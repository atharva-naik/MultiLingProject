{
    "accuracy": 0,
    "bleu": {
        "bleu": 0.9148578874542489,
        "precisions": [
            0.9892060892544757,
            0.9885656970912738,
            0.9881160330513631,
            0.9877021478365263
        ],
        "brevity_penalty": 0.9255972832199999,
        "length_ratio": 0.9282327232524594,
        "translation_length": 107468,
        "reference_length": 115777
    },
    "codebleu": 0,
    "preds": [
        "Stops monitoring the predefined directory.",
        "Called when a file in the monitored directory has been moved.\n\n        Breaks move down into a delete and a create (which it is sometimes detected as!).\n        :param event: the file system event",
        "Tears down all temp files and directories.",
        "Test whether a file target is not exists or it exists but allow\n        overwrite.",
        "Copy this file to other place.",
        "Clients a Docker client.\n\n    Will raise a `ConnectionError` if the Docker daemon is not accessible.\n    :return: the Docker client",
        "Decorate methods when repository path is required.",
        "clean repository given before and after states",
        "Get repository descriptive stats\n\n        :Returns:\n            #. numberOfDirectories (integer): Number of diretories in repository\n            #. numberOfFiles (integer): Number of files in repository",
        "Reset repository instance.",
        "Load repository from a directory path and update the current instance.\n        First, new repository still will be loaded. If failed, then old\n        style repository load will be tried.\n\n        :Parameters:\n            #. path (string): The path of the directory from where to load\n               the repository from. If '.' or an empty string is passed,\n               the current working directory will be used.\n            #. verbose (boolean): Whether to be verbose about abnormalities\n            #. ntrials (int): After aquiring all locks, ntrials is the maximum\n               number of trials allowed before failing.\n               In rare cases, when multiple processes\n               are accessing the same repository components, different processes\n               can alter repository components between successive lock releases\n               of some other process. Bigger number of trials lowers the\n               likelyhood of failure due to multiple processes same time\n               alteration.\n",
        "Remove all repository from path along with all repository tracked files.\n\n        :Parameters:\n            #. path (None, string): The path the repository to remove.\n            #. removeEmptyDirs (boolean): Whether to remove remaining empty\n               directories.",
        "Get whether creating a file or a directory from the basenane of the given\n        path is allowed\n\n        :Parameters:\n            #. path (str): The absolute or relative path or simply the file\n               or directory name.\n\n        :Returns:\n            #. allowed (bool): Whether name is allowed.\n            #. message (None, str): Reason for the name to be forbidden.",
        "Given a path, return relative path to diretory\n\n        :Parameters:\n            #. path (str): Path as a string\n            #. split (boolean): Whether to split path to its components\n\n        :Returns:\n            #. relativePath (str, list): Relative path as a string or as a list\n               of components if split is True",
        "Get a list representation of repository state along with useful\n        information. List state is ordered relativeley to directories level\n\n        :Parameters:\n            #. relaPath (None, str): relative directory path from where to\n               start. If None all repository representation is returned.\n\n        :Returns:\n            #. state (list): List representation of the repository.\n               List items are all dictionaries. Every dictionary has a single\n               key which is the file or the directory name and the value is a\n               dictionary of information including:\n\n                   * 'type': the type of the tracked whether it's file, dir, or objectdir\n                   * 'exists': whether file or directory actually exists on disk\n                   * 'pyrepfileinfo': In case of a file or an objectdir whether .%s_pyrepfileinfo exists\n                   * 'pyrepdirinfo': In case of a directory whether .pyrep",
        "Get file information dict from the repository given its relative path.\n\n        :Parameters:\n            #. relativePath (string): The relative to the repository path of\n               the file.\n\n        :Returns:\n            #. info (None, dictionary): The file information dictionary.\n               If None, it means an error has occurred.\n            #. errorMessage (string): The error message if any error occurred.",
        "Check whether a given relative path is a repository file path\n\n        :Parameters:\n            #. relativePath (string): File relative path\n\n        :Returns:\n            #. isRepoFile (boolean): Whether file is a repository file.\n            #. isFileOnDisk (boolean): Whether file is found on disk.\n            #. isFileInfoOnDisk (boolean): Whether file info is found on disk.\n            #. isFileClassOnDisk (boolean): Whether file class is found on disk.",
        "Create a tar file package of all the repository files and directories.\n        Only files and directories that are tracked in the repository\n        are stored in the package tar file.\n\n        **N.B. On some systems packaging requires root permissions.**\n\n        :Parameters:\n            #. path (None, string): The real absolute path where to create the\n               package. If None, it will be created in the same directory as\n               the repository. If '.' or an empty string is passed, the current\n               working directory will be used.\n            #. name (None, string): The name to give to the package file\n               If None, the package directory name will be used with the\n               appropriate extension added.\n            #. mode (None, string): The writing mode of the tarfile.\n               If None, automatically the best compression mode will be chose.\n               Available modes are ('w', 'w:",
        "Renames an item in this collection as a transaction.\n\n        Will override if new key name already exists.\n        :param key: the current name of the item\n        :param new_key: the new name that the item should have",
        "Use default hash method to return hash value of a piece of string\n    default setting use 'utf-8' encoding.",
        "Return md5 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file\n\n    CPU = i7-4600U 2.10GHz - 2.70GHz, RAM = 8.00 GB\n    1 second can process 0.25GB data\n\n    - 0.59G - 2.43 sec\n    - 1.3G - 5.68 sec\n    - 1.9G - 7.72 sec\n    - 2.5G - 10.32 sec\n    - 3.9G - 16.0 sec",
        "Return sha256 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file",
        "Return sha512 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file",
        "A command line auto complete similar behavior. Find all item with same\n        prefix of this one.\n\n        :param case_sensitive: toggle if it is case sensitive.\n        :return: list of :class:`pathlib_mate.pathlib2.Path`.",
        "Print ``top_n`` big dir in this dir.",
        "Print ``top_n`` big file in this dir.",
        "Print ``top_n`` big dir and ``top_n`` big file in each dir.",
        "Create a new folder having exactly same structure with this directory.\n        However, all files are just empty file with same file name.\n\n        :param dst: destination directory. The directory can't exists before\n        you execute this.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u521b\u5efa\u4e00\u4e2a\u76ee\u5f55\u7684\u955c\u50cf\u62f7\u8d1d, \u4e0e\u62f7\u8d1d\u64cd\u4f5c\u4e0d\u540c\u7684\u662f, \u6587\u4ef6\u7684\u526f\u672c\u53ea\u662f\u5728\u6587\u4ef6\u540d\u4e0a\n        \u4e0e\u539f\u4ef6\u4e00\u81f4, \u4f46\u662f\ufffd",
        "Execute every ``.py`` file as main script.\n\n        :param py_exe: str, python command or python executable path.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709Python\u6587\u4ef6\u4f5c\u4e3a\u4e3b\u811a\u672c\u7528\u5f53\u524d\u89e3\u91ca\u5668\u8fd0\u884c\u3002",
        "Trail white space at end of each line for every ``.py`` file.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u88ab\u9009\u62e9\u7684\u6587\u4ef6\u4e2d\u884c\u672b\u7684\u7a7a\u683c\u5220\u9664\u3002",
        "Auto convert your python code in a directory to pep8 styled code.\n\n        :param kwargs: arguments for ``autopep8.fix_code`` method.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709Python\u6587\u4ef6\u7528pep8\u98ce\u683c\u683c\u5f0f\u5316\u3002\u589e\u52a0\u5176\u53ef\u8bfb\u6027\u548c\u89c4\u8303\u6027\u3002",
        "File size in bytes.",
        "Get most recent modify time in timestamp.",
        "Get most recent access time in timestamp.",
        "Get most recent create time in timestamp.",
        "Lists options that have not been used to format other values in \n        their sections. \n        \n        Good for finding out if the user has misspelled any of the options.",
        "List names of options and positional arguments.",
        "Add an Option object to the user interface.",
        "Append a positional argument to the user interface.\n\n        Optional positional arguments must be added after the required ones. \n        The user interface can have at most one recurring positional argument, \n        and if present, that argument must be the last one.",
        "Read program documentation from a DocParser compatible file.\n\n        docsfiles is a list of paths to potential docsfiles: parse if present.\n        A string is taken as a list of one item.",
        "Return user friendly help on program options.",
        "Return user friendly help on positional arguments in the program.",
        "Return user friendly help on positional arguments.        \n\n        indent is the number of spaces preceeding the text on each line. \n        \n        The indent of the documentation is dependent on the length of the \n        longest label that is shorter than maxindent. A label longer than \n        maxindent will be printed on its own line.\n        \n        width is maximum allowed page width, use self.width if 0.",
        "Return a summary of program options, their values and origins.\n        \n        width is maximum allowed page width, use self.width if 0.",
        "Parse text blocks from a file.",
        "Pop, parse and return the first self.nargs items from args.\n\n        if self.nargs > 1 a list of parsed values will be returned.\n        \n        Raise BadNumberOfArguments or BadArgument on errors.\n         \n        NOTE: argv may be modified in place by this method.",
        "Parse arguments found in settings files.\n        \n        Use the values in self.true for True in settings files, or those in \n        self.false for False, case insensitive.",
        "Return the separator that preceding format i, or '' for i == 0.",
        "Return a URL to redirect the user to for OAuth authentication.",
        "Exchange the authorization code for an access token.",
        "Wraps Lock.acquire",
        "Wraps Lock.release",
        "Handle a dict that might contain a wrapped state for a custom type.",
        "Wrap the marshalled state in a dictionary.\n\n        The returned dictionary has two keys, corresponding to the ``type_key`` and ``state_key``\n        options. The former holds the type name and the latter holds the marshalled state.\n\n        :param typename: registered name of the custom type\n        :param state: the marshalled state of the object\n        :return: an object serializable by the serializer",
        "Enable HTTP access to a dataset.\n\n    This only works on datasets in some systems. For example, datasets stored\n    in AWS S3 object storage and Microsoft Azure Storage can be published as\n    datasets accessible over HTTP. A published dataset is world readable.",
        "Update the descriptive metadata interactively.\n\n    Uses values entered by the user. Note that the function keeps recursing\n    whenever a value is another ``CommentedMap`` or a ``list``. The\n    function works as passing dictionaries and lists into a function edits\n    the values in place.",
        "Create a proto dataset.",
        "Interactive prompting to populate the readme.",
        "Default editor updating of readme content.",
        "Show the descriptive metadata in the readme.",
        "Use YAML from a file or stdin to populate the readme.\n\n    To stream content from stdin use \"-\", e.g.\n\n    echo \"desc: my data\" | dtool readme write <DS_URI> -",
        "Add a file to the proto dataset.",
        "Add metadata to a file in the proto dataset.",
        "Convert a proto dataset into a dataset.\n\n    This step is carried out after all files have been added to the dataset.\n    Freezing a dataset finalizes it with a stamp marking it as frozen.",
        "Copy a dataset to a different location.",
        "Compress anything to bytes or string.\n\n    :params obj: \n    :params level: \n    :params return_type: if bytes, then return bytes; if str, then return\n      base64.b64encode bytes in utf-8 string.",
        "attempt to deduce if a pre 100 year was lost\n         due to padded zeros being taken off",
        "Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings",
        "The CPython version of ``fromutc`` checks that the input is a ``datetime``\n    object and that ``self`` is attached as its ``tzinfo``.",
        "Given a datetime in UTC, return local time",
        "Strip comments from line string.",
        "Strip comments from json string.\n\n    :param string: A string containing json with comments started by comment_symbols.\n    :param comment_symbols: Iterable of symbols that start a line comment (default # or //).\n    :return: The string with the comments removed.",
        "dayofweek == 0 means Sunday, whichweek 5 means last instance",
        "Convert a registry key's values to a dictionary.",
        "Parse strings as returned from the Windows registry into the time zone\n        name as defined in the registry.\n\n        >>> from dateutil.tzwin import tzres\n        >>> tzr = tzres()\n        >>> print(tzr.name_from_string('@tzres.dll,-251'))\n        'Dateline Daylight Time'\n        >>> print(tzr.name_from_string('Eastern Standard Time'))\n        'Eastern Standard Time'\n\n        :param tzname_str:\n            A timezone name string as returned from a Windows registry key.\n\n        :return:\n            Returns the localized timezone string from tzres.dll if the string\n            is of the form `@tzres.dll,-offset`, else returns the input string.",
        "This retrieves a time zone from the local zoneinfo tarball that is packaged\n    with dateutil.\n\n    :param name:\n        An IANA-style time zone name, as found in the zoneinfo file.\n\n    :return:\n        Returns a :class:`dateutil.tz.tzfile` time zone object.\n\n    .. warning::\n        It is generally inadvisable to use this function, and it is only\n        provided for API compatibility with earlier versions. This is *not*\n        equivalent to ``dateutil.tz.gettz()``, which selects an appropriate\n        time zone based on the inputs, favoring system zoneinfo. This is ONLY\n        for accessing the dateutil-specific zoneinfo (which may be out of\n        date compared to the system zoneinfo).\n\n    .. deprecated:: 2.6\n        If you need to use a specific zoneinfofile over the",
        "Get the zonefile metadata\n\n    See `zonefile_metadata`_\n\n    :returns:\n        A dictionary with the database metadata\n\n    .. deprecated:: 2.6\n        See deprecation warning in :func:`zoneinfo.gettz`. To get metadata,\n        query the attribute ``zoneinfo.ZoneInfoFile.metadata``.",
        "Get the configuration for the given JID based on XMPP_HTTP_UPLOAD_ACCESS.\n\n    If the JID does not match any rule, ``False`` is returned.",
        "Given a datetime and a time zone, determine whether or not a given datetime\n    would fall in a gap.\n\n    :param dt:\n        A :class:`datetime.datetime` (whose time zone will be ignored if ``tz``\n        is provided.)\n\n    :param tz:\n        A :class:`datetime.tzinfo` with support for the ``fold`` attribute. If\n        ``None`` or not provided, the datetime's own time zone will be used.\n\n    :return:\n        Returns a boolean value whether or not the \"wall time\" exists in ``tz``.",
        "Set the time zone data of this object from a _tzfile object",
        "Return a version of this object represented entirely using integer\n        values for the relative attributes.\n\n        >>> relativedelta(days=1.5, hours=2).normalized()\n        relativedelta(days=1, hours=14)\n\n        :return:\n            Returns a :class:`dateutil.relativedelta.relativedelta` object.",
        "Create a new HMAC hash.\n\n    :param secret: The secret used when hashing data.\n    :type secret: bytes\n    :param data: The data to hash.\n    :type data: bytes\n    :param alg: The algorithm to use when hashing `data`.\n    :type alg: str\n    :return: New HMAC hash.\n    :rtype: bytes",
        "Decodes the given token's header and payload and validates the signature.\n\n    :param secret: The secret used to decode the token. Must match the\n        secret used when creating the token.\n    :type secret: Union[str, bytes]\n    :param token: The token to decode.\n    :type token: Union[str, bytes]\n    :param alg: The algorithm used to decode the token. Must match the\n        algorithm used when creating the token.\n    :type alg: str\n    :return: The decoded header and payload.\n    :rtype: Tuple[dict, dict]",
        "Compares the given signatures.\n\n    :param expected: The expected signature.\n    :type expected: Union[str, bytes]\n    :param actual: The actual signature.\n    :type actual: Union[str, bytes]\n    :return: Do the signatures match?\n    :rtype: bool",
        "Compares the given tokens.\n\n    :param expected: The expected token.\n    :type expected: Union[str, bytes]\n    :param actual: The actual token.\n    :type actual: Union[str, bytes]\n    :return: Do the tokens match?\n    :rtype: bool",
        "Is the token valid? This method only checks the timestamps within the\n        token and compares them against the current time if none is provided.\n\n        :param time: The timestamp to validate against\n        :type time: Union[int, None]\n        :return: The validity of the token.\n        :rtype: bool",
        "Check for registered claims in the payload and move them to the\n        registered_claims property, overwriting any extant claims.",
        "Create a token based on the data held in the class.\n\n        :return: A new token\n        :rtype: str",
        "Decodes the given token into an instance of `Jwt`.\n\n        :param secret: The secret used to decode the token. Must match the\n            secret used when creating the token.\n        :type secret: Union[str, bytes]\n        :param token: The token to decode.\n        :type token: Union[str, bytes]\n        :param alg: The algorithm used to decode the token. Must match the\n            algorithm used when creating the token.\n        :type alg: str\n        :return: The decoded token.\n        :rtype: `Jwt`",
        "Compare against another `Jwt`.\n\n        :param jwt: The token to compare against.\n        :type jwt: Jwt\n        :param compare_dates: Should the comparision take dates into account?\n        :type compare_dates: bool\n        :return: Are the two Jwt's the same?\n        :rtype: bool",
        "Download a file.",
        "Test a file is a valid json file.\n\n    - *.json: uncompressed, utf-8 encode json file\n    - *.js: uncompressed, utf-8 encode json file\n    - *.gz: compressed, utf-8 encode json file",
        "``set`` dumper.",
        "``collections.deque`` dumper.",
        "``collections.OrderedDict`` dumper.",
        "``numpy.ndarray`` dumper.",
        "Decorator for rruleset methods which may invalidate the\n    cached length.",
        "Returns the last recurrence before the given datetime instance. The\n            inc keyword defines what happens if dt is an occurrence. With\n            inc=True, if dt itself is an occurrence, it will be returned.",
        "Returns the first recurrence after the given datetime instance. The\n            inc keyword defines what happens if dt is an occurrence. With\n            inc=True, if dt itself is an occurrence, it will be returned.",
        "Generator which yields up to `count` recurrences after the given\n        datetime instance, equivalent to `after`.\n\n        :param dt:\n            The datetime at which to start generating recurrences.\n\n        :param count:\n            The maximum number of recurrences to generate. If `None` (default),\n            dates are generated until the recurrence rule is exhausted.\n\n        :param inc:\n            If `dt` is an instance of the rule and `inc` is `True`, it is\n            included in the output.\n\n        :yields: Yields a sequence of `datetime` objects.",
        "Return new rrule with same attributes except for those attributes given new\n           values by whichever keyword arguments are specified.",
        "Run the excel_to_html function from the\n    command-line.\n\n    Args:\n        -p path to file\n        -s name of the sheet to convert\n        -css classes to apply\n        -m attempt to combine merged cells\n        -c caption for accessibility\n        -su summary for accessibility\n        -d details for accessibility\n\n    Example use:\n\n        excel_to_html -p myfile.xlsx -s SheetName -css diablo-python -m true",
        "Gets the requested template for the given language.\n\n        Args:\n            language: string, the language of the template to look for.\n\n            template_type: string, 'iterable' or 'singular'. \n            An iterable template is needed when the value is an iterable\n            and needs more unpacking, e.g. list, tuple. A singular template \n            is needed when unpacking is complete and the value is singular, \n            e.g. string, int, float.\n\n            indentation: int, the indentation level.\n    \n            key: multiple types, the array key.\n\n            val: multiple types, the array values\n\n        Returns:\n            string, template formatting for arrays by language.",
        "Unserializes a serialized php array and prints it to\n        the console as a data structure in the specified language.\n        Used to translate or convert a php array into a data structure \n        in another language. Currently supports, PHP, Python, Javascript,\n        and JSON. \n\n        Args:\n            string: a string of serialized php\n        \n            language: a string representing the desired output \n            format for the array.\n\n            level: integer, indentation level in spaces. \n            Defaults to 3.\n\n            retdata: boolean, the method will return the string\n            in addition to printing it if set to True. Defaults \n            to false.\n\n        Returns:\n            None but prints a string to the console if retdata is \n            False, otherwise returns a string.",
        "Only API function for the config module.\n\n    :return: {dict}     loaded validated configuration.",
        "Create a reusable class from a generator function\n\n    Parameters\n    ----------\n    func: GeneratorCallable[T_yield, T_send, T_return]\n        the function to wrap\n\n    Note\n    ----\n    * the callable must have an inspectable signature\n    * If bound to a class, the new reusable generator is callable as a method.\n      To opt out of this, add a :func:`staticmethod` decorator above\n      this decorator.",
        "Send an item into a generator expecting a final return value\n\n    Parameters\n    ----------\n    gen: ~typing.Generator[T_yield, T_send, T_return]\n        the generator to send the value to\n    value: T_send\n        the value to send\n\n    Raises\n    ------\n    RuntimeError\n        if the generator did not return as expected\n\n    Returns\n    -------\n    T_return\n        the generator's return value",
        "Apply a function to all ``send`` values of a generator\n\n    Parameters\n    ----------\n    func: ~typing.Callable[[T_send], T_mapped]\n        the function to apply\n    gen: Generable[T_yield, T_mapped, T_return]\n        the generator iterable.\n\n    Returns\n    -------\n    ~typing.Generator[T_yield, T_send, T_return]\n        the mapped generator",
        "Prints the traceback and invokes the ipython debugger on any exception\n\n    Only invokes ipydb if you are outside ipython or python interactive session.\n    So scripts must be called from OS shell in order for exceptions to ipy-shell-out.\n\n    Dependencies:\n      Needs `pip install ipdb`\n\n    Arguments:\n      exc_type (type): The exception type/class (e.g. RuntimeError)\n      exc_value (Exception): The exception instance (e.g. the error message passed to the Exception constructor)\n      exc_trace (Traceback): The traceback instance\n    \n    References:\n      http://stackoverflow.com/a/242531/623735\n\n    Example Usage:\n      $  python -c 'from pug import debug;x=[];x[0]'\n      Traceback (most recent call last):\n        File \"<string>\"",
        "Copies a file from its location on the web to a designated \n    place on the local machine.\n\n    Args:\n        file_path: Complete url of the file to copy, string (e.g. http://fool.com/input.css).\n\n        target_path: Path and name of file on the local machine, string. (e.g. /directory/output.css)\n\n    Returns:\n        None.",
        "Counts the number of lines in a file.\n\n    Args:\n        fname: string, name of the file.\n\n    Returns:\n        integer, the number of lines in the file.",
        "Indentes css that has not been indented and saves it to a new file.\n    A new file is created if the output destination does not already exist.\n\n    Args:\n        f: string, path to file.\n\n        output: string, path/name of the output file (e.g. /directory/output.css).\n    print type(response.read())\n\n    Returns:\n        None.",
        "Adds line breaks after every occurance of a given character in a file.\n\n    Args:\n        f: string, path to input file.\n\n        output: string, path to output file.\n\n    Returns:\n        None.",
        "Reformats poorly written css. This function does not validate or fix errors in the code.\n    It only gives code the proper indentation. \n\n    Args:\n        input_file: string, path to the input file.\n\n        output_file: string, path to where the reformatted css should be saved. If the target file\n        doesn't exist, a new file is created.\n\n    Returns:\n        None.",
        "Take a list of strings and clear whitespace \n    on each one. If a value in the list is not a \n    string pass it through untouched.\n\n    Args:\n        iterable: mixed list\n\n    Returns: \n        mixed list",
        "Calculates the future value of money invested at an anual interest rate,\n    x times per year, for a given number of years.\n\n    Args:\n        present_value: int or float, the current value of the money (principal).\n\n        annual_rate: float 0 to 1 e.g., .5 = 50%), the interest rate paid out.\n\n        periods_per_year: int, the number of times money is invested per year.\n\n        years: int, the number of years invested.\n\n    Returns:\n        Float, the future value of the money invested with compound interest.",
        "Uses Heron's formula to find the area of a triangle\n    based on the coordinates of three points.\n\n    Args:\n        point1: list or tuple, the x y coordinate of point one.\n\n        point2: list or tuple, the x y coordinate of point two.\n\n        point3: list or tuple, the x y coordinate of point three.\n\n    Returns:\n        The area of a triangle as a floating point number.\n\n    Requires:\n        The math module, point_distance().",
        "Calculates  the median of a list of integers or floating point numbers.\n\n    Args:\n        data: A list of integers or floating point numbers\n\n    Returns:\n        Sorts the list numerically and returns the middle number if the list has an odd number\n        of items. If the list contains an even number of items the mean of the two middle numbers\n        is returned.",
        "Calculates the average or mean of a list of numbers\n\n    Args:\n        numbers: a list of integers or floating point numbers.\n\n        numtype: string, 'decimal' or 'float'; the type of number to return.\n\n    Returns:\n        The average (mean) of the numbers as a floating point number\n        or a Decimal object.\n\n    Requires:\n        The math module",
        "Calculates the population or sample variance of a list of numbers.\n    A large number means the results are all over the place, while a\n    small number means the results are comparatively close to the average.\n\n    Args:\n        numbers: a list  of integers or floating point numbers to compare.\n\n        type: string, 'population' or 'sample', the kind of variance to be computed.\n\n    Returns:\n        The computed population or sample variance.\n        Defaults to population variance.\n\n    Requires:\n        The math module, average()",
        "Finds the percentage of one number over another.\n\n    Args:\n        a: The number that is a percent, int or float.\n\n        b: The base number that a is a percent of, int or float.\n\n        i: Optional boolean integer. True if the user wants the result returned as\n        a whole number. Assumes False.\n\n        r: Optional boolean round. True if the user wants the result rounded.\n        Rounds to the second decimal point on floating point numbers. Assumes False.\n\n    Returns:\n        The argument a as a percentage of b. Throws a warning if integer is set to True\n        and round is set to False.",
        "Get datetime string from datetime object\n\n        :param datetime datetime_obj: datetime object\n        :return: datetime string\n        :rtype: str",
        "attr pipe can extract attribute value of object.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_name: The name of attribute\n    :type attr_name: str\n    :returns: generator",
        "attrs pipe can extract attribute values of object.\n\n    If attr_names is a list and its item is not a valid attribute of\n    prev's object. It will be excluded from yielded dict.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_names: The list of attribute names\n    :type attr_names: str of list\n    :returns: generator",
        "attrdict pipe can extract attribute values of object into a dict.\n\n    The argument attr_names can be a list or a dict.\n\n    If attr_names is a list and its item is not a valid attribute of\n    prev's object. It will be excluded from yielded dict.\n\n    If attr_names is dict and the key doesn't exist in prev's object.\n    the value of corresponding attr_names key will be copy to yielded dict.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_names: The list or dict of attribute names\n    :type attr_names: str of list or dict\n    :returns: generator",
        "flatten pipe extracts nested item from previous pipe.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param depth: The deepest nested level to be extracted. 0 means no extraction.\n    :type depth: integer\n    :returns: generator",
        "values pipe extract value from previous pipe.\n\n    If previous pipe send a dictionary to values pipe, keys should contains\n    the key of dictionary which you want to get. If previous pipe send list or\n    tuple,\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :returns: generator",
        "pack pipe takes n elements from previous generator and yield one\n    list to next.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param rest: Set True to allow to output the rest part of last elements.\n    :type prev: boolean\n    :param padding: Specify the padding element for the rest part of last elements.\n    :type prev: boolean\n    :returns: generator\n\n    :Example:\n    >>> result([1,2,3,4,5,6,7] | pack(3))\n    [[1, 2, 3], [4, 5, 6]]\n\n    >>> result([1,2,3,4,5,6,7] | pack(3, rest=True))\n    [[1, 2, 3], [4, 5, 6], [7,]]\n\n    >>> result([1,2",
        "The pipe greps the data passed from previous generator according to\n    given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter out data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :param kw:\n    :type kw: dict\n    :returns: generator",
        "The pipe greps the data passed from previous generator according to\n    given regular expression. The data passed to next pipe is MatchObject\n    , dict or tuple which determined by 'to' in keyword argument.\n\n    By default, match pipe yields MatchObject. Use 'to' in keyword argument\n    to change the type of match result.\n\n    If 'to' is dict, yield MatchObject.groupdict().\n    If 'to' is tuple, yield MatchObject.groups().\n    If 'to' is list, yield list(MatchObject.groups()).\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter data.\n    :type pattern: str|unicode\n    :param to: What data type the result should be stored. dict|tuple|list\n    :type to: type\n    :returns: generator",
        "The resplit pipe split previous pipe input by regular expression.\n\n    Use 'maxsplit' keyword argument to limit the number of split.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to split string.\n    :type pattern: str|unicode",
        "sub pipe is a wrapper of re.sub method.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern string.\n    :type pattern: str|unicode\n    :param repl: Check repl argument in re.sub method.\n    :type repl: str|unicode|callable",
        "wildcard pipe greps data passed from previous generator\n    according to given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The wildcard string which used to filter data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :returns: generator",
        "This pipe read data from previous iterator and write it to stdout.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param endl: The end-of-line symbol for each output.\n    :type endl: str\n    :param thru: If true, data will passed to next generator. If false, data\n                 will be dropped.\n    :type thru: bool\n    :returns: generator",
        "This pipe get filenames or file object from previous pipe and read the\n    content of file. Then, send the content of file line by line to next pipe.\n\n    The start and end parameters are used to limit the range of reading from file.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param filename: The files to be read. If None, use previous pipe input as filenames.\n    :type filename: None|str|unicode|list|tuple\n    :param mode: The mode to open file. default is 'r'\n    :type mode: str\n    :param trim: The function to trim the line before send to next pipe.\n    :type trim: function object.\n    :param start: if star is specified, only line number larger or equal to start will be sent.\n    :type start: integer\n    :param end: The last line",
        "sh pipe execute shell command specified by args. If previous pipe exists,\n    read data from it and write it to stdin of shell process. The stdout of\n    shell process will be passed to next pipe object line by line.\n\n    A optional keyword argument 'trim' can pass a function into sh pipe. It is\n    used to trim the output from shell process. The default trim function is\n    str.rstrip. Therefore, any space characters in tail of\n    shell process output line will be removed.\n\n    For example:\n\n    py_files = result(sh('ls') | strip | wildcard('*.py'))\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param args: The command line arguments. It will be joined by space character.\n    :type args: list of string.\n    :param kw: arguments for subprocess.Popen.\n",
        "This pipe wrap os.walk and yield absolute path one by one.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param args: The end-of-line symbol for each output.\n    :type args: list of string.\n    :param kw: The end-of-line symbol for each output.\n    :type kw: dictionary of options. Add 'endl' in kw to specify end-of-line symbol.\n    :returns: generator",
        "alias of str.join",
        "alias of string.Template.substitute",
        "alias of string.Template.safe_substitute",
        "Convert data from previous pipe with specified encoding.",
        "Regiser all default type-to-pipe convertors.",
        "Convert Paginator instance to dict\n\n        :return: Paging data\n        :rtype: dict",
        "Check that a process is not running more than once, using PIDFILE",
        "This function will check whether a PID is currently running",
        "This function will disown, so the Ardexa service can be restarted",
        "Run a  program and check program return code Note that some commands don't work\n    well with Popen.  So if this function is specifically called with 'shell=True',\n    then it will run the old 'os.system'. In which case, there is no program output",
        "Yield each integer from a complex range string like \"1-9,12,15-20,23\"\n\n    >>> list(parse_address_list('1-9,12,15-20,23'))\n    [1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 15, 16, 17, 18, 19, 20, 23]\n\n    >>> list(parse_address_list('1-9,12,15-20,2-3-4'))\n    Traceback (most recent call last):\n        ...\n    ValueError: format error in 2-3-4",
        "Do url-encode resource ids",
        "Get item creator according registered item type.\n\n    :param item_type: The type of item to be checed.\n    :type item_type: types.TypeType.\n    :returns: Creator function. None if type not found.",
        "Self-cloning. All its next Pipe objects are cloned too.\n\n        :returns: cloned object",
        "Append next object to pipe tail.\n\n        :param next: The Pipe object to be appended to tail.\n        :type next: Pipe object.",
        "Return an generator as iterator object.\n\n        :param prev: Previous Pipe object which used for data input.\n        :returns: A generator for iteration.",
        "Wrap a reduce function to Pipe object. Reduce function is a function\n        with at least two arguments. It works like built-in reduce function.\n        It takes first argument for accumulated result, second argument for\n        the new data to process. A keyword-based argument named 'init' is\n        optional. If init is provided, it is used for the initial value of\n        accumulated result. Or, the initial value is None.\n\n        The first argument is the data to be converted. The return data from\n        filter function should be a boolean value. If true, data can pass.\n        Otherwise, data is omitted.\n\n        :param func: The filter function to be wrapped.\n        :type func: function object\n        :param args: The default arguments to be used for filter function.\n        :param kw: The default keyword arguments to be used for filter function.\n        :returns: Pipe object",
        "Return a dictionary of network name to active status bools.\n\n        Sample virsh net-list output::\n\n    Name                 State      Autostart\n    -----------------------------------------\n    default              active     yes\n    juju-test            inactive   no\n    foobar               inactive   no\n\n    Parsing the above would return::\n    {\"default\": True, \"juju-test\": False, \"foobar\": False}\n\n    See: http://goo.gl/kXwfC",
        "flush the line to stdout",
        "runs the passed in arguments and returns an iterator on the output of\n        running command",
        "Build a basic 035 subfield with basic information from the OAI-PMH request.\n\n    :param root: ElementTree root node\n\n    :return: list of subfield tuples [(..),(..)]",
        "Strip out namespace data from an ElementTree.\n\n    This function is recursive and will traverse all\n    subnodes to the root element\n\n    @param root: the root element\n\n    @return: the same root element, minus namespace",
        "Load values from a dictionary structure. Nesting can be used to\n            represent namespaces.\n\n            >>> c = ConfigDict()\n            >>> c.load_dict({'some': {'namespace': {'key': 'value'} } })\n            {'some.namespace.key': 'value'}",
        "The oembed endpoint, or the url to which requests for metadata are passed.\n    Third parties will want to access this view with URLs for your site's\n    content and be returned OEmbed metadata.",
        "Extract and return oembed content for given urls.\n\n    Required GET params:\n        urls - list of urls to consume\n\n    Optional GET params:\n        width - maxwidth attribute for oembed content\n        height - maxheight attribute for oembed content\n        template_dir - template_dir to use when rendering oembed\n\n    Returns:\n        list of dictionaries with oembed metadata and renderings, json encoded",
        "A site profile detailing valid endpoints for a given domain.  Allows for\n    better auto-discovery of embeddable content.\n\n    OEmbed-able content lives at a URL that maps to a provider.",
        "scan path directory and any subdirectories for valid captain scripts",
        "Make the request params given location data",
        "Get the tax rate from the ZipTax response",
        "Check if there are exceptions that should be raised",
        "Recursively extract all text from node.",
        "Registers a provider with the site.",
        "Unregisters a provider from the site.",
        "Populate the internal registry's dictionary with the regexes for each\n        provider instance",
        "Find the right provider for a URL",
        "A hook for django-based oembed providers to delete any stored oembeds",
        "The heart of the matter",
        "Load up StoredProviders from url if it is an oembed scheme",
        "Iterate over the returned json and try to sort out any new providers",
        "A kind of cheesy method that allows for callables or attributes to\n        be used interchangably",
        "Return an ImageFileField instance",
        "Build a dictionary of metadata for the requested object.",
        "Parses the date from a url and uses it in the query.  For objects which\n        are unique for date.",
        "Override the base.",
        "Add the 909 OAI info to 035.",
        "Check if we shall add cnum in 035.",
        "Remove hidden notes and tag a CERN if detected.",
        "Remove INSPIRE specific notes.",
        "Move title info from 245 to 111 proceeding style.",
        "Update reportnumbers.",
        "Remove dashes from ISBN.",
        "Remove duplicate BibMatch DOIs.",
        "260 Date normalization.",
        "041 Language.",
        "Generate directory listing HTML\n\n    Arguments:\n        FS (FS): filesystem object to read files from\n        filepath (str): path to generate directory listings for\n\n    Keyword Arguments:\n        list_dir (callable: list[str]): list file names in a directory\n        isdir (callable: bool): os.path.isdir\n\n    Yields:\n        str: lines of an HTML table",
        "Checks if files are not being uploaded to server.\n    @timeout - time after which the script will register an error.",
        "Converts capital letters to lower keeps first letter capital.",
        "Scans a block of text and extracts oembed data on any urls,\n        returning it in a list of dictionaries",
        "Try to maintain parity with what is extracted by extract since strip\n        will most likely be used in conjunction with extract",
        "Automatically build the provider index.",
        "pass in a list of options, promt the user to select one, and return the selected option or None",
        "Transforms the argparse arguments from Namespace to dict and then to Bunch\n    Therefore it is not necessary to access the arguments using the dict syntax\n    The settings can be called like regular vars on the settings object",
        "Reads a dom xml element in oaidc format and\n            returns the bibrecord object",
        "display a progress that can update in place\n\n    example -- \n        total_length = 1000\n        with echo.progress(total_length) as p:\n            for x in range(total_length):\n                # do something crazy\n                p.update(x)\n\n    length -- int -- the total size of what you will be updating progress on",
        "print format_msg to stderr",
        "prints a banner\n\n    sep -- string -- the character that will be on the line on the top and bottom\n        and before any of the lines, defaults to *\n    count -- integer -- the line width, defaults to 80",
        "format columned data so we can easily print it out on a console, this just takes\n    columns of data and it will format it into properly aligned columns, it's not\n    fancy, but it works for most type of strings that I need it for, like server name\n    lists.\n\n    other formatting options:\n        http://stackoverflow.com/a/8234511/5006\n\n    other packages that probably do this way better:\n        https://stackoverflow.com/a/26937531/5006\n\n    :Example:\n        >>> echo.table([(1, 2), (3, 4), (5, 6), (7, 8), (9, 0)])\n        1  2\n        3  4\n        5  6\n        7  8\n        9  0\n        >>> echo.table([1, 3, 5, 7, 9], [2, 4",
        "echo a prompt to the user and wait for an answer\n\n    question -- string -- the prompt for the user\n    choices -- list -- if given, only exit when prompt matches one of the choices\n    return -- string -- the answer that was given by the user",
        "Returns the records listed in the webpage given as\n        parameter as a xml String.\n\n        @param url: the url of the Journal, Book, Protocol or Reference work",
        "Logs into the specified ftp server and returns connector.",
        "Set the thermostat mode\n\n        :param mode: The desired mode integer value.\n                     Auto = 1\n                     Temporary hold = 2\n                     Permanent hold = 3",
        "Set the target temperature to the desired fahrenheit, with more granular control of the\n        hold mode\n\n        :param fahrenheit: The desired temperature in F\n        :param mode: The desired mode to operate in",
        "Set the target temperature to the desired celsius, with more granular control of the hold\n        mode\n\n        :param celsius: The desired temperature in C\n        :param mode: The desired mode to operate in",
        "Updates the target temperature on the NuHeat API\n\n        :param temperature: The desired temperature in NuHeat format\n        :param permanent: Permanently hold the temperature. If set to False, the schedule will\n                          resume at the next programmed event",
        "This function returns a Bunch object from the stated config file.\n\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-",
        "Authenticate against the NuHeat API",
        "Make a request to the NuHeat API\n\n        :param url: The URL to request\n        :param method: The type of request to make (GET, POST)\n        :param data: Data to be sent along with POST requests\n        :param params: Querystring parameters\n        :param retry: Attempt to re-authenticate and retry request if necessary",
        "Return representation of html start tag and attributes.",
        "Return representation of html end tag.",
        "Return stripped HTML, keeping only MathML.",
        "return True if callback is an instance of a class",
        "return True if callback is a vanilla plain jane function",
        "these kwargs come from the @arg decorator, they are then merged into any\n        keyword arguments that were automatically generated from the main function\n        introspection",
        "find any matching parser_args from list_args and merge them into this\n        instance\n\n        list_args -- list -- an array of (args, kwargs) tuples",
        "Overridden to not get rid of newlines\n\n        https://github.com/python/cpython/blob/2.7/Lib/argparse.py#L620",
        "create string suitable for HTTP User-Agent header",
        "Add a MARCXML datafield as a new child to a XML document.",
        "Given a document, return XML prettified.",
        "Transform & and < to XML valid &amp; and &lt.\n\n    Pass a list of tags as string to enable replacement of\n    '<' globally but keep any XML tags in the list.",
        "Properly format arXiv IDs.",
        "Convert journal name to Inspire's short form.",
        "Add correct nations field according to mapping in NATIONS_DEFAULT_MAP.",
        "Fix bad Unicode special dashes in string.",
        "Try to capitalize properly a title string.",
        "Convert some HTML tags to latex equivalents.",
        "Download URL to a file.",
        "Run a shell command.",
        "Create a logger object.",
        "Perform the actual uncompression.",
        "Locate all files matching supplied filename pattern recursively.",
        "Punctuate author names properly.\n\n    Expects input in the form 'Bloggs, J K' and will return 'Bloggs, J. K.'.",
        "Convert a date-value to the ISO date standard.",
        "Convert a date-value to the ISO date standard for humans.",
        "Convert list of images to PNG format.\n\n    @param: image_list ([string, string, ...]): the list of image files\n        extracted from the tarball in step 1\n\n    @return: image_list ([str, str, ...]): The list of image files when all\n        have been converted to PNG format.",
        "Generate a safe and closed filepath.",
        "Get letters from string only.",
        "Return True if license is compatible with Open Access",
        "Information about the current volume, issue, etc. is available\n        in a file called issue.xml that is available in a higher directory.",
        "issue.xml from Elsevier assume the existence of a local DTD.\n        This procedure install the DTDs next to the issue.xml file\n        and normalize it using xmllint in order to resolve all namespaces\n        and references.",
        "main.xml from Elsevier assume the existence of a local DTD.\n        This procedure install the DTDs next to the main.xml file\n        and normalize it using xmllint in order to resolve all namespaces\n        and references.",
        "Return the best effort start_date.",
        "Extract oembed resources from a block of text.  Returns a list\n    of dictionaries.\n\n    Max width & height can be specified:\n    {% for embed in block_of_text|extract_oembeds:\"400x300\" %}\n\n    Resource type can be specified:\n    {% for photo_embed in block_of_text|extract_oembeds:\"photo\" %}\n\n    Or both:\n    {% for embed in block_of_text|extract_oembeds:\"400x300xphoto\" %}",
        "A node which parses everything between its two nodes, and replaces any links\n    with OEmbed-provided objects, if possible.\n\n    Supports two optional argument, which is the maximum width and height,\n    specified like so:\n\n    {% oembed 640x480 %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    and or the name of a sub tempalte directory to render templates from:\n\n    {% oembed 320x240 in \"comments\" %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    or:\n\n    {% oembed in \"comments\" %}http://www.viddler.com/explore/SYSTM/videos/49/{% end",
        "Generates a &lt;link&gt; tag with oembed autodiscovery bits for an object.\n\n    {% oembed_autodiscover video %}",
        "Generates a &lt;link&gt; tag with oembed autodiscovery bits.\n\n    {% oembed_url_scheme %}",
        "return the parser for the current name",
        "load the module so we can actually run the script's function",
        "get the contents of the script",
        "parse and import the script, and then run the script's main function",
        "return that path to be able to call this script from the passed in\n        basename\n\n        example -- \n            basepath = /foo/bar\n            self.path = /foo/bar/che/baz.py\n            self.call_path(basepath) # che/baz.py\n\n        basepath -- string -- the directory you would be calling this script in\n        return -- string -- the minimum path that you could use to execute this script\n            in basepath",
        "load the script and set the parser and argument info\n\n        I feel that this is way too brittle to be used long term, I think it just\n        might be best to import the stupid module, the thing I don't like about that\n        is then we import basically everything, which seems bad?",
        "return True if this script can be run from the command line",
        "Handles registering the fields with the FieldRegistry and creating a \n    post-save signal for the model.",
        "I need a way to ensure that this signal gets created for all child\n        models, and since model inheritance doesn't have a 'contrubite_to_class'\n        style hook, I am creating a fake virtual field which will be added to\n        all subclasses and handles creating the signal",
        "Fetch response headers and data from a URL, raising a generic exception\n    for any kind of failure.",
        "Given a url which may or may not be a relative url, convert it to a full\n    url path given another full url as an example",
        "Generate a fake request object to allow oEmbeds to use context processors.",
        "dynamically load a class given a string of the format\n    \n    package.Class",
        "Override the base get_record.",
        "Special handling if record is a CMS NOTE.",
        "Handle reportnumbers.",
        "653 Free Keywords.",
        "710 Collaboration.",
        "Return a field created with the provided elements.\n\n    Global position is set arbitrary to -1.",
        "Create a list of records from the marcxml description.\n\n    :returns: a list of objects initiated by the function create_record().\n              Please see that function's docstring.",
        "Create a record object from the marcxml description.\n\n    Uses the lxml parser.\n\n    The returned object is a tuple (record, status_code, list_of_errors),\n    where status_code is 0 when there are errors, 1 when no errors.\n\n    The return record structure is as follows::\n\n        Record := {tag : [Field]}\n        Field := (Subfields, ind1, ind2, value)\n        Subfields := [(code, value)]\n\n    .. code-block:: none\n\n                                    .--------.\n                                    | record |\n                                    '---+----'\n                                        |\n               .------------------------+------------------------------------.\n               |record['001']           |record['909']        |record['520'] |\n               |                        |                     |              |\n        [list of fields]           [list of fields]     [list",
        "Filter the given field.\n\n    Filters given field and returns only that field instances that contain\n    filter_subcode with given filter_value. As an input for search function\n    accepts output from record_get_field_instances function. Function can be\n    run in three modes:\n\n    - 'e' - looking for exact match in subfield value\n    - 's' - looking for substring in subfield value\n    - 'r' - looking for regular expression in subfield value\n\n    Example:\n\n    record_filter_field(record_get_field_instances(rec, '999', '%', '%'),\n                        'y', '2001')\n\n    In this case filter_subcode is 'y' and filter_value is '2001'.\n\n    :param field_instances: output from record_get_field_instances\n    :param filter_subcode: name of the subfield\n",
        "Return a record where all the duplicate fields have been removed.\n\n    Fields are considered identical considering also the order of their\n    subfields.",
        "Return True if rec1 is identical to rec2.\n\n    It does so regardless of a difference in the 005 tag (i.e. the timestamp).",
        "Return the list of field instances for the specified tag and indications.\n\n    Return empty list if not found.\n    If tag is empty string, returns all fields\n\n    Parameters (tag, ind1, ind2) can contain wildcard %.\n\n    :param rec: a record structure as returned by create_record()\n    :param tag: a 3 characters long string\n    :param ind1: a 1 character long string\n    :param ind2: a 1 character long string\n    :param code: a 1 character long string\n    :return: a list of field tuples (Subfields, ind1, ind2, value,\n             field_position_global) where subfields is list of (code, value)",
        "Delete the field with the given position.\n\n    If global field position is specified, deletes the field with the\n    corresponding global field position.\n    If field_position_local is specified, deletes the field with the\n    corresponding local field position and tag.\n    Else deletes all the fields matching tag and optionally ind1 and\n    ind2.\n\n    If both field_position_global and field_position_local are present,\n    then field_position_local takes precedence.\n\n    :param rec: the record data structure\n    :param tag: the tag of the field to be deleted\n    :param ind1: the first indicator of the field to be deleted\n    :param ind2: the second indicator of the field to be deleted\n    :param field_position_global: the global field position (record wise)\n    :param field_position_local: the local field position (tag wise)\n",
        "Add the fields into the record at the required position.\n\n    The position is specified by the tag and the field_position_local in the\n    list of fields.\n\n    :param rec: a record structure\n    :param tag: the tag of the fields to be moved\n    :param field_position_local: the field_position_local to which the field\n                                 will be inserted. If not specified, appends\n                                 the fields to the tag.\n    :param a: list of fields to be added\n    :return: -1 if the operation failed, or the field_position_local if it was\n             successful",
        "Move some fields to the position specified by 'field_position_local'.\n\n    :param rec: a record structure as returned by create_record()\n    :param tag: the tag of the fields to be moved\n    :param field_positions_local: the positions of the fields to move\n    :param field_position_local: insert the field before that\n                                 field_position_local. If unspecified, appends\n                                 the fields :return: the field_position_local\n                                 is the operation was successful",
        "Delete all subfields with subfield_code in the record.",
        "Return the the matching field.\n\n    One has to enter either a global field position or a local field position.\n\n    :return: a list of subfield tuples (subfield code, value).\n    :rtype: list",
        "Replace a field with a new field.",
        "Return the subfield of the matching field.\n\n    One has to enter either a global field position or a local field position.\n\n    :return: a list of subfield tuples (subfield code, value).\n    :rtype:  list",
        "Delete subfield from position specified.\n\n    Specify the subfield by tag, field number and subfield position.",
        "Add subfield into specified position.\n\n    Specify the subfield by tag, field number and optionally by subfield\n    position.",
        "Modify controlfield at position specified by tag and field number.",
        "Modify subfield at specified position.\n\n    Specify the subfield by tag, field number and subfield position.",
        "Move subfield at specified position.\n\n    Sspecify the subfield by tag, field number and subfield position to new\n    subfield position.",
        "Generate the XML for record 'rec'.\n\n    :param rec: record\n    :param tags: list of tags to be printed\n    :return: string",
        "Generate the XML for field 'field' and returns it as a string.",
        "Print a record.\n\n    :param format: 1 XML, 2 HTML (not implemented)\n    :param tags: list of tags to be printed",
        "Print a list of records.\n\n    :param format: 1 XML, 2 HTML (not implemented)\n    :param tags: list of tags to be printed\n           if 'listofrec' is not a list it returns empty string",
        "Return the global and local positions of the first occurrence of the field.\n\n    :param rec:    A record dictionary structure\n    :type  rec:    dictionary\n    :param tag:    The tag of the field to search for\n    :type  tag:    string\n    :param field:  A field tuple as returned by create_field()\n    :type  field:  tuple\n    :param strict: A boolean describing the search method. If strict\n                   is False, then the order of the subfields doesn't\n                   matter. Default search method is strict.\n    :type  strict: boolean\n    :return:       A tuple of (global_position, local_position) or a\n                   tuple (None, None) if the field is not present.\n    :rtype:        tuple\n    :raise InvenioBibRecordFieldError: If the provided field is invalid.",
        "Find subfield instances in a particular field.\n\n    It tests values in 1 of 3 possible ways:\n     - Does a subfield code exist? (ie does 773__a exist?)\n     - Does a subfield have a particular value? (ie 773__a == 'PhysX')\n     - Do a pair of subfields have particular values?\n        (ie 035__2 == 'CDS' and 035__a == '123456')\n\n    Parameters:\n     * rec - dictionary: a bibrecord structure\n     * tag - string: the tag of the field (ie '773')\n     * ind1, ind2 - char: a single characters for the MARC indicators\n     * sub_key - char: subfield key to find\n     * sub_value - string: subfield value of that key\n     * sub_key2 - char: key of subfield to compare against\n     * sub",
        "Remove unchanged volatile subfields from the record.",
        "Turns all subfields to volatile",
        "Remove empty subfields and fields from the record.\n\n    If 'tag' is not None, only a specific tag of the record will be stripped,\n    otherwise the whole record.\n\n    :param rec:  A record dictionary structure\n    :type  rec:  dictionary\n    :param tag:  The tag of the field to strip empty fields from\n    :type  tag:  string",
        "Remove all non-empty controlfields from the record.\n\n    :param rec:  A record dictionary structure\n    :type  rec:  dictionary",
        "Order subfields from a record alphabetically based on subfield code.\n\n    If 'tag' is not None, only a specific tag of the record will be reordered,\n    otherwise the whole record.\n\n    :param rec: bibrecord\n    :type rec: bibrec\n    :param tag: tag where the subfields will be ordered\n    :type tag: str",
        "Compare 2 fields.\n\n    If strict is True, then the order of the subfield will be taken care of, if\n    not then the order of the subfields doesn't matter.\n\n    :return: True if the field are equivalent, False otherwise.",
        "Check if a field is well-formed.\n\n    :param field: A field tuple as returned by create_field()\n    :type field:  tuple\n    :raise InvenioBibRecordFieldError: If the field is invalid.",
        "Shift all global field positions.\n\n    Shift all global field positions with global field positions\n    higher or equal to 'start' from the value 'delta'.",
        "Return true if MARC 'tag' matches a 'pattern'.\n\n    'pattern' is plain text, with % as wildcard\n\n    Both parameters must be 3 characters long strings.\n\n    .. doctest::\n\n        >>> _tag_matches_pattern(\"909\", \"909\")\n        True\n        >>> _tag_matches_pattern(\"909\", \"9%9\")\n        True\n        >>> _tag_matches_pattern(\"909\", \"9%8\")\n        False\n\n    :param tag: a 3 characters long string\n    :param pattern: a 3 characters long string\n    :return: False or True",
        "Check if the global field positions in the record are valid.\n\n    I.e., no duplicate global field positions and local field positions in the\n    list of fields are ascending.\n\n    :param record: the record data structure\n    :return: the first error found as a string or None if no error was found",
        "Sort the fields inside the record by indicators.",
        "Sort a set of fields by their indicators.\n\n    Return a sorted list with correct global field positions.",
        "Create a record object using the LXML parser.\n\n    If correct == 1, then perform DTD validation\n    If correct == 0, then do not perform DTD validation\n\n    If verbose == 0, the parser will not give warnings.\n    If 1 <= verbose <= 3, the parser will not give errors, but will warn\n        the user about possible mistakes (implement me!)\n    If verbose > 3 then the parser will be strict and will stop in case of\n        well-formedness errors or DTD errors.",
        "Retrieve all children from node 'node' with name 'name'.",
        "Iterate through all the children of a node.\n\n    Returns one string containing the values from all the text-nodes\n    recursively.",
        "Check and correct the structure of the record.\n\n    :param record: the record data structure\n    :return: a list of errors found",
        "Return a warning message of code 'code'.\n\n    If code = (cd, str) it returns the warning message of code 'cd' and appends\n    str at the end",
        "Compare twolists using given comparing function.\n\n    :param list1: first list to compare\n    :param list2: second list to compare\n    :param custom_cmp: a function taking two arguments (element of\n        list 1, element of list 2) and\n    :return: True or False depending if the values are the same",
        "Parse an XML document and clean any namespaces.",
        "Clean MARCXML harvested from OAI.\n\n        Allows the xml to be used with BibUpload or BibRecord.\n\n        :param xml: either XML as a string or path to an XML file\n\n        :return: ElementTree of clean data",
        "Generate the record deletion if deleted form OAI-PMH.",
        "Return a session for yesss.at.",
        "Check for working login data.",
        "Send an SMS.",
        "Return the date of the article in file.",
        "Return this articles' collection.",
        "Attach fulltext FFT.",
        "Convert the list of bibrecs into one MARCXML.\n\n        >>> from harvestingkit.bibrecord import BibRecordPackage\n        >>> from harvestingkit.inspire_cds_package import Inspire2CDS\n        >>> bibrecs = BibRecordPackage(\"inspire.xml\")\n        >>> bibrecs.parse()\n        >>> xml = Inspire2CDS.convert_all(bibrecs.get_records())\n\n        :param records: list of BibRecord dicts\n        :type records: list\n\n        :returns: MARCXML as string",
        "Yield single conversion objects from a MARCXML file or string.\n\n        >>> from harvestingkit.inspire_cds_package import Inspire2CDS\n        >>> for record in Inspire2CDS.from_source(\"inspire.xml\"):\n        >>>     xml = record.convert()",
        "Return the opposite mapping by searching the imported KB.",
        "Load configuration from config.\n\n        Meant to run only once per system process as\n        class variable in subclasses.",
        "Try to match the current record to the database.",
        "Keep only fields listed in field_list.",
        "Clear any fields listed in field_list.",
        "Add 035 number from 001 recid with given source.",
        "Add a control-number 00x for given tag with value.",
        "650 Translate Categories.",
        "Connects and logins to the server.",
        "Downloads a file from the FTP server to target folder\n\n        :param source_file: the absolute path for the file on the server\n                   it can be the one of the files coming from\n                   FtpHandler.dir().\n        :type source_file: string\n        :param target_folder: relative or absolute path of the\n                              destination folder default is the\n                              working directory.\n        :type target_folder: string",
        "Changes the working directory on the server.\n\n        :param folder: the desired directory.\n        :type folder: string",
        "Lists the files and folders of a specific directory\n        default is the current working directory.\n\n        :param folder: the folder to be listed.\n        :type folder: string\n\n        :returns: a tuple with the list of files in the folder\n                  and the list of subfolders in the folder.",
        "Creates a folder in the server\n\n        :param folder: the folder to be created.\n        :type folder: string",
        "Delete a file from the server.\n\n        :param filename: the file to be deleted.\n        :type filename: string",
        "Delete a folder from the server.\n\n        :param foldername: the folder to be deleted.\n        :type foldername: string",
        "Returns the filesize of a file\n\n        :param filename: the full path to the file on the server.\n        :type filename: string\n\n        :returns: string representation of the filesize.",
        "Uploads a file on the server to the desired location\n\n        :param filename: the name of the file to be uploaded.\n        :type filename: string\n        :param location: the directory in which the file will\n                         be stored.\n        :type location: string",
        "Parses a block of text indiscriminately",
        "Parses a block of text rendering links that occur on their own line\n        normally but rendering inline links using a special template dir",
        "Do the legwork of logging into the Midas Server instance, storing the API\n    key and token.\n\n    :param email: (optional) Email address to login with. If not set, the\n        console will be prompted.\n    :type email: None | string\n    :param password: (optional) User password to login with. If not set and no\n        'api_key' is set, the console will be prompted.\n    :type password: None | string\n    :param api_key: (optional) API key to login with. If not set, password\n        login with be used.\n    :type api_key: None | string\n    :param application: (optional) Application name to be used with 'api_key'.\n    :type application: string\n    :param url: (optional) URL address of the Midas Server instance to login\n        to. If",
        "Renew or get a token to use for transactions with the Midas Server\n    instance.\n\n    :returns: API token.\n    :rtype: string",
        "Create an item from the local file in the Midas Server folder corresponding\n    to the parent folder id.\n\n    :param local_file: full path to a file on the local file system\n    :type local_file: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the item will be added\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool",
        "Create a folder from the local file in the midas folder corresponding to\n    the parent folder id.\n\n    :param local_folder: full path to a directory on the local file system\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the folder will be added\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing folder of\n       the same name in the same location, or create a new one instead\n    :type reuse_existing: bool",
        "Create and return a hex checksum using the MD5 sum of the passed in file.\n    This will stream the file, rather than load it all into memory.\n\n    :param file_path: full path to the file\n    :type file_path: string\n    :returns: a hex checksum\n    :rtype: string",
        "Create a bitstream in the given item.\n\n    :param file_path: full path to the local file\n    :type file_path: string\n    :param local_file: name of the local file\n    :type local_file: string\n    :param log_ind: (optional) any additional message to log upon creation of\n        the bitstream\n    :type log_ind: None | string",
        "Function for doing an upload of a file as an item. This should be a\n    building block for user-level functions.\n\n    :param local_file: name of local file to upload\n    :type local_file: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the item will be added\n    :type parent_folder_id: int | long\n    :param file_path: full path to the file\n    :type file_path: string\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool",
        "Function for creating a remote folder and returning the id. This should be\n    a building block for user-level functions.\n\n    :param local_folder: full path to a local folder\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the new folder will be added\n    :type parent_folder_id: int | long\n    :returns: id of the remote folder that was created\n    :rtype: int | long",
        "Function to recursively upload a folder and all of its descendants.\n\n    :param local_folder: full path to local folder to be uploaded\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the new folder will be added\n    :type parent_folder_id: int | long\n    :param leaf_folders_as_items: (optional) whether leaf folders should have\n        all files uploaded as single items\n    :type leaf_folders_as_items: bool\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool",
        "Return whether a folder contains only files. This will be False if the\n    folder contains any subdirectories.\n\n    :param local_folder: full path to the local folder\n    :type local_folder: string\n    :returns: True if the folder contains only files\n    :rtype: bool",
        "Upload a folder as a new item. Take a folder and use its base name as the\n    name of a new item. Then, upload its containing files into the new item as\n    bitstreams.\n\n    :param local_folder: The path to the folder to be uploaded\n    :type local_folder: string\n    :param parent_folder_id: The id of the destination folder for the new item.\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool",
        "Upload a pattern of files. This will recursively walk down every tree in\n    the file pattern to create a hierarchy on the server. As of right now, this\n    places the file into the currently logged in user's home directory.\n\n    :param file_pattern: a glob type pattern for files\n    :type file_pattern: string\n    :param destination: (optional) name of the midas destination folder,\n        defaults to Private\n    :type destination: string\n    :param leaf_folders_as_items: (optional) whether leaf folders should have\n        all files uploaded as single items\n    :type leaf_folders_as_items: bool\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool",
        "Descend a path to return a folder id starting from the given folder id.\n\n    :param parsed_path: a list of folders from top to bottom of a hierarchy\n    :type parsed_path: list[string]\n    :param folder_id: The id of the folder from which to start the descent\n    :type folder_id: int | long\n    :returns: The id of the found folder or -1\n    :rtype: int | long",
        "Find an item or folder matching the name. A folder will be found first if\n    both are present.\n\n    :param name: The name of the resource\n    :type name: string\n    :param folder_id: The folder to search within\n    :type folder_id: int | long\n    :returns: A tuple indicating whether the resource is an item an the id of\n        said resource. i.e. (True, item_id) or (False, folder_id). Note that in\n        the event that we do not find a result return (False, -1)\n    :rtype: (bool, int | long)",
        "Get a folder id from a path on the server.\n\n    Warning: This is NOT efficient at all.\n\n    The schema for this path is:\n    path := \"/users/<name>/\" | \"/communities/<name>\" , {<subfolder>/}\n    name := <firstname> , \"_\" , <lastname>\n\n    :param path: The virtual path on the server.\n    :type path: string\n    :returns: a tuple indicating True or False about whether the resource is an\n        item and id of the resource i.e. (True, item_id) or (False, folder_id)\n    :rtype: (bool, int | long)",
        "Download a folder to the specified path along with any children.\n\n    :param folder_id: The id of the target folder\n    :type folder_id: int | long\n    :param path: (optional) the location to download the folder\n    :type path: string",
        "Download the requested item to the specified path.\n\n    :param item_id: The id of the item to be downloaded\n    :type item_id: int | long\n    :param path: (optional) the location to download the item\n    :type path: string\n    :param item: The dict of item info\n    :type item: dict | None",
        "Recursively download a file or item from the Midas Server instance.\n\n    :param server_path: The location on the server to find the resource to\n        download\n    :type server_path: string\n    :param local_path: The location on the client to store the downloaded data\n    :type local_path: string",
        "Login and get a token. If you do not specify a specific application,\n        'Default' will be used.\n\n        :param email: Email address of the user\n        :type email: string\n        :param api_key: API key assigned to the user\n        :type api_key: string\n        :param application: (optional) Application designated for this API key\n        :type application: string\n        :returns: Token to be used for interaction with the API until\n            expiration\n        :rtype: string",
        "List the folders in the users home area.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :returns: List of dictionaries containing folder information.\n        :rtype: list[dict]",
        "Get the default API key for a user.\n\n        :param email: The email of the user.\n        :type email: string\n        :param password: The user's password.\n        :type password: string\n        :returns: API key to confirm that it was fetched successfully.\n        :rtype: string",
        "List the public users in the system.\n\n        :param limit: (optional) The number of users to fetch.\n        :type limit: int | long\n        :returns: The list of users.\n        :rtype: list[dict]",
        "Get a user by the email of that user.\n\n        :param email: The email of the desired user.\n        :type email: string\n        :returns: The user requested.\n        :rtype: dict",
        "Create a new community or update an existing one using the uuid.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param name: The community name.\n        :type name: string\n        :param description: (optional) The community description.\n        :type description: string\n        :param uuid: (optional) uuid of the community. If none is passed, will\n            generate one.\n        :type uuid: string\n        :param privacy: (optional) Default 'Public', possible values\n            [Public|Private].\n        :type privacy: string\n        :param can_join: (optional) Default 'Everyone', possible values\n            [Everyone|Invitation].\n        :type can_join: string\n        :returns: The community dao that was created.\n        :rtype: dict",
        "Get a community based on its name.\n\n        :param name: The name of the target community.\n        :type name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The requested community.\n        :rtype: dict",
        "Get a community based on its id.\n\n        :param community_id: The id of the target community.\n        :type community_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The requested community.\n        :rtype: dict",
        "Get the non-recursive children of the passed in community_id.\n\n        :param community_id: The id of the requested community.\n        :type community_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: List of the folders in the community.\n        :rtype: dict[string, list]",
        "List all communities visible to a user.\n\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The list of communities.\n        :rtype: list[dict]",
        "Get the attributes of the specified folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the requested folder.\n        :type folder_id: int | long\n        :returns: Dictionary of the folder attributes.\n        :rtype: dict",
        "Get the non-recursive children of the passed in folder_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the requested folder.\n        :type folder_id: int | long\n        :returns: Dictionary of two lists: 'folders' and 'items'.\n        :rtype: dict[string, list]",
        "Delete the folder with the passed in folder_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder to be deleted.\n        :type folder_id: int | long\n        :returns: None.\n        :rtype: None",
        "Move a folder to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder to be moved.\n        :type folder_id: int | long\n        :param dest_folder_id: The id of destination (new parent) folder.\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the moved folder.\n        :rtype: dict",
        "Create an item to the server.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param name: The name of the item to be created.\n        :type name: string\n        :param parent_id: The id of the destination folder.\n        :type parent_id: int | long\n        :param description: (optional) The description text of the item.\n        :type description: string\n        :param uuid: (optional) The UUID for the item. It will be generated if\n            not given.\n        :type uuid: string\n        :param privacy: (optional) The privacy state of the item\n            ('Public' or 'Private').\n        :type privacy: string\n        :returns: Dictionary containing the details of the created item.\n        :rtype: dict",
        "Get the attributes of the specified item.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the requested item.\n        :type item_id: int | string\n        :returns: Dictionary of the item attributes.\n        :rtype: dict",
        "Download an item to disk.\n\n        :param item_id: The id of the item to be downloaded.\n        :type item_id: int | long\n        :param token: (optional) The authentication token of the user\n            requesting the download.\n        :type token: None | string\n        :param revision: (optional) The revision of the item to download, this\n            defaults to HEAD.\n        :type revision: None | int | long\n        :returns: A tuple of the filename and the content iterator.\n        :rtype: (string, unknown)",
        "Delete the item with the passed in item_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be deleted.\n        :type item_id: int | long\n        :returns: None.\n        :rtype: None",
        "Get the metadata associated with an item.\n\n        :param item_id: The id of the item for which metadata will be returned\n        :type item_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :param revision: (optional) Revision of the item. Defaults to latest\n            revision.\n        :type revision: int | long\n        :returns: List of dictionaries containing item metadata.\n        :rtype: list[dict]",
        "Set the metadata associated with an item.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item for which metadata will be set.\n        :type item_id: int | long\n        :param element: The metadata element name.\n        :type element: string\n        :param value: The metadata value for the field.\n        :type value: string\n        :param qualifier: (optional) The metadata qualifier. Defaults to empty\n            string.\n        :type qualifier: None | string\n        :returns: None.\n        :rtype: None",
        "Share an item to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be shared.\n        :type item_id: int | long\n        :param dest_folder_id: The id of destination folder where the item is\n            shared to.\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the shared item.\n        :rtype: dict",
        "Move an item from the source folder to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be moved\n        :type item_id: int | long\n        :param src_folder_id: The id of source folder where the item is located\n        :type src_folder_id: int | long\n        :param dest_folder_id: The id of destination folder where the item is\n            moved to\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the moved item\n        :rtype: dict",
        "Return all items.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name.\n        :rtype: list[dict]",
        "Return all items with a given name and parent folder id.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param folder_id: The id of the parent folder to search by.\n        :type folder_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name and parent folder id.\n        :rtype: list[dict]",
        "Return all items with a given name and parent folder name.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param folder_name: The name of the parent folder to search by.\n        :type folder_name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name and parent folder\n            name.\n        :rtype: list[dict]",
        "Create a link bitstream.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder in which to create a new item\n            that will contain the link. The new item will have the same name as\n            the URL unless an item name is supplied.\n        :type folder_id: int | long\n        :param url: The URL of the link you will create, will be used as the\n            name of the bitstream and of the item unless an item name is\n            supplied.\n        :type url: string\n        :param item_name: (optional)  The name of the newly created item, if\n            not supplied, the item will have the same name as the URL.\n        :type item_name: string\n        :param length: (optional) The length in bytes of the file to which the",
        "Generate a token to use for upload.\n\n        Midas Server uses a individual token for each upload. The token\n        corresponds to the file specified and that file only. Passing the MD5\n        checksum allows the server to determine if the file is already in the\n        asset store.\n\n        If :param:`checksum` is passed and the token returned is blank, the\n        server already has this file and there is no need to follow this\n        call with a call to `perform_upload`, as the passed in file will have\n        been added as a bitstream to the item's latest revision, creating a\n        new revision if one doesn't exist.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item in which to upload the file as a\n            bitstream.\n        :type item_id: int |",
        "Upload a file into a given item (or just to the public folder if the\n        item is not specified.\n\n        :param upload_token: The upload token (returned by\n            generate_upload_token)\n        :type upload_token: string\n        :param filename: The upload filename. Also used as the path to the\n            file, if 'filepath' is not set.\n        :type filename: string\n        :param mode: (optional) Stream or multipart. Default is stream.\n        :type mode: string\n        :param folder_id: (optional) The id of the folder to upload into.\n        :type folder_id: int | long\n        :param item_id: (optional) If set, will append item ``bitstreams`` to\n            the latest revision (or the one set using :param:`revision` ) of\n            the existing item.\n        :type item",
        "Get the resources corresponding to a given query.\n\n        :param search: The search criterion.\n        :type search: string\n        :param token: (optional) The credentials to use when searching.\n        :type token: None | string\n        :returns: Dictionary containing the search result. Notable is the\n            dictionary item 'results', which is a list of item details.\n        :rtype: dict",
        "Add a Condor DAG to the given Batchmake task.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param batchmaketaskid: id of the Batchmake task for this DAG\n        :type batchmaketaskid: int | long\n        :param dagfilename: Filename of the DAG file\n        :type dagfilename: string\n        :param dagmanoutfilename: Filename of the DAG processing output\n        :type dagmanoutfilename: string\n        :returns: The created Condor DAG DAO\n        :rtype: dict",
        "Add a Condor DAG job to the Condor DAG associated with this\n        Batchmake task\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param batchmaketaskid: id of the Batchmake task for this DAG\n        :type batchmaketaskid: int | long\n        :param jobdefinitionfilename: Filename of the definition file for the\n            job\n        :type jobdefinitionfilename: string\n        :param outputfilename: Filename of the output file for the job\n        :type outputfilename: string\n        :param errorfilename: Filename of the error file for the job\n        :type errorfilename: string\n        :param logfilename: Filename of the log file for the job\n        :type logfilename: string\n        :param postfilename: Filename of the post script log file for the job\n        :type post",
        "Extract DICOM metadata from the given item\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: id of the item to be extracted\n        :type item_id: int | long\n        :return: the item revision DAO\n        :rtype: dict",
        "Log in to get the real token using the temporary token and otp.\n\n        :param temp_token: The temporary token or id returned from normal login\n        :type temp_token: string\n        :param one_time_pass: The one-time pass to be sent to the underlying\n            multi-factor engine.\n        :type one_time_pass: string\n        :returns: A standard token for interacting with the web api.\n        :rtype: string",
        "Create a big thumbnail for the given bitstream with the given width.\n        It is used as the main image of the given item and shown in the item\n        view page.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param bitstream_id: The bitstream from which to create the thumbnail.\n        :type bitstream_id: int | long\n        :param item_id: The item on which to set the thumbnail.\n        :type item_id: int | long\n        :param width: (optional) The width in pixels to which to resize (aspect\n            ratio will be preserved). Defaults to 575.\n        :type width: int | long\n        :returns: The ItemthumbnailDao object that was created.\n        :rtype: dict",
        "Create a 100x100 small thumbnail for the given item. It is used for\n        preview purpose and displayed in the 'preview' and 'thumbnails'\n        sidebar sections.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The item on which to set the thumbnail.\n        :type item_id: int | long\n        :returns: The item object (with the new thumbnail id) and the path\n            where the newly created thumbnail is stored.\n        :rtype: dict",
        "Search item metadata using Apache Solr.\n\n        :param query: The Apache Lucene search query.\n        :type query: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :param limit: (optional) The limit of the search.\n        :type limit: int | long\n        :returns: The list of items that match the search query.\n        :rtype: list[dict]",
        "Create a new scalar data point.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param community_id: The id of the community that owns the producer.\n        :type community_id: int | long\n        :param producer_display_name: The display name of the producer.\n        :type producer_display_name: string\n        :param metric_name: The metric name that identifies which trend this\n            point belongs to.\n        :type metric_name: string\n        :param producer_revision: The repository revision that identifies which trend this\n            point belongs to.\n        :type producer_revision: int | long | string\n        :param submit_time: The submit timestamp. Must be parsable with PHP\n            strtotime().\n        :type submit_time: string\n        :param value: The value of the scalar.",
        "Upload a JSON file containing numeric scoring results to be added as\n        scalars. File is parsed and then deleted from the server.\n\n        :param token: A valid token for the user in question.\n        :param filepath: The path to the JSON file.\n        :param community_id: The id of the community that owns the producer.\n        :param producer_display_name: The display name of the producer.\n        :param producer_revision: The repository revision of the producer\n            that produced this value.\n        :param submit_time: The submit timestamp. Must be parsable with PHP\n            strtotime().\n        :param config_item_id: (optional) If this value pertains to a specific\n            configuration item, pass its id here.\n        :param test_dataset_id: (optional) If this value pertains to a\n            specific test dataset, pass its id here.\n        :",
        "Obtain particular version of the doc at key.",
        "Find a hash value for the linear combination of invocation methods.",
        "Connects to a Siemens S7 PLC.\n\n        Connects to a Siemens S7 using the Snap7 library.\n        See [the snap7 documentation](http://snap7.sourceforge.net/) for\n        supported models and more details.\n\n        It's not currently possible to query the device for available pins,\n        so `available_pins()` returns an empty list. Instead, you should use\n        `map_pin()` to map to a Merker, Input or Output in the PLC. The\n        internal id you should use is a string following this format:\n        '[DMQI][XBWD][0-9]+.?[0-9]*' where:\n\n        * [DMQI]: D for DB, M for Merker, Q for Output, I for Input\n        * [XBWD]: X for bit, B for byte, W for",
        "Connects to an Arduino UNO on serial port `port`.\n\n        @throw RuntimeError can't connect to Arduino",
        "Returns a map of nodename to average fitness value for this block.\n        Assumes that required resources have been checked on all nodes.",
        "Returns a list of available drivers names.",
        "Maps a pin number to a physical device pin.\n\n        To make it easy to change drivers without having to refactor a lot of\n        code, this library does not use the names set by the driver to identify\n        a pin. This function will map a number, that will be used by other\n        functions, to a physical pin represented by the drivers pin id. That\n        way, if you need to use another pin or change the underlying driver\n        completly, you only need to redo the mapping.\n\n        If you're developing a driver, keep in mind that your driver will not\n        know about this. The other functions will translate the mapped pin to\n        your id before calling your function.\n\n        @arg abstract_pin_id the id that will identify this pin in the\n        other function calls. You can choose what you want.\n\n        @arg physical_pin_id the id returned in the driver.",
        "Sets pin `pin` to `direction`.\n\n        The pin should support the requested mode. Calling this function\n        on a unmapped pin does nothing. Calling it with a unsupported direction\n        throws RuntimeError.\n\n        If you're developing a driver, you should implement\n        _set_pin_direction(self, pin, direction) where `pin` will be one of\n        your internal IDs. If a pin is set to OUTPUT, put it on LOW state.\n\n        @arg pin pin id you've set using `AbstractDriver.map_pin`\n        @arg mode a value from `AbstractDriver.Direction`\n\n        @throw KeyError if pin isn't mapped.\n        @throw RuntimeError if direction is not supported by pin.",
        "Gets the `ahio.Direction` this pin was set to.\n\n        If you're developing a driver, implement _pin_direction(self, pin)\n\n        @arg pin the pin you want to see the mode\n        @returns the `ahio.Direction` the pin is set to\n\n        @throw KeyError if pin isn't mapped.",
        "Sets pin `pin` to `type`.\n\n        The pin should support the requested mode. Calling this function\n        on a unmapped pin does nothing. Calling it with a unsupported mode\n        throws RuntimeError.\n\n        If you're developing a driver, you should implement\n        _set_pin_type(self, pin, ptype) where `pin` will be one of your\n        internal IDs. If a pin is set to OUTPUT, put it on LOW state.\n\n        @arg pin pin id you've set using `AbstractDriver.map_pin`\n        @arg mode a value from `AbstractDriver.PortType`\n\n        @throw KeyError if pin isn't mapped.\n        @throw RuntimeError if type is not supported by pin.",
        "Gets the `ahio.PortType` this pin was set to.\n\n        If you're developing a driver, implement _pin_type(self, pin)\n\n        @arg pin the pin you want to see the mode\n        @returns the `ahio.PortType` the pin is set to\n\n        @throw KeyError if pin isn't mapped.",
        "Sets the output to the given value.\n\n        Sets `pin` output to given value. If the pin is in INPUT mode, do\n        nothing. If it's an analog pin, value should be in write_range.\n        If it's not in the allowed range, it will be clamped. If pin is in\n        digital mode, value can be `ahio.LogicValue` if `pwm` = False, or a\n        number between 0 and 1 if `pwm` = True. If PWM is False, the pin will\n        be set to HIGH or LOW, if `pwm` is True, a PWM wave with the given\n        cycle will be created. If the pin does not support PWM and `pwm` is\n        True, raise RuntimeError. The `pwm` argument should be ignored in case\n        the pin is analog. If value is not valid for the given\n",
        "Reads value from pin `pin`.\n\n        Returns the value read from pin `pin`. If it's an analog pin, returns\n        a number in analog.input_range. If it's digital, returns\n        `ahio.LogicValue`.\n\n        If you're developing a driver, implement _read(self, pin)\n\n        @arg pin the pin to read from\n        @returns the value read from the pin\n\n        @throw KeyError if pin isn't mapped.",
        "Sets the analog reference to `reference`\n\n        If the driver supports per pin reference setting, set pin to the\n        desired reference. If not, passing None means set to all, which is the\n        default in most hardware. If only per pin reference is supported and\n        pin is None, raise RuntimeError.\n\n        If you're developing a driver, implement\n        _set_analog_reference(self, reference, pin). Raise RuntimeError if pin\n        was set but is not supported by the platform.\n\n        @arg reference the value that describes the analog reference. See\n            `AbstractDriver.analog_references`\n        @arg pin if the the driver supports it, the pin that will use\n            `reference` as reference. None for all.\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only analog reference hardware",
        "Returns the analog reference.\n\n        If the driver supports per pin analog reference setting, returns the\n        reference for pin `pin`. If pin is None, returns the global analog\n        reference. If only per pin reference is supported and pin is None,\n        raise RuntimeError.\n\n        If you're developing a driver, implement _analog_reference(self, pin)\n\n        @arg pin if the the driver supports it, the pin that will use\n            `reference` as reference. None for all.\n\n        @returns the reference used for pin\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only analog reference hardware.\n        @throw KeyError if pin isn't mapped.",
        "Sets PWM frequency, if supported by hardware\n\n        If the driver supports per pin frequency setting, set pin to the\n        desired frequency. If not, passing None means set to all. If only per\n        pin frequency is supported and pin is None, raise RuntimeError.\n\n        If you're developing a driver, implement\n        _set_pwm_frequency(self, frequency, pin). Raise RuntimeError if pin\n        was set but is not supported by the platform.\n\n        @arg frequency pwm frequency to be set, in Hz\n        @arg pin if the the driver supports it, the pin that will use\n            `frequency` as pwm frequency. None for all/global.\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only hardware.\n        @throw KeyError if pin isn't mapped.",
        "Integrate SIR epidemic model\n\n    Simulate a very basic deterministic SIR system.\n\n    :param 2x1 numpy array y0: initial conditions\n    :param Ntimestep length numpy array time: Vector of time points that \\\n    solution is returned at\n    :param float beta: transmission rate\n    :param float gamma: recovery rate\n\n    :returns: (2)x(Ntimestep) numpy array Xsim: first row S(t), second row I(t)",
        "Return the URL of the server.\n\n        :returns: URL of the server\n        :rtype: string",
        "Returns an estimate for the maximum amount of memory to be consumed by numpy arrays.",
        "Create coverage reports and open them in the browser.",
        "Start a Modbus server.\n\n        The following classes are available with their respective named\n        parameters:\n        \n        ModbusTcpClient\n            host: The host to connect to (default 127.0.0.1)\n            port: The modbus port to connect to (default 502)\n            source_address: The source address tuple to bind to (default ('', 0))\n            timeout: The timeout to use for this socket (default Defaults.Timeout)\n\n        ModbusUdpClient\n            host: The host to connect to (default 127.0.0.1)\n            port: The modbus port to connect to (default 502)\n            timeout: The timeout to use for this socket (default None)\n\n        ModbusSerialClient\n            method: The method to use for connection (asii, rtu, binary)\n            port: The serial port to attach to\n            stopbits",
        "Return an exception given status and error codes.\n\n    :param status_code: HTTP status code.\n    :type status_code: None | int\n    :param error_code: Midas Server error code.\n    :type error_code: None | int\n    :param value: Message to display.\n    :type value: string\n    :returns: Exception.\n    :rtype : pydas.exceptions.ResponseError",
        "Retrieve the last analog data value received for the specified pin.\n\n        :param pin: Selected pin\n\n        :return: The last value entered into the analog response table.",
        "Disables analog reporting for a single analog pin.\n\n        :param pin: Analog pin number. For example for A0, the number is 0.\n\n        :return: No return value",
        "Disables digital reporting. By turning reporting off for this pin, reporting\n        is disabled for all 8 bits in the \"port\" -\n\n        :param pin: Pin and all pins for this port\n\n        :return: No return value",
        "Enables analog reporting. By turning reporting on for a single pin.\n\n        :param pin: Analog pin number. For example for A0, the number is 0.\n\n        :return: No return value",
        "Enables digital reporting. By turning reporting on for all 8 bits in the \"port\" -\n        this is part of Firmata's protocol specification.\n\n        :param pin: Pin and all pins for this port\n\n        :return: No return value",
        "This method will send an extended data analog output command to the selected pin\n\n        :param pin: 0 - 127\n\n        :param data: 0 - 0xfffff",
        "Get the stepper library version number.\n\n        :param timeout: specify a time to allow arduino to process and return a version\n\n        :return: the stepper version number if it was set.",
        "Write data to an i2c device.\n\n        :param address: i2c device address\n\n        :param args: A variable number of bytes to be sent to the device",
        "This method stops an I2C_READ_CONTINUOUSLY operation for the i2c device address specified.\n\n        :param address: address of i2c device",
        "This method will call the Tone library for the selected pin.\n        If the tone command is set to TONE_TONE, then the specified tone will be played.\n        Else, if the tone command is TONE_NO_TONE, then any currently playing tone will be disabled.\n        It is intended for a future release of Arduino Firmata\n\n        :param pin: Pin number\n\n        :param tone_command: Either TONE_TONE, or TONE_NO_TONE\n\n        :param frequency: Frequency of tone in hz\n\n        :param duration: Duration of tone in milliseconds\n\n        :return: No return value",
        "This method \"arms\" an analog pin for its data to be latched and saved in the latching table\n        If a callback method is provided, when latching criteria is achieved, the callback function is called\n        with latching data notification. In that case, the latching table is not updated.\n\n        :param pin: Analog pin number (value following an 'A' designator, i.e. A5 = 5\n\n        :param threshold_type: ANALOG_LATCH_GT | ANALOG_LATCH_LT  | ANALOG_LATCH_GTE | ANALOG_LATCH_LTE\n\n        :param threshold_value: numerical value - between 0 and 1023\n\n        :param cb: callback method\n\n        :return: True if successful, False if parameter data is invalid",
        "This method \"arms\" a digital pin for its data to be latched and saved in the latching table\n        If a callback method is provided, when latching criteria is achieved, the callback function is called\n        with latching data notification. In that case, the latching table is not updated.\n\n        :param pin: Digital pin number\n\n        :param threshold_type: DIGITAL_LATCH_HIGH | DIGITAL_LATCH_LOW\n\n        :param cb: callback function\n\n        :return: True if successful, False if parameter data is invalid",
        "Configure a pin as a servo pin. Set pulse min, max in ms.\n\n        :param pin: Servo Pin.\n\n        :param min_pulse: Min pulse width in ms.\n\n        :param max_pulse: Max pulse width in ms.\n\n        :return: No return value",
        "Configure stepper motor prior to operation.\n\n        :param steps_per_revolution: number of steps per motor revolution\n\n        :param stepper_pins: a list of control pin numbers - either 4 or 2",
        "Move a stepper motor for the number of steps at the specified speed\n\n        :param motor_speed: 21 bits of data to set motor speed\n\n        :param number_of_steps: 14 bits for number of steps & direction\n                                positive is forward, negative is reverse",
        "Request the stepper library version from the Arduino.\n        To retrieve the version after this command is called, call\n        get_stepper_version",
        "open the serial port using the configuration data\n        returns a reference to this instance",
        "This method continually runs. If an incoming character is available on the serial port\n        it is read and placed on the _command_deque\n        @return: Never Returns",
        "Set the brightness level for the entire display\n        @param brightness: brightness level (0 -15)",
        "Populate the bit map with the supplied \"shape\" and color\n        and then write the entire bitmap to the display\n        @param shape: pattern to display\n        @param color: color for the pattern",
        "Write the entire buffer to the display",
        "Set all led's to off.",
        "This method handles the incoming digital message.\n        It stores the data values in the digital response table.\n        Data is stored for all 8 bits of a  digital port\n\n        :param data: Message data from Firmata\n\n        :return: No return value.",
        "This method handles the incoming encoder data message and stores\n        the data in the digital response table.\n\n        :param data: Message data from Firmata\n\n        :return: No return value.",
        "This method handles the incoming sonar data message and stores\n        the data in the response table.\n\n        :param data: Message data from Firmata\n\n        :return: No return value.",
        "This method will send a Sysex command to Firmata with any accompanying data\n\n        :param sysex_command: sysex command\n\n        :param sysex_data: data for command\n\n        :return : No return value.",
        "This method is used to transmit a non-sysex command.\n\n        :param command: Command to send to firmata includes command + data formatted by caller\n\n        :return : No return value.",
        "Send the reset command to the Arduino.\n        It resets the response tables to their initial values\n\n        :return: No return value",
        "This method handles the incoming string data message from Firmata.\n        The string is printed to the console\n\n        :param data: Message data from Firmata\n\n        :return: No return value.s",
        "This method starts the thread that continuously runs to receive and interpret\n        messages coming from Firmata. This must be the last method in this file\n        It also checks the deque for messages to be sent to Firmata.",
        "Use requests to fetch remote content",
        "Combine finder_image_urls and extender_image_urls,\n        remove duplicate but keep order",
        "Find image URL in background-image\n\n    Example:\n    <div style=\"width: 100%; height: 100%; background-image: url(http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg);\" class=\"Image iLoaded iWithTransition Frame\" src=\"http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg\"></div>\n    to\n    http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg",
        "Return the node name where the ``name`` would land to",
        "Return the node where the ``name`` would land to",
        "Return the encoding, idletime, or refcount about the key",
        "Pop a value off the tail of ``src``, push it on the head of ``dst``\n        and then return it.\n\n        This command blocks until a value is in ``src`` or until ``timeout``\n        seconds elapse, whichever is first. A ``timeout`` value of 0 blocks\n        forever.\n        Not atomic",
        "RPOP a value off of the ``src`` list and LPUSH it\n        on to the ``dst`` list.  Returns the value.",
        "Move ``value`` from set ``src`` to set ``dst``\n        not atomic",
        "Returns the members of the set resulting from the union between\n        the first set and all the successive sets.",
        "Store the union of sets ``src``,  ``args`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.",
        "Sets each key in the ``mapping`` dict to its corresponding value if\n        none of the keys are already set",
        "Rename key ``src`` to ``dst``",
        "Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist",
        "Returns a list of keys matching ``pattern``",
        "Returns the number of keys in the current database",
        "Prepare the date in the instance state for serialization.",
        "Verify the signaure of an XML document with the given certificate.\n    Returns `True` if the document is signed with a valid signature.\n    Returns `False` if the document is not signed or if the signature is\n    invalid.\n\n    :param lxml.etree._Element xml: The document to sign\n    :param file stream: The private key to sign the document with\n\n    :rtype: Boolean",
        "Add number of photos to each gallery.",
        "Set currently authenticated user as the author of the gallery.",
        "For each photo set it's author to currently authenticated user.",
        "Outputs a list of tuples with ranges or the empty list\n        According to the rfc, start or end values can be omitted",
        "Removes errored ranges",
        "Converts to valid byte ranges",
        "Sorts and removes overlaps",
        "Renders the selected social widget. You can specify optional settings\n    that will be passed  to widget template.\n\n    Sample usage:\n    {% social_widget_render widget_template ke1=val1 key2=val2 %}\n\n    For example to render Twitter follow button you can use code like this:\n    {% social_widget_render 'twitter/follow_button.html' username=\"ev\" %}",
        "In-place addition\n\n        :param addend_mat: A matrix to be added on the Sparse3DMatrix object\n        :param axis: The dimension along the addend_mat is added\n        :return: Nothing (as it performs in-place operations)",
        "In-place multiplication\n\n        :param multiplier: A matrix or vector to be multiplied\n        :param axis: The dim along which 'multiplier' is multiplied\n        :return: Nothing (as it performs in-place operations)",
        "Updates the probability of read origin at read level\n\n        :param model: Normalization model (1: Gene->Allele->Isoform, 2: Gene->Isoform->Allele, 3: Gene->Isoform*Allele, 4: Gene*Isoform*Allele)\n        :return: Nothing (as it performs in-place operations)",
        "Runs EM iterations\n\n        :param model: Normalization model (1: Gene->Allele->Isoform, 2: Gene->Isoform->Allele, 3: Gene->Isoform*Allele, 4: Gene*Isoform*Allele)\n        :param tol: Tolerance for termination\n        :param max_iters: Maximum number of iterations until termination\n        :param verbose: Display information on how EM is running\n        :return: Nothing (as it performs in-place operations)",
        "Exports expected read counts\n\n        :param filename: File name for output\n        :param grp_wise: whether the report is at isoform level or gene level\n        :param reorder: whether the report should be either 'decreasing' or 'increasing' order or just 'as-is'\n        :return: Nothing but the method writes a file",
        "Exports expected depths\n\n        :param filename: File name for output\n        :param grp_wise: whether the report is at isoform level or gene level\n        :param reorder: whether the report should be either 'decreasing' or 'increasing' order or just 'as-is'\n        :return: Nothing but the method writes a file",
        "Writes the posterior probability of read origin\n\n        :param filename: File name for output\n        :param title: The title of the posterior probability matrix\n        :return: Nothing but the method writes a file in EMASE format (PyTables)",
        "Prints nonzero rows of the read wanted",
        "Transliterate `data` with the given `scheme_map`. This function is used\n  when the source scheme is a Roman scheme.\n\n  :param data: the data to transliterate\n  :param scheme_map: a dict that maps between characters in the old scheme\n                     and characters in the new scheme",
        "Transliterate `data` with the given `scheme_map`. This function is used\n  when the source scheme is a Brahmic scheme.\n\n  :param data: the data to transliterate\n  :param scheme_map: a dict that maps between characters in the old scheme\n                     and characters in the new scheme",
        "Detect the input's transliteration scheme.\n\n    :param text: some text data, either a `unicode` or a `str` encoded\n                 in UTF-8.",
        "Add a variety of default schemes.",
        "converts an array of integers to utf8 string",
        "set the value of delta to reflect the current codepage",
        "Handle unrecognised characters.",
        "Transliterate a Latin character equivalent to Devanagari.\n        \n        Add VIRAMA for ligatures.\n        Convert standalone to dependent vowels.",
        "A convenience method",
        "Load and generate ``num`` number of top-level rules from the specified grammar.\n\n    :param list grammar: The grammar file to load and generate data from\n    :param int num: The number of times to generate data\n    :param output: The output destination (an open, writable stream-type object. default=``sys.stdout``)\n    :param int max_recursion: The maximum reference-recursion when generating data (default=``10``)\n    :param int seed: The seed to initialize the PRNG with. If None, will not initialize it.",
        "Build the ``Quote`` instance\n\n        :param list pre: The prerequisites list\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.",
        "Make the list of verbs into present participles\n\n    E.g.:\n\n        empower -> empowering\n        drive -> driving",
        "Deletes sent MailerMessage records",
        "Load the includes of an encoding Namelist files.\n\n  This is an implementation detail of readNamelist.",
        "Return a dict with the data of an encoding Namelist file.\n\n  This is an implementation detail of readNamelist.",
        "Detect infinite recursion and prevent it.\n\n  This is an implementation detail of readNamelist.\n\n  Raises NamelistRecursionError if namFilename is in the process of being included",
        "Returns the set of codepoints contained in a given Namelist file.\n\n  This is a replacement CodepointsInSubset and implements the \"#$ include\"\n  header format.\n\n  Args:\n    namFilename: The path to the  Namelist file.\n    unique_glyphs: Optional, whether to only include glyphs unique to subset.\n  Returns:\n    A set containing the glyphs in the subset.",
        "Returns list of CharsetInfo about supported orthographies",
        "Generates header for oauth2",
        "Parse oauth2 access",
        "Refresh access token",
        "Calls right function according to file extension",
        "Call right func to save data according to file extension",
        "Write json data into a file",
        "Get data from json file",
        "Get data from .yml file",
        "Write data into a .yml file",
        "Turns distances into RBF values.\n\n        Parameters\n        ----------\n        X : array\n            The raw pairwise distances.\n\n        Returns\n        -------\n        X_rbf : array of same shape as X\n            The distances in X passed through the RBF kernel.",
        "Learn the linear transformation to clipped eigenvalues.\n\n        Note that if min_eig isn't zero and any of the original eigenvalues\n        were exactly zero, this will leave those eigenvalues as zero.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.",
        "Learn the linear transformation to flipped eigenvalues.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.",
        "Transforms X according to the linear transformation corresponding to\n        flipping the input eigenvalues.\n\n        Parameters\n        ----------\n        X : array, shape [n_test, n]\n            The test similarities to training points.\n\n        Returns\n        -------\n        Xt : array, shape [n_test, n]\n            The transformed test similarites to training points.",
        "Flips the negative eigenvalues of X.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.\n\n        Returns\n        -------\n        Xt : array, shape [n, n]\n            The transformed training similarities.",
        "Learn the transformation to shifted eigenvalues. Only depends\n        on the input dimension.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities.",
        "Transforms X according to the linear transformation corresponding to\n        shifting the input eigenvalues to all be at least ``self.min_eig``.\n\n        Parameters\n        ----------\n        X : array, shape [n_test, n]\n            The test similarities to training points.\n\n        Returns\n        -------\n        Xt : array, shape [n_test, n]\n            The transformed test similarites to training points. Only different\n            from X if X is the training data.",
        "Picks the elements of the basis to use for the given data.\n\n        Only depends on the dimension of X. If it's more convenient, you can\n        pass a single integer for X, which is the dimension to use.\n\n        Parameters\n        ----------\n        X : an integer, a :class:`Features` instance, or a list of bag features\n            The input data, or just its dimension, since only the dimension is\n            needed here.",
        "Transform a list of bag features into its projection series\n        representation.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            New data to transform. The data should all lie in [0, 1];\n            use :class:`skl_groups.preprocessing.BagMinMaxScaler` if not.\n\n        Returns\n        -------\n        X_new : integer array, shape ``[len(X), dim_]``\n            X transformed into the new space.",
        "Get distribution version.\n\n        This method is enhanced compared to original distutils implementation.\n        If the version string is set to a special value then instead of using\n        the actual value the real version is obtained by querying versiontools.\n\n        If versiontools package is not installed then the version is obtained\n        from the standard section of the ``PKG-INFO`` file. This file is\n        automatically created by any source distribution. This method is less\n        useful as it cannot take advantage of version control information that\n        is automatically loaded by versiontools. It has the advantage of not\n        requiring versiontools installation and that it does not depend on\n        ``setup_requires`` feature of ``setuptools``.",
        "Get a live version string using versiontools",
        "Fit the transformer on the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of arrays of shape ``[n_samples[i], n_features]``\n            Training set. If a Features object, it will be stacked.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``fit()``.",
        "Transform the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of bag feature arrays\n            New data to transform.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``transform()``.\n\n        Returns\n        -------\n        X_new : :class:`Features`\n            Transformed features.",
        "Fit and transform the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of bag feature arrays\n            Data to train on and transform.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``transform()``.\n\n        Returns\n        -------\n        X_new : :class:`Features`\n            Transformed features.",
        "Compute the minimum and maximum to be used for later scaling.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.",
        "Scaling features of X according to feature_range.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed.",
        "Undo the scaling of X according to feature_range.\n\n        Note that if truncate is true, any truncated points will not\n        be restored exactly.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed.",
        "Choose the codewords based on a training set.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of arrays of shape ``[n_samples[i], n_features]``\n            Training set. If a Features object, it will be stacked.",
        "Transform a list of bag features into its bag-of-words representation.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            New data to transform.\n\n        Returns\n        -------\n        X_new : integer array, shape [len(X), kmeans.n_clusters]\n            X transformed into the new space.",
        "Checks whether the array is either integral or boolean.",
        "Returns argument as an integer array, converting floats if convertable.\n    Raises ValueError if it's a float array with nonintegral values.",
        "Signal the start of the process.\n\n        Parameters\n        ----------\n        total : int\n            The total number of steps in the process, or None if unknown.",
        "Builds FLANN indices for each bag.",
        "Gets within-bag distances for each bag.",
        "r'''\n    Estimates the linear inner product \\int p q between two distributions,\n    based on kNN distances.",
        "r'''\n    Estimates \\int p^2 based on kNN distances.\n\n    In here because it's used in the l2 distance, above.\n\n    Returns array of shape (num_Ks,).",
        "Topologically sort a DAG, represented by a dict of child => set of parents.\n    The dependency dict is destroyed during operation.\n\n    Uses the Kahn algorithm: http://en.wikipedia.org/wiki/Topological_sorting\n    Not a particularly good implementation, but we're just running it on tiny\n    graphs.",
        "Ks as an array and type-checked.",
        "The dictionary of arguments to give to FLANN.",
        "Sets up for divergence estimation \"from\" new data \"to\" X.\n        Builds FLANN indices for each bag, and maybe gets within-bag distances.\n\n        Parameters\n        ----------\n        X : list of arrays or :class:`skl_groups.features.Features`\n            The bags to search \"to\".\n\n        get_rhos : boolean, optional, default False\n            Compute within-bag distances :attr:`rhos_`. These are only needed\n            for some divergence functions or if do_sym is passed, and they'll\n            be computed (and saved) during :meth:`transform` if they're not\n            computed here.\n\n            If you're using Jensen-Shannon divergence, a higher max_K may\n            be needed once it sees the number of points in the transformed bags,\n            so the computation here might be wasted.",
        "If unstacked, convert to stacked. If stacked, do nothing.",
        "Copies the Feature object. Makes a copy of the features array.\n\n        Parameters\n        ----------\n        stack : boolean, optional, default False\n            Whether to stack the copy if this one is unstacked.\n\n        copy_meta : boolean, optional, default False\n            Also copy the metadata. If False, metadata in both points to the\n            same object.",
        "Make a Features object with no metadata; points to the same features.",
        "Specify the data to which kernel values should be computed.\n\n        Parameters\n        ----------\n        X : list of arrays or :class:`skl_groups.features.Features`\n            The bags to compute \"to\".",
        "Transform a list of bag features into a matrix of its mean features.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            Data to transform.\n\n        Returns\n        -------\n        X_new : array, shape ``[len(X), X.dim]``\n            X transformed into its means.",
        "Start listening to the server",
        "Connect to the server\n\n        :raise ConnectionError: If socket cannot establish a connection",
        "Disconnect from the server",
        "Send a command to the server\n\n        :param string command: command to send",
        "Read a line from the server. Data is read from the socket until a character ``\\n`` is found\n\n        :return: the read line\n        :rtype: string",
        "Read a block from the server. Lines are read until a character ``.`` is found\n\n        :return: the read block\n        :rtype: string",
        "Read a block and return the result as XML\n\n        :return: block as xml\n        :rtype: xml.etree.ElementTree",
        "Analyse an OpenStreetMap changeset.",
        "Get information about number of changesets, blocks and mapping days of a\n    user, using both the OSM API and the Mapbox comments APIself.",
        "Return a dictionary with id, user, user_id, bounds, date of creation\n    and all the tags of the changeset.\n\n    Args:\n        changeset: the XML string of the changeset.",
        "Get the changeset using the OSM API and return the content as a XML\n    ElementTree.\n\n    Args:\n        changeset: the id of the changeset.",
        "Get the metadata of a changeset using the OSM API and return it as a XML\n    ElementTree.\n\n    Args:\n        changeset: the id of the changeset.",
        "Read the first feature from the geojson and return it as a Polygon\n        object.",
        "Filter the changesets that intersects with the geojson geometry.",
        "Set the fields of this class with the metadata of the analysed\n        changeset.",
        "Add suspicion reason and set the suspicious flag.",
        "Execute the count and verify_words methods.",
        "Verify the fields source, imagery_used and comment of the changeset\n        for some suspect words.",
        "Verify if the software used in the changeset is a powerfull_editor.",
        "Count the number of elements created, modified and deleted by the\n        changeset and analyses if it is a possible import, mass modification or\n        a mass deletion.",
        "Get a stream URI from a playlist URI, ``uri``.\n    Unwraps nested playlists until something that's not a playlist is found or\n    the ``timeout`` is reached.",
        "Start asynchronous HTTP Server on an individual process.\n\n        :param request_handler: Sanic request handler with middleware\n        :param error_handler: Sanic error handler with middleware\n        :param debug: enables debug output (slows server)\n        :param request_timeout: time in seconds\n        :param ssl: SSLContext\n        :param sock: Socket for the server to accept connections from\n        :param request_max_size: size in bytes, `None` for no limit\n        :param reuse_port: `True` for multiple workers\n        :param loop: asyncio compatible event loop\n        :param protocol: subclass of asyncio protocol class\n        :return: Nothing",
        "Grow this Pantheon by multiplying Gods.",
        "Get it on.",
        "Compare vectors. Borrowed from A. Parish.",
        "This model recognizes that sex chromosomes don't always line up with\n        gender. Assign M, F, or NB according to the probabilities in p_gender.",
        "Accept either strings or Gods as inputs.",
        "Produce two gametes, an egg and a sperm, from the input strings.\n        Combine them to produce a genome a la sexual reproduction.",
        "Produce two gametes, an egg and a sperm, from input Gods. Combine\n        them to produce a genome a la sexual reproduction. Assign divinity\n        according to probabilities in p_divinity. The more divine the parents,\n        the more divine their offspring.",
        "Extract 23 'chromosomes' aka words from 'gene pool' aka list of tokens\n        by searching the list of tokens for words that are related to the given\n        egg_or_sperm_word.",
        "Print parents' names and epithets.",
        "Returns all the information regarding a specific stage run\n\n        See the `Go stage instance documentation`__ for examples.\n\n        .. __: http://api.go.cd/current/#get-stage-instance\n\n        Args:\n          counter (int): The stage instance to fetch.\n            If falsey returns the latest stage instance from :meth:`history`.\n          pipeline_counter (int): The pipeline instance for which to fetch\n            the stage. If falsey returns the latest pipeline instance.\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object",
        "Performs a HTTP request to the Go server\n\n        Args:\n          path (str): The full path on the Go server to request.\n            This includes any query string attributes.\n          data (str, dict, bool, optional): If any data is present this\n            request will become a POST request.\n          headers (dict, optional): Headers to set for this particular\n            request\n\n        Raises:\n          HTTPError: when the HTTP request fails.\n\n        Returns:\n          file like object: The response from a\n            :func:`urllib2.urlopen` call",
        "Make the request appear to be coming from a browser\n\n        This is to interact with older parts of Go that doesn't have a\n        proper API call to be made. What will be done:\n\n        1. If no response passed in a call to `go/api/pipelines.xml` is\n           made to get a valid session\n        2. `JSESSIONID` will be populated from this request\n        3. A request to `go/pipelines` will be so the\n           `authenticity_token` (CSRF) can be extracted. It will then\n           silently be injected into `post_args` on any POST calls that\n           doesn't start with `go/api` from this point.\n\n        Args:\n          response: a :class:`Response` object from a previously successful\n            API call. So we won't have to query `go/api/pipelines.xml`\n",
        "Return a dict as a list of lists.\n\n    >>> flatten({\"a\": \"b\"})\n    [['a', 'b']]\n    >>> flatten({\"a\": [1, 2, 3]})\n    [['a', [1, 2, 3]]]\n    >>> flatten({\"a\": {\"b\": \"c\"}})\n    [['a', 'b', 'c']]\n    >>> flatten({\"a\": {\"b\": {\"c\": \"e\"}}})\n    [['a', 'b', 'c', 'e']]\n    >>> flatten({\"a\": {\"b\": \"c\", \"d\": \"e\"}})\n    [['a', 'b', 'c'], ['a', 'd', 'e']]\n    >>> flatten({\"a\": {\"b\": \"c\", \"d\": \"e\"}, \"b\": {\"c\": \"",
        "Returns all the information regarding a specific pipeline run\n\n        See the `Go pipeline instance documentation`__ for examples.\n\n        .. __: http://api.go.cd/current/#get-pipeline-instance\n\n        Args:\n          counter (int): The pipeline instance to fetch.\n            If falsey returns the latest pipeline instance from :meth:`history`.\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object",
        "Schedule a pipeline run\n\n        Aliased as :meth:`run`, :meth:`schedule`, and :meth:`trigger`.\n\n        Args:\n          variables (dict, optional): Variables to set/override\n          secure_variables (dict, optional): Secure variables to set/override\n          materials (dict, optional): Material revisions to be used for\n            this pipeline run. The exact format for this is a bit iffy,\n            have a look at the official\n            `Go pipeline scheduling documentation`__ or inspect a call\n            from triggering manually in the UI.\n          return_new_instance (bool): Returns a :meth:`history` compatible\n            response for the newly scheduled instance. This is primarily so\n            users easily can get the new instance number. **Note:** This is done\n            in a very naive way, it just checks that the instance number is\n            higher than before the pipeline was triggered.",
        "Yields the output and metadata from all jobs in the pipeline\n\n        Args:\n          instance: The result of a :meth:`instance` call, if not supplied\n            the latest of the pipeline will be used.\n\n        Yields:\n          tuple: (metadata (dict), output (str)).\n\n          metadata contains:\n            - pipeline\n            - pipeline_counter\n            - stage\n            - stage_counter\n            - job\n            - job_result",
        "Update template config for specified template name.\n\n        .. __: https://api.go.cd/current/#edit-template-config\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object",
        "Create template config for specified template name.\n\n        .. __: https://api.go.cd/current/#create-template-config\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object",
        "Delete template config for specified template name.\n\n        .. __: https://api.go.cd/current/#delete-a-template\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object",
        "Returns a set of all pipelines from the last response\n\n        Returns:\n          set: Response success: all the pipelines available in the response\n               Response failure: an empty set",
        "Gets an artifact directory by its path.\n\n        See the `Go artifact directory documentation`__ for example responses.\n\n        .. __: http://api.go.cd/current/#get-artifact-directory\n\n        .. note::\n          Getting a directory relies on Go creating a zip file of the\n          directory in question. Because of this Go will zip the file in\n          the background and return a 202 Accepted response. It's then up\n          to the client to check again later and get the final file.\n\n          To work with normal assumptions this :meth:`get_directory` will\n          retry itself up to ``timeout`` seconds to get a 200 response to\n          return. At that point it will then return the response as is, no\n          matter whether it's still 202 or 200. The retry is done with an\n          exponential backoff with a max value between retries. See the\n          ``backoff``",
        "Configuration loader.\n\n    Adds support for loading templates from the Flask application's instance\n    folder (``<instance_folder>/templates``).",
        "Create Flask application class.\n\n    Invenio-Files-REST needs to patch the Werkzeug form parsing in order to\n    support streaming large file uploads. This is done by subclassing the Flask\n    application class.",
        "Initialize application object.\n\n        :param app: An instance of :class:`~flask.Flask`.",
        "Initialize configuration.\n\n        :param app: An instance of :class:`~flask.Flask`.",
        "Covert name from CamelCase to \"Normal case\".\n\n    >>> camel2word('CamelCase')\n    'Camel case'\n    >>> camel2word('CaseWithSpec')\n    'Case with spec'",
        "Format a time in seconds.",
        "Indent representation of a dict",
        "Test for existence of ``needle`` regex within ``haystack``.\n\n    Say ``escape`` to escape the ``needle`` if you aren't really using the\n    regex feature & have special characters in it.",
        "Mutates any attributes on ``obj`` which are classes, with link to ``obj``.\n\n    Adds a convenience accessor which instantiates ``obj`` and then calls its\n    ``setup`` method.\n\n    Recurses on those objects as well.",
        "Procesa TCU, CP, FEU diario.\n\n    :param df:\n    :param verbose:\n    :param convert_kwh:\n    :return:",
        "Compress the log message in order to send less bytes to the wire.",
        "Internal bookkeeping to handle nested classes",
        "Needs to be its own method so it can be called from both wantClass and\n        registerGoodClass.",
        "Obtiene los dataframes de los datos de PVPC con resampling diario y mensual.",
        "Performs sanitation of the path after validating\n\n    :param path: path to sanitize\n    :return: path\n    :raises:\n        - InvalidPath if the path doesn't start with a slash",
        "Ensures the passed schema instance is compatible\n\n    :param obj: object to validate\n    :return: obj\n    :raises:\n        - IncompatibleSchema if the passed schema is of an incompatible type",
        "Journey route decorator\n\n    Enables simple serialization, deserialization and validation of Flask routes with the help of Marshmallow.\n\n    :param bp: :class:`flask.Blueprint` object\n    :param args: args to pass along to `Blueprint.route`\n    :param kwargs:\n        - :strict_slashes: Enable / disable strict slashes (default False)\n        - :validate: Enable / disable body/query validation (default True)\n        - :_query: Unmarshal Query string into this schema\n        - :_body: Unmarshal JSON body into this schema\n        - :marshal_with: Serialize the output with this schema\n    :raises:\n        - ValidationError if the query parameters or JSON body fails validation",
        "Attaches a flask.Blueprint to the bundle\n\n        :param bp: :class:`flask.Blueprint` object\n        :param description: Optional description string\n        :raises:\n            - InvalidBlueprint if the Blueprint is not of type `flask.Blueprint`",
        "Returns the DottedRule that results from moving the dot.",
        "Computes the intermediate FIRST set using symbols.",
        "Computes the FIRST set for every symbol in the grammar.\n\n        Tenatively based on _compute_first in PLY.",
        "Computes the FOLLOW set for every non-terminal in the grammar.\n\n        Tenatively based on _compute_follow in PLY.",
        "Computes the initial closure using the START_foo production.",
        "Computes the next closure for rules based on the symbol we got.\n\n        Args:\n            rules - an iterable of DottedRules\n            symbol - a string denoting the symbol we've just seen\n\n        Returns: frozenset of DottedRules",
        "Fills out the entire closure based on some initial dotted rules.\n\n        Args:\n            rules - an iterable of DottedRules\n\n        Returns: frozenset of DottedRules",
        "Initializes Journey extension\n\n        :param app: App passed from constructor or directly to init_app\n        :raises:\n            - NoBundlesAttached if no bundles has been attached attached",
        "Returns simple info about registered blueprints\n\n        :return: Tuple containing endpoint, path and allowed methods for each route",
        "Checks if a bundle exists at the provided path\n\n        :param path: Bundle path\n        :return: bool",
        "Attaches a bundle object\n\n        :param bundle: :class:`flask_journey.BlueprintBundle` object\n        :raises:\n            - IncompatibleBundle if the bundle is not of type `BlueprintBundle`\n            - ConflictingPath if a bundle already exists at bundle.path\n            - MissingBlueprints if the bundle doesn't contain any blueprints",
        "Register and return info about the registered blueprint\n\n        :param bp: :class:`flask.Blueprint` object\n        :param bundle_path: the URL prefix of the bundle\n        :param child_path: blueprint relative to the bundle path\n        :return: Dict with info about the blueprint",
        "Returns detailed information about registered blueprint routes matching the `BlueprintBundle` path\n\n        :param app: App instance to obtain rules from\n        :param base_path: Base path to return detailed route info for\n        :return: List of route detail dicts",
        "Computes the precedence of terminal and production.\n\n        The precedence of a terminal is it's level in the PRECEDENCE tuple. For\n        a production, the precedence is the right-most terminal (if it exists).\n        The default precedence is DEFAULT_PREC - (LEFT, 0).\n\n        Returns:\n            precedence - dict[terminal | production] = (assoc, level)",
        "Generates the ACTION and GOTO tables for the grammar.\n\n        Returns:\n            action - dict[state][lookahead] = (action, ...)\n            goto - dict[state][just_reduced] = new_state",
        "Return the antecedents and the consequent of a definite clause.",
        "Auxiliary routine to implement tt_entails.",
        "Return a list of all propositional symbols in x.",
        "Return True if the propositional logic expression is true in the model,\n    and False if it is false. If the model does not specify the value for\n    every proposition, this may return None to indicate 'not obvious';\n    this may happen even when the expression is tautological.",
        "See if the clauses are true in a partial model.",
        "A variable is an Expr with no args and a lowercase symbol as the op.",
        "Remove the sentence's clauses from the KB.",
        "Updates the cache with setting values from the database.",
        "Search game to determine best action; use alpha-beta pruning.\n    This version cuts off search and uses an evaluation function.",
        "Return the value to player; 1 for win, -1 for loss, 0 otherwise.",
        "If X wins with this move, return 1; if O return -1; else return 0.",
        "Return true if there is a line through move on board for player.",
        "Update a dict, or an object with slots, according to `entries` dict.\n\n    >>> update({'a': 1}, a=10, b=20)\n    {'a': 10, 'b': 20}\n    >>> update(Struct(a=1), a=10, b=20)\n    Struct(a=10, b=20)",
        "Pick n samples from seq at random, with replacement, with the\n    probability of each element in proportion to its corresponding\n    weight.",
        "Return a random-sample function that picks from seq weighted by weights.",
        "Format args with the first argument as format string, and write.\n    Return the last arg, or format itself if there are no args.",
        "Try to find some reasonable name for the object.",
        "Open a file based at the AIMA root directory.",
        "Just count how many times each value of each input attribute\n    occurs, conditional on the target value. Count the different\n    target values too.",
        "Number of bits to represent the probability distribution in values.",
        "Layered feed-forward network.",
        "Given a list of learning algorithms, have them vote.",
        "Return a predictor that takes a weighted vote.",
        "Copy dataset, replicating each example in proportion to its weight.",
        "Leave one out cross-validation over the dataset.",
        "Generate a DataSet with n examples.",
        "2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints.",
        "Compare various learners on various datasets using cross-validation.\n    Print results as a table.",
        "Check that my fields make sense.",
        "Add an example to the list of examples, checking it first.",
        "Raise ValueError if example has any invalid values.",
        "Returns the number used for attr, which can be a name, or -n .. n-1.",
        "Return a copy of example, with non-input attributes replaced by None.",
        "Add an observation o to the distribution.",
        "Include o among the possible observations, whether or not\n        it's been observed yet.",
        "Return a random sample from the distribution.",
        "Return true if we remove a value.",
        "Minimum-remaining-values heuristic.",
        "Least-constraining-values heuristic.",
        "Prune neighbor values inconsistent with var=value.",
        "Maintain arc consistency.",
        "Solve a CSP by stochastic hillclimbing on the number of conflicts.",
        "Return the value that will give var the least number of conflicts.\n    If there is a tie, choose at random.",
        "Return the number of conflicts var=val has with other variables.",
        "Start accumulating inferences from assuming var=value.",
        "Rule out var=value.",
        "Return the partial assignment implied by the current inferences.",
        "Undo a supposition and all inferences from it.",
        "Return a list of variables in current assignment that are in conflict",
        "The number of conflicts, as recorded with each assignment.\n        Count conflicts in row and in up, down diagonals. If there\n        is a queen there, it can't conflict with itself, so subtract 3.",
        "Assign var, and keep track of conflicts.",
        "Record conflicts caused by addition or deletion of a Queen.",
        "Find the best segmentation of the string of characters, given the\n    UnigramTextModel P.",
        "Encodes text, using a code which is a permutation of the alphabet.",
        "Build up a random sample of text nwords words long, using\n        the conditional probability given the n-1 preceding words.",
        "Index a whole collection of files.",
        "Index the text of a document.",
        "Compute a score for this word on this docid.",
        "Present the results as a list.",
        "Get results for the query and present them.",
        "Return a score for text based on how common letters pairs are.",
        "Search for a decoding of the ciphertext.",
        "Score is product of word scores, unigram scores, and bigram scores.\n        This can get very small, so we use logs and exp.",
        "Returns a ``SettingDict`` object.",
        "The expected utility of doing a in state s, according to the MDP and U.",
        "Return the state that results from going in this direction.",
        "Returns a ``SettingDict`` object for this queryset.",
        "Creates and returns an object of the appropriate type for ``value``.",
        "Returns ``True`` if this model should be used to store ``value``.\n\n        Checks if ``value`` is an instance of ``value_type``. Override this\n        method if you need more advanced behaviour. For example, to distinguish\n        between single and multi-line text.",
        "One possible schedule function for simulated annealing",
        "Call genetic_algorithm on the appropriate parts of a problem.\n    This requires the problem to have states that can mate and mutate,\n    plus a value method that scores states.",
        "Return a random Boggle board of size n x n.\n    We represent a board as a linear list of letters.",
        "Print the board in a 2-d array.",
        "Return a list of lists, where the i-th element is the list of indexes\n    for the neighbors of square i.",
        "If n2 is a perfect square, return its square root, else raise error.",
        "List the nodes reachable in one step from this node.",
        "Fig. 3.10",
        "Return a list of nodes forming the path from the root to this node.",
        "Return a new individual crossing self and other.",
        "Make a digraph into an undirected graph by adding symmetric edges.",
        "Add a link from A and B of given distance, and also add the inverse\n        link if the graph is undirected.",
        "Add a link from A to B of given distance, in one direction only.",
        "h function is straight-line distance from a node's state to goal.",
        "In the leftmost empty column, try all non-conflicting rows.",
        "Place the next queen at the given row.",
        "Set the board, and find all the words in it.",
        "The total score for the words found, according to the rules.",
        "Wrap the agent's program to print its input and output. This will let\n    you see what the agent is doing in the environment.",
        "An agent that keeps track of what locations are clean or dirty.",
        "Run the environment for one time step. If the\n        actions and exogenous changes are independent, this method will\n        do.  If there are interactions between them, you'll need to\n        override this method.",
        "Run the Environment for given number of time steps.",
        "Return all things exactly at a given location.",
        "Add a thing to the environment, setting its location. For\n        convenience, if thing is an agent program we make a new agent\n        for it. (Shouldn't need to override this.",
        "Remove a thing from the environment.",
        "Return all things within radius of location.",
        "By default, agent perceives things within a default radius.",
        "Move a thing to a new location.",
        "Put walls around the entire perimeter of the grid.",
        "Parse a list of words; according to the grammar.\n        Leave results in the chart.",
        "Add edge to chart, and see if it extends or predicts another edge.",
        "For each edge expecting a word of this category here, extend the edge.",
        "Add to chart any rules for B that could help extend this edge.",
        "See what edges can be extended by this edge.",
        "Adds a ``SettingDict`` object for the ``Setting`` model to the context as\n    ``SETTINGS``. Automatically creates non-existent settings with an empty\n    string as the default value.",
        "Return the factor for var in bn's joint distribution given e.\n    That is, bn's full joint distribution, projected to accord with e,\n    is the pointwise product of these factors for bn's variables.",
        "Eliminate var from all factors by summing over its values.",
        "Yield every way of extending e with values for all vars.",
        "Is event consistent with the given evidence?",
        "Sample an event from bn that's consistent with the evidence e;\n    return the event and its weight, the likelihood that the event\n    accords to the evidence.",
        "Show the probabilities rounded and sorted by key, for the\n        sake of portable doctests.",
        "Add a node to the net. Its parents must already be in the\n        net, and its variable must not.",
        "Multiply two factors, combining their variables.",
        "Make a factor eliminating var by summing over its values.",
        "Return my probabilities; must be down to one variable.",
        "Strips all whitespace from a minidom XML node and its children\n\n    This operation is made in-place.",
        "Takes a hls color and converts to proper hue \n        Bulbs use a BGR order instead of RGB",
        "Takes your standard rgb color \n        and converts it to a proper hue value",
        "Takes an HTML hex code\n        and converts it to a proper hue value",
        "Wait for x seconds\n            each wait command is 100ms",
        "Return json from querying Web Api\n\n\t\tArgs:\n\t\t\tview: django view function.\n\t\t\trequest: http request object got from django.\n\t\t\t\t\n\t\tReturns: json format dictionary",
        "put text on on screen\n    a tuple as first argument tells absolute position for the text\n    does not change TermCursor position\n    args = list of optional position, formatting tokens and strings",
        "get user input without echo",
        "get character. waiting for key",
        "tweaked from source of base",
        "getProcessOwner - Get the process owner of a pid\n\n        @param pid <int> - process id\n\n        @return - None if process not found or can't be determined. Otherwise, a dict: \n            {\n                uid  - Owner UID\n                name - Owner name, or None if one cannot be determined\n            }",
        "scanProcessForCwd - Searches a given pid's cwd for a given pattern\n\n            @param pid <int> - A running process ID on this system\n            @param searchPortion <str> - Any portion of directory to search\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n\n            @return <dict> - If result is found, the following dict is returned. If no match found on the given pid, or pid is not found running, None is returned.\n                {\n                    'searchPortion' : The passed search pattern\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or uid if no mapping can be found, or \"unknown\" if neither could be determined.\n                    'cmdline'       : Commandline string\n                    'cwd'           : The exact cwd",
        "scanAllProcessesForCwd - Scans all processes on the system for a given search pattern.\n\n            @param searchPortion <str> - Any portion of directory to search\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n\n        @return - <dict> - A dictionary of pid -> cwdResults for each pid that matched the search pattern. For format of \"cwdResults\", @see scanProcessForCwd",
        "scanProcessForMapping - Searches a given pid's mappings for a certain pattern.\n\n            @param pid <int> - A running process ID on this system\n            @param searchPortion <str> - A mapping for which to search, example: libc or python or libz.so.1. Give empty string to return all mappings.\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n            @return <dict> - If result is found, the following dict is returned. If no match found on the given pid, or pid is not found running, None is returned.\n                {\n                    'searchPortion' : The passed search pattern\n                    'pid'           : The passed pid (as an integer)\n                    'owner'",
        "scanAllProcessesForMapping - Scans all processes on the system for a given search pattern.\n\n            @param searchPortion <str> - A mapping for which to search, example: libc or python or libz.so.1. Give empty string to return all mappings.\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return - <dict> - A dictionary of pid -> mappingResults for each pid that matched the search pattern. For format of \"mappingResults\", @see scanProcessForMapping",
        "scanProcessForOpenFile - Scans open FDs for a given pid to see if any are the provided searchPortion\n\n            @param searchPortion <str> - Filename to check\n            @param isExactMatch <bool> Default True - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return -  If result is found, the following dict is returned. If no match found on the given pid, or the pid is not found running, None is returned.\n                {\n                    'searchPortion' : The search portion provided\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or \"unknown\" if one could not be determined\n                    'cmdline'       : Commandline string\n                    'fds'          ",
        "scanAllProcessessForOpenFile - Scans all processes on the system for a given filename\n\n            @param searchPortion <str> - Filename to check\n            @param isExactMatch <bool> Default True - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return - <dict> - A dictionary of pid -> mappingResults for each pid that matched the search pattern. For format of \"mappingResults\", @see scanProcessForOpenFile",
        "Create and connect to socket for TCP communication with hub.",
        "Send TCP command to hub and return response.",
        "Receive TCP response, looping to get whole thing or timeout.",
        "Get current light data as dictionary with light zids as keys.",
        "Get current light data, set and return as list of Bulb objects.",
        "Set brightness of bulb.",
        "Set color and brightness of bulb.",
        "Update light objects to their current values.",
        "This function takes a file path beginning with edgar and stores the form in a directory.\n        The default directory is sec_filings but can be changed through a keyword argument.",
        "read file as is",
        "Clean up after ourselves, removing created files.\n    @param {[String]} A list of file paths specifying the files we've created\n        during run. Will all be deleted.\n    @return {None}",
        "Create an index file in the given location, supplying known lists of\n    present image files and subdirectories.\n    @param {String} root_dir - The root directory of the entire crawl. Used to\n        ascertain whether the given location is the top level.\n    @param {String} location - The current directory of the crawl. The index\n        file will be created here.\n    @param {[String]} image_files - A list of image file names in the location.\n        These will be displayed in the index file's gallery.\n    @param {[String]} dirs - The subdirectories of the location directory.\n        These will be displayed as links further down the file structure.\n    @param {Boolean=False} force_no_processing - If True, do not attempt to\n        actually process thumbnails, PIL images or anything. Simply index\n        <img> tags with original file src attributes.\n   ",
        "Crawl the root directory downwards, generating an index HTML file in each\n    directory on the way down.\n    @param {String} root_dir - The top level directory to crawl down from. In\n        normal usage, this will be '.'.\n    @param {Boolean=False} force_no_processing - If True, do not attempt to\n        actually process thumbnails, PIL images or anything. Simply index\n        <img> tags with original file src attributes.\n    @return {[String]} Full file paths of all created files.",
        "Get an instance of PIL.Image from the given file.\n    @param {String} dir_path - The directory containing the image file\n    @param {String} image_file - The filename of the image file within dir_path\n    @return {PIL.Image} An instance of the image file as a PIL Image, or None\n        if the functionality is not available. This could be because PIL is not\n        present, or because it can't process the given file type.",
        "Get base-64 encoded data as a string for the given image. Fallback to return\n    fallback_image_file if cannot get the image data or img is None.\n    @param {Image} img - The PIL Image to get src data for\n    @param {String} fallback_image_file - The filename of the image file,\n        to be used when image data capture fails\n    @return {String} The base-64 encoded image data string, or path to the file\n        itself if not supported.",
        "Get a PIL.Image from the given image file which has been scaled down to\n    THUMBNAIL_WIDTH wide.\n    @param {String} dir_path - The directory containing the image file\n    @param {String} image_file - The filename of the image file within dir_path\n    @return {PIL.Image} An instance of the thumbnail as a PIL Image, or None\n        if the functionality is not available. See _get_image_from_file for\n        details.",
        "Run the image server. This is blocking. Will handle user KeyboardInterrupt\n    and other exceptions appropriately and return control once the server is\n    stopped.\n    @return {None}",
        "Generate indexes and run server from the given directory downwards.\n    @param {String} dir_path - The directory path (absolute, or relative to CWD)\n    @return {None}",
        "USE carefully ^^",
        "random blending masks",
        "z value as like a seed",
        "Converts a permutation into a permutation matrix.\n\n    `matches` is a dictionary whose keys are vertices and whose values are\n    partners. For each vertex ``u`` and ``v``, entry (``u``, ``v``) in the\n    returned matrix will be a ``1`` if and only if ``matches[u] == v``.\n\n    Pre-condition: `matches` must be a permutation on an initial subset of the\n    natural numbers.\n\n    Returns a permutation matrix as a square NumPy array.",
        "Convenience function that creates a block matrix with the specified\n    blocks.\n\n    Each argument must be a NumPy matrix. The two top matrices must have the\n    same number of rows, as must the two bottom matrices. The two left matrices\n    must have the same number of columns, as must the two right matrices.",
        "Returns the adjacency matrix of a bipartite graph whose biadjacency\n    matrix is `A`.\n\n    `A` must be a NumPy array.\n\n    If `A` has **m** rows and **n** columns, then the returned matrix has **m +\n    n** rows and columns.",
        "Returns the Boolean matrix in the same shape as `D` with ones exactly\n    where there are nonzero entries in `D`.\n\n    `D` must be a NumPy array.",
        "Returns the result of incrementing `version`.\n\n    If `which` is not specified, the \"patch\" part of the version number will be\n    incremented.  If `which` is specified, it must be ``'major'``, ``'minor'``,\n    or ``'patch'``. If it is one of these three strings, the corresponding part\n    of the version number will be incremented instead of the patch number.\n\n    Returns a string representing the next version number.\n\n    Example::\n\n        >>> bump_version('2.7.1')\n        '2.7.2'\n        >>> bump_version('2.7.1', 'minor')\n        '2.8.0'\n        >>> bump_version('2.7.1', 'major')\n        '3.0.0'",
        "Gets the current version from the specified file.\n\n    This function assumes the file includes a string of the form::\n\n        <pattern> = <version>",
        "Prints the specified message and exits the program with the specified\n    exit status.",
        "Tags the current version.",
        "initialize with templates' path\n        parameters\n          templates_path    str    the position of templates directory\n          global_data       dict   globa data can be got in any templates",
        "Render data with template, return html unicodes.\n        parameters\n          template   str  the template's filename\n          data       dict the data to render",
        "Render data with template and then write to path",
        "shortcut to render data with `template`. Just add exception\n    catch to `renderer.render`",
        "Get the DataFrame for this view.\n        Defaults to using `self.dataframe`.\n\n        This method should always be used rather than accessing `self.dataframe`\n        directly, as `self.dataframe` gets evaluated only once, and those results\n        are cached for all subsequent requests.\n\n        You may want to override this if you need to provide different\n        dataframes depending on the incoming request.",
        "Indexes the row based on the request parameters.",
        "Returns the row the view is displaying.\n\n        You may want to override this if you need to provide non-standard\n        queryset lookups.  Eg if objects are referenced using multiple\n        keyword arguments in the url conf.",
        "The paginator instance associated with the view, or `None`.",
        "Return a single page of results, or `None` if pagination is disabled.",
        "parse config, return a dict",
        "shortcut to render data with `template` and then write to `path`.\n    Just add exception catch to `renderer.render_to`",
        "Parse ascii post source, return dict",
        "parse post source files name to datetime object",
        "run a server binding to port",
        "get source files' update time",
        "watch files for changes, if changed, rebuild blog. this thread\n        will quit if the main process ends",
        "Deploy new blog to current directory",
        "Temporarily update the context to use the BlockContext for the given alias.",
        "Find the first matching block in the current block_context",
        "Load a series of widget libraries.",
        "Return a list of widget names for the provided field.",
        "Allow reuse of a block within a template.\n\n    {% reuse '_myblock' foo=bar %}\n\n    If passed a list of block names, will use the first that matches:\n\n    {% reuse list_of_block_names .... %}",
        "When dealing with optgroups, ensure that the value is properly force_text'd.",
        "Message instances are namedtuples of type `Message`.\n        The date field is already serialized in datetime.isoformat ECMA-262 format",
        "Send a message to a list of users without passing through `django.contrib.messages`\n\n    :param users: an iterable containing the recipients of the messages\n    :param level: message level\n    :param message_text: the string containing the message\n    :param extra_tags: like the Django api, a string containing extra tags for the message\n    :param date: a date, different than the default timezone.now\n    :param url: an optional url\n    :param fail_silently: not used at the moment",
        "Send a message to all users aka broadcast.\n\n    :param level: message level\n    :param message_text: the string containing the message\n    :param extra_tags: like the Django api, a string containing extra tags for the message\n    :param date: a date, different than the default timezone.now\n    :param url: an optional url\n    :param fail_silently: not used at the moment",
        "Mark message instance as read for user.\n    Returns True if the message was `unread` and thus actually marked as `read` or False in case\n    it is already `read` or it does not exist at all.\n\n    :param user: user instance for the recipient\n    :param message: a Message instance to mark as read",
        "Mark all message instances for a user as read.\n\n    :param user: user instance for the recipient",
        "Renders a list of archived messages for the current user",
        "Retrieve unread messages for current user, both from the inbox and\n        from other storages",
        "If the message level was configured for being stored and request.user\n        is not anonymous, save it to the database. Otherwise, let some other\n        class handle the message.\n\n        Notice: controls like checking the message is not empty and the level\n        is above the filter need to be performed here, but it could happen\n        they'll be performed again later if the message does not need to be\n        stored.",
        "persistent messages are already in the database inside the 'archive',\n        so we can say they're already \"stored\".\n        Here we put them in the inbox, or remove from the inbox in case the\n        messages were iterated.\n\n        messages contains only new msgs if self.used==True\n        else contains both new and unread messages",
        "Like the base class method, prepares a list of messages for storage\n        but avoid to do this for `models.Message` instances.",
        "Main entry point for script.",
        "initializes a base logger\n\n    you can use this to init a logger in any of your files.\n    this will use config.py's LOGGER param and logging.dictConfig to configure\n    the logger for you.\n\n    :param int|logging.LEVEL base_level: desired base logging level\n    :param int|logging.LEVEL verbose_level: desired verbose logging level\n    :param dict logging_dict: dictConfig based configuration.\n     used to override the default configuration from config.py\n    :rtype: `python logger`",
        "Configure an object with a user-supplied factory.",
        "sets the global verbosity level for console and the jocker_lgr logger.\n\n    :param bool is_verbose_output: should be output be verbose",
        "returns a configuration object\n\n    :param string config_file: path to config file",
        "generates a Dockerfile, builds an image and pushes it to DockerHub\n\n    A `Dockerfile` will be generated by Jinja2 according to the `varsfile`\n    imported. If build is true, an image will be generated from the\n    `outputfile` which is the generated Dockerfile and committed to the\n    image:tag string supplied to `build`.\n    If push is true, a build will be triggered and the produced image\n    will be pushed to DockerHub upon completion.\n\n    :param string varsfile: path to file with variables.\n    :param string templatefile: path to template file to use.\n    :param string outputfile: path to output Dockerfile.\n    :param string configfile: path to yaml file with docker-py config.\n    :param bool dryrun: mock run.\n    :param build: False or the image:tag to build to.\n   ",
        "since the push process outputs a single unicode string consisting of\n        multiple JSON formatted \"status\" lines, we need to parse it so that it\n        can be read as multiple strings.\n\n        This will receive the string as an input, count curly braces and ignore\n        any newlines. When the curly braces stack is 0, it will append the\n        entire string it has read up until then to a list and so forth.\n\n        :param string: the string to parse\n        :rtype: list of JSON's",
        "Uploads an image file to Imgur",
        "Return true if the IP address is in dotted decimal notation.",
        "Return true if the IP address is in binary notation.",
        "Return true if the IP address is in octal notation.",
        "Return true if the IP address is in decimal notation.",
        "Function internally used to check if the given netmask\n    is of the specified notation.",
        "Return true if the netmask is in bits notatation.",
        "Return true if the netmask is in wildcard bits notatation.",
        "Dotted decimal notation to decimal conversion.",
        "Decimal to dotted decimal notation conversion.",
        "Hexadecimal to decimal conversion.",
        "Octal to decimal conversion.",
        "Binary to decimal conversion.",
        "Generate a table to convert a whole byte to binary.\n    This code was taken from the Python Cookbook, 2nd edition - O'Reilly.",
        "Decimal to binary conversion.",
        "Bits to decimal conversion.",
        "Wildcard bits to decimal conversion.",
        "Function internally used to detect the notation of the\n    given IP or netmask.",
        "Internally used to convert IPs and netmasks to other notations.",
        "Convert among IP address notations.\n\n    Given an IP address, this function returns the address\n    in another notation.\n\n    @param ip: the IP address.\n    @type ip: integers, strings or object with an appropriate __str()__ method.\n\n    @param notation: the notation of the output (default: IP_DOT).\n    @type notation: one of the IP_* constants, or the equivalent strings.\n\n    @param inotation: force the input to be considered in the given notation\n                    (default the notation of the input is autodetected).\n    @type inotation: one of the IP_* constants, or the equivalent strings.\n\n    @param check: force the notation check on the input.\n    @type check: True force the check, False force not to check and None\n                do the check only if the inotation is unknown.\n\n",
        "Convert a netmask to another notation.",
        "Sum two IP addresses.",
        "Subtract two IP addresses.",
        "Return the bits notation of the netmask.",
        "Return the wildcard bits notation of the netmask.",
        "Set the IP address and the netmask.",
        "Change the current IP.",
        "Change the current netmask.",
        "Return true if the given address in amongst the usable addresses,\n        or if the given CIDR is contained in this one.",
        "Upload a file to S3 possibly using the multi-part uploader\n        Return the key uploaded",
        "Copy a file from one bucket into another",
        "Recursively upload a ``folder`` into a backet.\n\n        :param bucket: bucket where to upload the folder to\n        :param folder: the folder location in the local file system\n        :param key: Optional key where the folder is uploaded\n        :param skip: Optional list of files to skip\n        :param content_types: Optional dictionary mapping suffixes to\n            content types\n        :return: a coroutine",
        "Coroutine for uploading a single file",
        "Trigger an ``event`` on this channel",
        "Connect to a Pusher websocket",
        "Handle websocket incoming messages",
        "Constant time string comparison",
        "Decodes a limited set of HTML entities.",
        "Set signature passphrases",
        "Set encryption passphrases",
        "Set algorithms used for sealing. Defaults can not be overridden.",
        "Get algorithms used for sealing",
        "Private function for setting options used for sealing",
        "Verify sealed data signature",
        "Encode data with specific algorithm",
        "Decode data with specific algorithm",
        "Add signature to data",
        "Verify and remove signature",
        "Verify and remove magic",
        "Add header to data",
        "Read header from data",
        "Remove header from data",
        "Read header version from data",
        "Get algorithm info",
        "Generate and return PBKDF2 key",
        "Update algorithm definition type dictionaries",
        "This function populates the internal tableOfContents list with the contents\n        of the zip file TOC. If the server does not support ranged requests, this will raise\n        and exception. It will also throw an exception if the TOC cannot be found.",
        "This function will extract a single file from the remote zip without downloading\n        the entire zip file. The filename argument should match whatever is in the 'filename'\n        key of the tableOfContents.",
        "Does photometry and estimates uncertainties by calculating the scatter around a linear fit to the data\n        in each orientation. This function is called by other functions and generally the user will not need\n        to interact with it directly.",
        "Creates the figure shown in ``adjust_aperture`` for visualization purposes. Called by other functions\n        and generally not called by the user directly.\n\n        Args: \n            img: The data frame to be passed through to be plotted. A cutout of the ``integrated_postcard``",
        "Identify the centroid positions for the target star at all epochs. Useful for verifying that there is\n        no correlation between flux and position, as might be expected for high proper motion stars.",
        "Identify the \"expected\" flux value at the time of each observation based on the \n        Kepler long-cadence data, to ensure variations observed are not the effects of a single\n        large starspot. Only works if the target star was targeted for long or short cadence\n        observations during the primary mission.",
        "Estimate the photometric uncertainties on each data point following Equation A.2 of The Paper.\n        Based on the kepcal package of Dan Foreman-Mackey.",
        "Dump single field.",
        "Disassemble serialized protocol buffers file.",
        "Find all missing imports in list of Pbd instances.",
        "Write fasta_dict to fasta_file\n\n    :param fasta_dict: returned by fasta_file_to_dict\n    :param fasta_file: output file can be a string path or a file object\n    :param line_char_limit: None = no limit (default)\n    :return: None",
        "Helper function to record and log an error message\n\n        :param line_data: dict\n        :param error_info: dict\n        :param logger:\n        :param log_level: int\n        :return:",
        "checks whether child features are within the coordinate boundaries of parent features\n\n        :return:",
        "1. get a list of CDS with the same parent\n        2. sort according to strand\n        3. calculate and validate phase",
        "Transfer children from old_parent to new_parent\n\n        :param old_parent: feature_id(str) or line_index(int) or line_data(dict) or feature\n        :param new_parent: feature_id(str) or line_index(int) or line_data(dict)\n        :return: List of children transferred",
        "Marks line_data and all of its associated feature's 'line_status' as 'removed', does not actually remove the line_data from the data structure.\n        The write function checks the 'line_status' when writing the gff file.\n        Find the root parent of line_data of type root_type, remove all of its descendants.\n        If the root parent has a parent with no children after the remove, remove the root parent's parent recursively.\n\n        :param line_data:\n        :param root_type:\n        :return:",
        "given a filename, return the ABFs ID string.",
        "Determine the protocol used to record an ABF file",
        "given the bytestring ABF header, make and launch HTML.",
        "iterate over every sweep",
        "read the header and populate self with information about comments",
        "given a sweep, return the protocol as condensed sequence.\n        This is better for comparing similarities and determining steps.\n        There should be no duplicate numbers.",
        "return the average of part of the current sweep.",
        "Return a sweep which is the average of multiple sweeps.\n        For now, standard deviation is lost.",
        "create kernel based on this ABF info.",
        "Get the filtered sweepY of the current sweep.\n        Only works if self.kernel has been generated.",
        "Given a list of list of dicts, return just the dicts.",
        "given a key, return a list of values from the matrix with that key.",
        "given a recarray, return it as a list of dicts.",
        "given text, make it a temporary HTML file and launch it.",
        "show everything we can about an object's projects and methods.",
        "Put 2d numpy data into a temporary HTML file.",
        "given a string or a path to an XML file, return an XML object.",
        "mono-exponential curve.",
        "return a list of Is where the data first crosses above threshold.",
        "Try to format anything as a 2D matrix with column names.",
        "save something to a pickle file",
        "convert a dictionary to a pretty formatted string.",
        "determine the comment cooked in the protocol.",
        "scan an ABF directory and subdirectory. Try to do this just once.\n    Returns ABF files, SWHLab files, and groups.",
        "given an ABF file name, return the ABF of its parent.",
        "given an ABF and the groups dict, return the ID of its parent.",
        "given an ABF, find the parent, return that line of experiments.txt",
        "given a path or list of files, return ABF IDs.",
        "May be given an ABF object or filename.",
        "return an \"FTP\" object after logging in.",
        "upload everything from localFolder into the current FTP folder.",
        "Only scott should do this. Upload new version to site.",
        "use the GUI to ask for a string.",
        "use the GUI to pop up a message.",
        "use the GUI to ask YES or NO.",
        "check out the arguments and figure out what to do.",
        "provide all stats on the first AP.",
        "return average of a feature divided by sweep.",
        "continuously monitor a folder for new abfs and try to analyze them.\n    This is intended to watch only one folder, but can run multiple copies.",
        "easy way to plot a gain function.",
        "draw vertical lines at comment points. Defaults to seconds.",
        "stamp the bottom with file info.",
        "makes a new matplotlib figure with default dims and DPI.\n    Also labels it with pA or mV depending on ABF.",
        "Save the pylab figure somewhere.\n    If fname==False, show it instead.\n    Height force > dpi force\n    if a tag is given instead of a filename, save it alongside the ABF",
        "if the module is in this path, load it from the local folder.",
        "Called to update the state of the iterator.  This methods\n        receives the set of task ids from the previous set of tasks\n        together with the launch information to allow the output\n        values to be parsed using the output_extractor. This data is then\n        used to determine the next desired point in the parameter\n        space by calling the _update_state method.",
        "When dynamic, not all argument values may be available.",
        "Summarizes the trace of values used to update the DynamicArgs\n        and the arguments subsequently returned. May be used to\n        implement the summary method.",
        "Takes as input a list or tuple of two elements. First the\n        value returned by incrementing by 'stepsize' followed by the\n        value returned after a 'stepsize' decrement.",
        "given a filename or ABF object, try to analyze it.",
        "frame the current matplotlib plot with ABF info, and optionally save it.\n    Note that this is entirely independent of the ABFplot class object.\n    if saveImage is False, show it instead.\n\n    Datatype should be:\n        * plot\n        * experiment",
        "make sure a figure is ready.",
        "save the existing figure. does not close it.",
        "plot every sweep of an ABF file.",
        "plot the current sweep protocol.",
        "plot the protocol of all sweeps.",
        "Given ABFs and TIFs formatted long style, rename each of them to prefix their number with a different number.\n\n    Example: 2017_10_11_0011.abf\n    Becomes: 2017_10_11_?011.abf\n    where ? can be any character.",
        "given a list of files, return a dict organized by extension.",
        "given files and cells, return a dict of files grouped by cell.",
        "populate class properties relating to files in the folder.",
        "generate list of cells with links. keep this simple.\n        automatically generates splash page and regnerates frames.",
        "generate a data view for every ABF in the project folder.",
        "hyperpolarization step. Use to calculate tau and stuff.",
        "IC steps. Use to determine gain function.",
        "IC steps. See how hyperpol. step affects things.",
        "repeated membrane tests.",
        "fast sweeps, 1 step per sweep, for clean IV without fast currents.",
        "repeated membrane tests, likely with drug added. Maybe IPSCs.",
        "combination of membrane test and IV steps.",
        "OBSOLETE WAY TO INDEX A FOLDER.",
        "A custom save method that handles figuring out when something is activated or deactivated.",
        "It is impossible to delete an activatable model unless force is True. This function instead sets it to inactive.",
        "Write to file_handle if supplied, othewise print output",
        "A helper method that supplies the root directory name given a\n        timestamp.",
        "The log contains the tids and corresponding specifications\n        used during launch with the specifications in JSON format.",
        "All launchers should call this method to write the info file\n        at the end of the launch. The .info file is saved given\n        setup_info supplied by _setup_launch into the\n        root_directory. When called without setup_info, the existing\n        info file is updated with the end-time.",
        "Launches processes defined by process_commands, but only\n        executes max_concurrency processes at a time; if a process\n        completes and there are still outstanding processes to be\n        executed, the next processes are run until max_concurrency is\n        reached again.",
        "A succinct summary of the Launcher configuration.  Unlike the\n        repr, a summary does not have to be complete but must supply\n        key information relevant to the user.",
        "The method that actually runs qsub to invoke the python\n        process with the necessary commands to trigger the next\n        collation step and next block of jobs.",
        "This method handles static argument specifiers and cases where\n        the dynamic specifiers cannot be queued before the arguments\n        are known.",
        "Aggregates all process_commands and the designated output files into a\n        list, and outputs it as JSON, after which the wrapper script is called.",
        "Performs consistency checks across all the launchers.",
        "Launches all available launchers.",
        "Runs the review process for all the launchers.",
        "Helper to prompt the user for input on the commandline.",
        "The implementation in the base class simply checks there is no\n        clash between the metadata and data keys.",
        "Returns the full path for saving the file, adding an extension\n        and making the filename unique as necessary.",
        "Returns a boolean indicating whether the filename has an\n        appropriate extension for this class.",
        "Data may be either a PIL Image object or a Numpy array.",
        "return \"YYYY-MM-DD\" when the file was modified.",
        "returns a dict of active folders with days as keys.",
        "given some data and a list of X posistions, return the normal\n    distribution curve as a Y point at each of those Xs.",
        "show basic info about ABF class variables.",
        "read the ABF header and save it HTML formatted.",
        "use 1 colormap for the whole abf. You can change it!.",
        "return self.dataY around a time point. All units are seconds.\n        if thisSweep==False, the time point is considered to be experiment time\n            and an appropriate sweep may be selected. i.e., with 10 second\n            sweeps and timePint=35, will select the 5s mark of the third sweep",
        "RETURNS filtered trace. Desn't filter it in place.",
        "Raises a ValidationError for any ActivatableModel that has ForeignKeys or OneToOneFields that will\n    cause cascading deletions to occur. This function also raises a ValidationError if the activatable\n    model has not defined a Boolean field with the field name defined by the ACTIVATABLE_FIELD_NAME variable\n    on the model.",
        "Helper function to convet an Args object to a HoloViews Table",
        "Method to define the positional arguments and keyword order\n        for pretty printing.",
        "Formats the elements of an argument set appropriately",
        "Returns a dictionary like object with the lists of values\n        collapsed by their respective key. Useful to find varying vs\n        constant keys and to find how fast keys vary.",
        "A succinct summary of the argument specifier. Unlike the repr,\n        a summary does not have to be complete but must supply the\n        most relevant information about the object to the user.",
        "Returns the specs, the remaining kwargs and whether or not the\n        constructor was called with kwarg or explicit specs.",
        "Convenience method to inspect the available argument values in\n        human-readable format. The ordering of keys is determined by\n        how quickly they vary.\n\n        The exclude list allows specific keys to be excluded for\n        readability (e.g. to hide long, absolute filenames).",
        "The lexical sort order is specified by a list of string\n        arguments. Each string is a key name prefixed by '+' or '-'\n        for ascending and descending sort respectively. If the key is\n        not found in the operand's set of varying keys, it is ignored.",
        "Simple replacement for numpy linspace",
        "Parses the log file generated by a launcher and returns\n        dictionary with tid keys and specification values.\n\n        Ordering can be maintained by setting dict_type to the\n        appropriate constructor (i.e. OrderedDict). Keys are converted\n        from unicode to strings for kwarg use.",
        "Writes the supplied specifications to the log path. The data\n        may be supplied as either as a an Args or as a list of\n        dictionaries.\n\n        By default, specifications will be appropriately appended to\n        an existing log file. This can be disabled by setting\n        allow_append to False.",
        "Load all the files in a given directory selecting only files\n        with the given extension if specified. The given kwargs are\n        passed through to the normal constructor.",
        "Return the fields specified in the pattern using Python's\n        formatting mini-language.",
        "Loads the files that match the given pattern.",
        "From the pattern decomposition, finds the absolute paths\n        matching the pattern.",
        "Convenience method to directly chain a pattern processed by\n        FilePattern into a FileInfo instance.\n\n        Note that if a default filetype has been set on FileInfo, the\n        filetype argument may be omitted.",
        "Load the file contents into the supplied Table using the\n        specified key and filetype. The input table should have the\n        filenames as values which will be replaced by the loaded\n        data. If data_key is specified, this key will be used to index\n        the loaded data to retrive the specified item.",
        "Load the file contents into the supplied dataframe using the\n        specified key and filetype.",
        "Generates the union of the source.specs and the metadata\n        dictionary loaded by the filetype object.",
        "Push new data into the buffer. Resume looping if paused.",
        "Create a plot of one area of interest of a single sweep.",
        "Inelegant for now, but lets you manually analyze every ABF in a folder.",
        "Reanalyze data for a single ABF. Also remakes child and parent html.",
        "scan folder1 and folder2 into files1 and files2.\n        since we are on windows, simplify things by making them all lowercase.\n        this WILL cause problems on 'nix operating systems.If this is the case,\n        just run a script to rename every file to all lowercase.",
        "run this to turn all folder1 TIFs and JPGs into folder2 data.\n        TIFs will be treated as micrographs and converted to JPG with enhanced\n        contrast. JPGs will simply be copied over.",
        "analyze every unanalyzed ABF in the folder.",
        "return appropriate HTML determined by file extension.",
        "generate a generic flat file html for an ABF parent. You could give\n        this a single ABF ID, its parent ID, or a list of ABF IDs.\n        If a child ABF is given, the parent will automatically be used.",
        "create ID_plot.html of just intrinsic properties.",
        "This applies a kernel to a signal through convolution and returns the result.\n\n    Some magic is done at the edges so the result doesn't apprach zero:\n        1. extend the signal's edges with len(kernel)/2 duplicated values\n        2. perform the convolution ('same' mode)\n        3. slice-off the ends we added\n        4. return the same number of points as the original",
        "simple timer. returns a time object, or a string.",
        "if the value is in the list, move it to the front and return it.",
        "if the value is in the list, move it to the back and return it.",
        "given a list and a list of items to be first, return the list in the\n    same order except that it begins with each of the first items.",
        "given a list of goofy ABF names, return it sorted intelligently.\n    This places things like 16o01001 after 16901001.",
        "when given a dictionary where every key contains a list of IDs, replace\n    the keys with the list of files matching those IDs. This is how you get a\n    list of files belonging to each child for each parent.",
        "given a groups dictionary and an ID, return its actual parent ID.",
        "return the semi-temporary user folder",
        "Coroutine wrapper to catch errors after async scheduling.\n\n    Args:\n        emitter (EventEmitter): The event emitter that is attempting to\n            call a listener.\n        event (str): The event that triggered the emitter.\n        listener (async def): The async def that was used to generate the coro.\n        coro (coroutine): The coroutine that should be tried.\n\n    If an exception is caught the function will use the emitter to emit the\n    failure event. If, however, the current event _is_ the failure event then\n    the method reraises. The reraised exception may show in debug mode for the\n    event loop but is otherwise silently dropped.",
        "Check if the listener limit is hit and warn if needed.",
        "Bind a listener to a particular event.\n\n        Args:\n            event (str): The name of the event to listen for. This may be any\n                string value.\n            listener (def or async def): The callback to execute when the event\n                fires. This may be a sync or async function.",
        "Add a listener that is only called once.",
        "Remove a listener from the emitter.\n\n        Args:\n            event (str): The event name on which the listener is bound.\n            listener: A reference to the same object given to add_listener.\n\n        Returns:\n            bool: True if a listener was removed else False.\n\n        This method only removes one listener at a time. If a listener is\n        attached multiple times then this method must be called repeatedly.\n        Additionally, this method removes listeners first from the those\n        registered with 'on' or 'add_listener'. If none are found it continue\n        to remove afterwards from those added with 'once'.",
        "Schedule a coroutine for execution.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (async def): The async def that needs to be executed.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        The values of *args and **kwargs are passed, unaltered, to the async\n        def when generating the coro. If there is an exception generating the\n        coro, such as the wrong number of arguments, the emitter's error event\n        is triggered. If the triggering event _is_ the emitter's error event\n        then the exception is reraised. The reraised exception may show in\n        debug mode for the event loop but is otherwise silently dropped.",
        "Execute a sync function.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def): The def that needs to be executed.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        The values of *args and **kwargs are passed, unaltered, to the def\n        when exceuting. If there is an exception executing the def, such as the\n        wrong number of arguments, the emitter's error event is triggered. If\n        the triggering event _is_ the emitter's error event then the exception\n        is reraised. The reraised exception may show in debug mode for the\n        event loop but is otherwise silently dropped.",
        "Dispatch an event to a listener.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def or async def): The listener to trigger.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method inspects the listener. If it is a def it dispatches the\n        listener to a method that will execute that def. If it is an async def\n        it dispatches it to a method that will schedule the resulting coro with\n        the event loop.",
        "Call each listener for the event with the given arguments.\n\n        Args:\n            event (str): The event to trigger listeners on.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method passes all arguments other than the event name directly\n        to the listeners. If a listener raises an exception for any reason the\n        'listener-error', or current value of LISTENER_ERROR_EVENT, is emitted.\n        Listeners to this event are given the event name, listener object, and\n        the exception raised. If an error listener fails it does so silently.\n\n        All event listeners are fired in a deferred way so this method returns\n        immediately. The calling coro must yield at some point for the event\n        to propagate to the listeners.",
        "Get the number of listeners for the event.\n\n        Args:\n            event (str): The event for which to count all listeners.\n\n        The resulting count is a combination of listeners added using\n        'on'/'add_listener' and 'once'.",
        "Convert each TIF to PNG. Return filenames of new PNGs.",
        "given an ID and the dict of files, generate a static html for that abf.",
        "expects a folder of ABFs.",
        "simple example how to load an ABF file and plot every sweep.",
        "plot X and Y data, then shade its background by variance.",
        "create some fancy graphs to show color-coded variances.",
        "run this before analysis. Checks if event detection occured.\n        If not, runs AP detection on all sweeps.",
        "runs AP detection on every sweep.",
        "Return package author and version as listed in `init.py`.",
        "Create an API subclass with fewer methods than its base class.\n\n    Arguments:\n      name (:py:class:`str`): The name of the new class.\n      docstring (:py:class:`str`): The docstring for the new class.\n      remove_methods (:py:class:`dict`): The methods to remove from\n        the base class's :py:attr:`API_METHODS` for the subclass. The\n        key is the name of the root method (e.g. ``'auth'`` for\n        ``'auth.test'``, the value is either a tuple of child method\n        names (e.g. ``('test',)``) or, if all children should be\n        removed, the special value :py:const:`ALL`.\n      base (:py:class:`type`, optional): The base class (defaults to\n        :py:class:`SlackApi`).\n\n",
        "Execute a specified Slack Web API method.\n\n        Arguments:\n          method (:py:class:`str`): The name of the method.\n          **params (:py:class:`dict`): Any additional parameters\n            required.\n\n        Returns:\n          :py:class:`dict`: The JSON data from the response.\n\n        Raises:\n          :py:class:`aiohttp.web_exceptions.HTTPException`: If the HTTP\n            request returns a code other than 200 (OK).\n          SlackApiError: If the Slack API is reached but the response\n           contains an error message.",
        "Whether a given method exists in the known API.\n\n        Arguments:\n          method (:py:class:`str`): The name of the method.\n\n        Returns:\n          :py:class:`bool`: Whether the method is in the known API.",
        "Extend XPath evaluation with Parsley extensions' namespace",
        "Try and convert matching Elements to unicode strings.\n\n        If this fails, the selector evaluation probably already\n        returned some string(s) of some sort, or boolean value,\n        or int/float, so return that instead.",
        "Join the real-time messaging service.\n\n        Arguments:\n          filters (:py:class:`dict`, optional): Dictionary mapping\n            message filters to the functions they should dispatch to.\n            Use a :py:class:`collections.OrderedDict` if precedence is\n            important; only one filter, the first match, will be\n            applied to each message.",
        "Handle an incoming message appropriately.\n\n        Arguments:\n          message (:py:class:`aiohttp.websocket.Message`): The incoming\n            message to handle.\n          filters (:py:class:`list`): The filters to apply to incoming\n            messages.",
        "If you send a message directly to me",
        "Create a new instance from the API token.\n\n        Arguments:\n          token (:py:class:`str`, optional): The bot's API token\n            (defaults to ``None``, which means looking in the\n            environment).\n          api_cls (:py:class:`type`, optional): The class to create\n            as the ``api`` argument for API access (defaults to\n            :py:class:`aslack.slack_api.SlackBotApi`).\n\n        Returns:\n          :py:class:`SlackBot`: The new instance.",
        "Format an outgoing message for transmission.\n\n        Note:\n          Adds the message type (``'message'``) and incremental ID.\n\n        Arguments:\n          channel (:py:class:`str`): The channel to send to.\n          text (:py:class:`str`): The message text to send.\n\n        Returns:\n          :py:class:`str`: The JSON string of the message.",
        "Get the WebSocket URL for the RTM session.\n\n        Warning:\n          The URL expires if the session is not joined within 30\n          seconds of the API call to the start endpoint.\n\n        Returns:\n          :py:class:`str`: The socket URL.",
        "Generates the instructions for a bot and its filters.\n\n        Note:\n          The guidance for each filter is generated by combining the\n          docstrings of the predicate filter and resulting dispatch\n          function with a single space between. The class's\n          :py:attr:`INSTRUCTIONS` and the default help command are\n          added.\n\n        Arguments:\n          filters (:py:class:`list`): The filters to apply to incoming\n            messages.\n\n        Returns:\n          :py:class:`str`: The bot's instructions.",
        "Respond to a message on the current socket.\n\n        Args:\n          channel (:py:class:`str`): The channel to send to.\n          text (:py:class:`str`): The message text to send.",
        "Check the first message matches the expected handshake.\n\n        Note:\n          The handshake is provided as :py:attr:`RTM_HANDSHAKE`.\n\n        Arguments:\n          msg (:py:class:`aiohttp.Message`): The message to validate.\n\n        Raises:\n          :py:class:`SlackApiError`: If the data doesn't match the\n            expected handshake.",
        "Returns list of paths to tested apps",
        "Get the imported task classes for each task that will be run",
        "Get the options for each task that will be run",
        "Write the data from the db to a CLDF dataset according to the metadata in `self.dataset`.\n\n        :param dest:\n        :param mdname:\n        :return: path of the metadata file",
        "A user-friendly description of the handler.\n\n        Returns:\n          :py:class:`str`: The handler's description.",
        "Create a Parselet instance from a file containing\n        the Parsley script as a JSON object\n\n        >>> import parslepy\n        >>> with open('parselet.json') as fp:\n        ...     parslepy.Parselet.from_jsonfile(fp)\n        ...\n        <parslepy.base.Parselet object at 0x2014e50>\n\n        :param file fp: an open file-like pointer containing the Parsley script\n        :rtype: :class:`.Parselet`\n\n        Other arguments: same as for :class:`.Parselet` contructor",
        "Create a Parselet instance from a file containing\n        the Parsley script as a YAML object\n\n        >>> import parslepy\n        >>> with open('parselet.yml') as fp:\n        ...     parslepy.Parselet.from_yamlfile(fp)\n        ...\n        <parslepy.base.Parselet object at 0x2014e50>\n\n        :param file fp: an open file-like pointer containing the Parsley script\n        :rtype: :class:`.Parselet`\n\n        Other arguments: same as for :class:`.Parselet` contructor",
        "Interpret input lines as a JSON Parsley script.\n        Python-style comment lines are skipped.",
        "Build part of the abstract Parsley extraction tree\n\n        Arguments:\n        parselet_node (dict) -- part of the Parsley tree to compile\n                                (can be the root dict/node)\n        level (int)          -- current recursion depth (used for debug)",
        "Use CLDF reference properties to implicitely create foreign key constraints.\n\n        :param component: A Table object or `None`.",
        "Create a URL for the specified endpoint.\n\n        Arguments:\n          endpoint (:py:class:`str`): The API endpoint to access.\n          root: (:py:class:`str`, optional): The root URL for the\n            service API.\n          params: (:py:class:`dict`, optional): The values for format\n            into the created URL (defaults to ``None``).\n          url_params: (:py:class:`dict`, optional): Parameters to add\n            to the end of the URL (defaults to ``None``).\n\n        Returns:\n          :py:class:`str`: The resulting URL.",
        "Raise an appropriate error for a given response.\n\n    Arguments:\n      response (:py:class:`aiohttp.ClientResponse`): The API response.\n\n    Raises:\n      :py:class:`aiohttp.web_exceptions.HTTPException`: The appropriate\n        error for the response's status.",
        "Truncate the supplied text for display.\n\n    Arguments:\n      text (:py:class:`str`): The text to truncate.\n      max_len (:py:class:`int`, optional): The maximum length of the\n        text before truncation (defaults to 350 characters).\n      end (:py:class:`str`, optional): The ending to use to show that\n        the text was truncated (defaults to ``'...'``).\n\n    Returns:\n      :py:class:`str`: The truncated text.",
        "Add a source, either specified by glottolog reference id, or as bibtex record.",
        "Returns a cache key consisten of a username and image size.",
        "Decorator to cache the result of functions that take a ``user`` and a\n    ``size`` value.",
        "Function to be called when saving or changing an user's avatars.",
        "Returns a field object instance for a given PrefProxy object.\n\n    :param PrefProxy pref_proxy:\n\n    :rtype: models.Field",
        "Updates field object with data from a PrefProxy object.\n\n    :param models.Field field_obj:\n\n    :param PrefProxy pref_proxy:",
        "Returns preferences model class dynamically crated for a given app or None on conflict.",
        "Returns locals dictionary from a given frame.\n\n    :param int stepback:\n\n    :rtype: dict",
        "Generator to walk through variables considered as preferences\n    in locals dict of a given frame.\n\n    :param int stepback:\n\n    :rtype: tuple",
        "Prints file details in the current directory",
        "Attempt to bind the args to the type signature. First try to just bind\n        to the signature, then ensure that all arguments match the parameter\n        types.",
        "For every parameter, create a matcher if the parameter has an\n        annotation.",
        "Makes a wrapper function that executes a dispatch call for func. The\n        wrapper has the dispatch and dispatch_first attributes, so that\n        additional overloads can be added to the group.",
        "Adds the decorated function to this dispatch.",
        "Adds the decorated function to this dispatch, at the FRONT of the order.\n        Useful for allowing third parties to add overloaded functionality\n        to be executed before default functionality.",
        "Dispatch a call. Call the first function whose type signature matches\n        the arguemts.",
        "reprojette en WGS84 et recupere l'extend",
        "Convert GRIB to Tif",
        "Triggered on dynamic preferences model save.\n     Issues DB save and reread.",
        "Binds PrefProxy objects to module variables used by apps as preferences.\n\n    :param list|tuple values: Preference values.\n\n    :param str|unicode category: Category name the preference belongs to.\n\n    :param Field field: Django model field to represent this preference.\n\n    :param str|unicode verbose_name: Field verbose name.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    :rtype: list",
        "Registers dynamically created preferences models for Admin interface.\n\n    :param admin.AdminSite admin_site: AdminSite object.",
        "Automatically discovers and registers all preferences available in all apps.\n\n    :param admin.AdminSite admin_site: Custom AdminSite object.",
        "Restores the original values of module variables\n    considered preferences if they are still PatchedLocal\n    and not PrefProxy.",
        "Replaces a settings module with a Module proxy to intercept\n    an access to settings.\n\n    :param int depth: Frame count to go backward.",
        "Registers preferences that should be handled by siteprefs.\n\n    Expects preferences as *args.\n\n    Use keyword arguments to batch apply params supported by\n    ``PrefProxy`` to all preferences not constructed by ``pref`` and ``pref_group``.\n\n    Batch kwargs:\n\n        :param str|unicode help_text: Field help text.\n\n        :param bool static: Leave this preference static (do not store in DB).\n\n        :param bool readonly: Make this field read only.\n\n    :param bool swap_settings_module: Whether to automatically replace settings module\n        with a special ``ProxyModule`` object to access dynamic values of settings\n        transparently (so not to bother with calling ``.value`` of ``PrefProxy`` object).",
        "Marks preferences group.\n\n    :param str|unicode title: Group title\n\n    :param list|tuple prefs: Preferences to group.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.",
        "Marks a preference.\n\n    :param preference: Preference variable.\n\n    :param Field field: Django model field to represent this preference.\n\n    :param str|unicode verbose_name: Field verbose name.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    :rtype: PrefProxy|None",
        "Generate the ``versionwarning-data.json`` file.\n\n    This file is included in the output and read by the AJAX request when\n    accessing to the documentation and used to compare the live versions with\n    the curent one.\n\n    Besides, this file contains meta data about the project, the API to use and\n    the banner itself.",
        "Gives objective functions a number of dimensions and parameter range\n\n    Parameters\n    ----------\n    param_scales : (int, int)\n        Scale (std. dev.) for choosing each parameter\n\n    xstar : array_like\n        Optimal parameters",
        "Pointwise minimum of two quadratic bowls",
        "Objective and gradient for the rosenbrock function",
        "Beale's function",
        "Booth's function",
        "Three-hump camel function",
        "One of the Bohachevsky functions",
        "Dixon-Price function",
        "Styblinski-Tang function",
        "Return a list of buckets in MimicDB.\n\n        :param boolean force: If true, API call is forced to S3",
        "Return a bucket from MimicDB if it exists. Return a\n        S3ResponseError if the bucket does not exist and validate is passed.\n\n        :param boolean force: If true, API call is forced to S3",
        "Add the bucket to MimicDB after successful creation.",
        "Sync either a list of buckets or the entire connection.\n\n        Force all API calls to S3 and populate the database with the current\n        state of S3.\n\n        :param \\*string \\*buckets: Buckets to sync",
        "Return the key from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3",
        "Return None if key is not in the bucket set.\n\n        Pass 'force' in the headers to check S3 for the key, and after fetching\n        the key from S3, save the metadata and key to the bucket set.",
        "Return a list of keys from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3",
        "Remove each key or key name in an iterable from the bucket set.",
        "Remove key name from bucket set.",
        "Return an iterable of keys from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3",
        "Sync a bucket.\n\n        Force all API calls to S3 and populate the database with the current state of S3.",
        "Minimize the proximal operator of a given objective using L-BFGS\n\n    Parameters\n    ----------\n    f_df : function\n        Returns the objective and gradient of the function to minimize\n\n    maxiter : int\n        Maximum number of L-BFGS iterations",
        "Applies a smoothing operator along one dimension\n\n    currently only accepts a matrix as input\n\n    Parameters\n    ----------\n    penalty : float\n\n    axis : int, optional\n        Axis along which to apply the smoothing (Default: 0)\n\n    newshape : tuple, optional\n        Desired shape of the parameters to apply the nuclear norm to. The given\n        parameters are reshaped to an array with this shape, or not reshaped if\n        the value of newshape is None. (Default: None)",
        "Projection onto the semidefinite cone",
        "Projection onto the probability simplex\n\n    http://arxiv.org/pdf/1309.1541v1.pdf",
        "Applies a proximal operator to the columns of a matrix",
        "Turns a coroutine into a gradient based optimizer.",
        "Adds a proximal operator to the list of operators",
        "Set key attributes to retrived metadata. Might be extended in the\n        future to support more attributes.",
        "Called internally for any type of upload. After upload finishes,\n        make sure the key is in the bucket set and save the metadata.",
        "Memoizes an objective + gradient function, and splits it into\n    two functions that return just the objective and gradient, respectively.\n\n    Parameters\n    ----------\n    f_df : function\n        Must be unary (takes a single argument)\n\n    xref : list, dict, or array_like\n        The form of the parameters\n\n    size : int, optional\n        Size of the cache (Default=1)",
        "Decorates a function with the given docstring\n\n    Parameters\n    ----------\n    docstr : string",
        "Compares the numerical gradient to the analytic gradient\n\n    Parameters\n    ----------\n    f_df : function\n        The analytic objective and gradient function to check\n\n    x0 : array_like\n        Parameter values to check the gradient at\n\n    stepsize : float, optional\n        Stepsize for the numerical gradient. Too big and this will poorly estimate the gradient.\n        Too small and you will run into precision issues (default: 1e-6)\n\n    tol : float, optional\n        Tolerance to use when coloring correct/incorrect gradients (default: 1e-5)\n\n    width : int, optional\n        Width of the table columns (default: 15)\n\n    style : string, optional\n        Style of the printed table, see tableprint for a list of styles (default: 'round')",
        "Evaluate the files identified for checksum.",
        "Check the integrity of the datapackage.json",
        "Guess the filetype and read the file into row sets",
        "Guess schema using messytables",
        "Calculates a checksum for a Finnish national reference number",
        "Helper to make sure the given character is valid for a reference number",
        "Creates the huge number from ISO alphanumeric ISO reference",
        "Validates ISO reference number",
        "Calculates virtual barcode for IBAN account number and ISO reference\n\n    Arguments:\n        iban {string} -- IBAN formed account number\n        reference {string} -- ISO 11649 creditor reference\n        amount {decimal.Decimal} -- Amount in euros, 0.01 - 999999.99\n        due {datetime.date} -- due date",
        "Add a normal file including its source",
        "Run the executable and capture the input and output...",
        "Add files to the repository by explicitly specifying them or by\n    specifying a pattern over files accessed during execution of an\n    executable.\n\n    Parameters\n    ----------\n\n    repo: Repository\n\n    args: files or command line\n         (a) If simply adding files, then the list of files that must\n         be added (including any additional arguments to be passed to\n         git\n         (b) If files to be added are an output of a command line, then\n         args is the command lined\n    targetdir: Target directory to store the files\n    execute: Args are not files to be added but scripts that must be run.\n    includes: patterns used to select files to\n    script: Is this a script?\n    generator: Is this a generator\n    source: Link to the original source of the data",
        "For various actions we need files that match patterns",
        "Run a specific command using the manager",
        "Get metadata for a given file",
        "Lookup all available repos",
        "Working directory for the repo",
        "Add repo to the internal lookup table...",
        "Lookup a repo based on username reponame",
        "Run a shell command within the repo's context\n\n    Parameters\n    ----------\n\n    repo: Repository object\n    args: Shell command",
        "Check if the datapackage exists...",
        "Create the datapackage file..",
        "Initialize an empty repository with datapackage.json\n\n    Parameters\n    ----------\n\n    username: Name of the user\n    reponame: Name of the repo\n    setup: Specify the 'configuration' (git only, git+s3 backend etc)\n    force: Force creation of the files\n    options: Dictionary with content of dgit.json, if available.\n    noinput: Automatic operation with no human interaction",
        "Update metadata with the content of the files",
        "Update metadata with the commit information",
        "Update metadata with the action history",
        "Update metadata host information",
        "Collect information from the dependent repo's",
        "Post to metadata server\n\n    Parameters\n    ----------\n\n    repo: Repository object (result of lookup)",
        "Show details of available plugins\n\n    Parameters\n    ----------\n    what: Class of plugins e.g., backend\n    name: Name of the plugin e.g., s3\n    version: Version of the plugin\n    details: Show details be shown?",
        "Load all plugins from dgit extension",
        "Registering a plugin\n\n        Params\n        ------\n        what: Nature of the plugin (backend, instrumentation, repo)\n        obj: Instance of the plugin",
        "Search for a plugin",
        "Instantiate the validation specification",
        "Validate the content of the files for consistency. Validators can\n    look as deeply as needed into the files. dgit treats them all as\n    black boxes.\n\n    Parameters\n    ----------\n\n    repo: Repository object\n    validator_name: Name of validator, if any. If none, then all validators specified in dgit.json will be included.\n    filename: Pattern that specifies files that must be processed by the validators selected. If none, then the default specification in dgit.json is used.\n    rules: Pattern specifying the files that have rules that validators will use\n    show: Print the validation results on the terminal\n\n    Returns\n    -------\n\n    status: A list of dictionaries, each with target file processed, rules file applied, status of the validation and any error  message.",
        "Check if a URL exists",
        "Post to the metadata server\n\n        Parameters\n        ----------\n\n        repo",
        "imports and returns module class from ``path.to.module.Class``\n    argument",
        "Find max 5 executables that are responsible for this repo.",
        "Automatically get repo\n\n    Parameters\n    ----------\n\n    autooptions: dgit.json content",
        "Look through the local directory to pick up files to check",
        "Cleanup the paths and add",
        "This will try to pull in a stream from an external source. Once a\n        stream has been successfully pulled it is assigned a 'local stream\n        name' which can be used to access the stream from the EMS.\n\n        :param uri: The URI of the external stream. Can be RTMP, RTSP or\n            unicast/multicast (d) mpegts\n        :type uri: str\n\n        :param keepAlive: If keepAlive is set to 1, the server will attempt to\n            reestablish connection with a stream from an external source. Once a\n        stream has been successfully pulled it is assigned a 'local stream\n        name' which can be used to access the stream from the EMS.\n\n        :param localStreamName: If provided, the stream will be given this\n            name. Otherwise, techniques used to determine the stream\n            name (based on the URI)\n        :type local",
        "Records any inbound stream. The record command allows users to record\n        a stream that may not yet exist. When a new stream is brought into\n        the server, it is checked against a list of streams to be recorded.\n\n        Streams can be recorded as FLV files, MPEG-TS files or as MP4 files.\n\n        :param localStreamName: The name of the stream to be used as input\n            for recording.\n        :type localStreamName: str\n\n        :param pathToFile: Specify path and file name to write to.\n        :type pathToFile: str\n\n        :param type: `ts`, `mp4` or `flv`\n        :type type: str\n\n        :param overwrite: If false, when a file already exists for the stream\n            name, a new file will be created with the next appropriate number\n            appended. If 1 (true), files with the same name",
        "Creates an RTMP ingest point, which mandates that streams pushed into\n        the EMS have a target stream name which matches one Ingest Point\n        privateStreamName.\n\n        :param privateStreamName: The name that RTMP Target Stream Names must\n            match.\n        :type privateStreamName: str\n\n        :param publicStreamName: The name that is used to access the stream\n            pushed to the privateStreamName. The publicStreamName becomes the\n            streams localStreamName.\n        :type publicStreamName: str\n\n        :link: http://docs.evostream.com/ems_api_definition/createingestpoint",
        "Instantiate the generator and filename specification",
        "Helper function to run commands\n\n        Parameters\n        ----------\n        cmd : list\n              Arguments to git command",
        "Run a generic command within the repo. Assumes that you are\n        in the repo's root directory",
        "Initialize a Git repo\n\n        Parameters\n        ----------\n\n        username, reponame : Repo name is tuple (name, reponame)\n        force: force initialization of the repo even if exists\n        backend: backend that must be used for this (e.g. s3)",
        "Delete files from the repo",
        "Cleanup the repo",
        "Get the permalink to command that generated the dataset",
        "Add files to the repo",
        "Marks the invoice as sent in Holvi\n\n        If send_email is False then the invoice is *not* automatically emailed to the recipient\n        and your must take care of sending the invoice yourself.",
        "Convert our Python object to JSON acceptable to Holvi API",
        "API wrapper documentation",
        "Saves this order to Holvi, returns a tuple with the order itself and checkout_uri",
        "Return source code based on tokens.\n\n    This is like tokenize.untokenize(), but it preserves spacing between\n    tokens. So if the original soure code had multiple spaces between\n    some tokens or if escaped newlines were used, those things will be\n    reflected by untokenize().",
        "Load profile INI",
        "Update the profile",
        "Insert hook into the repo",
        "Try the library. If it doesnt work, use the command line..",
        "Run a shell command",
        "Get the commit history for a given dataset",
        "Look at files and compute the diffs intelligently",
        "Execute command and wait for it to finish. Proceed with caution because\n        if you run a command that causes a prompt this will hang",
        "Enter sudo mode",
        "Install specified packages using apt-get. -y options are\n        automatically used. Waits for command to finish.\n\n        Parameters\n        ----------\n        package_names: list-like of str\n        raise_on_error: bool, default False\n            If True then raise ValueError if stderr is not empty\n            debconf often gives tty error",
        "Install specified python packages using pip. -U option added\n        Waits for command to finish.\n\n        Parameters\n        ----------\n        package_names: list-like of str\n        raise_on_error: bool, default True\n            If True then raise ValueError if stderr is not empty",
        "Install all requirements contained in the given file path\n        Waits for command to finish.\n\n        Parameters\n        ----------\n        requirements: str\n            Path to requirements.txt\n        raise_on_error: bool, default True\n            If True then raise ValueError if stderr is not empty",
        "Create fiji-macros for stitching all channels and z-stacks for a well.\n\n    Parameters\n    ----------\n    path : string\n        Well path.\n    output_folder : string\n        Folder to store images. If not given well path is used.\n\n    Returns\n    -------\n    output_files, macros : tuple\n        Tuple with filenames and macros for stitched well.",
        "Lossless compression. Save images as PNG and TIFF tags to json. Can be\n    reversed with `decompress`. Will run in multiprocessing, where\n    number of workers is decided by ``leicaexperiment.experiment._pools``.\n\n    Parameters\n    ----------\n    images : list of filenames\n        Images to lossless compress.\n    delete_tif : bool\n        Wheter to delete original images.\n    folder : string\n        Where to store images. Basename will be kept.\n\n    Returns\n    -------\n    list of filenames\n        List of compressed files.",
        "Lossless compression. Save image as PNG and TIFF tags to json. Process\n    can be reversed with `decompress`.\n\n    Parameters\n    ----------\n    image : string\n        TIF-image which should be compressed lossless.\n    delete_tif : bool\n        Wheter to delete original images.\n    force : bool\n        Wheter to compress even if .png already exists.\n\n    Returns\n    -------\n    string\n        Filename of compressed image, or empty string if compress failed.",
        "Set self.path, self.dirname and self.basename.",
        "List of paths to images.",
        "Get path of specified image.\n\n        Parameters\n        ----------\n        well_row : int\n            Starts at 0. Same as --U in files.\n        well_column : int\n            Starts at 0. Same as --V in files.\n        field_row : int\n            Starts at 0. Same as --Y in files.\n        field_column : int\n            Starts at 0. Same as --X in files.\n\n        Returns\n        -------\n        string\n            Path to image or empty string if image is not found.",
        "Get list of paths to images in specified well.\n\n\n        Parameters\n        ----------\n        well_row : int\n            Starts at 0. Same as --V in files.\n        well_column : int\n            Starts at 0. Save as --U in files.\n\n        Returns\n        -------\n        list of strings\n            Paths to images or empty list if no images are found.",
        "Stitches all wells in experiment with ImageJ. Stitched images are\n        saved in experiment root.\n\n        Images which already exists are omitted stitching.\n\n        Parameters\n        ----------\n        folder : string\n            Where to store stitched images. Defaults to experiment path.\n\n        Returns\n        -------\n        list\n            Filenames of stitched images. Files which already exists before\n            stitching are also returned.",
        "Lossless compress all images in experiment to PNG. If folder is\n        omitted, images will not be moved.\n\n        Images which already exists in PNG are omitted.\n\n        Parameters\n        ----------\n        folder : string\n            Where to store PNGs. Defaults to the folder they are in.\n        delete_tif : bool\n            If set to truthy value, ome.tifs will be deleted after compression.\n\n        Returns\n        -------\n        list\n            Filenames of PNG images. Files which already exists before\n            compression are also returned.",
        "Get OME-XML metadata of given field.\n\n        Parameters\n        ----------\n        well_row : int\n            Y well coordinate. Same as --V in files.\n        well_column : int\n            X well coordinate. Same as --U in files.\n        field_row : int\n            Y field coordinate. Same as --Y in files.\n        field_column : int\n            X field coordinate. Same as --X in files.\n\n        Returns\n        -------\n        lxml.objectify.ObjectifiedElement\n            lxml object of OME-XML found in slide/chamber/field/metadata.",
        "Get a list of stitch coordinates for the given well.\n\n        Parameters\n        ----------\n        well_row : int\n            Y well coordinate. Same as --V in files.\n        well_column : int\n            X well coordinate. Same as --U in files.\n\n        Returns\n        -------\n        (xs, ys, attr) : tuples with float and collections.OrderedDict\n            Tuple of x's, y's and attributes.",
        "Create a new droplet\n\n        Parameters\n        ----------\n        name: str\n            Name of new droplet\n        region: str\n            slug for region (e.g., sfo1, nyc1)\n        size: str\n            slug for droplet size (e.g., 512mb, 1024mb)\n        image: int or str\n            image id (e.g., 12352) or slug (e.g., 'ubuntu-14-04-x64')\n        ssh_keys: list, optional\n            default SSH keys to be added on creation\n            this is highly recommended for ssh access\n        backups: bool, optional\n            whether automated backups should be enabled for the Droplet.\n            Automated backups can only be enabled when the Droplet is created.\n        ipv6: bool, optional\n            whether IPv6 is enabled on the Droplet\n",
        "Retrieve a droplet by id\n\n        Parameters\n        ----------\n        id: int\n            droplet id\n\n        Returns\n        -------\n        droplet: DropletActions",
        "Restore this droplet with given image id\n\n        A Droplet restoration will rebuild an image using a backup image.\n        The image ID that is passed in must be a backup of the current Droplet\n        instance. The operation will leave any embedded SSH keys intact.\n\n        Parameters\n        ----------\n        image: int or str\n            int for image id and str for image slug\n        wait: bool, default True\n            Whether to block until the pending action is completed",
        "Rebuild this droplet with given image id\n\n        Parameters\n        ----------\n        image: int or str\n            int for image id and str for image slug\n        wait: bool, default True\n            Whether to block until the pending action is completed",
        "Change the name of this droplet\n\n        Parameters\n        ----------\n        name: str\n            New name for the droplet\n        wait: bool, default True\n            Whether to block until the pending action is completed\n\n        Raises\n        ------\n        APIError if region does not support private networking",
        "Change the kernel of this droplet\n\n        Parameters\n        ----------\n        kernel_id: int\n            Can be retrieved from output of self.kernels()\n        wait: bool, default True\n            Whether to block until the pending action is completed\n\n        Raises\n        ------\n        APIError if region does not support private networking",
        "Delete this droplet\n\n        Parameters\n        ----------\n        wait: bool, default True\n            Whether to block until the pending action is completed",
        "wait for all actions to complete on a droplet",
        "Open SSH connection to droplet\n\n        Parameters\n        ----------\n        interactive: bool, default False\n            If True then SSH client will prompt for password when necessary\n            and also print output to console",
        "Send a request to the REST API\n\n        Parameters\n        ----------\n        kind: str, {get, delete, put, post, head}\n        resource: str\n        url_components: list or tuple to be appended to the request URL\n\n        Notes\n        -----\n        kwargs contain request parameters to be sent as request data",
        "Properly formats array types",
        "create request url for resource",
        "Send a request for this resource to the API\n\n        Parameters\n        ----------\n        kind: str, {'get', 'delete', 'put', 'post', 'head'}",
        "Send list request for all members of a collection",
        "Get single unit of collection",
        "Transfer this image to given region\n\n        Parameters\n        ----------\n        region: str\n            region slug to transfer to (e.g., sfo1, nyc1)",
        "id or slug",
        "id or fingerprint",
        "Creates a new domain\n\n        Parameters\n        ----------\n        name: str\n            new domain name\n        ip_address: str\n            IP address for the new domain",
        "Get a list of all domain records for the given domain name\n\n        Parameters\n        ----------\n        name: str\n            domain name",
        "Change the name of this domain record\n\n        Parameters\n        ----------\n        id: int\n            domain record id\n        name: str\n            new name of record",
        "Retrieve a single domain record given the id",
        "Logs the user on to FogBugz.\n\n        Returns None for a successful login.",
        "Chop list_ into n chunks. Returns a list.",
        "return first droplet",
        "Take a snapshot of a droplet\n\n    Parameters\n    ----------\n    name: str\n        name for snapshot",
        "Retrieves the allowed operations for this request.",
        "Assets if the requested operations are allowed in this context.",
        "Fills the response object from the passed data.",
        "Processes a `GET` request.",
        "Processes a `POST` request.",
        "Processes a `PUT` request.",
        "Processes a `DELETE` request.",
        "Processes a `LINK` request.\n\n        A `LINK` request is asking to create a relation from the currently\n        represented URI to all of the `Link` request headers.",
        "Creates a base Django project",
        "Helper function that performs an `ilike` query if a string value\n    is passed, otherwise the normal default operation.",
        "Parse the querystring into a normalized form.",
        "Return objects representing segments.",
        "we expect foo=bar",
        "Set the value of this attribute for the passed object.",
        "Consumes set specifiers as text and forms a generator to retrieve\n    the requested ranges.\n\n    @param[in] specifiers\n        Expected syntax is from the byte-range-specifier ABNF found in the\n        [RFC 2616]; eg. 15-17,151,-16,26-278,15\n\n    @returns\n        Consecutive tuples that describe the requested range; eg. (1, 72) or\n        (1, 1) [read as 1 to 72 or 1 to 1].",
        "Paginate an iterable during a request.\n\n    Magically splicling an iterable in our supported ORMs allows LIMIT and\n    OFFSET queries. We should probably delegate this to the ORM or something\n    in the future.",
        "Decorate test methods with this if you don't require strict index checking",
        "Read and return the request data.\n\n        @param[in] deserialize\n            True to deserialize the resultant text using a determiend format\n            or the passed format.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.",
        "Updates the active resource configuration to the passed\n    keyword arguments.\n\n    Invoking this method without passing arguments will just return the\n    active resource configuration.\n\n    @returns\n        The previous configuration.",
        "This decorator wraps descriptor methods with a new method that tries\n    to delegate to a function of the same name defined on the owner instance\n    for convenience for dispatcher clients.",
        "Given a single decorated handler function,\n        prepare, append desired data to self.registry.",
        "Find the first method this input dispatches to.",
        "Given a node, return the string to use in computing the\n        matching visitor methodname. Can also be a generator of strings.",
        "Find all method names this input dispatches to.",
        "Parse string to create an instance\n\n          :param str s: String with requirement to parse\n          :param bool required: Is this requirement required to be fulfilled? If not, then it is a filter.",
        "Add requirements to be managed\n\n        :param list/Requirement requirements: List of :class:`BumpRequirement` or :class:`pkg_resources.Requirement`\n        :param bool required: Set required flag for each requirement if provided.",
        "Check if requirement is already satisfied by what was previously checked\n\n        :param Requirement req: Requirement to check",
        "Add new requirements that must be fulfilled for this bump to occur",
        "Parse changes for requirements\n\n        :param list changes:",
        "Bump dependencies using given requirements.\n\n          :param RequirementsManager bump_reqs: Bump requirements manager\n          :param dict kwargs: Additional args from argparse. Some bumpers accept user options, and some not.\n          :return: List of :class:`Bump` changes made.",
        "Restore content in target file to be before any changes",
        "Transforms the object into an acceptable format for transmission.\n\n        @throws ValueError\n            To indicate this serializer does not support the encoding of the\n            specified object.",
        "Extends a collection with a value.",
        "Merges a named option collection.",
        "All package info for given package",
        "All versions for package",
        "Flush and close the stream.\n\n        This is called automatically by the base resource on resources\n        unless the resource is operating asynchronously; in that case,\n        this method MUST be called in order to signal the end of the request.\n        If not the request will simply hang as it is waiting for some\n        thread to tell it to return to the client.",
        "Writes the given chunk to the output buffer.\n\n        @param[in] chunk\n            Either a byte array, a unicode string, or a generator. If `chunk`\n            is a generator then calling `self.write(<generator>)` is\n            equivalent to:\n\n            @code\n                for x in <generator>:\n                    self.write(x)\n                    self.flush()\n            @endcode\n\n        @param[in] serialize\n            True to serialize the lines in a determined serializer.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.",
        "Serializes the data into this response using a serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n\n        @returns\n            A tuple of the serialized text and an instance of the\n            serializer used.",
        "Flush the write buffers of the stream.\n\n        This results in writing the current contents of the write buffer to\n        the transport layer, initiating the HTTP/1.1 response. This initiates\n        a streaming response. If the `Content-Length` header is not given\n        then the chunked `Transfer-Encoding` is applied.",
        "Writes the passed chunk and flushes it to the client.",
        "Writes the passed chunk, flushes it to the client,\n        and terminates the connection.",
        "This ``Context Manager`` is used to move the contents of a directory\n    elsewhere temporarily and put them back upon exit.  This allows testing\n    code to use the same file directories as normal code without fear of\n    damage.\n\n    The name of the temporary directory which contains your files is yielded.\n\n    :param dirname:\n        Path name of the directory to be replaced.\n\n\n    Example:\n\n    .. code-block:: python\n\n        with replaced_directory('/foo/bar/') as rd:\n            # \"/foo/bar/\" has been moved & renamed\n            with open('/foo/bar/thing.txt', 'w') as f:\n                f.write('stuff')\n                f.close()\n\n\n        # got here? => \"/foo/bar/ is now restored and temp has been wiped, \n        # \"thing.txt\" is gone",
        "This ``Context Manager`` redirects STDOUT to a ``StringIO`` objects\n    which is returned from the ``Context``.  On exit STDOUT is restored.\n\n    Example:\n\n    .. code-block:: python\n\n        with capture_stdout() as capture:\n            print('foo')\n\n        # got here? => capture.getvalue() will now have \"foo\\\\n\"",
        "This ``Context Manager`` redirects STDERR to a ``StringIO`` objects\n    which is returned from the ``Context``.  On exit STDERR is restored.\n\n    Example:\n\n    .. code-block:: python\n\n        with capture_stderr() as capture:\n            print('foo')\n\n        # got here? => capture.getvalue() will now have \"foo\\\\n\"",
        "Builds the URL configuration for this resource.",
        "Dump an object in req format to the fp given.\n\n    :param Mapping obj: The object to serialize.  Must have a keys method.\n    :param fp: A writable that can accept all the types given.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.",
        "Dump an object in req format to a string.\n\n    :param Mapping obj: The object to serialize.  Must have a keys method.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.",
        "Load an object from the file pointer.\n\n    :param fp: A readable filehandle.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    :param cls: A callable that returns a Mapping that is filled with pairs.  The most common alternate option would be OrderedDict.\n    :param list_cls: A callable that takes an iterable and returns a sequence.",
        "Loads an object from a string.\n\n    :param s: An object to parse\n    :type s: bytes or str\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    :param cls: A callable that returns a Mapping that is filled with pairs.  The most common alternate option would be OrderedDict.\n    :param list_cls: A callable that takes an iterable and returns a sequence.",
        "Reverse all bumpers",
        "Expand targets by looking for '-r' in targets.",
        "Gets the Nginx config for the project",
        "Creates base directories for app, virtualenv, and nginx",
        "Creates the virtualenv for the project",
        "Creates the Nginx configuration for the project",
        "Creates scripts to start and stop the application",
        "Creates the full project",
        "Dasherizes the passed value.",
        "Redirect to the canonical URI for this resource.",
        "Parses out parameters and separates them out of the path.\n\n        This uses one of the many defined patterns on the options class. But,\n        it defaults to a no-op if there are no defined patterns.",
        "Traverses down the path and determines the accessed resource.\n\n        This makes use of the patterns array to implement simple traversal.\n        This defaults to a no-op if there are no defined patterns.",
        "Helper method used in conjunction with the view handler to\n        stream responses to the client.",
        "Deserializes the text using a determined deserializer.\n\n        @param[in] request\n            The request object to pull information from; normally used to\n            determine the deserialization format (when `format` is\n            not provided).\n\n        @param[in] text\n            The text to be deserialized. Can be left blank and the\n            request will be read.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.\n\n        @returns\n            A tuple of the deserialized data and an instance of the\n            deserializer used.",
        "Serializes the data using a determined serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] response\n            The response object to serialize the data to.\n            If this method is invoked as an instance method, the response\n            object can be omitted and it will be taken from the instance.\n\n        @param[in] request\n            The request object to pull information from; normally used to\n            determine the serialization format (when `format` is not provided).\n            May be used by some serializers as well to pull additional headers.\n            If this method is invoked as an instance method, the request\n            object can be omitted and it will be taken from the instance.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension",
        "Entry-point of the dispatch cycle for this resource.\n\n        Performs common work such as authentication, decoding, etc. before\n        handing complete control of the result to a function with the\n        same name as the request method.",
        "Ensure we are authenticated.",
        "Ensure we are allowed to access this resource.",
        "Ensure that we're allowed to use this HTTP method.",
        "Processes every request.\n\n        Directs control flow to the appropriate HTTP/1.1 method.",
        "Process an `OPTIONS` request.\n\n        Used to initiate a cross-origin request. All handling specific to\n        CORS requests is done on every request however this method also\n        returns a list of available methods.",
        "Wraps the decorated function in a lightweight resource.",
        "Render to cookie strings.",
        "update self with cookie_string.",
        "Adds a method to the internal lists of allowed or denied methods.\n        Each object in the internal list contains a resource ARN and a\n        condition statement. The condition statement can be null.",
        "This function loops over an array of objects containing\n        a resourceArn and conditions statement and generates\n        the array of statements for the policy.",
        "AWS doesn't quite have Swagger 2.0 validation right and will fail\n        on some refs. So, we need to convert to deref before\n        upload.",
        "Check all necessary system requirements to exist.\n\n    :param pre_requirements:\n        Sequence of pre-requirements to check by running\n        ``where <pre_requirement>`` on Windows and ``which ...`` elsewhere.",
        "Convert config dict to arguments list.\n\n    :param config: Configuration dict.",
        "Create virtual environment.\n\n    :param env: Virtual environment name.\n    :param args: Pass given arguments to ``virtualenv`` script.\n    :param recerate: Recreate virtual environment? By default: False\n    :param ignore_activated:\n        Ignore already activated virtual environment and create new one. By\n        default: False\n    :param quiet: Do not output messages into terminal. By default: False",
        "Decorator to error handling.",
        "Install library or project into virtual environment.\n\n    :param env: Use given virtual environment name.\n    :param requirements: Use given requirements file for pip.\n    :param args: Pass given arguments to pip script.\n    :param ignore_activated:\n        Do not run pip inside already activated virtual environment. By\n        default: False\n    :param install_dev_requirements:\n        When enabled install prefixed or suffixed dev requirements after\n        original installation process completed. By default: False\n    :param quiet: Do not output message to terminal. By default: False",
        "Iterate over dict items.",
        "Iterate over dict keys.",
        "r\"\"\"Bootstrap Python projects and libraries with virtualenv and pip.\n\n    Also check system requirements before bootstrap and run post bootstrap\n    hook if any.\n\n    :param \\*args: Command line arguments list.",
        "Parse args from command line by creating argument parser instance and\n    process it.\n\n    :param args: Command line arguments list.",
        "r\"\"\"Run pip command in given or activated virtual environment.\n\n    :param env: Virtual environment name.\n    :param cmd: Pip subcommand to run.\n    :param ignore_activated:\n        Ignore activated virtual environment and use given venv instead. By\n        default: False\n    :param \\*\\*kwargs:\n        Additional keyword arguments to be passed to :func:`~run_cmd`",
        "Convert config dict to command line args line.\n\n    :param config: Configuration dict.\n    :param bootstrap: Bootstrapper configuration dict.",
        "Print error message to stderr, using ANSI-colors.\n\n    :param message: Message to print\n    :param wrap:\n        Wrap message into ``ERROR: <message>. Exit...`` template. By default:\n        True",
        "Print message via ``subprocess.call`` function.\n\n    This helps to ensure consistent output and avoid situations where print\n    messages actually shown after messages from all inner threads.\n\n    :param message: Text message to print.",
        "Read and parse configuration file. By default, ``filename`` is relative\n    path to current work directory.\n\n    If no config file found, default ``CONFIG`` would be used.\n\n    :param filename: Read config from given filename.\n    :param args: Parsed command line arguments.",
        "r\"\"\"Call given command with ``subprocess.call`` function.\n\n    :param cmd: Command to run.\n    :type cmd: tuple or str\n    :param echo:\n        If enabled show command to call and its output in STDOUT, otherwise\n        hide all output. By default: False\n    :param fail_silently: Do not raise exception on error. By default: False\n    :param \\*\\*kwargs:\n        Additional keyword arguments to be passed to ``subprocess.call``\n        function. STDOUT and STDERR streams would be setup inside of function\n        to ensure hiding command output in case of disabling ``echo``.",
        "Run post-bootstrap hook if any.\n\n    :param hook: Hook to run.\n    :param config: Configuration dict.\n    :param quiet: Do not output messages to STDOUT/STDERR. By default: False",
        "Save error traceback to bootstrapper log file.\n\n    :param err: Catched exception.",
        "Convert Python object to string.\n\n    :param value: Python object to convert.\n    :param encoding: Encoding to use if in Python 2 given object is unicode.\n    :param errors: Errors mode to use if in Python 2 given object is unicode.",
        "Copy file from `src` path to `dst` path. If `dst` already exists, will add '+' characters\n    to the end of the basename without extension.\n\n    Parameters\n    ----------\n    src: str\n\n    dst: str\n\n    Returns\n    -------\n    dstpath: str",
        "Returns the absolute path of folderpath.\n    If the path does not exist, will raise IOError.",
        "Return the extension of fpath.\n\n    Parameters\n    ----------\n    fpath: string\n    File name or path\n\n    check_if_exists: bool\n\n    allowed_exts: dict\n    Dictionary of strings, where the key if the last part of a complex ('.' separated) extension\n    and the value is the previous part.\n    For example: for the '.nii.gz' extension I would have a dict as {'.gz': ['.nii',]}\n\n    Returns\n    -------\n    str\n    The extension of the file name or path",
        "Add the extension ext to fpath if it doesn't have it.\n\n    Parameters\n    ----------\n    filepath: str\n    File name or path\n\n    ext: str\n    File extension\n\n    check_if_exists: bool\n\n    Returns\n    -------\n    File name or path with extension added, if needed.",
        "Joins path to each line in filelist\n\n    Parameters\n    ----------\n    path: str\n\n    filelist: list of str\n\n    Returns\n    -------\n    list of filepaths",
        "Deletes all files in filelist\n\n    Parameters\n    ----------\n    filelist: list of str\n        List of the file paths to be removed\n\n    folder: str\n        Path to be used as common directory for all file paths in filelist",
        "Returns the length of the file using the 'wc' GNU command\n\n    Parameters\n    ----------\n    filepath: str\n\n    Returns\n    -------\n    float",
        "Merge two dictionaries.\n\n    Values that evaluate to true take priority over falsy values.\n    `dict_1` takes priority over `dict_2`.",
        "Return a folder path if it exists.\n\n    First will check if it is an existing system path, if it is, will return it\n    expanded and absoluted.\n\n    If this fails will look for the rcpath variable in the app_name rcfiles or\n    exclusively within the given section_name, if given.\n\n    Parameters\n    ----------\n    rcpath: str\n        Existing folder path or variable name in app_name rcfile with an\n        existing one.\n\n    section_name: str\n        Name of a section in the app_name rcfile to look exclusively there for\n        variable names.\n\n    app_name: str\n        Name of the application to look for rcfile configuration files.\n\n    Returns\n    -------\n    sys_path: str\n        A expanded absolute file or folder path if the path exists.\n\n    Raises\n    ------\n    IOError",
        "Read environment variables and config files and return them merged with\n    predefined list of arguments.\n\n    Parameters\n    ----------\n    appname: str\n        Application name, used for config files and environment variable\n        names.\n\n    section: str\n        Name of the section to be read. If this is not set: appname.\n\n    args:\n        arguments from command line (optparse, docopt, etc).\n\n    strip_dashes: bool\n        Strip dashes prefixing key names from args dict.\n\n    Returns\n    --------\n    dict\n        containing the merged variables of environment variables, config\n        files and args.\n\n    Raises\n    ------\n    IOError\n        In case the return value is empty.\n\n    Notes\n    -----\n    Environment variables are read if they start with appname in uppercase\n    with underscore, for example:\n\n        TEST_VAR=1\n\n    Config",
        "Return the dictionary containing the rcfile section configuration\n    variables.\n\n    Parameters\n    ----------\n    section_name: str\n        Name of the section in the rcfiles.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    settings: dict\n        Dict with variable values",
        "Return the value of the variable in the section_name section of the\n    app_name rc file.\n\n    Parameters\n    ----------\n    var_name: str\n        Name of the variable to be searched for.\n\n    section_name: str\n        Name of the section in the rcfiles.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    var_value: str\n        The value of the variable with given var_name.",
        "Return the section and the value of the variable where the first\n    var_name is found in the app_name rcfiles.\n\n    Parameters\n    ----------\n    var_name: str\n        Name of the variable to be searched for.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    section_name: str\n        Name of the section in the rcfiles where var_name was first found.\n\n    var_value: str\n        The value of the first variable with given var_name.",
        "Filters the lst using pattern.\n    If pattern starts with '(' it will be considered a re regular expression,\n    otherwise it will use fnmatch filter.\n\n    :param lst: list of strings\n\n    :param pattern: string\n\n    :return: list of strings\n    Filtered list of strings",
        "Given a nested dictionary adict.\n    This returns its childen just below the path.\n    The path is a string composed of adict keys separated by sep.\n\n    :param adict: nested dict\n\n    :param path: str\n\n    :param sep: str\n\n    :return: dict or list or leaf of treemap",
        "Given a nested dictionary, this returns all its leave elements in a list.\n\n    :param adict:\n\n    :return: list",
        "Looks for path_regex within base_path. Each match is append\n    in the returned list.\n    path_regex may contain subfolder structure.\n    If any part of the folder structure is a\n\n    :param base_path: str\n\n    :param path_regex: str\n\n    :return list of strings",
        "Will create dirpath folder. If dirpath already exists and overwrite is False,\n        will append a '+' suffix to dirpath until dirpath does not exist.",
        "Imports filetree and root_path variable values from the filepath.\n\n        :param filepath:\n        :return: root_path and filetree",
        "Remove the nodes that match the pattern.",
        "Return the number of nodes that match the pattern.\n\n        :param pattern:\n\n        :param adict:\n        :return: int",
        "Converts an array-like to an array of floats\n\n    The new dtype will be np.float32 or np.float64, depending on the original\n    type. The function can create a copy or modify the argument depending\n    on the argument copy.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n\n    copy : bool, optional\n        If True, a copy of X will be created. If False, a copy may still be\n        returned if X's dtype is not a floating point type.\n\n    Returns\n    -------\n    XT : {array, sparse matrix}\n        An array of type np.float",
        "Make arrays indexable for cross-validation.\n\n    Checks consistent length, passes through None, and ensures that everything\n    can be indexed by converting sparse matrices to csr and converting\n    non-interable objects to arrays.\n\n    Parameters\n    ----------\n    iterables : lists, dataframes, arrays, sparse matrices\n        List of objects to ensure sliceability.",
        "Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is converted to an at least 2nd numpy array.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc.  None means that sparse matrix input will raise an error.\n        If the input is sparse but not in the allowed format, it will be\n        converted to the first listed format.\n\n    order : 'F', 'C' or None (default=None)\n        Whether an array will be forced to be fortran or c-style.\n\n    copy : boolean (default=False)\n        Whether a forced copy will be triggered. If copy=False, a copy might",
        "Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X 2d and y 1d.\n    Standard input checks are only applied to y. For multi-label y,\n    set multi_ouput=True to allow 2d and sparse y.\n\n    Parameters\n    ----------\n    X : nd-array, list or sparse matrix\n        Input data.\n\n    y : nd-array, list or sparse matrix\n        Labels.\n\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc.  None means that sparse matrix input will raise an error.\n        If the input is sparse but not in the allowed format, it will be\n        converted to the first listed format.\n\n    order : 'F', 'C",
        "Ravel column or 1d numpy array, else raises an error\n\n    Parameters\n    ----------\n    y : array-like\n\n    Returns\n    -------\n    y : array",
        "Warning utility function to check that data type is floating point.\n\n    Returns True if a warning was raised (i.e. the input is not float) and\n    False otherwise, for easier input validation.",
        "Convert an arbitrary array to numpy.ndarray.\n\n    In the case of a memmap array, a copy is automatically made to break the\n    link with the underlying file (whatever the value of the \"copy\" keyword).\n\n    The purpose of this function is mainly to get rid of memmap objects, but\n    it can be used for other purposes. In particular, combining copying and\n    casting can lead to performance improvements in some cases, by avoiding\n    unnecessary copies.\n\n    If not specified, input array order is preserved, in all cases, even when\n    a copy is requested.\n\n    Caveat: this function does not copy during bool to/from 1-byte dtype\n    conversions. This can lead to some surprising results in some rare cases.\n    Example:\n\n        a = numpy.asarray([0, 1, 2], dtype=numpy.int8)\n",
        "Call FSL tools to apply transformations to a given atlas to a functional image.\n    Given the transformation matrices.\n\n    Parameters\n    ----------\n    atlas_filepath: str\n        Path to the 3D atlas volume file.\n\n    anatbrain_filepath: str\n        Path to the anatomical brain volume file (skull-stripped and registered to the same space as the atlas,\n        e.g., MNI).\n\n    meanfunc_filepath: str\n        Path to the average functional image to be used as reference in the last applywarp step.\n\n    atlas2anat_nonlin_xfm_filepath: str\n        Path to the 3D atlas volume file.\n\n    is_atlas2anat_inverted: bool\n        If False will have to calculate the inverse fields I need\n        anat_to_mni_nl_inv = op.",
        "Convert a FWHM value to sigma in a Gaussian kernel.\n\n    Parameters\n    ----------\n    fwhm: float or numpy.array\n       fwhm value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       sigma values",
        "Convert a sigma in a Gaussian kernel to a FWHM value.\n\n    Parameters\n    ----------\n    sigma: float or numpy.array\n       sigma value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       fwhm values corresponding to `sigma` values",
        "Smooth images with a a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        3D or 4D array, with image number as last dimension.\n\n    affine: numpy.ndarray\n        Image affine transformation matrix for image.\n\n    fwhm: scalar, numpy.ndarray\n        Smoothing kernel size, as Full-Width at Half Maximum (FWHM) in millimeters.\n        If a scalar is given, kernel width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n\n    copy: bool\n        if True, will make a copy of the input array. Otherwise will directly smooth the input array.\n\n    Returns\n    -------\n    smooth_arr: numpy.ndarray",
        "Smooth images using a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of each image in images.\n    In all cases, non-finite values in input are zeroed.\n\n    Parameters\n    ----------\n    imgs: str or img-like object or iterable of img-like objects\n        See boyle.nifti.read.read_img\n        Image(s) to smooth.\n\n    fwhm: scalar or numpy.ndarray\n        Smoothing kernel size, as Full-Width at Half Maximum (FWHM) in millimeters.\n        If a scalar is given, kernel width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n\n    Returns\n    -------\n    smooth_imgs: nibabel.Nifti1Image or list of.\n        Smooth input image/",
        "Smooth images by applying a Gaussian filter.\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    This is copied and slightly modified from nilearn:\n    https://github.com/nilearn/nilearn/blob/master/nilearn/image/image.py\n    Added the **kwargs argument.\n\n    Parameters\n    ==========\n    arr: numpy.ndarray\n        4D array, with image number as last dimension. 3D arrays are also\n        accepted.\n    affine: numpy.ndarray\n        (4, 4) matrix, giving affine transformation for image. (3, 3) matrices\n        are also accepted (only these coefficients are used).\n        If fwhm='fast', the affine is not used and can be None\n    fwhm: scalar, numpy.ndarray, 'fast' or None\n        Smoothing strength, as a full-width at",
        "Smooth images by applying a Gaussian filter.\n    Apply a Gaussian filter along the three first dimensions of arr.\n    In all cases, non-finite values in input image are replaced by zeros.\n\n    This is copied and slightly modified from nilearn:\n    https://github.com/nilearn/nilearn/blob/master/nilearn/image/image.py\n    Added the **kwargs argument.\n\n    Parameters\n    ==========\n    imgs: Niimg-like object or iterable of Niimg-like objects\n        See http://nilearn.github.io/manipulating_images/manipulating_images.html#niimg.\n        Image(s) to smooth.\n    fwhm: scalar, numpy.ndarray, 'fast' or None\n        Smoothing strength, as a Full-Width at Half Maximum, in millimeters.\n",
        "Create requests session with any required auth headers\n        applied.\n\n        :rtype: requests.Session.",
        "Create requests session with AAD auth headers\n\n        :rtype: requests.Session.",
        "Return a grid with coordinates in 3D physical space for `img`.",
        "Gets a 3D CoordinateMap from img.\n\n    Parameters\n    ----------\n    img: nib.Nifti1Image or nipy Image\n\n    Returns\n    -------\n    nipy.core.reference.coordinate_map.CoordinateMap",
        "Return the header and affine matrix from a Nifti file.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    hdr, aff",
        "Return the voxel matrix of the Nifti file.\n    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    copy: bool\n    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.\n\n    Returns\n   ",
        "Read a Nifti file and return as nipy.Image\n\n    Parameters\n    ----------\n    param nii_file: str\n        Nifti file path\n\n    Returns\n    -------\n    nipy.Image",
        "From the list of absolute paths to nifti files, creates a Numpy array\n    with the data.\n\n    Parameters\n    ----------\n    img_filelist:  list of str\n        List of absolute file paths to nifti files. All nifti files must have\n        the same shape.\n\n    outdtype: dtype\n        Type of the elements of the array, if not set will obtain the dtype from\n        the first nifti file.\n\n    Returns\n    -------\n    outmat: Numpy array with shape N x prod(vol.shape)\n            containing the N files as flat vectors.\n\n    vol_shape: Tuple with shape of the volumes, for reshaping.",
        "Crops image to a smaller size\n\n    Crop img to size indicated by slices and modify the affine accordingly.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n        Image to be cropped.\n\n    slices: list of slices\n        Defines the range of the crop.\n        E.g. [slice(20, 200), slice(40, 150), slice(0, 100)]",
        "Crops img as much as possible\n\n    Will crop img, removing as many zero entries as possible\n    without touching non-zero entries. Will leave one voxel of\n    zero padding around the obtained non-zero area in order to\n    avoid sampling issues later on.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n        Image to be cropped.\n\n    rtol: float\n",
        "Create a new image of the same class as the reference image\n\n    Parameters\n    ----------\n    ref_niimg: image\n        Reference image. The new image will be of the same type.\n\n    data: numpy array\n        Data to be stored in the image\n\n    affine: 4x4 numpy array, optional\n        Transformation matrix\n\n    copy_header: boolean, optional\n        Indicated if the header of the reference image should be used to\n        create the new image\n\n    Returns\n    -------\n    new_img: image\n        A loaded image with the same type (and header) as the reference image.",
        "Return the h5py.File given its file path.\n\n    Parameters\n    ----------\n    file_path: string\n        HDF5 file path\n\n    mode: string\n        r   Readonly, file must exist\n        r+  Read/write, file must exist\n        w   Create file, truncate if exists\n        w-  Create file, fail if exists\n        a   Read/write if exists, create otherwise (default)\n\n    Returns\n    -------\n    h5file: h5py.File",
        "Return all dataset contents from h5path group in h5file in an OrderedDict.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to read datasets from\n\n    Returns\n    -------\n    datasets: OrderedDict\n        Dict with variables contained in file_path/h5path",
        "Return the node of type node_type names within h5path of h5file.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to get the group names from\n\n    node_type: h5py object type\n        HDF5 object type\n\n    Returns\n    -------\n    names: list of str\n        List of names",
        "self.mask setter\n\n        Parameters\n        ----------\n        image: str or img-like object.\n            See NeuroImage constructor docstring.",
        "Read the images, load them into self.items and set the labels.",
        "Save the Numpy array created from to_matrix function to the output_file.\n\n        Will save into the file: outmat, mask_indices, vol_shape and self.others (put here whatever you want)\n\n            data: Numpy array with shape N x prod(vol.shape)\n                  containing the N files as flat vectors.\n\n            mask_indices: matrix with indices of the voxels in the mask\n\n            vol_shape: Tuple with shape of the volumes, for reshaping.\n\n        Parameters\n        ----------\n        output_file: str\n            Path to the output file. The extension of the file will be taken into account for the file format.\n            Choices of extensions: '.pyshelf' or '.shelf' (Python shelve)\n                                   '.mat' (Matlab archive),\n                                   '.hdf5' or '.h5' (HDF5 file",
        "Writes msg to stderr and exits with return code",
        "Calls the command\n\n    Parameters\n    ----------\n    cmd_args: list of str\n        Command name to call and its arguments in a list.\n\n    Returns\n    -------\n    Command output",
        "Call CLI command with arguments and returns its return value.\n\n    Parameters\n    ----------\n    cmd_name: str\n        Command name or full path to the binary file.\n\n    arg_strings: list of str\n        Argument strings list.\n\n    Returns\n    -------\n    return_value\n        Command return value.",
        "Tries to submit cmd to HTCondor, if it does not succeed, it will\n    be called with subprocess.call.\n\n    Parameters\n    ----------\n    cmd: string\n        Command to be submitted\n\n    Returns\n    -------",
        "Submits cmd to HTCondor queue\n\n    Parameters\n    ----------\n    cmd: string\n        Command to be submitted\n\n    Returns\n    -------\n    int\n        returncode value from calling the submission command.",
        "Clean previously built package artifacts.",
        "Upload the package to an index server.\n\n    This implies cleaning and re-building the package.\n\n    :param repo: Required. Name of the index server to upload to, as specifies\n        in your .pypirc configuration file.",
        "Load all Service Fabric commands",
        "Open a volumetric file using the tools following the file extension.\n\n    Parameters\n    ----------\n    filepath: str\n        Path to a volume file\n\n    Returns\n    -------\n    volume_data: np.ndarray\n        Volume data\n\n    pixdim: 1xN np.ndarray\n        Vector with the description of the voxels physical size (usually in mm) for each volume dimension.\n\n    Raises\n    ------\n    IOError\n        In case the file is not found.",
        "Will rename all files in file_lst to a padded serial\n    number plus its extension\n\n    :param file_lst: list of path.py paths",
        "Search for dicoms in folders and save file paths into\n        self.dicom_paths set.\n\n        :param folders: str or list of str",
        "Overwrites self.items with the given set of files.\n        Will filter the fileset and keep only Dicom files.\n\n        Parameters\n        ----------\n        fileset: iterable of str\n        Paths to files\n\n        check_if_dicoms: bool\n        Whether to check if the items in fileset are dicom file paths",
        "Update this set with the union of itself and dicomset.\n\n        Parameters\n        ----------\n        dicomset: DicomFileSet",
        "Copies all files within this set to the output_folder\n\n        Parameters\n        ----------\n        output_folder: str\n        Path of the destination folder of the files\n\n        rename_files: bool\n        Whether or not rename the files to a sequential format\n\n        mkdir: bool\n        Whether to make the folder if it does not exist\n\n        verbose: bool\n        Whether to print to stdout the files that are beind copied",
        "Creates a lambda function to read DICOM files.\n        If store_store_metadata is False, will only return the file path.\n        Else if you give header_fields, will return only the set of of\n        header_fields within a DicomFile object or the whole DICOM file if\n        None.\n\n        :return: function\n        This function has only one parameter: file_path",
        "Generator that yields one by one the return value for self.read_dcm\n        for each file within this set",
        "Return a set of unique field values from a list of DICOM files\n\n    Parameters\n    ----------\n    dcm_file_list: iterable of DICOM file paths\n\n    field_name: str\n     Name of the field from where to get each value\n\n    Returns\n    -------\n    Set of field values",
        "Returns a list of the dicom files within root_path\n\n    Parameters\n    ----------\n    root_path: str\n    Path to the directory to be recursively searched for DICOM files.\n\n    Returns\n    -------\n    dicoms: set\n    Set of DICOM absolute file paths",
        "Tries to read the file using dicom.read_file,\n    if the file exists and dicom.read_file does not raise\n    and Exception returns True. False otherwise.\n\n    :param filepath: str\n     Path to DICOM file\n\n    :return: bool",
        "Group in a dictionary all the DICOM files in dicom_paths\n    separated by the given `hdr_field` tag value.\n\n    Parameters\n    ----------\n    dicom_paths: str\n        Iterable of DICOM file paths.\n\n    hdr_field: str\n        Name of the DICOM tag whose values will be used as key for the group.\n\n    Returns\n    -------\n    dicom_groups: dict of dicom_paths",
        "Return the attributes values from this DicomFile\n\n        Parameters\n        ----------\n        attributes: str or list of str\n         DICOM field names\n\n        default: str\n         Default value if the attribute does not exist.\n\n        Returns\n        -------\n        Value of the field or list of values.",
        "Concatenate `images` in the direction determined in `axis`.\n\n    Parameters\n    ----------\n    images: list of str or img-like object.\n        See NeuroImage constructor docstring.\n\n    axis: str\n      't' : concatenate images in time\n      'x' : concatenate images in the x direction\n      'y' : concatenate images in the y direction\n      'z' : concatenate images in the z direction\n\n    Returns\n    -------\n    merged: img-like object",
        "Picks a function whose first argument is an `img`, processes its\n    data and returns a numpy array. This decorator wraps this numpy array\n    into a nibabel.Nifti1Image.",
        "Pixelwise division or divide by a number",
        "Return the image with the given `mask` applied.",
        "Return an image with the binarised version of the data of `img`.",
        "Return a z-scored version of `icc`.\n    This function is based on GIFT `icatb_convertImageToZScores` function.",
        "Return the thresholded z-scored `icc`.",
        "Write the content of the `meta_dict` into `filename`.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file\n\n    meta_dict: dict\n        Dictionary with the fields of the metadata .mhd file",
        "Write the data into a raw format file. Big endian is always used.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file\n\n    data: numpy.ndarray\n        n-dimensional image data array.",
        "Write the `data` and `meta_dict` in two files with names\n    that use `filename` as a prefix.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file.\n        This is going to be used as a preffix.\n        Two files will be created, one with a '.mhd' extension\n        and another with '.raw'. If `filename` has any of these already\n        they will be taken into account to build the filenames.\n\n    data: numpy.ndarray\n        n-dimensional image data array.\n\n    shape: tuple\n        Tuple describing the shape of `data`\n        Default: data.shape\n\n    meta_dict: dict\n        Dictionary with the fields of the metadata .mhd file\n        Default: {}\n\n    Returns\n    -------\n    mhd_filename: str\n        Path to the .mhd file\n\n    raw",
        "Copy .mhd and .raw files to dst.\n\n    If dst is a folder, won't change the file, but if dst is another filepath,\n    will modify the ElementDataFile field in the .mhd to point to the\n    new renamed .raw file.\n\n    Parameters\n    ----------\n    src: str\n        Path to the .mhd file to be copied\n\n    dst: str\n        Path to the destination of the .mhd and .raw files.\n        If a new file name is given, the extension will be ignored.\n\n    Returns\n    -------\n    dst: str",
        "SPSS .sav files to Pandas DataFrame through Rpy2\n\n    :param input_file: string\n\n    :return:",
        "SPSS .sav files to Pandas DataFrame through savreader module\n\n    :param input_file: string\n\n    :return:",
        "Valid extensions '.pyshelf', '.mat', '.hdf5' or '.h5'\n\n        @param filename: string\n\n        @param varnames: list of strings\n        Names of the variables\n\n        @param varlist: list of objects\n        The objects to be saved",
        "Create CLI environment",
        "Find all the ROIs in img and returns a similar volume with the ROIs\n    emptied, keeping only their border voxels.\n\n    This is useful for DTI tractography.\n\n    Parameters\n    ----------\n    img: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    np.ndarray\n        an array of same shape as img_data",
        "Return the largest connected component of a 3D array.\n\n    Parameters\n    -----------\n    volume: numpy.array\n        3D boolean array.\n\n    Returns\n    --------\n    volume: numpy.array\n        3D boolean array with only one connected component.",
        "Return as mask for `volume` that includes only areas where\n    the connected components have a size bigger than `min_cluster_size`\n    in number of voxels.\n\n    Parameters\n    -----------\n    volume: numpy.array\n        3D boolean array.\n\n    min_cluster_size: int\n        Minimum size in voxels that the connected component must have.\n\n    Returns\n    --------\n    volume: numpy.array\n        3D int array with a mask excluding small connected components.",
        "Look for the files in filelist containing the names in roislist, these files will be opened, binarised\n    and merged in one mask.\n\n    Parameters\n    ----------\n    roislist: list of strings\n        Names of the ROIs, which will have to be in the names of the files in filelist.\n\n    filelist: list of strings\n        List of paths to the volume files containing the ROIs.\n\n    Returns\n    -------\n    numpy.ndarray\n        Mask volume",
        "Return a sorted list of the non-zero unique values of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        The data array\n\n    Returns\n    -------\n    list of items of arr.",
        "Get the center of mass for each ROI in the given volume.\n\n    Parameters\n    ----------\n    vol: numpy ndarray\n        Volume with different values for each ROI.\n\n    Returns\n    -------\n    OrderedDict\n        Each entry in the dict has the ROI value as key and the center_of_mass coordinate as value.",
        "Extracts the values in `datavol` that are in the ROI with value `roivalue` in `roivol`.\n    The ROI can be masked by `maskvol`.\n\n    Parameters\n    ----------\n    datavol: numpy.ndarray\n        4D timeseries volume or a 3D volume to be partitioned\n\n    roivol: numpy.ndarray\n        3D ROIs volume\n\n    roivalue: int or float\n        A value from roivol that represents the ROI to be used for extraction.\n\n    maskvol: numpy.ndarray\n        3D mask volume\n\n    zeroe: bool\n        If true will remove the null timeseries voxels.  Only applied to timeseries (4D) data.\n\n    Returns\n    -------\n    values: np.array\n        An array of the values in the indicated ROI.\n        A 2D matrix if `datavol`",
        "Pick one 3D volume from a 4D nifti image file\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Volume defining different ROIs.\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    vol_idx: int\n        Index of the 3D volume to be extracted from the 4D volume.\n\n    Returns\n    -------\n    vol, hdr, aff\n        The data array, the image header and the affine transform matrix.",
        "Returns a h5py dataset given its registered name.\n\n        :param ds_name: string\n        Name of the dataset to be returned.\n\n        :return:",
        "Creates a Dataset with unknown size.\n        Resize it before using.\n\n        :param ds_name: string\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py DataSet",
        "Saves a Numpy array in a dataset in the HDF file, registers it as\n        ds_name and returns the h5py dataset.\n\n        :param ds_name: string\n        Registration name of the dataset to be registered.\n\n        :param data: Numpy ndarray\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py dataset",
        "See create_dataset.",
        "Will get the names of the index colums of df, obtain their ranges from\n        range_values dict and return a reindexed version of df with the given\n        range values.\n\n        :param df: pandas DataFrame\n\n        :param range_values: dict or array-like\n        Must contain for each index column of df an entry with all the values\n        within the range of the column.\n\n        :param fill_value: scalar or 'nearest', default 0\n        Value to use for missing values. Defaults to 0, but can be any\n        \"compatible\" value, e.g., NaN.\n        The 'nearest' mode will fill the missing value with the nearest value in\n         the column.\n\n        :param fill_method:  {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n        Method to use for filling holes in reindexed DataFrame\n        'pad",
        "Retrieve pandas object or group of Numpy ndarrays\n        stored in file\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        obj : type of object stored in file",
        "Store object in HDFStore\n\n        Parameters\n        ----------\n        key : str\n\n        value : {Series, DataFrame, Panel, Numpy ndarray}\n\n        format : 'fixed(f)|table(t)', default is 'fixed'\n            fixed(f) : Fixed format\n                Fast writing/reading. Not-appendable, nor searchable\n\n            table(t) : Table format\n                Write as a PyTables Table structure which may perform worse but allow more flexible operations\n                like searching/selecting subsets of the data\n\n        append : boolean, default False\n            This will force Table format, append the input data to the\n            existing.\n\n        encoding : default None, provide an encoding for strings",
        "Returns a PyTables HDF Array from df in the shape given by its index columns range values.\n\n        :param key: string object\n\n        :param df: pandas DataFrame\n\n        :param range_values: dict or array-like\n        Must contain for each index column of df an entry with all the values\n        within the range of the column.\n\n        :param loop_multiindex: bool\n        Will loop through the first index in a multiindex dataframe, extract a\n        dataframe only for one value, complete and fill the missing values and\n        store in the HDF.\n        If this is True, it will not use unstack.\n        This is as fast as unstacking.\n\n        :param unstack: bool\n        Unstack means that this will use the first index name to\n        unfold the DataFrame, and will create a group with as many datasets\n        as valus has this first index.",
        "Set a smoothing Gaussian kernel given its FWHM in mm.",
        "First set_mask and the get_masked_data.\n\n        Parameters\n        ----------\n        mask_img:  nifti-like image, NeuroImage or str\n            3D mask array: True where a voxel should be used.\n            Can either be:\n            - a file path to a Nifti image\n            - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n            If niimg is a string, consider it as a path to Nifti image and\n            call nibabel.load on it. If it is an object, check if get_data()\n            and get_affine() methods are present, raise TypeError otherwise.\n\n        Returns\n        -------\n        The masked data deepcopied",
        "Sets a mask img to this. So every operation to self, this mask will be taken into account.\n\n        Parameters\n        ----------\n        mask_img: nifti-like image, NeuroImage or str\n            3D mask array: True where a voxel should be used.\n            Can either be:\n            - a file path to a Nifti image\n            - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n            If niimg is a string, consider it as a path to Nifti image and\n            call nibabel.load on it. If it is an object, check if get_data()\n            and get_affine() methods are present, raise TypeError otherwise.\n\n        Note\n        ----\n        self.img and mask_file must have the same shape.\n\n        Raises\n        ------\n        FileNotFound",
        "Return the data masked with self.mask\n\n        Parameters\n        ----------\n        data: np.ndarray\n\n        Returns\n        -------\n        masked np.ndarray\n\n        Raises\n        ------\n        ValueError if the data and mask dimensions are not compatible.\n        Other exceptions related to numpy computations.",
        "Set self._smooth_fwhm and then smooths the data.\n        See boyle.nifti.smooth.smooth_imgs.\n\n        Returns\n        -------\n        the smoothed data deepcopied.",
        "Return a vector of the masked data.\n\n        Returns\n        -------\n        np.ndarray, tuple of indices (np.ndarray), tuple of the mask shape",
        "Save this object instance in outpath.\n\n        Parameters\n        ----------\n        outpath: str\n            Output file path",
        "Setup logging configuration.",
        "Return a 3D volume from a 4D nifti image file\n\n    Parameters\n    ----------\n    filename: str\n        Path to the 4D .mhd file\n\n    vol_idx: int\n        Index of the 3D volume to be extracted from the 4D volume.\n\n    Returns\n    -------\n    vol, hdr\n        The data array and the new 3D image header.",
        "A wrapper for mem.cache that flushes the cache if the version\n        number of nibabel has changed.",
        "Saves a Nifti1Image into an HDF5 group.\n\n    Parameters\n    ----------\n    h5group: h5py Group\n        Output HDF5 file path\n\n    spatial_img: nibabel SpatialImage\n        Image to be saved\n\n    h5path: str\n        HDF5 group path where the image data will be saved.\n        Datasets will be created inside the given group path:\n        'data', 'extra', 'affine', the header information will\n        be set as attributes of the 'data' dataset.",
        "Saves a Nifti1Image into an HDF5 file.\n\n    Parameters\n    ----------\n    file_path: string\n        Output HDF5 file path\n\n    spatial_img: nibabel SpatialImage\n        Image to be saved\n\n    h5path: string\n        HDF5 group path where the image data will be saved.\n        Datasets will be created inside the given group path:\n        'data', 'extra', 'affine', the header information will\n        be set as attributes of the 'data' dataset.\n        Default: '/img'\n\n    append: bool\n        True if you don't want to erase the content of the file\n        if it already exists, False otherwise.\n\n    Note\n    ----\n    HDF5 open modes\n    >>> 'r' Readonly, file must exist\n    >>> 'r+' Read/write, file must exist\n    >>> 'w'",
        "Transforms an H5py Attributes set to a dict.\n    Converts unicode string keys into standard strings\n    and each value into a numpy array.\n\n    Parameters\n    ----------\n    h5attrs: H5py Attributes\n\n    Returns\n    --------\n    dict",
        "Returns in a list all images found under h5group.\n\n    Parameters\n    ----------\n    h5group: h5py.Group\n        HDF group\n\n    Returns\n    -------\n    list of nifti1Image",
        "Inserts all given nifti files from file_list into one dataset in fname.\n    This will not check if the dimensionality of all files match.\n\n    Parameters\n    ----------\n    file_path: string\n        HDF5 file path\n\n    h5path: string\n\n    file_list: list of strings\n\n    newshape: tuple or lambda function\n        If None, it will not reshape the images.\n        If a lambda function, this lambda will receive only the shape array.\n        e.g., newshape = lambda x: (np.prod(x[0:3]), x[3])\n        If a tuple, it will try to reshape all the images with the same shape.\n        It must work for all the images in file_list.\n\n    concat_axis: int\n        Axis of concatenation after reshaping\n\n    dtype: data type\n    Dataset data type\n   ",
        "Generate all combinations of the elements of iterable and its subsets.\n\n    Parameters\n    ----------\n    iterable: list, set or dict or any iterable object\n\n    Returns\n    -------\n    A generator of all possible combinations of the iterable.\n\n    Example:\n    -------\n    >>> for i in treefall([1, 2, 3, 4, 5]): print(i)\n    >>> (1, 2, 3)\n    >>> (1, 2)\n    >>> (1, 3)\n    >>> (2, 3)\n    >>> (1,)\n    >>> (2,)\n    >>> (3,)\n    >>> ()",
        "List existing reliable dictionaries.\n\n    List existing reliable dictionaries and respective schema for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str",
        "Query Schema information for existing reliable dictionaries.\n\n    Query Schema information existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param dictionary: Name of the reliable dictionary.\n    :type dictionary: str\n    :param output_file: Optional file to save the schema.",
        "Query existing reliable dictionary.\n\n    Query existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param dictionary_name: Name of the reliable dictionary.\n    :type dictionary_name: str\n    :param query_string: An OData query string. For example $top=10. Check https://www.odata.org/documentation/ for more information.\n    :type query_string: str\n    :param partition_key: Optional partition key of the desired partition, either a string if named schema or int if Int64 schema\n    :type partition_id: str\n    :param partition_id: Optional partition GUID of the owning reliable dictionary.\n    :type partition_id: str\n    :param output",
        "Execute create, update, delete operations on existing reliable dictionaries.\n\n    carry out create, update and delete operations on existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param output_file: input file with list of json to provide the operation information for reliable dictionaries.",
        "Verify arguments for select command",
        "Get AAD token",
        "Use openpyxl to read an Excel file.",
        "Return the expanded absolute path of `xl_path` if\n    if exists and 'xlrd' or 'openpyxl' depending on\n    which module should be used for the Excel file in `xl_path`.\n\n    Parameters\n    ----------\n    xl_path: str\n        Path to an Excel file\n\n    Returns\n    -------\n    xl_path: str\n        User expanded and absolute path to `xl_path`\n\n    module: str\n        The name of the module you should use to process the\n        Excel file.\n        Choices: 'xlrd', 'pyopenxl'\n\n    Raises\n    ------\n    IOError\n        If the file does not exist\n\n    RuntimError\n        If a suitable reader for xl_path is not found",
        "Return the workbook from the Excel file in `xl_path`.",
        "Return a list with the name of the sheets in\n    the Excel file in `xl_path`.",
        "Return a pandas DataFrame with the concat'ed\n    content of the `sheetnames` from the Excel file in\n    `xl_path`.\n\n    Parameters\n    ----------\n    xl_path: str\n        Path to the Excel file\n\n    sheetnames: list of str\n        List of existing sheet names of `xl_path`.\n        If None, will use all sheets from `xl_path`.\n\n    add_tab_names: bool\n        If True will add a 'Tab' column which says from which\n        tab the row comes from.\n\n    Returns\n    -------\n    df: pandas.DataFrame",
        "Raise an AttributeError if `df` does not have a column named as an item of\n    the list of strings `col_names`.",
        "Return a list of not null values from the `col_name` column of `df`.",
        "Return a DataFrame with the duplicated values of the column `col_name`\n    in `df`.",
        "Return the duplicated items in `values`",
        "Convert to string all values in `data`.\n\n    Parameters\n    ----------\n    data: dict[str]->object\n\n    Returns\n    -------\n    string_data: dict[str]->str",
        "Search for items in `table` that have the same field sub-set values as in `sample`.\n    Expecting it to be unique, otherwise will raise an exception.\n\n    Parameters\n    ----------\n    table: tinydb.table\n    sample: dict\n        Sample data\n\n    Returns\n    -------\n    search_result: tinydb.database.Element\n        Unique item result of the search.\n\n    Raises\n    ------\n    KeyError:\n        If the search returns for more than one entry.",
        "Search in `table` an item with the value of the `unique_fields` in the `sample` sample.\n    Check if the the obtained result is unique. If nothing is found will return an empty list,\n    if there is more than one item found, will raise an IndexError.\n\n    Parameters\n    ----------\n    table: tinydb.table\n\n    sample: dict\n        Sample data\n\n    unique_fields: list of str\n        Name of fields (keys) from `data` which are going to be used to build\n        a sample to look for exactly the same values in the database.\n        If None, will use every key in `data`.\n\n    Returns\n    -------\n    eid: int\n        Id of the object found with same `unique_fields`.\n        None if none is found.\n\n    Raises\n    ------\n    MoreThanOneItemError\n        If more than one example is found.",
        "Create a TinyDB query that looks for items that have each field in `sample` with a value\n    compared with the correspondent operation in `operators`.\n\n    Parameters\n    ----------\n    sample: dict\n        The sample data\n\n    operators: str or list of str\n        A list of comparison operations for each field value in `sample`.\n        If this is a str, will use the same operator for all `sample` fields.\n        If you want different operators for each field, remember to use an OrderedDict for `sample`.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query",
        "Create a tinyDB Query object that looks for items that confirms the correspondent operator\n    from `operators` for each `field_names` field values from `data`.\n\n    Parameters\n    ----------\n    data: dict\n        The data sample\n\n    field_names: str or list of str\n        The name of the fields in `data` that will be used for the query.\n\n    operators: str or list of str\n        A list of comparison operations for each field value in `field_names`.\n        If this is a str, will use the same operator for all `field_names`.\n        If you want different operators for each field, remember to use an OrderedDict for `data`.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query",
        "Create a tinyDB Query object that is the concatenation of each query in `queries`.\n    The concatenation operator is taken from `operators`.\n\n    Parameters\n    ----------\n    queries: list of tinydb.Query\n        The list of tinydb.Query to be joined.\n\n    operators: str or list of str\n        List of binary operators to join `queries` into one query.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query",
        "Return the element in `table_name` with Object ID `eid`.\n        If None is found will raise a KeyError exception.\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table to look in.\n\n        eid: int\n            The Object ID of the element to look for.\n\n        Returns\n        -------\n        elem: tinydb.database.Element\n\n        Raises\n        ------\n        KeyError\n            If the element with ID `eid` is not found.",
        "Search in `table` an item with the value of the `unique_fields` in the `data` sample.\n        Check if the the obtained result is unique. If nothing is found will return an empty list,\n        if there is more than one item found, will raise an IndexError.\n\n        Parameters\n        ----------\n        table_name: str\n\n        sample: dict\n            Sample data\n\n        unique_fields: list of str\n            Name of fields (keys) from `data` which are going to be used to build\n            a sample to look for exactly the same values in the database.\n            If None, will use every key in `data`.\n\n        Returns\n        -------\n        eid: int\n            Id of the object found with same `unique_fields`.\n            None if none is found.\n\n        Raises\n        ------\n        MoreThanOneItemError\n            If more than one example is found.",
        "Return True if an item with the value of `unique_fields`\n        from `data` is unique in the table with `table_name`.\n        False if no sample is found or more than one is found.\n\n        See function `find_unique` for more details.\n\n        Parameters\n        ----------\n        table_name: str\n\n        sample: dict\n            Sample data for query\n\n        unique_fields: str or list of str\n\n        Returns\n        -------\n        is_unique: bool",
        "Update the unique matching element to have a given set of fields.\n\n        Parameters\n        ----------\n        table_name: str\n\n        fields: dict or function[dict -> None]\n            new data/values to insert into the unique element\n            or a method that will update the elements.\n\n        data: dict\n            Sample data for query\n\n        cond: tinydb.Query\n            which elements to update\n\n        unique_fields: list of str\n\n        raise_if_not_found: bool\n            Will raise an exception if the element is not found for update.\n\n        Returns\n        -------\n        eid: int\n            The eid of the updated element if found, None otherwise.",
        "Return the number of items that match the `sample` field values\n        in table `table_name`.\n        Check function search_sample for more details.",
        "Check for get_data and get_affine method in an object\n\n    Parameters\n    ----------\n    obj: any object\n        Tested object\n\n    Returns\n    -------\n    is_img: boolean\n        True if get_data and get_affine methods are present and callable,\n        False otherwise.",
        "Get the data in the image without having a side effect on the Nifti1Image object\n\n    Parameters\n    ----------\n    img: Nifti1Image\n\n    Returns\n    -------\n    np.ndarray",
        "Return the shape of img.\n\n    Paramerers\n    -----------\n    img:\n\n    Returns\n    -------\n    shape: tuple",
        "Return true if one_img and another_img have the same shape.\n    False otherwise.\n    If both are nibabel.Nifti1Image will also check for affine matrices.\n\n    Parameters\n    ----------\n    one_img: nibabel.Nifti1Image or np.ndarray\n\n    another_img: nibabel.Nifti1Image  or np.ndarray\n\n    only_check_3d: bool\n        If True will check only the 3D part of the affine matrices when they have more dimensions.\n\n    Raises\n    ------\n    NiftiFilesNotCompatible",
        "Return True if the affine matrix of one_img is close to the affine matrix of another_img.\n    False otherwise.\n\n    Parameters\n    ----------\n    one_img: nibabel.Nifti1Image\n\n    another_img: nibabel.Nifti1Image\n\n    only_check_3d: bool\n        If True will extract only the 3D part of the affine matrices when they have more dimensions.\n\n    Returns\n    -------\n    bool\n\n    Raises\n    ------\n    ValueError",
        "Printing of img or imgs",
        "Returns true if array1 and array2 have the same shapes, false\n    otherwise.\n\n    Parameters\n    ----------\n    array1: numpy.ndarray\n\n    array2: numpy.ndarray\n\n    nd_to_check: int\n        Number of the dimensions to check, i.e., if == 3 then will check only the 3 first numbers of array.shape.\n    Returns\n    -------\n    bool",
        "Create a list of regex matches that result from the match_regex\n    of all file names within wd.\n    The list of files will have wd as path prefix.\n\n    @param regex: string\n    @param wd: string\n    working directory\n    @return:",
        "Returns absolute paths of folders that match the regex within folder_path and\n    all its children folders.\n\n    Note: The regex matching is done using the match function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: string\n\n    Returns\n    -------\n    A list of strings.",
        "Creates a list of files that match the search_regex within file_dir.\n    The list of files will have file_dir as path prefix.\n\n    Parameters\n    ----------\n    @param file_dir:\n\n    @param search_regex:\n\n    Returns:\n    --------\n    List of paths to files that match the search_regex",
        "Returns absolute paths of files that match the regex within file_dir and\n    all its children folders.\n\n    Note: The regex matching is done using the search function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: string\n\n    Returns\n    -------\n    A list of strings.",
        "Returns absolute paths of files that match the regexs within folder_path and\n    all its children folders.\n\n    This is an iterator function that will use yield to return each set of\n    file_paths in one iteration.\n\n    Will only return value if all the strings in regex match a file name.\n\n    Note: The regex matching is done using the search function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: strings\n\n    Returns\n    -------\n    A list of strings.",
        "Generator that loops through all absolute paths of the files within folder\n\n    Parameters\n    ----------\n    folder: str\n    Root folder start point for recursive search.\n\n    Yields\n    ------\n    fpath: str\n    Absolute path of one file in the folders",
        "Uses glob to find all files or folders that match the regex\n    starting from the base_directory.\n\n    Parameters\n    ----------\n    base_directory: str\n\n    regex: str\n\n    Returns\n    -------\n    files: list",
        "Append key-value pairs to msg, for display.\n\n    Parameters\n    ----------\n    msg: string\n        arbitrary message\n    kwargs: dict\n        arbitrary dictionary\n\n    Returns\n    -------\n    updated_msg: string\n        msg, with \"key: value\" appended. Only string values are appended.\n\n    Example\n    -------\n    >>> compose_err_msg('Error message with arguments...', arg_num=123, \\\n        arg_str='filename.nii', arg_bool=True)\n    'Error message with arguments...\\\\narg_str: filename.nii'\n    >>>",
        "Gets a list of DICOM file absolute paths and returns a list of lists of\n    DICOM file paths. Each group contains a set of DICOM files that have\n    exactly the same headers.\n\n    Parameters\n    ----------\n    dicom_file_paths: list of str\n        List or set of DICOM file paths\n\n    header_fields: list of str\n        List of header field names to check on the comparisons of the DICOM files.\n\n    Returns\n    -------\n    dict of DicomFileSets\n        The key is one filepath representing the group (the first found).",
        "Copy the DICOM file groups to folder_path. Each group will be copied into\n    a subfolder with named given by groupby_field.\n\n    Parameters\n    ----------\n    dicom_groups: boyle.dicom.sets.DicomFileSet\n\n    folder_path: str\n     Path to where copy the DICOM files.\n\n    groupby_field_name: str\n     DICOM field name. Will get the value of this field to name the group\n     folder.",
        "Calculates the DicomFileDistance between all files in dicom_files, using an\n    weighted Levenshtein measure between all field names in field_weights and\n    their corresponding weights.\n\n    Parameters\n    ----------\n    dicom_files: iterable of str\n        Dicom file paths\n\n    field_weights: dict of str to float\n        A dict with header field names to float scalar values, that\n        indicate a distance measure ratio for the levenshtein distance\n        averaging of all the header field names in it. e.g., {'PatientID': 1}\n\n    dist_method_cls: DicomFileDistance class\n        Distance method object to compare the files.\n        If None, the default DicomFileDistance method using Levenshtein\n        distance between the field_wieghts will be used.\n\n    kwargs: DicomFileDistance instantiation named arguments\n        Apart from the field_weitghts argument.\n\n    Returns\n    -------\n    file_dists: np.ndarray or scipy.sparse.lil_matrix of shape NxN\n        Levenshtein distances between each of the N items in dicom_files.",
        "Check the field values in self.dcmf1 and self.dcmf2 and returns True\n        if all the field values are the same, False otherwise.\n\n        Returns\n        -------\n        bool",
        "Updates the status of the file clusters comparing the cluster\n        key files with a levenshtein weighted measure using either the\n        header_fields or self.header_fields.\n\n        Parameters\n        ----------\n        field_weights: dict of strings with floats\n            A dict with header field names to float scalar values, that indicate a distance measure\n            ratio for the levenshtein distance averaging of all the header field names in it.\n            e.g., {'PatientID': 1}",
        "Thresholds a distance matrix and returns the result.\n\n        Parameters\n        ----------\n\n        dist_matrix: array_like\n        Input array or object that can be converted to an array.\n\n        perc_thr: float in range of [0,100]\n        Percentile to compute which must be between 0 and 100 inclusive.\n\n        k: int, optional\n        Diagonal above which to zero elements.\n        k = 0 (the default) is the main diagonal,\n        k < 0 is below it and k > 0 is above.\n\n        Returns\n        -------\n        array_like",
        "Returns a list of 2-tuples with pairs of dicom groups that\n        are in the same folder within given depth.\n\n        Parameters\n        ----------\n        folder_depth: int\n        Path depth to check for folder equality.\n\n        Returns\n        -------\n        list of tuples of str",
        "Extend the lists within the DICOM groups dictionary.\n        The indices will indicate which list have to be extended by which\n        other list.\n\n        Parameters\n        ----------\n        indices: list or tuple of 2 iterables of int, bot having the same len\n             The indices of the lists that have to be merged, both iterables\n             items will be read pair by pair, the first is the index to the\n             list that will be extended with the list of the second index.\n             The indices can be constructed with Numpy e.g.,\n             indices = np.where(square_matrix)",
        "Copy the file groups to folder_path. Each group will be copied into\n        a subfolder with named given by groupby_field.\n\n        Parameters\n        ----------\n        folder_path: str\n         Path to where copy the DICOM files.\n\n        groupby_field_name: str\n         DICOM field name. Will get the value of this field to name the group\n         folder. If empty or None will use the basename of the group key file.",
        "Return a dictionary where the key is the group key file path and\n        the values are sets of unique values of the field name of all DICOM\n        files in the group.\n\n        Parameters\n        ----------\n        field_name: str\n         Name of the field to read from all files\n\n        field_to_use_as_key: str\n         Name of the field to get the value and use as key.\n         If None, will use the same key as the dicom_groups.\n\n        Returns\n        -------\n        Dict of sets",
        "Gets a config by name.\n\n    In the case where the config name is not found, will use fallback value.",
        "Checks if a config value is set to a valid bool value.",
        "Set a config by name to a value.",
        "Path to certificate related files, either a single file path or a\n    tuple. In the case of no security, returns None.",
        "Set AAD token cache.",
        "Set AAD metadata.",
        "Set certificate usage paths",
        "Returns a list with of the objects in olist that have a fieldname valued as fieldval\n\n    Parameters\n    ----------\n    olist: list of objects\n\n    fieldname: string\n\n    fieldval: anything\n\n    Returns\n    -------\n    list of objets",
        "Checks whether the re module can compile the given regular expression.\n\n    Parameters\n    ----------\n    string: str\n\n    Returns\n    -------\n    boolean",
        "Returns True if the given string is considered a fnmatch\n    regular expression, False otherwise.\n    It will look for\n\n    :param string: str",
        "Return index of the nth match found of pattern in strings\n\n    Parameters\n    ----------\n    strings: list of str\n        List of strings\n\n    pattern: str\n        Pattern to be matched\n\n    nth: int\n        Number of times the match must happen to return the item index.\n\n    lookup_func: callable\n        Function to match each item in strings to the pattern, e.g., re.match or re.search.\n\n    Returns\n    -------\n    index: int\n        Index of the nth item that matches the pattern.\n        If there are no n matches will return -1",
        "Generate a dcm2nii configuration file that disable the interactive\n    mode.",
        "Converts all DICOM files within `work_dir` into one or more\n    NifTi files by calling dcm2nii on this folder.\n\n    Parameters\n    ----------\n    work_dir: str\n        Path to the folder that contain the DICOM files\n\n    arguments: str\n        String containing all the flag arguments for `dcm2nii` CLI.\n\n    Returns\n    -------\n    sys_code: int\n        dcm2nii execution return code",
        "Call MRICron's `dcm2nii` to convert the DICOM files inside `input_dir`\n    to Nifti and save the Nifti file in `output_dir` with a `filename` prefix.\n\n    Parameters\n    ----------\n    input_dir: str\n        Path to the folder that contains the DICOM files\n\n    output_dir: str\n        Path to the folder where to save the NifTI file\n\n    filename: str\n        Output file basename\n\n    Returns\n    -------\n    filepaths: list of str\n        List of file paths created in `output_dir`.",
        "Return a subset of `filepaths`. Keep only the files that have a basename longer than the\n    others with same suffix.\n    This works based on that dcm2nii appends a preffix character for each processing\n    step it does automatically in the DICOM to NifTI conversion.\n\n    Parameters\n    ----------\n    filepaths: iterable of str\n\n    Returns\n    -------\n    cleaned_paths: iterable of str",
        "Transform a named tuple into a dictionary",
        "Extend the within a dict of lists. The indices will indicate which\n    list have to be extended by which other list.\n\n    Parameters\n    ----------\n    adict: OrderedDict\n        An ordered dictionary of lists\n\n    indices: list or tuple of 2 iterables of int, bot having the same length\n        The indices of the lists that have to be merged, both iterables items\n         will be read pair by pair, the first is the index to the list that\n         will be extended with the list of the second index.\n         The indices can be constructed with Numpy e.g.,\n         indices = np.where(square_matrix)\n\n    pop_later: bool\n        If True will oop out the lists that are indicated in the second\n         list of indices.\n\n    copy: bool\n        If True will perform a deep copy of the input adict before\n         modifying it, hence not changing the original input.",
        "Return a dict of lists from a list of dicts with the same keys.\n    For each dict in list_of_dicts with look for the values of the\n    given keys and append it to the output dict.\n\n    Parameters\n    ----------\n    list_of_dicts: list of dicts\n\n    keys: list of str\n        List of keys to create in the output dict\n        If None will use all keys in the first element of list_of_dicts\n    Returns\n    -------\n    DefaultOrderedDict of lists",
        "Imports the contents of filepath as a Python module.\n\n    :param filepath: string\n\n    :param mod_name: string\n    Name of the module when imported\n\n    :return: module\n    Imported module",
        "Copies the files in the built file tree map\n    to despath.\n\n    :param configfile: string\n     Path to the FileTreeMap config file\n\n    :param destpath: string\n     Path to the files destination\n\n    :param overwrite: bool\n     Overwrite files if they already exist.\n\n    :param sub_node: string\n     Tree map configuration sub path.\n     Will copy only the contents within this sub-node",
        "Transforms the input .sav SPSS file into other format.\n    If you don't specify an outputfile, it will use the\n    inputfile and change its extension to .csv",
        "Load a Nifti mask volume.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n    -------\n    nibabel.Nifti1Image with boolean data.",
        "Load a Nifti mask volume and return its data matrix as boolean and affine.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n    -------\n    numpy.ndarray with dtype==bool, numpy.ndarray of affine transformation",
        "Creates a binarised mask with the union of the files in filelist.\n\n    Parameters\n    ----------\n    filelist: list of img-like object or boyle.nifti.NeuroImage or str\n        List of paths to the volume files containing the ROIs.\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    ndarray of bools\n        Mask volume\n\n    Raises\n    ------\n    ValueError",
        "Read a Nifti file nii_file and a mask Nifti file.\n    Returns the voxels in nii_file that are within the mask, the mask indices\n    and the mask shape.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    mask_img: img-like object or boyle.nifti",
        "Read a Nifti file nii_file and a mask Nifti file.\n    Extract the signals in nii_file that are within the mask, the mask indices\n    and the mask shape.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    mask_img: img-like object or boyle.nifti.",
        "Transform a given vector to a volume. This is a reshape function for\n    3D flattened and maybe masked vectors.\n\n    Parameters\n    ----------\n    arr: np.array\n        1-Dimensional array\n\n    mask: numpy.ndarray\n        Mask image. Must have 3 dimensions, bool dtype.\n\n    Returns\n    -------\n    np.ndarray",
        "Transform a given vector to a volume. This is a reshape function for\n    4D flattened masked matrices where the second dimension of the matrix\n    corresponds to the original 4th dimension.\n\n    Parameters\n    ----------\n    arr: numpy.array\n        2D numpy.array\n\n    mask: numpy.ndarray\n        Mask image. Must have 3 dimensions, bool dtype.\n\n    dtype: return type\n        If None, will get the type from vector\n\n    Returns\n    -------\n    data: numpy.ndarray\n        Unmasked data.\n        Shape: (mask.shape[0], mask.shape[1], mask.shape[2], X.shape[1])",
        "From the list of absolute paths to nifti files, creates a Numpy array\n    with the masked data.\n\n    Parameters\n    ----------\n    img_filelist: list of str\n        List of absolute file paths to nifti files. All nifti files must have\n        the same shape.\n\n    mask_file: str\n        Path to a Nifti mask file.\n        Should be the same shape as the files in nii_filelist.\n\n    outdtype: dtype\n        Type of the elements of the array, if not set will obtain the dtype from\n        the first nifti file.\n\n    Returns\n    -------\n    outmat:\n        Numpy array with shape N x prod(vol.shape) containing the N files as flat vectors.\n\n    mask_indices:\n        Tuple with the 3D spatial indices of the masking voxels, for resh",
        "Create a client for Service Fabric APIs.",
        "Aggregate the rows of the DataFrame into a single value.\n\n        :param clazz: name of a class that extends class Callable\n        :type clazz: class\n        :param new_col: name of the new column\n        :type new_col: str\n        :param args: list of column names of the object that function \n        should be applied to\n        :type args: tuple\n        :return: returns a new dataframe object with the aggregated value\n        :rtype: DataFrame",
        "Pipeable grouping method.\n\n    Takes either\n      - a dataframe and a tuple of strings for grouping,\n      - a tuple of strings if a dataframe has already been piped into.\n    \n    :Example:\n        \n    group(dataframe, \"column\")\n    \n    :Example:\n    \n    dataframe >> group(\"column\")\n    \n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a grouped dataframe object\n    :rtype: GroupedDataFrame",
        "Pipeable aggregation method.\n    \n    Takes either \n     - a dataframe and a tuple of arguments required for aggregation,\n     - a tuple of arguments if a dataframe has already been piped into.\n    In any case one argument has to be a class that extends callable.\n\n    :Example:\n\n    aggregate(dataframe, Function, \"new_col_name\", \"old_col_name\")\n\n    :Example:\n\n    dataframe >> aggregate(Function, \"new_col_name\", \"old_col_name\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame",
        "Pipeable subsetting method.\n\n    Takes either\n     - a dataframe and a tuple of arguments required for subsetting,\n     - a tuple of arguments if a dataframe has already been piped into.\n\n    :Example:\n        \n    subset(dataframe, \"column\")\n    \n    :Example:\n    \n    dataframe >> subset(\"column\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame",
        "Pipeable modification method \n    \n    Takes either \n     - a dataframe and a tuple of arguments required for modification,\n     - a tuple of arguments if a dataframe has already been piped into.\n    In any case one argument has to be a class that extends callable.\n\n    :Example:\n\n    modify(dataframe, Function, \"new_col_name\", \"old_col_name\")\n    \n    :Example:\n\n    dataframe >> modify(Function, \"new_col_name\", \"old_col_name\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame",
        "Escape a single character",
        "Escape a string so that it only contains characters in a safe set.\n\n    Characters outside the safe list will be escaped with _%x_,\n    where %x is the hex value of the character.\n\n    If `allow_collisions` is True, occurrences of `escape_char`\n    in the input will not be escaped.\n\n    In this case, `unescape` cannot be used to reverse the transform\n    because occurrences of the escape char in the resulting string are ambiguous.\n    Only use this mode when:\n\n    1. collisions cannot occur or do not matter, and\n    2. unescape will never be called.\n\n    .. versionadded: 1.0\n        allow_collisions argument.\n        Prior to 1.0, behavior was the same as allow_collisions=False (default).",
        "Unescape a string escaped with `escape`\n    \n    escape_char must be the same as that used in the call to escape.",
        "Determines whether this backend is allowed to send a notification to\n        the given user and notice_type.",
        "Returns a dictionary with the format identifier as the key. The values are\n        are fully rendered templates with the given context.",
        "Copy the attributes from a source object to a destination object.",
        "Returns DataFrameRow of the DataFrame given its index.\n\n        :param idx: the index of the row in the DataFrame.\n        :return: returns a DataFrameRow",
        "The notice settings view.\n\n    Template: :template:`notification/notice_settings.html`\n\n    Context:\n\n        notice_types\n            A list of all :model:`notification.NoticeType` objects.\n\n        notice_settings\n            A dictionary containing ``column_headers`` for each ``NOTICE_MEDIA``\n            and ``rows`` containing a list of dictionaries: ``notice_type``, a\n            :model:`notification.NoticeType` object and ``cells``, a list of\n            tuples whose first value is suitable for use in forms and the second\n            value is ``True`` or ``False`` depending on a ``request.POST``\n            variable called ``form_label``, whose valid value is ``on``.",
        "Query Wolfram Alpha and return a Result object",
        "Return list of all Pod objects in result",
        "Find a node in the tree. If the node is not found it is added first and then returned.\n\n        :param args: a tuple\n        :return: returns the node",
        "Returns site-specific notification language for this user. Raises\n    LanguageStoreNotAvailable if this site does not use translated\n    notifications.",
        "Creates a new notice.\n\n    This is intended to be how other apps create new notices.\n\n    notification.send(user, \"friends_invite_sent\", {\n        \"spam\": \"eggs\",\n        \"foo\": \"bar\",\n    )",
        "A basic interface around both queue and send_now. This honors a global\n    flag NOTIFICATION_QUEUE_ALL that helps determine whether all calls should\n    be queued or not. A per call ``queue`` or ``now`` keyword argument can be\n    used to always override the default global behavior.",
        "Queue the notification in NoticeQueueBatch. This allows for large amounts\n    of user notifications to be deferred to a seperate process running outside\n    the webserver.",
        "A helper function to write lammps pair potentials to string. Assumes that\n    functions are vectorized.\n\n    Parameters\n    ----------\n    func: function\n       A function that will be evaluated for the force at each radius. Required to\n       be numpy vectorizable.\n    dfunc: function\n       Optional. A function that will be evaluated for the energy at each\n       radius. If not supplied the centered difference method will be\n       used. Required to be numpy vectorizable.\n    bounds: tuple, list\n       Optional. specifies min and max radius to evaluate the\n       potential. Default 1 length unit, 10 length unit.\n    samples: int\n       Number of points to evaluate potential. Default 1000. Note that\n       a low number of sample points will reduce accuracy.\n    tollerance: float\n       Value used to centered difference differentiation.\n    keyword: string\n       Lammps keyword to use to",
        "Write tersoff potential file from parameters to string\n\n    Parameters\n    ----------\n    parameters: dict\n       keys are tuple of elements with the values being the parameters length 14",
        "Aggregate the rows of each group into a single value.\n\n        :param clazz: name of a class that extends class Callable\n        :type clazz: class\n        :param new_col: name of the new column\n        :type new_col: str\n        :param args: list of column names of the object that\n         function should be applied to\n        :type args: varargs\n        :return: returns a new dataframe object with the aggregated value\n        :rtype: DataFrame",
        "Checks if elements of set2 are in set1.\n\n    :param set1: a set of values\n    :param set2: a set of values\n    :param warn: the error message that should be thrown\n     when the sets are NOT disjoint\n    :return: returns true no elements of set2 are in set1",
        "Checks if all elements from set2 are in set1.\n\n    :param set1:  a set of values\n    :param set2:  a set of values\n    :param warn: the error message that should be thrown \n     when the sets are not containd\n    :return: returns true if all values of set2 are in set1",
        "Serialize object back to XML string.\n\n        Returns:\n            str: String which should be same as original input, if everything\\\n                 works as expected.",
        "Parse MARC XML document to dicts, which are contained in\n        self.controlfields and self.datafields.\n\n        Args:\n            xml (str or HTMLElement): input data\n\n        Also detect if this is oai marc format or not (see elf.oai_marc).",
        "Parse control fields.\n\n        Args:\n            fields (list): list of HTMLElements\n            tag_id (str):  parameter name, which holds the information, about\n                           field name this is normally \"tag\", but in case of\n                           oai_marc \"id\".",
        "Parse data fields.\n\n        Args:\n            fields (list): of HTMLElements\n            tag_id (str): parameter name, which holds the information, about\n                          field name this is normally \"tag\", but in case of\n                          oai_marc \"id\"\n            sub_id (str): id of parameter, which holds informations about\n                          subfield name this is normally \"code\" but in case of\n                          oai_marc \"label\"",
        "This method is used mainly internally, but it can be handy if you work\n        with with raw MARC XML object and not using getters.\n\n        Args:\n            num (int): Which indicator you need (1/2).\n            is_oai (bool/None): If None, :attr:`.oai_marc` is\n                   used.\n\n        Returns:\n            str: current name of ``i1``/``ind1`` parameter based on \\\n                 :attr:`oai_marc` property.",
        "Return content of given `subfield` in `datafield`.\n\n        Args:\n            datafield (str): Section name (for example \"001\", \"100\", \"700\").\n            subfield (str):  Subfield name (for example \"a\", \"1\", etc..).\n            i1 (str, default None): Optional i1/ind1 parameter value, which\n               will be used for search.\n            i2 (str, default None): Optional i2/ind2 parameter value, which\n               will be used for search.\n            exception (bool): If ``True``, :exc:`~exceptions.KeyError` is\n                      raised when method couldn't found given `datafield` /\n                      `subfield`. If ``False``, blank array ``[]`` is returned.\n\n        Returns:\n            list: of :class:`.MARCSubrecord`.\n\n        Raises:\n            KeyError:",
        "Get the given param from each of the DOFs for a joint.",
        "Set the given param for each of the DOFs for a joint.",
        "Given an angle and an axis, create a quaternion.",
        "Given a set of bodies, compute their center of mass in world coordinates.",
        "Set the state of this body.\n\n        Parameters\n        ----------\n        state : BodyState tuple\n            The desired state of the body.",
        "Set the rotation of this body using a rotation matrix.\n\n        Parameters\n        ----------\n        rotation : sequence of 9 floats\n            The desired rotation matrix for this body.",
        "Convert a body-relative offset to world coordinates.\n\n        Parameters\n        ----------\n        position : 3-tuple of float\n            A tuple giving body-relative offsets.\n\n        Returns\n        -------\n        position : 3-tuple of float\n            A tuple giving the world coordinates of the given offset.",
        "Convert a point in world coordinates to a body-relative offset.\n\n        Parameters\n        ----------\n        position : 3-tuple of float\n            A world coordinates position.\n\n        Returns\n        -------\n        offset : 3-tuple of float\n            A tuple giving the body-relative offset of the given position.",
        "Convert a relative body offset to world coordinates.\n\n        Parameters\n        ----------\n        offset : 3-tuple of float\n            The offset of the desired point, given as a relative fraction of the\n            size of this body. For example, offset (0, 0, 0) is the center of\n            the body, while (0.5, -0.2, 0.1) describes a point halfway from the\n            center towards the maximum x-extent of the body, 20% of the way from\n            the center towards the minimum y-extent, and 10% of the way from the\n            center towards the maximum z-extent.\n\n        Returns\n        -------\n        position : 3-tuple of float\n            A position in world coordinates of the given body offset.",
        "Add a force to this body.\n\n        Parameters\n        ----------\n        force : 3-tuple of float\n            A vector giving the forces along each world or body coordinate axis.\n        relative : bool, optional\n            If False, the force values are assumed to be given in the world\n            coordinate frame. If True, they are assumed to be given in the\n            body-relative coordinate frame. Defaults to False.\n        position : 3-tuple of float, optional\n            If given, apply the force at this location in world coordinates.\n            Defaults to the current position of the body.\n        relative_position : 3-tuple of float, optional\n            If given, apply the force at this relative location on the body. If\n            given, this method ignores the ``position`` parameter.",
        "Add a torque to this body.\n\n        Parameters\n        ----------\n        force : 3-tuple of float\n            A vector giving the torque along each world or body coordinate axis.\n        relative : bool, optional\n            If False, the torque values are assumed to be given in the world\n            coordinate frame. If True, they are assumed to be given in the\n            body-relative coordinate frame. Defaults to False.",
        "Connect this body to another one using a joint.\n\n        This method creates a joint to fasten this body to the other one. See\n        :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body : :class:`Body` or str, optional\n            The other body to join with this one. If not given, connects this\n            body to the world.",
        "Move another body next to this one and join them together.\n\n        This method will move the ``other_body`` so that the anchor points for\n        the joint coincide. It then creates a joint to fasten the two bodies\n        together. See :func:`World.move_next_to` and :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body : :class:`Body` or str\n            The other body to join with this one.\n        offset : 3-tuple of float, optional\n            The body-relative offset where the anchor for the joint should be\n            placed. Defaults to (0, 0, 0). See :func:`World.move_next_to` for a\n            description of how offsets are specified.\n        other_offset : 3-tuple of float, optional\n",
        "List of positions for linear degrees of freedom.",
        "List of position rates for linear degrees of freedom.",
        "List of angles for rotational degrees of freedom.",
        "List of angle rates for rotational degrees of freedom.",
        "List of axes for this object's degrees of freedom.",
        "Set the lo stop values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        lo_stops : float or sequence of float\n            A lo stop value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians.",
        "Set the hi stop values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        hi_stops : float or sequence of float\n            A hi stop value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians.",
        "Set the target velocities for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        velocities : float or sequence of float\n            A target velocity value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians / second.",
        "Set the maximum forces for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        max_forces : float or sequence of float\n            A maximum force value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.",
        "Set the ERP values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        erps : float or sequence of float\n            An ERP value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.",
        "Set the CFM values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.",
        "Set the CFM values for this object's DOF limits.\n\n        Parameters\n        ----------\n        stop_cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom limits, or a list\n            containing one such value for each degree of freedom limit.",
        "Set the ERP values for this object's DOF limits.\n\n        Parameters\n        ----------\n        stop_erps : float or sequence of float\n            An ERP value to set on all degrees of freedom limits, or a list\n            containing one such value for each degree of freedom limit.",
        "Set the linear axis of displacement for this joint.\n\n        Parameters\n        ----------\n        axes : list containing one 3-tuple of floats\n            A list of the axes for this joint. For a slider joint, which has one\n            degree of freedom, this must contain one 3-tuple specifying the X,\n            Y, and Z axis for the joint.",
        "Set the angular axis of rotation for this joint.\n\n        Parameters\n        ----------\n        axes : list containing one 3-tuple of floats\n            A list of the axes for this joint. For a hinge joint, which has one\n            degree of freedom, this must contain one 3-tuple specifying the X,\n            Y, and Z axis for the joint.",
        "A list of axes of rotation for this joint.",
        "Create a new body.\n\n        Parameters\n        ----------\n        shape : str\n            The \"shape\" of the body to be created. This should name a type of\n            body object, e.g., \"box\" or \"cap\".\n        name : str, optional\n            The name to use for this body. If not given, a default name will be\n            constructed of the form \"{shape}{# of objects in the world}\".\n\n        Returns\n        -------\n        body : :class:`Body`\n            The created body object.",
        "Create a new joint that connects two bodies together.\n\n        Parameters\n        ----------\n        shape : str\n            The \"shape\" of the joint to use for joining together two bodies.\n            This should name a type of joint, such as \"ball\" or \"piston\".\n        body_a : str or :class:`Body`\n            The first body to join together with this joint. If a string is\n            given, it will be used as the name of a body to look up in the\n            world.\n        body_b : str or :class:`Body`, optional\n            If given, identifies the second body to join together with\n            ``body_a``. If not given, ``body_a`` is joined to the world.\n        name : str, optional\n            If given, use this name for the created joint. If not given, a name\n            will be constructed of the form\n            \"{",
        "Move one body to be near another one.\n\n        After moving, the location described by ``offset_a`` on ``body_a`` will\n        be coincident with the location described by ``offset_b`` on ``body_b``.\n\n        Parameters\n        ----------\n        body_a : str or :class:`Body`\n            The body to use as a reference for moving the other body. If this is\n            a string, it is treated as the name of a body to look up in the\n            world.\n        body_b : str or :class:`Body`\n            The body to move next to ``body_a``. If this is a string, it is\n            treated as the name of a body to look up in the world.\n        offset_a : 3-tuple of float\n            The offset of the anchor point, given as a relative fraction of the\n            size of ``body",
        "Set the states of some bodies in the world.\n\n        Parameters\n        ----------\n        states : sequence of states\n            A complete state tuple for one or more bodies in the world. See\n            :func:`get_body_states`.",
        "Step the world forward by one frame.\n\n        Parameters\n        ----------\n        substeps : int, optional\n            Split the step into this many sub-steps. This helps to prevent the\n            time delta for an update from being too large.",
        "Determine whether the given bodies are currently connected.\n\n        Parameters\n        ----------\n        body_a : str or :class:`Body`\n            One body to test for connectedness. If this is a string, it is\n            treated as the name of a body to look up.\n        body_b : str or :class:`Body`\n            One body to test for connectedness. If this is a string, it is\n            treated as the name of a body to look up.\n\n        Returns\n        -------\n        connected : bool\n            Return True iff the two bodies are connected.",
        "Parse an AMC motion capture data file.\n\n    Parameters\n    ----------\n    source : file\n        A file-like object that contains AMC motion capture text.\n\n    Yields\n    ------\n    frame : dict\n        Yields a series of motion capture frames. Each frame is a dictionary\n        that maps a bone name to a list of the DOF configurations for that bone.",
        "Traverse the bone hierarchy and create physics bodies.",
        "Traverse the bone hierarchy and create physics joints.",
        "Parse informations about corporations from given field identified\n        by `datafield` parameter.\n\n        Args:\n            datafield (str): MARC field ID (\"``110``\", \"``610``\", etc..)\n            subfield (str):  MARC subfield ID with name, which is typically\n                             stored in \"``a``\" subfield.\n            roles (str): specify which roles you need. Set to ``[\"any\"]`` for\n                         any role, ``[\"dst\"]`` for distributors, etc.. For\n                         details, see\n                         http://www.loc.gov/marc/relators/relaterm.html\n\n        Returns:\n            list: :class:`Corporation` objects.",
        "Parse persons from given datafield.\n\n        Args:\n            datafield (str): code of datafield (\"010\", \"730\", etc..)\n            subfield (char):  code of subfield (\"a\", \"z\", \"4\", etc..)\n            role (list of str): set to [\"any\"] for any role, [\"aut\"] for\n                 authors, etc.. For details see\n                 http://www.loc.gov/marc/relators/relaterm.html\n\n        Main records for persons are: \"100\", \"600\" and \"700\", subrecords \"c\".\n\n        Returns:\n            list: Person objects.",
        "Get list of VALID ISBN.\n\n        Returns:\n            list: List with *valid* ISBN strings.",
        "Content of field ``856u42``. Typically URL pointing to producers\n        homepage.\n\n        Returns:\n            list: List of URLs defined by producer.",
        "URL's, which may point to edeposit, aleph, kramerius and so on.\n\n        Fields ``856u40``, ``998a`` and ``URLu``.\n\n        Returns:\n            list: List of internal URLs.",
        "r'''Create a callable that implements a PID controller.\n\n    A PID controller returns a control signal :math:`u(t)` given a history of\n    error measurements :math:`e(0) \\dots e(t)`, using proportional (P), integral\n    (I), and derivative (D) terms, according to:\n\n    .. math::\n\n       u(t) = kp * e(t) + ki * \\int_{s=0}^t e(s) ds + kd * \\frac{de(s)}{ds}(t)\n\n    The proportional term is just the current error, the integral term is the\n    sum of all error measurements, and the derivative term is the instantaneous\n    derivative of the error measurement.\n\n    Parameters\n    ----------\n    kp : float\n        The weight associated with the proportional term of the PID controller.\n    ki",
        "Given a sequence of sequences, return a flat numpy array.\n\n    Parameters\n    ----------\n    iterables : sequence of sequence of number\n        A sequence of tuples or lists containing numbers. Typically these come\n        from something that represents each joint in a skeleton, like angle.\n\n    Returns\n    -------\n    ndarray :\n        An array of flattened data from each of the source iterables.",
        "Load a skeleton definition from a file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton. See :class:`pagoda.parser.Parser` for more\n            information about the format of the text file.",
        "Load a skeleton definition from a text file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton. See :class:`pagoda.parser.BodyParser` for\n            more information about the format of the text file.",
        "Load a skeleton definition from an ASF text file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton, in ASF format.",
        "Set PID parameters for all joints in the skeleton.\n\n        Parameters for this method are passed directly to the `pid` constructor.",
        "Get a list of all current joint torques in the skeleton.",
        "Get a list of the indices for a specific joint.\n\n        Parameters\n        ----------\n        name : str\n            The name of the joint to look up.\n\n        Returns\n        -------\n        list of int :\n            A list of the index values for quantities related to the named\n            joint. Often useful for getting, say, the angles for a specific\n            joint in the skeleton.",
        "Get a list of the indices for a specific body.\n\n        Parameters\n        ----------\n        name : str\n            The name of the body to look up.\n        step : int, optional\n            The number of numbers for each body. Defaults to 3, should be set\n            to 4 for body rotation (since quaternions have 4 values).\n\n        Returns\n        -------\n        list of int :\n            A list of the index values for quantities related to the named body.",
        "Get the current joint separations for the skeleton.\n\n        Returns\n        -------\n        distances : list of float\n            A list expressing the distance between the two joint anchor points,\n            for each joint in the skeleton. These quantities describe how\n            \"exploded\" the bodies in the skeleton are; a value of 0 indicates\n            that the constraints are perfectly satisfied for that joint.",
        "Enable the joint motors in this skeleton.\n\n        This method sets the maximum force that can be applied by each joint to\n        attain the desired target velocities. It also enables torque feedback\n        for all joint motors.\n\n        Parameters\n        ----------\n        max_force : float\n            The maximum force that each joint is allowed to apply to attain its\n            target velocity.",
        "Move each joint toward a target angle.\n\n        This method uses a PID controller to set a target angular velocity for\n        each degree of freedom in the skeleton, based on the difference between\n        the current and the target angle for the respective DOF.\n\n        PID parameters are by default set to achieve a tiny bit less than\n        complete convergence in one time step, using only the P term (i.e., the\n        P coefficient is set to 1 - \\delta, while I and D coefficients are set\n        to 0). PID parameters can be updated by calling the `set_pid_params`\n        method.\n\n        Parameters\n        ----------\n        angles : list of float\n            A list of the target angles for every joint in the skeleton.",
        "Add torques for each degree of freedom in the skeleton.\n\n        Parameters\n        ----------\n        torques : list of float\n            A list of the torques to add to each degree of freedom in the\n            skeleton.",
        "Return the names of our marker labels in canonical order.",
        "Load marker data from a CSV file.\n\n        The file will be imported using Pandas, which must be installed to use\n        this method. (``pip install pandas``)\n\n        The first line of the CSV file will be used for header information. The\n        \"time\" column will be used as the index for the data frame. There must\n        be columns named 'markerAB-foo-x','markerAB-foo-y','markerAB-foo-z', and\n        'markerAB-foo-c' for marker 'foo' to be included in the model.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the CSV file to load.",
        "Load marker data from a C3D file.\n\n        The file will be imported using the c3d module, which must be installed\n        to use this method. (``pip install c3d``)\n\n        Parameters\n        ----------\n        filename : str\n            Name of the C3D file to load.\n        start_frame : int, optional\n            Discard the first N frames. Defaults to 0.\n        max_frames : int, optional\n            Maximum number of frames to load. Defaults to loading all frames.",
        "Process data to produce velocity and dropout information.",
        "Create physics bodies corresponding to each marker in our data.",
        "Load attachment configuration from the given text source.\n\n        The attachment configuration file has a simple format. After discarding\n        Unix-style comments (any part of a line that starts with the pound (#)\n        character), each line in the file is then expected to have the following\n        format::\n\n            marker-name body-name X Y Z\n\n        The marker name must correspond to an existing \"channel\" in our marker\n        data. The body name must correspond to a rigid body in the skeleton. The\n        X, Y, and Z coordinates specify the body-relative offsets where the\n        marker should be attached: 0 corresponds to the center of the body along\n        the given axis, while -1 and 1 correspond to the minimal (maximal,\n        respectively) extent of the body's bounding box along the corresponding\n        dimension.\n\n        Parameters\n        ----------\n        source : str or file-like",
        "Attach marker bodies to the corresponding skeleton bodies.\n\n        Attachments are only made for markers that are not in a dropout state in\n        the given frame.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data we will use for attaching marker bodies.",
        "Reposition markers to a specific frame of data.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data where we should reposition marker bodies. Markers\n            will be positioned in the appropriate places in world coordinates.\n            In addition, linear velocities of the markers will be set according\n            to the data as long as there are no dropouts in neighboring frames.",
        "Get a list of the distances between markers and their attachments.\n\n        Returns\n        -------\n        distances : ndarray of shape (num-markers, 3)\n            Array of distances for each marker joint in our attachment setup. If\n            a marker does not currently have an associated joint (e.g. because\n            it is not currently visible) this will contain NaN for that row.",
        "Return an array of the forces exerted by marker springs.\n\n        Notes\n        -----\n\n        The forces exerted by the marker springs can be approximated by::\n\n          F = kp * dx\n\n        where ``dx`` is the current array of marker distances. An even more\n        accurate value is computed by approximating the velocity of the spring\n        displacement::\n\n          F = kp * dx + kd * (dx - dx_tm1) / dt\n\n        where ``dx_tm1`` is an array of distances from the previous time step.\n\n        Parameters\n        ----------\n        dx_tm1 : ndarray\n            An array of distances from markers to their attachment targets,\n            measured at the previous time step.\n\n        Returns\n        -------\n        F : ndarray\n            An array of forces that the markers are exerting on the skeleton.",
        "Create and configure a skeleton in our model.\n\n        Parameters\n        ----------\n        filename : str\n            The name of a file containing skeleton configuration data.\n        pid_params : dict, optional\n            If given, use this dictionary to set the PID controller\n            parameters on each joint in the skeleton. See\n            :func:`pagoda.skeleton.pid` for more information.",
        "Load marker data and attachment preferences into the model.\n\n        Parameters\n        ----------\n        filename : str\n            The name of a file containing marker data. This currently needs to\n            be either a .C3D or a .CSV file. CSV files must adhere to a fairly\n            strict column naming convention; see :func:`Markers.load_csv` for\n            more information.\n        attachments : str\n            The name of a text file specifying how markers are attached to\n            skeleton bodies.\n        max_frames : number, optional\n            Only read in this many frames of marker data. By default, the entire\n            data file is read into memory.\n\n        Returns\n        -------\n        markers : :class:`Markers`\n            Returns a markers object containing loaded marker data as well as\n            skeleton attachment configuration.",
        "Advance the physics world by one step.\n\n        Typically this is called as part of a :class:`pagoda.viewer.Viewer`, but\n        it can also be called manually (or some other stepping mechanism\n        entirely can be used).",
        "Settle the skeleton to our marker data at a specific frame.\n\n        Parameters\n        ----------\n        frame_no : int, optional\n            Settle the skeleton to marker data at this frame. Defaults to 0.\n        max_distance : float, optional\n            The settling process will stop when the mean marker distance falls\n            below this threshold. Defaults to 0.1m (10cm). Setting this too\n            small prevents the settling process from finishing (it will loop\n            indefinitely), and setting it too large prevents the skeleton from\n            settling to a stable state near the markers.\n        max_iters : int, optional\n            Attempt to settle markers for at most this many iterations. Defaults\n            to 1000.\n        states : list of body states, optional\n            If given, set the bodies in our skeleton to these kinematic states\n            before starting the settling process.",
        "Iterate over a set of marker data, dragging its skeleton along.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start following marker data after this frame. Defaults to 0.\n        end : int, optional\n            Stop following marker data after this frame. Defaults to the end of\n            the marker data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.",
        "Update the simulator to a specific frame of marker data.\n\n        This method returns a generator of body states for the skeleton! This\n        generator must be exhausted (e.g., by consuming this call in a for loop)\n        for the simulator to work properly.\n\n        This process involves the following steps:\n\n        - Move the markers to their new location:\n          - Detach from the skeleton\n          - Update marker locations\n          - Reattach to the skeleton\n        - Detect ODE collisions\n        - Yield the states of the bodies in the skeleton\n        - Advance the ODE world one step\n\n        Parameters\n        ----------\n        frame_no : int\n            Step to this frame of marker data.\n        dt : float, optional\n            Step with this time duration. Defaults to ``self.dt``.\n\n        Returns\n        -------\n        states : sequence of state tuples\n            A generator of",
        "Follow a set of marker data, yielding kinematic joint angles.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start following marker data after this frame. Defaults to 0.\n        end : int, optional\n            Stop following marker data after this frame. Defaults to the end of\n            the marker data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        max_force : float, optional\n            Allow each degree of freedom in the skeleton to exert at most this\n            force when attempting to maintain its equilibrium position. This\n            defaults to 20N. Set this value higher to simulate a stiff skeleton\n            while following marker data.\n\n        Returns\n        -------\n        angles : sequence of angle frames\n            Returns a generator of joint angle data for the skeleton.",
        "Follow a set of angle data, yielding dynamic joint torques.\n\n        Parameters\n        ----------\n        angles : ndarray (num-frames x num-dofs)\n            Follow angle data provided by this array of angle values.\n        start : int, optional\n            Start following angle data after this frame. Defaults to the start\n            of the angle data.\n        end : int, optional\n            Stop following angle data after this frame. Defaults to the end of\n            the angle data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        max_force : float, optional\n            Allow each degree of freedom in the skeleton to exert at most this\n            force when attempting to follow the given joint angles. Defaults to\n            100N. Setting this value to be large results in more accurate\n            following but",
        "Move the body according to a set of torque data.",
        "Sort values, but put numbers after alphabetically sorted words.\n\n    This function is here to make outputs diff-compatible with Aleph.\n\n    Example::\n        >>> sorted([\"b\", \"1\", \"a\"])\n        ['1', 'a', 'b']\n        >>> resorted([\"b\", \"1\", \"a\"])\n        ['a', 'b', '1']\n\n    Args:\n        values (iterable): any iterable object/list/tuple/whatever.\n\n    Returns:\n        list of sorted values, but with numbers after words",
        "Draw all bodies in the world.",
        "Get room stream to listen for messages.\n\n        Kwargs:\n            error_callback (func): Callback to call when an error occurred (parameters: exception)\n            live (bool): If True, issue a live stream, otherwise an offline stream\n\n        Returns:\n            :class:`Stream`. Stream",
        "Get list of users in the room.\n\n        Kwargs:\n            sort (bool): If True, sort rooms by name\n\n        Returns:\n            array. List of users",
        "Set the room name.\n\n        Args:\n            name (str): Name\n\n        Returns:\n            bool. Success",
        "Set the room topic.\n\n        Args:\n            topic (str): Topic\n\n        Returns:\n            bool. Success",
        "Post a message.\n\n        Args:\n            message (:class:`Message` or string): Message\n\n        Returns:\n            bool. Success",
        "Returns a list of paths specified by the XDG_CONFIG_DIRS environment\n        variable or the appropriate default.\n\n        The list is sorted by precedence, with the most important item coming\n        *last* (required by the existing config_resolver logic).",
        "Returns the value specified in the XDG_CONFIG_HOME environment variable\n        or the appropriate default.",
        "Returns the filename which is effectively used by the application. If\n        overridden by an environment variable, it will return that filename.",
        "Check if ``filename`` can be read. Will return boolean which is True if\n        the file can be read, False otherwise.",
        "Searches for an appropriate config file. If found, loads the file into\n        the current instance. This method can also be used to reload a\n        configuration. Note that you may want to set ``reload`` to ``True`` to\n        clear the configuration before loading in that case.  Without doing\n        that, values will remain available even if they have been removed from\n        the config files.\n\n        :param reload: if set to ``True``, the existing values are cleared\n                       before reloading.\n        :param require_load: If set to ``True`` this will raise a\n                             :py:exc:`IOError` if no config file has been found\n                             to load.",
        "Get styles.",
        "Create a connection with given settings.\n\n        Args:\n            settings (dict): A dictionary of settings\n\n        Returns:\n            :class:`Connection`. The connection",
        "Issue a PUT request.\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (dict): Dictionary of parameter and values\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n        \n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception",
        "Issue a POST request.\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (dict): Dictionary of parameter and values\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n            listener (func): callback called when uploading a file\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n        \n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception",
        "Issue a GET request.\n\n        Kwargs:\n            url (str): Destination URL\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n\n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception",
        "Get headers.\n\n        Returns:\n            tuple: Headers",
        "Get URL used for authentication\n\n        Returns:\n            string: URL",
        "Parses a response.\n\n        Args:\n            text (str): Text to parse\n\n        Kwargs:\n            key (str): Key to look for, if any\n\n        Returns:\n            Parsed value\n\n        Raises:\n            ValueError",
        "Build a request for twisted\n\n        Args:\n            method (str): Request method (GET/POST/PUT/DELETE/etc.) If not specified, it will be POST if post_data is not None\n            url (str): Destination URL (full, or relative)\n\n        Kwargs:\n            extra_headers (dict): Headers (override default connection headers, if any)\n            body_producer (:class:`twisted.web.iweb.IBodyProducer`): Object producing request body\n            full_url (bool): If False, URL is relative\n\n        Returns:\n            tuple. Tuple with two elements: reactor, and request",
        "Issue a request.\n\n        Args:\n            method (str): Request method (GET/POST/PUT/DELETE/etc.) If not specified, it will be POST if post_data is not None\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (str): A string of what to POST\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n            listener (func): callback called when uploading a file\n            full_return (bool): If set to True, get a full response (with success, data, info, body)\n\n        Returns:\n            dict. Response. If full_return==True, a dict with keys: success, data, info, body, otherwise the parsed data\n\n        Raises:\n",
        "Build destination URL.\n\n        Kwargs:\n            url (str): Destination URL\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            str. URL",
        "Tells if this message is a text message.\n\n        Returns:\n            bool. Success",
        "Get rooms list.\n\n        Kwargs:\n            sort (bool): If True, sort rooms by name\n\n        Returns:\n            array. List of rooms (each room is a dict)",
        "Get a room by name.\n\n        Returns:\n            :class:`Room`. Room\n\n        Raises:\n            RoomNotFoundException",
        "Get room.\n\n        Returns:\n            :class:`Room`. Room",
        "Get user.\n\n        Returns:\n            :class:`User`. User",
        "Search transcripts.\n\n        Args:\n            terms (str): Terms for search\n\n        Returns:\n            array. Messages",
        "Attach an observer.\n\n        Args:\n            observer (func): A function to be called when new messages arrive\n\n        Returns:\n            :class:`Stream`. Current instance to allow chaining",
        "Called when incoming messages arrive.\n\n        Args:\n            messages (tuple): Messages (each message is a dict)",
        "Fetch new messages.",
        "Called when new messages arrive.\n\n        Args:\n            messages (tuple): Messages",
        "Called when a connection is made, and used to send out headers",
        "Callback issued by twisted when new line arrives.\n\n        Args:\n            line (str): Incoming line",
        "Process data.\n\n        Args:\n            data (str): Incoming data",
        "Get a dictionary of CSL styles.",
        "Start producing.\n\n        Args:\n            consumer: Consumer",
        "Cleanup code after asked to stop producing.\n\n        Kwargs:\n            forced (bool): If True, we were forced to stop",
        "Send a block of bytes to the consumer.\n\n        Args:\n            block (str): Block of bytes",
        "Returns total length for this request.\n\n        Returns:\n            int. Length",
        "Build headers for each field.",
        "Returns the file size for given file field.\n\n        Args:\n            field (str): File field\n\n        Returns:\n            int. File size",
        "Generate a path value of type result_type.\n\n    result_type can either be bytes or text_type",
        "Given an ASCII str, returns a path of the given type.",
        "Generates a root component for a path.",
        "A strategy which generates filesystem path values.\n\n    The generated values include everything which the builtin\n    :func:`python:open` function accepts i.e. which won't lead to\n    :exc:`ValueError` or :exc:`TypeError` being raised.\n\n    Note that the range of the returned values depends on the operating\n    system, the Python version, and the filesystem encoding as returned by\n    :func:`sys.getfilesystemencoding`.\n\n    :param allow_pathlike:\n        If :obj:`python:None` makes the strategy include objects implementing\n        the :class:`python:os.PathLike` interface when Python >= 3.6 is used.\n        If :obj:`python:False` no pathlike objects will be generated. If\n        :obj:`python:True` pathlike will be generated (Python >= 3.6 required)\n\n    :type allow_pathlike: :obj:`",
        "exec compiled code",
        "replace all blocks in extends with current blocks",
        "flush all buffered string into code",
        "Add POST data.\n\n        Args:\n            data (dict): key => value dictionary",
        "Given some error text it will log the text if self.log_errors is True\n\n        :param text: Error text to log",
        "Processes the texts using TweeboParse and returns them in CoNLL format.\n\n        :param texts: The List of Strings to be processed by TweeboParse.\n        :param retry_count: The number of times it has retried for. Default\n                            0 does not require setting, main purpose is for\n                            recursion.\n        :return: A list of CoNLL formated strings.\n        :raises ServerError: Caused when the server is not running.\n        :raises :py:class:`requests.exceptions.HTTPError`: Caused when the\n                input texts is not formated correctly e.g. When you give it a\n                String not a list of Strings.\n        :raises :py:class:`json.JSONDecodeError`: Caused if after self.retries\n                attempts to parse the data it cannot decode the data.\n\n        :Example:",
        "Set entity data\n\n        Args:\n            data (dict): Entity data\n            datetime_fields (array): Fields that should be parsed as datetimes",
        "validates XML text",
        "validates XML name",
        "Prepare the actors, the world, and the messaging system to begin \n        playing the game.\n        \n        This method is guaranteed to be called exactly once upon entering the \n        game stage.",
        "Sequentially update the actors, the world, and the messaging system.  \n        The theater terminates once all of the actors indicate that they are done.",
        "Give the actors, the world, and the messaging system a chance to react \n        to the end of the game.",
        "Template variables.",
        "Use when application is starting.",
        "Catch a connection asyncrounosly.",
        "Initialize self.",
        "Asyncronously wait for a connection from the pool.",
        "Release waiters.",
        "Listen for an id from the server.\n\n        At the beginning of a game, each client receives an IdFactory from the \n        server.  This factory are used to give id numbers that are guaranteed \n        to be unique to tokens that created locally.  This method checks to see if such \n        a factory has been received.  If it hasn't, this method does not block \n        and immediately returns False.  If it has, this method returns True \n        after saving the factory internally.  At this point it is safe to enter \n        the GameStage.",
        "Respond when the server indicates that the client is out of sync.\n\n        The server can request a sync when this client sends a message that \n        fails the check() on the server.  If the reason for the failure isn't \n        very serious, then the server can decide to send it as usual in the \n        interest of a smooth gameplay experience.  When this happens, the \n        server sends out an extra response providing the clients with the\n        information they need to resync themselves.",
        "Manage the response when the server rejects a message.\n\n        An undo is when required this client sends a message that the server \n        refuses to pass on to the other clients playing the game.  When this \n        happens, the client must undo the changes that the message made to the \n        world before being sent or crash.  Note that unlike sync requests, undo \n        requests are only reported to the client that sent the offending \n        message.",
        "Relay messages from the forum on the server to the client represented \n        by this actor.",
        "Create a new DataItem.",
        "Raise an ApiUsageError if the given object is not a token that is currently \n    participating in the game.  To be participating in the game, the given \n    token must have an id number and be associated with the world.",
        "Iterate through each member of the class being created and add a \n        safety check to every method that isn't marked as read-only.",
        "Register the given callback to be called whenever the method with the \n        given name is called.  You can easily take advantage of this feature in \n        token extensions by using the @watch_token decorator.",
        "Clear all the internal data the token needed while it was part of \n        the world.\n\n        Note that this method doesn't actually remove the token from the \n        world.  That's what World._remove_token() does.  This method is just \n        responsible for setting the internal state of the token being removed.",
        "Allow tokens to modify the world for the duration of a with-block.\n\n        It's important that tokens only modify the world at appropriate times, \n        otherwise the changes they make may not be communicated across the \n        network to other clients.  To help catch and prevent these kinds of \n        errors, the game engine keeps the world locked most of the time and \n        only briefly unlocks it (using this method) when tokens are allowed to \n        make changes.  When the world is locked, token methods that aren't \n        marked as being read-only can't be called.  When the world is unlocked, \n        any token method can be called.  These checks can be disabled by \n        running python with optimization enabled.\n\n        You should never call this method manually from within your own game.  \n        This method is intended to be used by the game engine, which was \n        carefully designed to allow the world to",
        "Converts XML tree to event generator",
        "Converts events stream into lXML tree",
        "Parses file content into events stream",
        "selects sub-tree events",
        "merges each run of successive text events into one text event",
        "locates ENTER peer for each EXIT object. Convenient when selectively\n    filtering out XML markup",
        "construct BusinessDate instance from datetime.date instance,\n        raise ValueError exception if not possible\n\n        :param datetime.date datetime_date: calendar day\n        :return bool:",
        "construct datetime.date instance represented calendar date of BusinessDate instance\n\n        :return datetime.date:",
        "addition of a period object\n\n        :param BusinessDate d:\n        :param p:\n        :type p: BusinessPeriod or str\n        :param list holiday_obj:\n        :return bankdate:",
        "addition of a number of months\n\n        :param BusinessDate d:\n        :param int month_int:\n        :return bankdate:",
        "private method for the addition of business days, used in the addition of a BusinessPeriod only\n\n        :param BusinessDate d:\n        :param int days_int:\n        :param list holiday_obj:\n        :return: BusinessDate",
        "Parses as much as possible until it encounters a matching closing quote.\n    \n    By default matches any_token, but can be provided with a more specific parser if required.\n    Returns a string",
        "returns number of days for the given year and month\n\n    :param int year: calendar year\n    :param int month: calendar month\n    :return int:",
        "Initialize the application.",
        "Register connection's middleware and prepare self database.",
        "Close all connections.",
        "Register a model in self.",
        "Manage a database connection.",
        "Write your migrations here.\n\n    > Model = migrator.orm['name']\n\n    > migrator.sql(sql)\n    > migrator.create_table(Model)\n    > migrator.drop_table(Model, cascade=True)\n    > migrator.add_columns(Model, **fields)\n    > migrator.change_columns(Model, **fields)\n    > migrator.drop_columns(Model, *field_names, cascade=True)\n    > migrator.rename_column(Model, old_field_name, new_field_name)\n    > migrator.rename_table(Model, new_table_name)\n    > migrator.add_index(Model, *col_names, unique=False)\n    > migrator.drop_index(Model, index_name)\n    > migrator.add_not_null(Model, field_name)\n    > migrator.drop",
        "Runs a series of parsers in sequence passing the result of each parser to the next.\n    The result of the last parser is returned.",
        "Returns the current token if is found in the collection provided.\n    \n    Fails otherwise.",
        "Returns the current token if it is not found in the collection provided.\n    \n    The negative of one_of.",
        "Returns the current token if it satisfies the guard function provided.\n    \n    Fails otherwise.\n    This is the a generalisation of one_of.",
        "Succeeds if the given parser cannot consume input",
        "Applies the parser to input zero or more times.\n    \n    Returns a list of parser results.",
        "Consumes as many of these as it can until it term is encountered.\n    \n    Returns a tuple of the list of these results and the term result",
        "Like many_until but must consume at least one of these.",
        "Like sep but must consume at least one of parser.",
        "fills the internal buffer from the source iterator",
        "Advances to and returns the next token or returns EndOfFile",
        "Run a game being developed with the kxg game engine.\n\nUsage:\n    {exe_name} sandbox [<num_ais>] [-v...]\n    {exe_name} client [--host HOST] [--port PORT] [-v...]\n    {exe_name} server <num_guis> [<num_ais>] [--host HOST] [--port PORT] [-v...] \n    {exe_name} debug <num_guis> [<num_ais>] [--host HOST] [--port PORT] [-v...]\n    {exe_name} --help\n\nCommands:\n    sandbox\n        Play a single-player game with the specified number of AIs.  None of \n        the multiplayer machinery will be used.\n\n    client\n        Launch a client that will try to connect to a server on the given",
        "Poll the queues that the worker can use to communicate with the \n        supervisor, until all the workers are done and all the queues are \n        empty.  Handle messages as they appear.",
        "Return database field type.",
        "Parse value from database.",
        "Parse the fsapi endpoint from the device url.",
        "Create a session on the frontier silicon device.",
        "Execute a frontier silicon API call.",
        "Helper method for setting a value by using the fsapi API.",
        "Helper method for fetching a text value.",
        "Helper method for fetching a integer value.",
        "Helper method for fetching a long value. Result is integer.",
        "Check if the device is on.",
        "Power on or off the device.",
        "Get the modes supported by this device.",
        "Read the maximum volume level of the device.",
        "Check if the device is muted.",
        "Mute or unmute the device.",
        "Get the play status of the device.",
        "Get the equaliser modes supported by this device.",
        "Set device sleep timer.",
        "Assumes that start and stop are already in 'buffer' coordinates. value is a byte iterable.\n        value_len is fractional.",
        "Parse genotype from VCF line data",
        "toIndex - An optional method which will return the value prepped for index.\n\n\t\t\tBy default, \"toStorage\" will be called. If you provide \"hashIndex=True\" on the constructor,\n\t\t\tthe field will be md5summed for indexing purposes. This is useful for large strings, etc.",
        "copy - Create a copy of this IRField.\n\n\t\t\t  Each subclass should implement this, as you'll need to pass in the args to constructor.\n\n\t\t\t@return <IRField (or subclass)> - Another IRField that has all the same values as this one.",
        "objHasUnsavedChanges - Check if any object has unsaved changes, cascading.",
        "Check that a value has a certain JSON type.\n\n    Raise TypeError if the type does not match.\n\n    Supported types: str, int, float, bool, list, dict, and None.\n    float will match any number, int will only match numbers without\n    fractional part.\n\n    The special type JList(x) will match a list value where each\n    item is of type x:\n\n    >>> assert_json_type([1, 2, 3], JList(int))",
        "Load json or yaml data from file handle.\n\n        Args:\n            fh (file): File handle to load from.\n\n        Examlple:\n            >>> with open('data.json', 'r') as json:\n            >>>    jsdata = composite.load(json)\n            >>>\n            >>> with open('data.yml', 'r') as yml:\n            >>>    ymldata = composite.load(yml)",
        "Load json from file handle.\n\n        Args:\n            fh (file): File handle to load from.\n\n        Examlple:\n            >>> with open('data.json', 'r') as json:\n            >>>    data = composite.load(json)",
        "Recursively compute intersection of data. For dictionaries, items\n        for specific keys will be reduced to unique items. For lists, items\n        will be reduced to unique items. This method is meant to be analogous\n        to set.intersection for composite objects.\n\n        Args:\n            other (composite): Other composite object to intersect with.\n            recursive (bool): Whether or not to perform the operation recursively,\n                for all nested composite objects.",
        "Recursively compute union of data. For dictionaries, items\n        for specific keys will be combined into a list, depending on the\n        status of the overwrite= parameter. For lists, items will be appended\n        and reduced to unique items. This method is meant to be analogous\n        to set.union for composite objects.\n\n        Args:\n            other (composite): Other composite object to union with.\n            recursive (bool): Whether or not to perform the operation recursively,\n                for all nested composite objects.\n            overwrite (bool): Whether or not to overwrite entries with the same\n                key in a nested dictionary.",
        "Append to object, if object is list.",
        "Extend list from object, if object is list.",
        "Write composite object to file handle in JSON format.\n\n        Args:\n            fh (file): File handle to write to.\n            pretty (bool): Sort keys and indent in output.",
        "Return list of files in filetree.",
        "Prune leaves of filetree according to specified\n        regular expression.\n\n        Args:\n            regex (str): Regular expression to use in pruning tree.",
        "Returns the value this reference is pointing to. This method uses 'ctx' to resolve the reference and return\n        the value this reference references.\n        If the call was already made, it returns a cached result.\n        It also makes sure there's no cyclic reference, and if so raises CyclicReferenceError.",
        "delete - Delete all objects in this list.\n\n\t\t\t@return <int> - Number of objects deleted",
        "save - Save all objects in this list",
        "reload - Reload all objects in this list. \n\t\t\t\tUpdates in-place. To just fetch all these objects again, use \"refetch\"\n\n\t\t\t@return - List (same order as current objects) of either exception (KeyError) if operation failed,\n\t\t\t  or a dict of fields changed -> (old, new)",
        "refetch - Fetch a fresh copy of all items in this list.\n\t\t\t\tReturns a new list. To update in-place, use \"reload\".\n\n\t\t\t@return IRQueryableList<IndexedRedisModel> - List of fetched items",
        "Renders as a str",
        "Returns the elements HTML start tag",
        "Returns a repr of an object and falls back to a minimal representation of type and ID if the call to repr raised\n    an error.\n\n    :param obj: object to safe repr\n    :returns: repr string or '(type<id> repr error)' string\n    :rtype: str",
        "Match a genome VCF to variants in the ClinVar VCF file\n\n    Acts as a generator, yielding tuples of:\n    (ClinVarVCFLine, ClinVarAllele, zygosity)\n\n    'zygosity' is a string and corresponds to the genome's zygosity for that\n    ClinVarAllele. It can be either: 'Het' (heterozygous), 'Hom' (homozygous),\n    or 'Hem' (hemizygous, e.g. X chromosome in XY individuals).",
        "Return Allele data as dict object.",
        "Create list of Alleles from VCF line data",
        "Parse the VCF info field",
        "Dict representation of parsed VCF data",
        "Very lightweight parsing of a vcf line to get position.\n\n        Returns a dict containing:\n        'chrom': index of chromosome (int), indicates sort order\n        'pos': position on chromosome (int)",
        "_toStorage - Convert the value to a string representation for storage.\n\n\t\t\t@param value - The value of the item to convert\n\t\t\t@return A string value suitable for storing.",
        "Navigate an open ftplib.FTP to appropriate directory for ClinVar VCF files.\n\n    Args:\n        ftp:   (type: ftplib.FTP) an open connection to ftp.ncbi.nlm.nih.gov\n        build: (type: string) genome build, either 'b37' or 'b38'",
        "Return ClinVarAllele data as dict object.",
        "Parse frequency data in ClinVar VCF",
        "Parse alleles for ClinVar VCF, overrides parent method.",
        "Returns back a class decorator that enables registering Blox to this factory",
        "Decorator for warning user of depricated functions before use.\n\n    Args:\n        newmethod (str): Name of method to use instead.",
        "setDefaultRedisConnectionParams - Sets the default parameters used when connecting to Redis.\n\n\t\t  This should be the args to redis.Redis in dict (kwargs) form.\n\n\t\t  @param connectionParams <dict> - A dict of connection parameters.\n\t\t    Common keys are:\n\n\t\t       host <str> - hostname/ip of Redis server (default '127.0.0.1')\n\t\t       port <int> - Port number\t\t\t(default 6379)\n\t\t       db  <int>  - Redis DB number\t\t(default 0)\n\n\t\t   Omitting any of those keys will ensure the default value listed is used.\n\n\t\t  This connection info will be used by default for all connections to Redis, unless explicitly set otherwise.\n\t\t  The common way to override is to define REDIS_CONNECTION_PARAMS on a model, or use AltConnectedModel = MyModel.connectAlt",
        "clearRedisPools - Disconnect all managed connection pools, \n\t\t   and clear the connectiobn_pool attribute on all stored managed connection pools.\n\n\t\t   A \"managed\" connection pool is one where REDIS_CONNECTION_PARAMS does not define the \"connection_pool\" attribute.\n\t\t   If you define your own pools, IndexedRedis will use them and leave them alone.\n\n\t\t  This method will be called automatically after calling setDefaultRedisConnectionParams.\n\n\t\t  Otherwise, you shouldn't have to call it.. Maybe as some sort of disaster-recovery call..",
        "getRedisPool - Returns and possibly also creates a Redis connection pool\n\t\t\tbased on the REDIS_CONNECTION_PARAMS passed in.\n\n\t\t\tThe goal of this method is to keep a small connection pool rolling\n\t\t\tto each unique Redis instance, otherwise during network issues etc\n\t\t\tpython-redis will leak connections and in short-order can exhaust\n\t\t\tall the ports on a system. There's probably also some minor\n\t\t\tperformance gain in sharing Pools.\n\n\t\t\tWill modify \"params\", if \"host\" and/or \"port\" are missing, will fill\n\t\t\tthem in with defaults, and prior to return will set \"connection_pool\"\n\t\t\ton params, which will allow immediate return on the next call,\n\t\t\tand allow access to the pool directly from the model object.\n\n\t\t\t@param params <dict> - REDIS_CONNECTION_PARAMS - kwargs to redis",
        "pprint - Pretty-print a dict representation of this object.\n\n\t\t\t@param stream <file/None> - Either a stream to output, or None to default to sys.stdout",
        "hasUnsavedChanges - Check if any unsaved changes are present in this model, or if it has never been saved.\n\n\t\t\t@param cascadeObjects <bool> default False, if True will check if any foreign linked objects themselves have unsaved changes (recursively).\n\t\t\t\tOtherwise, will just check if the pk has changed.\n\n\t\t\t@return <bool> - True if any fields have changed since last fetch, or if never saved. Otherwise, False",
        "diff - Compare the field values on two IndexedRedisModels.\n\n\t\t\t@param firstObj < IndexedRedisModel instance> - First object (or self)\n\n\t\t\t@param otherObj < IndexedRedisModel instance> - Second object\n\n\t\t\t@param includeMeta <bool> - If meta information (like pk) should be in the diff results.\n\n\n\t\t\t@return <dict> - Dict of  'field' : ( value_firstObjForField, value_otherObjForField ).\n\t\t\t\t\n\t\t\t\tKeys are names of fields with different values.\n\t\t\t\tValue is a tuple of ( value_firstObjForField, value_otherObjForField )\n\n\t\t\tCan be called statically, like: IndexedRedisModel.diff ( obj1, obj2 )\n\n\t\t\t  or in reference to an obj   : obj1.diff(obj2)",
        "save - Save this object.\n\t\t\t\n\t\t\tWill perform an \"insert\" if this object had not been saved before,\n\t\t\t  otherwise will update JUST the fields changed on THIS INSTANCE of the model.\n\n\t\t\t  i.e. If you have two processes fetch the same object and change different fields, they will not overwrite\n\t\t\t  eachother, but only save the ones each process changed.\n\n\t\t\tIf you want to save multiple objects of type MyModel in a single transaction,\n\t\t\tand you have those objects in a list, myObjs, you can do the following:\n\n\t\t\t\tMyModel.saver.save(myObjs)\n\n\t\t\t@param cascadeSave <bool> Default True - If True, any Foreign models linked as attributes that have been altered\n\t\t\t   or created will be saved with this object. If False, only this object (and the reference to an already-saved foreign model",
        "hasSameValues - Check if this and another model have the same fields and values.\n\n\t\t\tThis does NOT include id, so the models can have the same values but be different objects in the database.\n\n\t\t\t@param other <IndexedRedisModel> - Another model\n\n\t\t\t@param cascadeObject <bool> default True - If True, foreign link values with changes will be considered a difference.\n\t\t\t\tOtherwise, only the immediate values are checked.\n\n\t\t\t@return <bool> - True if all fields have the same value, otherwise False",
        "copy - Copies this object.\n\n                    @param copyPrimaryKey <bool> default False - If True, any changes to the copy will save over-top the existing entry in Redis.\n                        If False, only the data is copied, and nothing is saved.\n\n\t\t    @param copyValues <bool> default False - If True, every field value on this object will be explicitly copied. If False,\n\t\t      an object will be created with the same values, and depending on the type may share the same reference.\n\t\t      \n\t\t      This is the difference between a copy and a deepcopy.\n\n\t            @return <IndexedRedisModel> - Copy of this object, per above\n\n\t\t    If you need a copy that IS linked, @see IndexedRedisModel.copy",
        "saveToExternal - Saves this object to a different Redis than that specified by REDIS_CONNECTION_PARAMS on this model.\n\n\t\t\t@param redisCon <dict/redis.Redis> - Either a dict of connection params, a la REDIS_CONNECTION_PARAMS, or an existing Redis connection.\n\t\t\t\tIf you are doing a lot of bulk copies, it is recommended that you create a Redis connection and pass it in rather than establish a new\n\t\t\t\tconnection with each call.\n\n\t\t\t@note - You will generate a new primary key relative to the external Redis environment. If you need to reference a \"shared\" primary key, it is better\n\t\t\t\t\tto use an indexed field than the internal pk.",
        "reload - Reload this object from the database, overriding any local changes and merging in any updates.\n\n\n\t\t    @param cascadeObjects <bool> Default True. If True, foreign-linked objects will be reloaded if their values have changed\n\t\t      since last save/fetch. If False, only if the pk changed will the foreign linked objects be reloaded.\n\n                    @raises KeyError - if this object has not been saved (no primary key)\n\n                    @return - Dict with the keys that were updated. Key is field name that was updated,\n       and value is tuple of (old value, new value). \n\n\t\t    NOTE: Currently, this will cause a fetch of all Foreign Link objects, one level",
        "copyModel - Copy this model, and return that copy.\n\n\t\t\t  The copied model will have all the same data, but will have a fresh instance of the FIELDS array and all members,\n\t\t\t    and the INDEXED_FIELDS array.\n\t\t\t  \n\t\t\t  This is useful for converting, like changing field types or whatever, where you can load from one model and save into the other.\n\n\t\t\t@return <IndexedRedisModel> - A copy class of this model class with a unique name.",
        "connectAlt - Create a class of this model which will use an alternate connection than the one specified by REDIS_CONNECTION_PARAMS on this model.\n\n\t\t\t@param redisConnectionParams <dict> - Dictionary of arguments to redis.Redis, same as REDIS_CONNECTION_PARAMS.\n\n\t\t\t@return - A class that can be used in all the same ways as the existing IndexedRedisModel, but that connects to a different instance.\n\n\t\t\t  The fields and key will be the same here, but the connection will be different. use #copyModel if you want an independent class for the model",
        "_get_new_connection - Get a new connection\n\t\t\tinternal",
        "_get_connection - Maybe get a new connection, or reuse if passed in.\n\t\t\t\tWill share a connection with a model\n\t\t\tinternal",
        "_add_id_to_keys - Adds primary key to table\n\t\t\tinternal",
        "_rem_id_from_keys - Remove primary key from table\n\t\t\tinternal",
        "_add_id_to_index - Adds an id to an index\n\t\t\tinternal",
        "_rem_id_from_index - Removes an id from an index\n\t\t\tinternal",
        "_get_key_for_index - Returns the key name that would hold the indexes on a value\n\t\t\tInternal - does not validate that indexedFields is actually indexed. Trusts you. Don't let it down.\n\n\t\t\t@param indexedField - string of field name\n\t\t\t@param val - Value of field\n\n\t\t\t@return - Key name string, potentially hashed.",
        "_compat_rem_str_id_from_index - Used in compat_convertHashedIndexes to remove the old string repr of a field,\n\t\t\t\tin order to later add the hashed value,",
        "_peekNextID - Look at, but don't increment the primary key for this model.\n\t\t\t\tInternal.\n\n\t\t\t@return int - next pk",
        "Internal for handling filters; the guts of .filter and .filterInline",
        "count - gets the number of records matching the filter criteria\n\n\t\t\tExample:\n\t\t\t\ttheCount = Model.objects.filter(field1='value').count()",
        "exists - Tests whether a record holding the given primary key exists.\n\n\t\t\t@param pk - Primary key (see getPk method)\n\n\t\t\tExample usage: Waiting for an object to be deleted without fetching the object or running a filter. \n\n\t\t\tThis is a very cheap operation.\n\n\t\t\t@return <bool> - True if object with given pk exists, otherwise False",
        "getPrimaryKeys - Returns all primary keys matching current filterset.\n\n\t\t\t@param sortByAge <bool> - If False, return will be a set and may not be ordered.\n\t\t\t\tIf True, return will be a list and is guarenteed to represent objects oldest->newest\n\n\t\t\t@return <set> - A set of all primary keys associated with current filters.",
        "all - Get the underlying objects which match the filter criteria.\n\n\t\t\tExample:   objs = Model.objects.filter(field1='value', field2='value2').all()\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@return - Objects of the Model instance associated with this query.",
        "allOnlyFields - Get the objects which match the filter criteria, only fetching given fields.\n\n\t\t\t@param fields - List of fields to fetch\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\n\t\t\t@return - Partial objects with only the given fields fetched",
        "allOnlyIndexedFields - Get the objects which match the filter criteria, only fetching indexed fields.\n\n\t\t\t@return - Partial objects with only the indexed fields fetched",
        "Random - Returns a random record in current filterset.\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@return - Instance of Model object, or None if no items math current filters",
        "delete - Deletes all entries matching the filter criteria",
        "get - Get a single value with the internal primary key.\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@param pk - internal primary key (can be found via .getPk() on an item)",
        "_doCascadeFetch - Takes an object and performs a cascading fetch on all foreign links, and all theirs, and so on.\n\n\t\t\t@param obj <IndexedRedisModel> - A fetched model",
        "getMultiple - Gets multiple objects with a single atomic operation\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@param pks - list of internal keys",
        "getOnlyFields - Gets only certain fields from a paticular primary key. For working on entire filter set, see allOnlyFields\n\n\t\t\t@param pk <int> - Primary Key\n\n\t\t\t@param fields list<str> - List of fields\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\n\t\t\treturn - Partial objects with only fields applied",
        "getMultipleOnlyFields - Gets only certain fields from a list of  primary keys. For working on entire filter set, see allOnlyFields\n\n\t\t\t@param pks list<str> - Primary Keys\n\n\t\t\t@param fields list<str> - List of fields\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\treturn - List of partial objects with only fields applied",
        "compat_convertHashedIndexes - Reindex fields, used for when you change the propery \"hashIndex\" on one or more fields.\n\n\t\t\tFor each field, this will delete both the hash and unhashed keys to an object, \n\t\t\t  and then save a hashed or unhashed value, depending on that field's value for \"hashIndex\".\n\n\t\t\tFor an IndexedRedisModel class named \"MyModel\", call as \"MyModel.objects.compat_convertHashedIndexes()\"\n\n\t\t\tNOTE: This works one object at a time (regardless of #fetchAll), so that an unhashable object does not trash all data.\n\n\t\t\tThis method is intended to be used while your application is offline,\n\t\t\t  as it doesn't make sense to be changing your model while applications are actively using it.\n\n\t\t\t@param fetchAll <bool>, Default True - If True, all",
        "_doSave - Internal function to save a single object. Don't call this directly. \n\t\t\t            Use \"save\" instead.\n\n\t\t\t  If a pipeline is provided, the operations (setting values, updating indexes, etc)\n\t\t\t    will be queued into that pipeline.\n\t\t\t  Otherwise, everything will be executed right away.\n\n\t\t\t  @param obj - Object to save\n\t\t\t  @param isInsert - Bool, if insert or update. Either way, obj._id is expected to be set.\n\t\t\t  @param conn - Redis connection\n\t\t\t  @param pipeline - Optional pipeline, if present the items will be queued onto it. Otherwise, go directly to conn.",
        "compat_convertHashedIndexes - Reindex all fields for the provided objects, where the field value is hashed or not.\n\t\t\tIf the field is unhashable, do not allow.\n\n\t\t\tNOTE: This works one object at a time. It is intended to be used while your application is offline,\n\t\t\t  as it doesn't make sense to be changing your model while applications are actively using it.\n\n\t\t\t@param objs <IndexedRedisModel objects to convert>\n\t\t\t@param conn <redis.Redis or None> - Specific Redis connection or None to reuse.",
        "deleteOne - Delete one object\n\n\t\t\t@param obj - object to delete\n\t\t\t@param conn - Connection to reuse, or None\n\n\t\t\t@return - number of items deleted (0 or 1)",
        "deleteByPk - Delete object associated with given primary key",
        "deleteMultiple - Delete multiple objects\n\n\t\t\t@param objs - List of objects\n\n\t\t\t@return - Number of objects deleted",
        "deleteMultipleByPks - Delete multiple objects given their primary keys\n\n\t\t\t@param pks - List of primary keys\n\n\t\t\t@return - Number of objects deleted",
        "Returns a blox template from an html string",
        "Returns a blox template from a file stream object",
        "Returns a blox template from a valid file path",
        "Accumulate all dictionary and named arguments as\n    keyword argument dictionary. This is generally useful for\n    functions that try to automatically resolve inputs.\n\n    Examples:\n        >>> @keywords\n        >>> def test(*args, **kwargs):\n        >>>     return kwargs\n        >>>\n        >>> print test({'one': 1}, two=2)\n        {'one': 1, 'two': 2}",
        "getCompressMod - Return the module used for compression on this field\n\n\t\t\t@return <module> - The module for compression",
        "toBytes - Convert a value to bytes using the encoding specified on this field\n\n\t\t\t@param value <str> - The field to convert to bytes\n\n\t\t\t@return <bytes> - The object encoded using the codec specified on this field.\n\n\t\t\tNOTE: This method may go away.",
        "Like functools.partial but instead of using the new kwargs, keeps the old ones.",
        "Callable to configure Bokeh's show method when a proxy must be\n    configured.\n\n    If port is None we're asking about the URL\n    for the origin header.",
        "Called at the start of notebook execution to setup the environment.\n\n    This will configure bokeh, and setup the logging library to be\n    reasonable.",
        "Creates a overview of the hosts per range.",
        "Create an OrderedDict\n\n    :param hierarchy: a dictionary\n    :param level: single key\n    :return: deeper dictionary",
        "Groups line reference together\n\n    :param text: Text object\n    :type text: MyCapytains.resources.text.api\n    :param getreffs: Callback function to retrieve text\n    :type getreffs: function(level)\n    :param lines: Number of lines to use by group\n    :type lines: int\n    :return: List of grouped urn references with their human readable version\n    :rtype: [(str, str)]",
        "Chunk a text at the passage level\n\n    :param text: Text object\n    :type text: MyCapytains.resources.text.api\n    :param getreffs: Callback function to retrieve text\n    :type getreffs: function(level)\n    :return: List of urn references with their human readable version\n    :rtype: [(str, str)]",
        "Create a numpy.ndarray with all observed fields and\n    computed teff and luminosity values.",
        "Return the numpy array with rounded teff and luminosity columns.",
        "Checks the arguments to brutefore and spawns greenlets to perform the bruteforcing.",
        "Given a cluster create a Bokeh plot figure using the\n    cluster's image.",
        "Returns rounded teff and luminosity lists.",
        "Given a cluster create a Bokeh plot figure creating an\n    H-R diagram.",
        "Given a numpy array calculate what the ranges of the H-R\n    diagram should be.",
        "Given a numpy array create a Bokeh plot figure creating an\n    H-R diagram.",
        "Filter the cluster data catalog into the filtered_data\n        catalog, which is what is shown in the H-R diagram.\n\n        Filter on the values of the sliders, as well as the lasso\n        selection in the skyviewer.",
        "Creates a tempfile and starts the given editor, returns the data afterwards.",
        "This functions gives the user a way to change the data that is given as input.",
        "Performs a bruteforce for the given users, password, domain on the given host.",
        "Set the access and modified times of the file specified by path.",
        "Strip \\\\?\\ prefix in init phase",
        "Return the path always without the \\\\?\\ prefix.",
        "Formats the output of another tool in the given way.\n        Has default styles for ranges, hosts and services.",
        "Print the given line to stdout",
        "Gets the IP from the inet interfaces.",
        "Create a pandas DataFrame from a numpy ndarray.\n\n    By default use temp and lum with max rows of 32 and precision of 2.\n\n    arr - An numpy.ndarray.\n    columns - The columns to include in the pandas DataFrame. Defaults to\n              temperature and luminosity.\n    names - The column names for the pandas DataFrame. Defaults to\n            Temperature and Luminosity.\n    max_rows - If max_rows is an integer then set the pandas\n               display.max_rows option to that value. If max_rows\n               is True then set display.max_rows option  to 1000.\n    precision - An integer to set the pandas precision option.",
        "Strips labels.",
        "Remove namespace in the passed document in place.",
        "Check to see if this URI is retrievable by this Retriever implementation\n\n        :param uri: the URI of the resource to be retrieved\n        :type uri: str\n        :return: True if it can be, False if not\n        :rtype: bool",
        "Decorator used to tag a method that should be used as a hook for the\n  specified `name` hook type.",
        "Subscribes `callable` to listen to events of `name` type. The\n    parameters passed to `callable` are dependent on the specific\n    event being triggered.",
        "Configures this engine based on the options array passed into\n    `argv`. If `argv` is ``None``, then ``sys.argv`` is used instead.\n    During configuration, the command line options are merged with\n    previously stored values. Then the logging subsystem and the\n    database model are initialized, and all storable settings are\n    serialized to configurations files.",
        "Alias for _assemble_with_columns",
        "Execute a query with provided parameters \n\n        Parameters\n        :query:     SQL string with parameter placeholders\n        :commit:    If True, the query will commit\n        :returns:   List of rows",
        "Handle provided columns and if necessary, convert columns to a list for \n        internal strage.\n\n        :columns: A sequence of columns for the table. Can be list, comma\n            -delimited string, or IntEnum.",
        "Execute a DML query \n\n        :sql_string:    An SQL string template\n        :*args:         Arguments to be passed for query parameters.\n        :commit:        Whether or not to commit the transaction after the query\n        :returns:       Psycopg2 result",
        "Execute a SELECT statement \n\n        :sql_string:    An SQL string template\n        :columns:       A list of columns to be returned by the query\n        :*args:         Arguments to be passed for query parameters.\n        :returns:       Psycopg2 result",
        "Retreive a single record from the table.  Lots of reasons this might be\n        best implemented in the model\n\n        :pk:            The primary key ID for the record\n        :returns:       List of single result",
        "Creates the final payload based on the x86 and x64 meterpreters.",
        "Combines the files 1 and 2 into 3.",
        "Runs the checker.py scripts to detect the os.",
        "Starts the exploiting phase, you should run setup before running this function.\n            if auto is set, this function will fire the exploit to all systems. Otherwise a curses interface is shown.",
        "Exploits a single ip, exploit is based on the given operating system.",
        "Create server instance with an optional WebSocket handler\n\n    For pure WebSocket server ``app`` may be ``None`` but an attempt to access\n    any path other than ``ws_path`` will cause server error.\n    \n    :param host: hostname or IP\n    :type host: str\n    :param port: server port\n    :type port: int\n    :param app: WSGI application\n    :param server_class: WSGI server class, defaults to AsyncWsgiServer\n    :param handler_class: WSGI handler class, defaults to AsyncWsgiHandler\n    :param ws_handler_class: WebSocket hanlder class, defaults to ``None``\n    :param ws_path: WebSocket path on the server, defaults to '/ws'\n    :type ws_path: str, optional\n    :return: initialized server instance",
        "Poll active sockets once\n\n        This method can be used to allow aborting server polling loop\n        on some condition.\n\n        :param timeout: polling timeout",
        "Start serving HTTP requests\n\n        This method blocks the current thread.\n\n        :param poll_interval: polling timeout\n        :return:",
        "write triples into a translation file.",
        "write triples to file.",
        "Returns protobuf mapcontainer. Read from translation file.",
        "Returns map with entity or relations from plain text.",
        "Prints an overview of the tags of the hosts.",
        "Main credentials tool",
        "Provides an overview of the duplicate credentials.",
        "Register nemo and parses annotations\n\n        .. note:: Process parses the annotation and extends informations about the target URNs by retrieving resource in range\n\n        :param nemo: Nemo",
        "Starts the loop to provide the data from jackal.",
        "Creates a search query based on the section of the config file.",
        "Creates the workers based on the given configfile to provide named pipes in the directory.",
        "Loads the config and handles the workers.",
        "Replace isocode by its language equivalent\n\n    :param isocode: Three character long language code\n    :param lang: Lang in which to return the language name\n    :return: Full Text Language Name",
        "A function to construct a hierarchical dictionary representing the different citation layers of a text\n\n    :param reffs: passage references with human-readable equivalent\n    :type reffs: [(str, str)]\n    :param citation: Main Citation\n    :type citation: Citation\n    :return: nested dictionary representing where keys represent the names of the levels and the final values represent the passage reference\n    :rtype: OrderedDict",
        "Take a string of form %citation_type|passage% and format it for human\n\n    :param string: String of formation %citation_type|passage%\n    :param lang: Language to translate to\n    :return: Human Readable string\n\n    .. note :: To Do : Use i18n tools and provide real i18n",
        "Annotation filtering filter\n\n    :param annotations: List of annotations\n    :type annotations: [AnnotationResource]\n    :param type_uri: URI Type on which to filter\n    :type type_uri: str\n    :param number: Number of the annotation to return\n    :type number: int\n    :return: Annotation(s) matching the request\n    :rtype: [AnnotationResource] or AnnotationResource",
        "Connect to a service to see if it is a http or https server.",
        "Retrieves services starts check_service in a gevent pool of 100.",
        "Imports the given nmap result.",
        "Start an nmap process with the given args on the given ips.",
        "Scans the given hosts with nmap.",
        "Scans available smb services in the database for smb signing and ms17-010.",
        "Function to create an overview of the services.\n        Will print a list of ports found an the number of times the port was seen.",
        "Rename endpoint function name to avoid conflict when namespacing is set to true\n\n    :param fn_name: Name of the route function\n    :param instance: Instance bound to the function\n    :return: Name of the new namespaced function name",
        "Retrieve the best matching locale using request headers\n\n        .. note:: Probably one of the thing to enhance quickly.\n\n        :rtype: str",
        "Transform input according to potentially registered XSLT\n\n        .. note:: Since 1.0.0, transform takes an objectId parameter which represent the passage which is called\n\n        .. note:: Due to XSLT not being able to be used twice, we rexsltise the xml at every call of xslt\n\n        .. warning:: Until a C libxslt error is fixed ( https://bugzilla.gnome.org/show_bug.cgi?id=620102 ), \\\n        it is not possible to use strip tags in the xslt given to this application\n\n        :param work: Work object containing metadata about the xml\n        :type work: MyCapytains.resources.inventory.Text\n        :param xml: XML to transform\n        :type xml: etree._Element\n        :param objectId: Object Identifier\n        :type objectId: str\n        :param subreference: Subreference\n        :type subreference:",
        "Request the api endpoint to retrieve information about the inventory\n\n        :return: Main Collection\n        :rtype: Collection",
        "Retrieve and transform a list of references.\n\n        Returns the inventory collection object with its metadata and a callback function taking a level parameter \\\n        and returning a list of strings.\n\n        :param objectId: Collection Identifier\n        :type objectId: str\n        :param subreference: Subreference from which to retrieve children\n        :type subreference: str\n        :param collection: Collection object bearing metadata\n        :type collection: Collection\n        :param export_collection: Return collection metadata\n        :type export_collection: bool\n        :return: Returns either the list of references, or the text collection object with its references as tuple\n        :rtype: (Collection, [str]) or [str]",
        "Retrieve the passage identified by the parameters\n\n        :param objectId: Collection Identifier\n        :type objectId: str\n        :param subreference: Subreference of the passage\n        :type subreference: str\n        :return: An object bearing metadata and its text\n        :rtype: InteractiveTextualNode",
        "Get siblings of a browsed subreference\n\n        .. note:: Since 1.0.0c, there is no more prevnext dict. Nemo uses the list of original\\\n        chunked references to retrieve next and previous, or simply relies on the resolver to get siblings\\\n        when the subreference is not found in given original chunks.\n\n        :param objectId: Id of the object\n        :param subreference: Subreference of the object\n        :param passage: Current Passage\n        :return: Previous and next references\n        :rtype: (str, str)",
        "Generates a SEO friendly string for given collection\n\n        :param collection: Collection object to generate string for\n        :param parent: Current collection parent\n        :return: SEO/URL Friendly string",
        "Creates a CoINS Title string from information\n\n        :param collection: Collection to create coins from\n        :param text: Text/Passage object\n        :param subreference: Subreference\n        :param lang: Locale information\n        :return: Coins HTML title value",
        "Build an ancestor or descendant dict view based on selected information\n\n        :param member: Current Member to build for\n        :param collection: Collection from which we retrieved it\n        :param lang: Language to express data in\n        :return:",
        "Build member list for given collection\n\n        :param collection: Collection to build dict view of for its members\n        :param lang: Language to express data in\n        :return: List of basic objects",
        "Build parents list for given collection\n\n        :param collection: Collection to build dict view of for its members\n        :param lang: Language to express data in\n        :return: List of basic objects",
        "Retrieve the top collections of the inventory\n\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Collections information and template\n        :rtype: {str: Any}",
        "Collection content browsing route function\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Template and collections contained in given collection\n        :rtype: {str: Any}",
        "Text exemplar references browsing route function\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type str\n        :return: Template and required information about text with its references",
        "Provides a redirect to the first passage of given objectId\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :return: Redirection to the first passage of given text",
        "Retrieve the text of the passage\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :param subreference: Reference identifier\n        :type subreference: str\n        :return: Template, collections metadata and Markup object representing the text\n        :rtype: {str: Any}",
        "Route for specific assets.\n\n        :param filetype: Asset Type\n        :param asset: Filename of an asset\n        :return: Response",
        "Merge and register assets, both as routes and dictionary\n\n        :return: None",
        "Create blueprint and register rules\n\n        :return: Blueprint of the current nemo app\n        :rtype: flask.Blueprint",
        "Create a view\n\n        :param name: Name of the route function to use for the view.\n        :type name: str\n        :return: Route function which makes use of Nemo context (such as menu informations)\n        :rtype: function",
        "Retrieve main parent collections of a repository\n\n        :param lang: Language to retrieve information in\n        :return: Sorted collections representations",
        "This function is built to provide cache keys for templates\n\n        :param endpoint: Current endpoint\n        :param kwargs: Keyword Arguments\n        :return: tuple of i18n dependant cache key and i18n ignoring cache key\n        :rtype: tuple(str)",
        "Render a route template and adds information to this route.\n\n        :param template: Template name.\n        :type template: str\n        :param kwargs: dictionary of named arguments used to be passed to the template\n        :type kwargs: dict\n        :return: Http Response with rendered template\n        :rtype: flask.Response",
        "Register the app using Blueprint\n\n        :return: Nemo blueprint\n        :rtype: flask.Blueprint",
        "Register filters for Jinja to use\n\n       .. note::  Extends the dictionary filters of jinja_env using self._filters list",
        "Register plugins in Nemo instance\n\n        - Clear routes first if asked by one plugin\n        - Clear assets if asked by one plugin and replace by the last plugin registered static_folder\n        - Register each plugin\n            - Append plugin routes to registered routes\n            - Append plugin filters to registered filters\n            - Append templates directory to given namespaces\n            - Append assets (CSS, JS, statics) to given resources \n            - Append render view (if exists) to Nemo.render stack",
        "Handle a list of references depending on the text identifier using the chunker dictionary.\n\n        :param text: Text object from which comes the references\n        :type text: MyCapytains.resources.texts.api.Text\n        :param reffs: List of references to transform\n        :type reffs: References\n        :return: Transformed list of references\n        :rtype: [str]",
        "Obtains the data from the pipe and appends the given tag.",
        "Creates the section value if it does not exists and sets the value.\n            Use write_config to actually set the value.",
        "This function tries to retrieve the value from the configfile\n            otherwise will return a default.",
        "Returns the configuration directory",
        "Write the current config to disk to store them.",
        "Track the specified remote branch if it is not already tracked.",
        "Checkout, update and branch from the specified branch.",
        "Returns the interface name of the first not link_local and not loopback interface.",
        "load_targets will load the services with smb signing disabled and if ldap is enabled the services with the ldap port open.",
        "write_targets will write the contents of ips and ldap_strings to the targets_file.",
        "Starts the ntlmrelayx.py and responder processes.\n            Assumes you have these programs in your path.",
        "Function that gets called on each event from pyinotify.",
        "Watches directory for changes",
        "Terminate the processes.",
        "This function waits for the relay and responding processes to exit.\n            Captures KeyboardInterrupt to shutdown these processes.",
        "Retrieve annotations from the query provider\n\n        :param targets: The CTS URN(s) to query as the target of annotations\n        :type targets: [MyCapytain.common.reference.URN], URN or None\n        :param wildcard: Wildcard specifier for how to match the URN\n        :type wildcard: str\n        :param include: URI(s) of Annotation types to include in the results\n        :type include: list(str)\n        :param exclude: URI(s) of Annotation types to include in the results\n        :type exclude: list(str)\n        :param limit: The max number of results to return (Default is None for no limit)\n        :type limit: int\n        :param start: the starting record to return (Default is 1)\n        :type start: int \n        :param expand: Flag to state whether Annotations are expanded (Default is False)\n",
        "Make breadcrumbs for a route\n\n        :param kwargs: dictionary of named arguments used to construct the view\n        :type kwargs: dict\n        :return: List of dict items the view can use to construct the link.\n        :rtype: {str: list({ \"link\": str, \"title\", str, \"args\", dict})}",
        "This function obtains hosts from core and starts a nessus scan on these hosts.\n        The nessus tag is appended to the host tags.",
        "Retrieves the uuid of the given template name.",
        "Creates a scan with the given host ips\n            Returns the scan id of the created object.",
        "Starts the scan identified by the scan_id.s",
        "Bases the comparison of the datastores on URI alone.",
        "Adds a tag to the list of tags and makes sure the result list contains only unique results.",
        "Removes a tag from this object",
        "Returns the result as a dictionary, provide the include_meta flag to als show information like index and doctype.",
        "Route to retrieve annotations by target\n\n        :param target_urn: The CTS URN for which to retrieve annotations  \n        :type target_urn: str\n        :return: a JSON string containing count and list of resources\n        :rtype: {str: Any}",
        "Returns the label for a given Enum key",
        "Returns the verbose name for a given enum value",
        "Returns the configured DNS servers with the use f nmcli.",
        "Tries to perform a zone transfer.",
        "Resolves the list of domains and returns the ips.",
        "Parses the list of ips, turns these into ranges based on the netmask given.\n        Set include_public to True to include public IP adresses.",
        "Creates a connection based upon the given configuration object.",
        "Searches the elasticsearch instance to retrieve the requested documents.",
        "Uses the command line arguments to fill the search function and call it.",
        "Returns the number of results after filtering with the given arguments.",
        "Uses the command line arguments to fill the count function and call it.",
        "Returns a generator that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.",
        "Resolves an ip adres to a range object, creating it if it doesn't exists.",
        "Argparser option with search functionality specific for ranges.",
        "Searches elasticsearch for objects with the same address, protocol, port and state.",
        "Resolves the given id to a user object, if it doesn't exists it will be created.",
        "Retrieves the domains of the users from elastic.",
        "Returns a list that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.",
        "Consumes an ET protocol tree and converts it to state.Command commands",
        "Initializes the indices",
        "Parse the entry into a computer object.",
        "Parse the file and extract the computers, import the computers that resolve into jackal.",
        "Parses a single entry from the domaindump",
        "Parses the domain users and groups files.",
        "Parses ldapdomaindump files and stores hosts and users in elasticsearch.",
        "Make an autocomplete API request\n\n    This can be used to find cities and/or hurricanes by name\n\n    :param string query: city\n    :param string country: restrict search to a specific country. Must be a two letter country code\n    :param boolean hurricanes: whether to search for hurricanes or not\n    :param boolean cities: whether to search for cities or not\n    :param integer timeout: timeout of the api request\n    :returns: result of the autocomplete API request\n    :rtype: dict",
        "Make an API request\n\n    :param string key: API key to use\n    :param list features: features to request. It must be a subset of :data:`FEATURES`\n    :param string query: query to send\n    :param integer timeout: timeout of the request\n    :returns: result of the API request\n    :rtype: dict",
        "Try to convert a string to unicode using different encodings",
        "Handle HTTP GET requests on an authentication endpoint.\n\n    Authentication flow begins when ``params`` has a ``login`` key with a value\n    of ``start``. For instance, ``/auth/twitter?login=start``.\n\n    :param str provider: An provider to obtain a user ID from.\n    :param str request_url: The authentication endpoint/callback.\n    :param dict params: GET parameters from the query string.\n    :param str token_secret: An app secret to encode/decode JSON web tokens.\n    :param str token_cookie: The current JSON web token, if available.\n    :return: A dict containing any of the following possible keys:\n\n        ``status``: an HTTP status code the server should sent\n\n        ``redirect``: where the client should be directed to continue the flow\n\n        ``set_token_cookie``: contains a JSON web token and should be stored",
        "Method to call to get a serializable object for json.dump or jsonify based on the target\n\n        :return: dict",
        "Read the contents of the Annotation Resource\n\n        :return: the contents of the resource\n        :rtype: str or bytes or flask.response",
        "index all triples into indexes and return their mappings",
        "recover triples from mapping.",
        "Transform triple index into a 1-D numpy array.",
        "Packs a list of triple indexes into a 2D numpy array.",
        "If entity pairs in a relation is as close as another relations, only keep one relation of such set.",
        "Remove direct links in the training sets.",
        "Uses a union find to find segment.",
        "Create a usable data structure for serializing.",
        "Logs an operation done on an entity, possibly with other arguments",
        "Logs a new state of an entity",
        "Logs an update done on an entity",
        "Logs an error",
        "Decorator that provides a dictionary cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.DICT) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side dictionary cursor",
        "Decorator that provides a cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor() coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side cursor",
        "Decorator that provides a namedtuple cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side namedtuple cursor",
        "Provides a transacted cursor which will run in autocommit=false mode\n\n    For any exception the transaction will be rolled back.\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side transacted named cursor",
        "gives the number of records in the table\n\n        Args:\n            table: a string indicating the name of the table\n\n        Returns:\n            an integer indicating the number of records in the table",
        "Creates an insert statement with only chosen fields\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n\n        Returns:\n            A 'Record' object with table columns as properties",
        "Creates an update query with only chosen fields\n        Supports only a single field where clause\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted",
        "Creates a delete query with where keys\n        Supports multiple where clause with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted",
        "Creates a select query for selective columns with where keys\n        Supports multiple where claus with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            order_by: a string indicating column name to order the results on\n            columns: list of columns to select from\n            where_keys: list of dictionary\n            limit: the limit on the number of results\n            offset: offset on the results\n\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and across dictionaries get 'OR'-ed\n\n        Returns:\n            A list of 'Record' object with table columns",
        "Run a raw sql query\n\n        Args:\n            query : query string to execute\n            values : tuple of values to be used with the query\n\n        Returns:\n            result of query as list of named tuple",
        "This method is used to append content of the `text`\n    argument to the `out` argument.\n\n    Depending on how many lines in the text, a\n    padding can be added to all lines except the first\n    one.\n\n    Concatenation result is appended to the `out` argument.",
        "This function should return unicode representation of the value",
        "Helper function to traverse an element tree rooted at element, yielding nodes matching the query.",
        "Given a simplified XPath query string, returns an array of normalized query parts.",
        "Inserts a new element as a child of this element, before the specified index or sibling.\n\n        :param before: An :class:`XmlElement` or a numeric index to insert the new node before\n        :param name: The tag name to add\n        :param attrs: Attributes for the new tag\n        :param data: CDATA for the new tag\n        :returns: The newly-created element\n        :rtype: :class:`XmlElement`",
        "A generator yielding children of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :param reverse: If ``True``, children will be yielded in reverse declaration order",
        "Helper function to determine if this node matches the given predicate.",
        "Returns a canonical path to this element, relative to the root node.\n\n        :param include_root: If ``True``, include the root node in the path. Defaults to ``False``.",
        "Recursively find any descendants of this node with the given tag name. If a tag name is omitted, this will\n        yield every descendant node.\n\n        :param name: If specified, only consider elements with this tag name\n        :returns: A generator yielding descendants of this node",
        "Returns the last child of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`",
        "Yields all parents of this element, back to the root element.\n\n        :param name: If specified, only consider elements with this tag name",
        "Returns the next sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`",
        "Returns the previous sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`",
        "Parses the HTML table into a list of dictionaries, each of which\n        represents a single observation.",
        "Calculates cache key based on `args` and `kwargs`.\n    `args` and `kwargs` must be instances of hashable types.",
        "Cache result of function execution into the django cache backend.\n    Calculate cache key based on `prefix`, `args` and `kwargs` of the function.\n    For using like object method set `method=True`.",
        "Wrapper around Django's ORM `get` functionality.\n    Wrap anything that raises ObjectDoesNotExist exception\n    and provide the default value if necessary.\n    `default` by default is None. `default` can be any callable,\n    if it is callable it will be called when ObjectDoesNotExist\n    exception will be raised.",
        "Turn column inputs from user into list of simple numbers.\n\n    Inputs can be:\n\n      - individual number: 1\n      - range: 1-3\n      - comma separated list: 1,2,3,4-6",
        "Return only the part of the row which should be printed.",
        "Writes a single observation to the output file.\n\n        If the ``observation_data`` parameter is a dictionary, it is\n        converted to a list to keep a consisted field order (as described\n        in format specification). Otherwise it is assumed that the data\n        is a raw record ready to be written to file.\n\n        :param observation_data: a single observation as a dictionary or list",
        "Takes a dictionary of observation data and converts it to a list\n        of fields according to AAVSO visual format specification.\n\n        :param cls: current class\n        :param observation_data: a single observation as a dictionary",
        "Converts a raw input record to a dictionary of observation data.\n\n        :param cls: current class\n        :param row: a single observation as a list or tuple",
        "Get the name of the view function used to prevent having to set the tag\n    manually for every endpoint",
        "Downloads all variable star observations by a given observer.\n\n    Performs a series of HTTP requests to AAVSO's WebObs search and\n    downloads the results page by page. Each page is then passed to\n    :py:class:`~pyaavso.parsers.webobs.WebObsResultsParser` and parse results\n    are added to the final observation list.",
        "Generates likely unique image path using md5 hashes",
        "Extract, transform, and load metadata from Lander-based projects.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional",
        "Upsert the technote resource into the projectmeta MongoDB collection.\n\n    Parameters\n    ----------\n    collection : `motor.motor_asyncio.AsyncIOMotorCollection`\n        The MongoDB collection.\n    jsonld : `dict`\n        The JSON-LD document that represents the document resource.",
        "Converts a Open511 JSON document to XML.\n\n    lang: the appropriate language code\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    Accepts only the full root-level JSON object from an Open511 response.",
        "Converts a Open511 JSON fragment to XML.\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    This won't provide a conforming document if you pass in a full JSON document;\n    it's for translating little fragments, and is mostly used internally.",
        "Given a dict deserialized from a GeoJSON object, returns an lxml Element\n    of the corresponding GML geometry.",
        "Transform a GEOS or OGR geometry object into an lxml Element\n    for the GML geometry.",
        "Delete latex comments from TeX source.\n\n    Parameters\n    ----------\n    tex_source : str\n        TeX source content.\n\n    Returns\n    -------\n    tex_source : str\n        TeX source without comments.",
        "r\"\"\"Replace macros in the TeX source with their content.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros. See\n        `lsstprojectmeta.tex.scraper.get_macros`.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source with known macros replaced.\n\n    Notes\n    -----\n    Macros with arguments are not supported.\n\n    Examples\n    --------\n    >>> macros = {r'\\handle': 'LDM-nnn'}\n    >>> sample = r'This is document \\handle.'\n    >>> replace_macros(sample, macros)\n    'This is document LDM-nnn.'\n\n   ",
        "Ensures that the provided document is an lxml Element or json dict.",
        "Convert an Open511 document between formats.\n    input_doc - either an lxml open511 Element or a deserialized JSON dict\n    output_format - short string name of a valid output format, as listed above",
        "Construct an `LsstLatexDoc` instance by reading and parsing the\n        LaTeX source.\n\n        Parameters\n        ----------\n        root_tex_path : `str`\n            Path to the LaTeX source on the filesystem. For multi-file LaTeX\n            projects this should be the path to the root document.\n\n        Notes\n        -----\n        This method implements the following pipeline:\n\n        1. `lsstprojectmeta.tex.normalizer.read_tex_file`\n        2. `lsstprojectmeta.tex.scraper.get_macros`\n        3. `lsstprojectmeta.tex.normalizer.replace_macros`\n\n        Thus ``input`` and ``includes`` are resolved along with simple macros.",
        "Get the document content in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content.",
        "Get the document title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document.",
        "Get the document short title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the short title is not available in\n            the document.",
        "Get the document abstract in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document.",
        "Get the document authors in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `list` of `str`\n            Sequence of author names in the specified output markup format.",
        "Parse documentclass options.\n\n        Sets the the ``_document_options`` attribute.",
        "Parse the title from TeX source.\n\n        Sets these attributes:\n\n        - ``_title``\n        - ``_short_title``",
        "Parse the document handle.\n\n        Sets the ``_series``, ``_serial``, and ``_handle`` attributes.",
        "r\"\"\"Parse the author from TeX source.\n\n        Sets the ``_authors`` attribute.\n\n        Goal is to parse::\n\n           \\author{\n           A.~Author,\n           B.~Author,\n           and\n           C.~Author}\n\n        Into::\n\n           ['A. Author', 'B. Author', 'C. Author']",
        "Parse the abstract from the TeX source.\n\n        Sets the ``_abstract`` attribute.",
        "Process a LaTeX snippet of content for better transformation\n        with pandoc.\n\n        Currently runs the CitationLinker to convert BibTeX citations to\n        href links.",
        "r\"\"\"Load the BibTeX bibliography referenced by the document.\n\n        This method triggered by the `bib_db` attribute and populates the\n        `_bib_db` private attribute.\n\n        The ``\\bibliography`` command is parsed to identify the bibliographies\n        referenced by the document.",
        "r\"\"\"Parse the ``\\date`` command, falling back to getting the\n        most recent Git commit date and the current datetime.\n\n        Result is available from the `revision_datetime` attribute.",
        "Create a JSON-LD representation of this LSST LaTeX document.\n\n        Parameters\n        ----------\n        url : `str`, optional\n            URL where this document is published to the web. Prefer\n            the LSST the Docs URL if possible.\n            Example: ``'https://ldm-151.lsst.io'``.\n        code_url : `str`, optional\n            Path the the document's repository, typically on GitHub.\n            Example: ``'https://github.com/lsst/LDM-151'``.\n        ci_url : `str`, optional\n            Path to the continuous integration service dashboard for this\n            document's repository.\n            Example: ``'https://travis-ci.org/lsst/LDM-151'``.\n        readme_url : `str`, optional\n            URL to the document repository's README file. Example:\n",
        "Renames an existing database.",
        "Returns True if database server is running, False otherwise.",
        "Saves the state of a database to a file.\n\n        Parameters\n        ----------\n        name: str\n            the database to be backed up.\n        filename: str\n            path to a file where database backup will be written.",
        "Loads state of a backup file to a database.\n\n        Note\n        ----\n        If database name does not exist, it will be created.\n\n        Parameters\n        ----------\n        name: str\n            the database to which backup will be restored.\n        filename: str\n            path to a file contain a postgres database backup.",
        "Provides a connection string for database.\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection string (e.g. 'dbname=db1 user=user1 host=localhost port=5432')",
        "Provides a connection string for database as a sqlalchemy compatible URL.\n\n        NB - this doesn't include special arguments related to SSL connectivity (which are outside the scope\n        of the connection URL format).\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection URL (e.g. postgresql://user1@localhost:5432/db1)",
        "Connects the database client shell to the database.\n\n        Parameters\n        ----------\n        expect_module: str\n            the database to which backup will be restored.",
        "Returns settings from the server.",
        "Say something in the morning",
        "Say something in the afternoon",
        "Say something in the evening",
        "Command line entrypoint to reduce technote metadata.",
        "Run a pipeline to process extract, transform, and load metadata for\n    multiple LSST the Docs-hosted projects\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    product_urls : `list` of `str`\n        List of LSST the Docs product URLs.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records.",
        "Ingest any kind of LSST document hosted on LSST the Docs from its\n    source.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_url : `str`\n        URL of the technote's product resource in the LTD Keeper API.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.",
        "Allows a decorator to be called with or without keyword arguments.",
        "Create a GitHub token for an integration installation.\n\n    Parameters\n    ----------\n    installation_id : `int`\n        Installation ID. This is available in the URL of the integration's\n        **installation** ID.\n    integration_jwt : `bytes`\n        The integration's JSON Web Token (JWT). You can create this with\n        `create_jwt`.\n\n    Returns\n    -------\n    token_obj : `dict`\n        GitHub token object. Includes the fields:\n\n        - ``token``: the token string itself.\n        - ``expires_at``: date time string when the token expires.\n\n    Example\n    -------\n    The typical workflow for authenticating to an integration installation is:\n\n    .. code-block:: python\n\n       from dochubadapter.github import auth\n       jwt = auth.create_jwt(integration_id, private_key_",
        "Create a JSON Web Token to authenticate a GitHub Integration or\n    installation.\n\n    Parameters\n    ----------\n    integration_id : `int`\n        Integration ID. This is available from the GitHub integration's\n        homepage.\n    private_key_path : `str`\n        Path to the integration's private key (a ``.pem`` file).\n\n    Returns\n    -------\n    jwt : `bytes`\n        JSON Web Token that is good for 9 minutes.\n\n    Notes\n    -----\n    The JWT is encoded with the RS256 algorithm. It includes a payload with\n    fields:\n\n    - ``'iat'``: The current time, as an `int` timestamp.\n    - ``'exp'``: Expiration time, as an `int timestamp. The expiration\n      time is set of 9 minutes in the future (maximum allowance is 10 minutes).\n    - ``'iss'``",
        "r\"\"\"Get all macro definitions from TeX source, supporting multiple\n    declaration patterns.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    This function uses the following function to scrape macros of different\n    types:\n\n    - `get_def_macros`\n    - `get_newcommand_macros`\n\n    This macro scraping has the following caveats:\n\n    - Macro definition (including content) must all occur on one line.\n    - Macros with arguments are not supported.",
        "r\"\"\"Get all ``\\def`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\def`` macros with arguments are not supported.",
        "r\"\"\"Get all ``\\newcommand`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\newcommand`` macros with arguments are not supported.",
        "Try to load and return a module\n\n    Will add DIRECTORY_NAME to sys.path and tries to import MODULE_NAME.\n\n    For example:\n    load(\"~/.yaz\", \"yaz_extension\")",
        "Makes a naive datetime.datetime in a given time zone aware.",
        "Makes an aware datetime.datetime naive in a given time zone.",
        "Converts a datetime to the timezone of this Schedule.",
        "Returns the next Period this event is in effect, or None if the event\n        has no remaining periods.",
        "Returns an iterator of Period tuples for every day this event is in effect, between range_start\n        and range_end.",
        "Returns an iterator of Period tuples for continuous stretches of time during\n        which this event is in effect, between range_start and range_end.",
        "Does this schedule include the provided time?\n        query_date and query_time are date and time objects, interpreted\n        in this schedule's timezone",
        "Returns an iterator of Period tuples for every day this schedule is in effect, between range_start\n        and range_end.",
        "A Period tuple representing the daily start and end time.",
        "A set of integers representing the weekdays the schedule recurs on,\n        with Monday = 0 and Sunday = 6.",
        "A context manager that creates a temporary database.\n\n    Useful for automated tests.\n\n    Parameters\n    ----------\n    db: object\n        a preconfigured DB object\n    name: str, optional\n        name of the database to be created. (default: globally unique name)",
        "Asynchronously request a URL and get the encoded text content of the\n    body.\n\n    Parameters\n    ----------\n    url : `str`\n        URL to download.\n    session : `aiohttp.ClientSession`\n        An open aiohttp session.\n\n    Returns\n    -------\n    content : `str`\n        Content downloaded from the URL.",
        "Asynchronously download a set of lsst-texmf BibTeX bibliographies from\n    GitHub.\n\n    Parameters\n    ----------\n    bibtex_names : sequence of `str`\n        Names of lsst-texmf BibTeX files to download. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtexs : `list` of `str`\n        List of BibTeX file content, in the same order as ``bibtex_names``.",
        "Get content of lsst-texmf bibliographies.\n\n    BibTeX content is downloaded from GitHub (``master`` branch of\n    https://github.com/lsst/lsst-texmf or retrieved from an in-memory cache.\n\n    Parameters\n    ----------\n    bibtex_filenames : sequence of `str`, optional\n        List of lsst-texmf BibTeX files to retrieve. These can be the filenames\n        of lsst-bibtex files (for example, ``['lsst.bib', 'lsst-dm.bib']``)\n        or names without an extension (``['lsst', 'lsst-dm']``). The default\n        (recommended) is to get *all* lsst-texmf bibliographies:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm',",
        "Make a pybtex BibliographyData instance from standard lsst-texmf\n    bibliography files and user-supplied bibtex content.\n\n    Parameters\n    ----------\n    lsst_bib_names : sequence of `str`, optional\n        Names of lsst-texmf BibTeX files to include. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n        Default is `None`, which includes all lsst-texmf bibtex files.\n\n    bibtex : `str`\n        BibTeX source content not included in lsst-texmf. This can be content\n        from a import ``local.bib`` file.\n\n    Returns\n    -------\n    bibliography : `pybtex.database.BibliographyData`\n",
        "Get a usable URL from a pybtex entry.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n\n    Returns\n    -------\n    url : `str`\n        Best available URL from the ``entry``.\n\n    Raises\n    ------\n    NoEntryUrlError\n        Raised when no URL can be made from the bibliography entry.\n\n    Notes\n    -----\n    The order of priority is:\n\n    1. ``url`` field\n    2. ``ls.st`` URL from the handle for ``@docushare`` entries.\n    3. ``adsurl``\n    4. DOI",
        "Get and format author-year text from a pybtex entry to emulate\n    natbib citations.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n    parens : `bool`, optional\n        Whether to add parentheses around the year. Default is `False`.\n\n    Returns\n    -------\n    authoryear : `str`\n        The author-year citation text.",
        "Extract, transform, and load Sphinx-based technote metadata.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n",
        "Reduce a technote project's metadata from multiple sources into a\n    single JSON-LD resource.\n\n    Parameters\n    ----------\n    github_url : `str`\n        URL of the technote's GitHub repository.\n    metadata : `dict`\n        The parsed contents of ``metadata.yaml`` found in a technote's\n        repository.\n    github_data : `dict`\n        The contents of the ``technote_repo`` GitHub GraphQL API query.\n    ltd_product_data : `dict`\n        JSON dataset for the technote corresponding to the\n        ``/products/<product>`` of LTD Keeper.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d",
        "Download the metadata.yaml file from a technote's GitHub repository.",
        "Return the timezone. If none is set use system timezone",
        "Convert any timestamp into a datetime and save as _time",
        "Return a dict that represents the DayOneEntry",
        "Saves a DayOneEntry as a plist",
        "Create and return full file path for DayOne entry",
        "Combine many files into a single file on disk.  Defaults to using the 'time' dimension.",
        "The entry point for a yaz script\n\n    This will almost always be called from a python script in\n    the following manner:\n\n        if __name__ == \"__main__\":\n            yaz.main()\n\n    This function will perform the following steps:\n\n    1. It will load any additional python code from\n       the yaz_extension python module located in the\n       ~/.yaz directory when LOAD_YAZ_EXTENSION is True\n       and the yaz_extension module exists\n\n    2. It collects all yaz tasks and plugins.  When WHITE_LIST\n       is a non-empty list, only the tasks and plugins located\n       therein will be considered\n\n    3. It will parse arguments from ARGV, or the command line\n       when ARGV is not given, resulting in a yaz task or a parser\n       help message.\n\n    4. When a suitable",
        "Returns a tree of Task instances\n\n    The tree is comprised of dictionaries containing strings for\n    keys and either dictionaries or Task instances for values.\n\n    When WHITE_LIST is given, only the tasks and plugins in this\n    list will become part of the task tree.  The WHITE_LIST may\n    contain either strings, corresponding to the task of plugin\n    __qualname__, or, preferable, the WHITE_LIST contains\n    links to the task function or plugin class instead.",
        "Declare a function or method to be a Yaz task\n\n    @yaz.task\n    def talk(message: str = \"Hello World!\"):\n        return message\n\n    Or... group multiple tasks together\n\n    class Tools(yaz.Plugin):\n        @yaz.task\n        def say(self, message: str = \"Hello World!\"):\n            return message\n\n        @yaz.task(option__choices=[\"A\", \"B\", \"C\"])\n        def choose(self, option: str = \"A\"):\n            return option",
        "Returns a list of parameters",
        "Returns the configuration for KEY",
        "Returns an instance of a fully initialized plugin class\n\n    Every plugin class is kept in a plugin cache, effectively making\n    every plugin into a singleton object.\n\n    When a plugin has a yaz.dependency decorator, it will be called\n    as well, before the instance is returned.",
        "Convert an Open511 XML document or document fragment to JSON.\n\n    Takes an lxml Element object. Returns a dict ready to be JSON-serialized.",
        "Given an lxml Element of a GML geometry, returns a dict in GeoJSON format.",
        "Translates a deprecated GML 2.0 geometry to GeoJSON",
        "Panflute filter function that converts content wrapped in a Para to\n    Plain.\n\n    Use this filter with pandoc as::\n\n        pandoc [..] --filter=lsstprojectmeta-deparagraph\n\n    Only lone paragraphs are affected. Para elements with siblings (like a\n    second Para) are left unaffected.\n\n    This filter is useful for processing strings like titles or author names so\n    that the output isn't wrapped in paragraph tags. For example, without\n    this filter, pandoc converts a string ``\"The title\"`` to\n    ``<p>The title</p>`` in HTML. These ``<p>`` tags aren't useful if you\n    intend to put the title text in ``<h1>`` tags using your own templating\n    system.",
        "Recursively generate of all the subclasses of class cls.",
        "List unique elements, preserving order. Remember only the element just seen.",
        "Returns a masked array with anything outside of values masked.\n    The minv and maxv parameters take precendence over any dict values.\n    The valid_range attribute takes precendence over the valid_min and\n    valid_max attributes.",
        "If input object is an ndarray it will be converted into a list",
        "If input object is an ndarray it will be converted into a dict\n        holding dtype, shape and the data, base64 encoded.",
        "leftSibling\n        previousSibling\n        leftSib\n        prevSib\n        lsib\n        psib\n        \n        have the same parent,and on the left",
        "rightSibling\n        nextSibling\n        rightSib\n        nextSib\n        rsib\n        nsib\n        \n        have the same parent,and on the right",
        "leftCousin\n        previousCousin\n        leftCin\n        prevCin\n        lcin\n        pcin\n        \n        parents are neighbors,and on the left",
        "rightCousin\n        nextCousin\n        rightCin\n        nextCin\n        rcin\n        ncin\n        \n        parents are neighbors,and on the right",
        "_creat_child_desc\n            update depth,parent_breadth_path,parent_path,sib_seq,path,lsib_path,rsib_path,lcin_path,rcin_path",
        "_upgrade_breadth_info\n            update breadth, breadth_path, and add desc to desc_level",
        "Parse command content from the LaTeX source.\n\n        Parameters\n        ----------\n        source : `str`\n            The full source of the tex document.\n\n        Yields\n        ------\n        parsed_command : `ParsedCommand`\n            Yields parsed commands instances for each occurence of the command\n            in the source.",
        "Parse a single command.\n\n        Parameters\n        ----------\n        source : `str`\n            The full source of the tex document.\n        start_index : `int`\n            Character index in ``source`` where the command begins.\n\n        Returns\n        -------\n        parsed_command : `ParsedCommand`\n            The parsed command from the source at the given index.",
        "r\"\"\"Attempt to parse a single token on the first line of this source.\n\n        This method is used for parsing whitespace-delimited arguments, like\n        ``\\input file``. The source should ideally contain `` file`` along\n        with a newline character.\n\n        >>> source = 'Line 1\\n' r'\\input test.tex' '\\nLine 2'\n        >>> LatexCommand._parse_whitespace_argument(source, 'input')\n        'test.tex'\n\n        Bracket delimited arguments (``\\input{test.tex}``) are handled in\n        the normal logic of `_parse_command`.",
        "Returns a list of TMDDEventConverter elements.\n\n        doc is an XML Element containing one or more <FEU> events",
        "Mostly ripped from nc3tonc4 in netCDF4-python.\n        Added ability to skip dimension and variables.\n        Removed all of the unpacking logic for shorts.",
        "Returns a Pandas DataFrame of the data.\n        This always returns positive down depths",
        "Load a pre-made query.\n\n        These queries are distributed with lsstprojectmeta. See\n        :file:`lsstrojectmeta/data/githubv4/README.rst` inside the\n        package repository for details on available queries.\n\n        Parameters\n        ----------\n        query_name : `str`\n            Name of the query, such as ``'technote_repo'``.\n\n        Returns\n        -------\n        github_query : `GitHubQuery\n            A GitHub query or mutation object that you can pass to\n            `github_request` to execute the request itself.",
        "Obtain the timestamp for the most recent commit to a given file in a\n    Git repository.\n\n    Parameters\n    ----------\n    filepath : `str`\n        Absolute or repository-relative path for a file.\n    repo_path : `str`, optional\n        Path to the Git repository. Leave as `None` to use the current working\n        directory or if a ``repo`` argument is provided.\n    repo : `git.Repo`, optional\n        A `git.Repo` instance.\n\n    Returns\n    -------\n    commit_timestamp : `datetime.datetime`\n        The datetime of the most recent commit to the given file.\n\n    Raises\n    ------\n    IOError\n        Raised if the ``filepath`` does not exist in the Git repository.",
        "Get the datetime for the most recent commit to a project that\n    affected certain types of content.\n\n    Parameters\n    ----------\n    extensions : sequence of 'str'\n        Extensions of files to consider in getting the most recent commit\n        date. For example, ``('rst', 'svg', 'png')`` are content extensions\n        for a Sphinx project. **Extension comparision is case sensitive.** add\n        uppercase variants to match uppercase extensions.\n    acceptance_callback : callable\n        Callable function whose sole argument is a file path, and returns\n        `True` or `False` depending on whether the file's commit date should\n        be considered or not. This callback is only run on files that are\n        included by ``extensions``. Thus this callback is a way to exclude\n        specific files that would otherwise be included by their extension.\n    root_dir : 'str`, optional\n        Only content contained within",
        "Iterative over relative filepaths of files in a directory, and\n    sub-directories, with the given extension.\n\n    Parameters\n    ----------\n    extname : `str`\n        Extension name (such as 'txt' or 'rst'). Extension comparison is\n        case sensitive.\n    root_dir : 'str`, optional\n        Root directory. Current working directory by default.\n\n    Yields\n    ------\n    filepath : `str`\n        File path, relative to ``root_dir``, with the given extension.",
        "Returns variables that match specific conditions.\n\n        * Can pass in key=value parameters and variables are returned that\n        contain all of the matches.  For example,\n\n        >>> # Get variables with x-axis attribute.\n        >>> vs = nc.get_variables_by_attributes(axis='X')\n        >>> # Get variables with matching \"standard_name\" attribute.\n        >>> nc.get_variables_by_attributes(standard_name='northward_sea_water_velocity')\n\n        * Can pass in key=callable parameter and variables are returned if the\n        callable returns True.  The callable should accept a single parameter,\n        the attribute value.  None is given as the attribute value when the\n        attribute does not exist on the variable. For example,\n\n        >>> # Get Axis variables.\n        >>> vs = nc.get_variables_by_attributes(axis=",
        "vfuncs can be any callable that accepts a single argument, the\n        Variable object, and returns a dictionary of new attributes to\n        set. These will overwrite existing attributes",
        "Decorate a function that uses pypandoc to ensure that pandoc is\n    installed if necessary.",
        "Convert text from one markup format to another using pandoc.\n\n    This function is a thin wrapper around `pypandoc.convert_text`.\n\n    Parameters\n    ----------\n    content : `str`\n        Original content.\n\n    from_fmt : `str`\n        Format of the original ``content``. Format identifier must be one of\n        those known by Pandoc. See https://pandoc.org/MANUAL.html for details.\n\n    to_fmt : `str`\n        Output format for the content.\n\n    deparagraph : `bool`, optional\n        If `True`, then the\n        `lsstprojectmeta.pandoc.filters.deparagraph.deparagraph` filter is\n        used to remove paragraph (``<p>``, for example) tags around a single\n        paragraph of content. That filter does not affect content that\n        consists of multiple blocks (se",
        "Convert lsstdoc-class LaTeX to another markup format.\n\n    This function is a thin wrapper around `convert_text` that automatically\n    includes common lsstdoc LaTeX macros.\n\n    Parameters\n    ----------\n    content : `str`\n        Original content.\n\n    to_fmt : `str`\n        Output format for the content (see https://pandoc.org/MANUAL.html).\n        For example, 'html5'.\n\n    deparagraph : `bool`, optional\n        If `True`, then the\n        `lsstprojectmeta.pandoc.filters.deparagraph.deparagraph` filter is\n        used to remove paragraph (``<p>``, for example) tags around a single\n        paragraph of content. That filter does not affect content that\n        consists of multiple blocks (several paragraphs, or lists, for\n        example). Default is `False",
        "Decode a JSON-LD dataset, including decoding datetime\n    strings into `datetime.datetime` objects.\n\n    Parameters\n    ----------\n    encoded_dataset : `str`\n        The JSON-LD dataset encoded as a string.\n\n    Returns\n    -------\n    jsonld_dataset : `dict`\n        A JSON-LD dataset.\n\n    Examples\n    --------\n\n    >>> doc = '{\"dt\": \"2018-01-01T12:00:00Z\"}'\n    >>> decode_jsonld(doc)\n    {'dt': datetime.datetime(2018, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)}",
        "Encode values as JSON strings.\n\n        This method overrides the default implementation from\n        `json.JSONEncoder`.",
        "Get all git repositories within this environment",
        "Install a python package using pip",
        "Update a python package using pip",
        "Returns the nb quantiles for datas in a dataframe",
        "Returns the root mean square error betwwen a and b",
        "Returns the normalized mean square error of a and b",
        "Returns the mean fractionalized bias error",
        "Returns the factor of exceedance",
        "Computes the correlation between a and b, says the Pearson's correlation\n    coefficient R",
        "Geometric mean bias",
        "Geometric mean variance",
        "Figure of merit in time",
        "Performs several stats on a against b, typically a is the predictions\n    array, and b the observations array\n\n    Returns:\n        A dataFrame of stat name, stat description, result",
        "Path to environments site-packages",
        "Prior to activating, store everything necessary to deactivate this\n        environment.",
        "Do some serious mangling to the current python environment...\n        This is necessary to activate an environment via python.",
        "Remove this environment",
        "Command used to launch this application module",
        "Create a virtual environment. You can pass either the name of a new\n    environment to create in your CPENV_HOME directory OR specify a full path\n    to create an environment outisde your CPENV_HOME.\n\n    Create an environment in CPENV_HOME::\n\n        >>> cpenv.create('myenv')\n\n    Create an environment elsewhere::\n\n        >>> cpenv.create('~/custom_location/myenv')\n\n    :param name_or_path: Name or full path of environment\n    :param config: Environment configuration including dependencies etc...",
        "Remove an environment or module\n\n    :param name_or_path: name or path to environment or module",
        "Activates and launches a module\n\n    :param module_name: name of module to launch",
        "Deactivates an environment by restoring all env vars to a clean state\n    stored prior to activating environments",
        "Returns a list of available modules.",
        "Add a module to CPENV_ACTIVE_MODULES environment variable",
        "Remove a module from CPENV_ACTIVE_MODULES environment variable",
        "Format a list of environments and modules for terminal output",
        "Show context info",
        "Activate an environment",
        "Create a new environment.",
        "Remove an environment",
        "Add an environment to the cache. Allows you to activate the environment\n    by name instead of by full path",
        "Remove a cached environment. Removed paths will no longer be able to\n    be activated by name",
        "Create a new template module.\n\n    You can also specify a filesystem path like \"./modules/new_module\"",
        "Add a module to an environment. PATH can be a git repository path or\n    a filesystem path.",
        "Copy a global module to the active environment.",
        "Resolves VirtualEnvironments with a relative or absolute path",
        "Resolves VirtualEnvironments in CPENV_HOME",
        "Resolves VirtualEnvironments in EnvironmentCache",
        "Resolves module in previously resolved environment.",
        "Resolves modules in currently active environment.",
        "Resolves environment from .cpenv file...recursively walks up the tree\n    in attempt to find a .cpenv file",
        "Returns a view of the array with axes transposed.\n\n    For a 1-D array, this has no effect.\n    For a 2-D array, this is the usual matrix transpose.\n    For an n-D array, if axes are given, their order indicates how the\n    axes are permuted\n\n    Args:\n      a (array_like): Input array.\n      axes (list of int, optional): By default, reverse the dimensions,\n        otherwise permute the axes according to the values given.",
        "Roll the specified axis backwards, until it lies in a given position.\n\n    Args:\n      a (array_like): Input array.\n      axis (int): The axis to roll backwards.  The positions of the other axes \n        do not change relative to one another.\n      start (int, optional): The axis is rolled until it lies before this \n        position.  The default, 0, results in a \"complete\" roll.\n\n    Returns:\n      res (ndarray)",
        "Insert a new axis, corresponding to a given position in the array shape\n\n    Args:\n      a (array_like): Input array.\n      axis (int): Position (amongst axes) where new axis is to be inserted.",
        "Join a sequence of arrays together. \n    Will aim to join `ndarray`, `RemoteArray`, and `DistArray` without moving \n    their data, if they happen to be on different engines.\n\n    Args:\n      tup (sequence of array_like): Arrays to be concatenated. They must have\n        the same shape, except in the dimension corresponding to `axis`.\n      axis (int, optional): The axis along which the arrays will be joined.\n\n    Returns: \n      res: `ndarray`, if inputs were all local\n           `RemoteArray`, if inputs were all on the same remote engine\n           `DistArray`, if inputs were already scattered on different engines",
        "Return the shape that would result from broadcasting the inputs",
        "Compute the arithmetic mean along the specified axis.\n\n    Returns the average of the array elements.  The average is taken over\n    the flattened array by default, otherwise over the specified axis.\n    `float64` intermediate and return values are used for integer inputs.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose mean is desired. If `a` is not an\n        array, a conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the means are computed. The default is to\n        compute the mean of the flattened array.\n        If this is a tuple of ints, a mean is performed over multiple axes,\n        instead of a single axis or all the axes as before.\n    dtype : data-type, optional\n        Type to use in computing the mean.  For integer inputs, the default\n        is `float",
        "`ax` is a valid candidate for a distributed axis if the given\n        subarray shapes are all the same when ignoring axis `ax`",
        "Returns True if successful, False if failure",
        "Return a command to launch a subshell",
        "Generate a prompt with a given prefix\n\n    linux/osx: [prefix] user@host cwd $\n          win: [prefix] cwd:",
        "Launch a subshell",
        "Append a file to file repository.\n\n        For file monitoring, monitor instance needs file.\n        Please put the name of file to `file` argument.\n\n        :param file: the name of file you want monitor.",
        "Append files to file repository.\n        \n        ModificationMonitor can append files to repository using this.\n        Please put the list of file names to `filelist` argument.\n\n        :param filelist: the list of file nmaes",
        "Run file modification monitor.\n\n        The monitor can catch file modification using timestamp and file body. \n        Monitor has timestamp data and file body data. And insert timestamp \n        data and file body data before into while roop. In while roop, monitor \n        get new timestamp and file body, and then monitor compare new timestamp\n        to originaltimestamp. If new timestamp and file body differ original,\n        monitor regard thease changes as `modification`. Then monitor create\n        instance of FileModificationObjectManager and FileModificationObject,\n        and monitor insert FileModificationObject to FileModificationObject-\n        Manager. Then, yield this object.\n\n        :param sleep: How times do you sleep in while roop.",
        "Decorator that invokes `add_status_job`.\n\n        ::\n\n            @app.status_job\n            def postgresql():\n                # query/ping postgres\n\n            @app.status_job(name=\"Active Directory\")\n            def active_directory():\n                # query active directory\n\n            @app.status_job(timeout=5)\n            def paypal():\n                # query paypal, timeout after 5 seconds",
        "Page through text by feeding it to another program.  Invoking a\n    pager through this might support colors.",
        "Calculation du profil annuel\n\n    Param\u00e8tres:\n    df: DataFrame de donn\u00e9es dont l'index est une s\u00e9rie temporelle\n        (cf module xair par exemple)\n    func: function permettant le calcul. Soit un nom de fonction numpy ('mean', 'max', ...)\n        soit la fonction elle-m\u00eame (np.mean, np.max, ...)\n    Retourne:\n    Un DataFrame de moyennes par mois",
        "Attempt to run a global hook by name with args",
        "Calcule de moyennes glissantes\n\n    Param\u00e8tres:\n    df: DataFrame de mesures sur lequel appliqu\u00e9 le calcul\n    sur: (int, par d\u00e9faut 8) Nombre d'observations sur lequel s'appuiera le\n    calcul\n    rep: (float, d\u00e9faut 0.75) Taux de r\u00e9pr\u00e9sentativit\u00e9 en dessous duquel le\n    calcul renverra NaN\n\n    Retourne:\n    Un DataFrame des moyennes glissantes calcul\u00e9es",
        "Calculation de l'AOT40 du 1er mai au 31 juillet\n\n    *AOT40 : AOT 40 ( exprim\u00e9 en micro g/m\u00b3 par heure ) signifie la somme des\n    diff\u00e9rences entre les concentrations horaires sup\u00e9rieures \u00e0 40 parties par\n    milliard ( 40 ppb soit 80 micro g/m\u00b3 ), durant une p\u00e9riode donn\u00e9e en\n    utilisant uniquement les valeurs sur 1 heure mesur\u00e9es quotidiennement\n    entre 8 heures (d\u00e9but de la mesure) et 20 heures (pile, fin de la mesure) CET,\n    ce qui correspond \u00e0 de 8h \u00e0 19h TU (donnant bien 12h",
        "Validate all the entries in the environment cache.",
        "Load the environment cache from disk.",
        "Save the environment cache to disk.",
        "Prompts a user for input.  This is a convenience function that can\n    be used to prompt a user for input later.\n\n    If the user aborts the input by sending a interrupt signal, this\n    function will catch it and raise a :exc:`Abort` exception.\n\n    .. versionadded:: 6.0\n       Added unicode support for cmd.exe on Windows.\n\n    .. versionadded:: 4.0\n       Added the `err` parameter.\n\n    :param text: the text to show for the prompt.\n    :param default: the default value to use if no input happens.  If this\n                    is not given it will prompt until it's aborted.\n    :param hide_input: if this is set to true then the input value will\n                       be hidden.\n    :param confirmation_prompt: asks for confirmation for the value.\n    :param type: the type to use to check",
        "This function takes a text and shows it via an environment specific\n    pager on stdout.\n\n    .. versionchanged:: 3.0\n       Added the `color` flag.\n\n    :param text: the text to page.\n    :param color: controls if the pager supports ANSI colors or not.  The\n                  default is autodetection.",
        "Prepare all iPython engines for distributed object processing.\n\n    Args:\n      client (ipyparallel.Client, optional): If None, will create a client\n        using the default ipyparallel profile.",
        "Retrieve objects that have been distributed, making them local again",
        "Apply a function in parallel to each element of the input",
        "Configure engines so that remote methods returning values of type\n        `real_type` will instead return by proxy, as type `proxy_type`",
        "Returns True if path is a git repository.",
        "Returns True if path is in CPENV_HOME",
        "Returns True if path contains a .cpenv file",
        "Get environment path from redirect file",
        "Returns an absolute expanded path",
        "Like os.path.join but also expands and normalizes path parts.",
        "Like os.path.join but acts relative to this packages bin path.",
        "Like os.makedirs but keeps quiet if path already exists",
        "Walk down a directory tree. Same as os.walk but allows for a depth limit\n    via depth argument",
        "Walk up a directory tree",
        "Preprocess a dict to be used as environment variables.\n\n    :param d: dict to be processed",
        "Add a sequence value to env dict",
        "Join a bunch of dicts",
        "Convert a dict containing environment variables into a standard dict.\n    Variables containing multiple values will be split into a list based on\n    the argument passed to pathsep.\n\n    :param env: Environment dict like os.environ.data\n    :param pathsep: Path separator used to split variables",
        "Convert a python dict to a dict containing valid environment variable\n    values.\n\n    :param d: Dict to convert to an env dict\n    :param pathsep: Path separator used to join lists(default os.pathsep)",
        "Expand all environment variables in an environment dict\n\n    :param env: Environment dict",
        "Returns an unused random filepath.",
        "Encode current environment as yaml and store in path or a temporary\n    file. Return the path to the stored environment.",
        "Returns the URL to the upstream data source for the given URI based on configuration",
        "Return request object for calling the upstream",
        "Returns time to live in seconds. 0 means no caching.\n\n        Criteria:\n        - response code 200\n        - read-only method (GET, HEAD, OPTIONS)\n        Plus http headers:\n        - cache-control: option1, option2, ...\n          where options are:\n          private | public\n          no-cache\n          no-store\n          max-age: seconds\n          s-maxage: seconds\n          must-revalidate\n          proxy-revalidate\n        - expires: Thu, 01 Dec 1983 20:00:00 GMT\n        - pragma: no-cache (=cache-control: no-cache)\n\n        See http://www.mobify.com/blog/beginners-guide-to-http-cache-headers/\n\n        TODO: tests",
        "Guarantee the existence of a basic MANIFEST.in.\n\n    manifest doc: http://docs.python.org/distutils/sourcedist.html#manifest\n\n    `options.paved.dist.manifest.include`: set of files (or globs) to include with the `include` directive.\n\n    `options.paved.dist.manifest.recursive_include`: set of files (or globs) to include with the `recursive-include` directive.\n\n    `options.paved.dist.manifest.prune`: set of files (or globs) to exclude with the `prune` directive.\n\n    `options.paved.dist.manifest.include_sphinx_docroot`: True -> sphinx docroot is added as `graft`\n\n    `options.paved.dist.manifest.include_sphinx_docroot`: True -> sphinx bu",
        "Format a pathname\n\n    :param str pathname: Pathname to format\n    :param int max_length: Maximum length of result pathname (> 3)\n    :return: Formatted pathname\n    :rtype: str\n    :raises ValueError: If *max_length* is not larger than 3\n\n    This function formats a pathname so it is not longer than *max_length*\n    characters. The resulting pathname is returned. It does so by replacing\n    characters at the start of the *pathname* with three dots, if necessary.\n    The idea is that the end of the *pathname* is the most important part\n    to be able to identify the file.",
        "Format a UUID string\n\n    :param str uuid: UUID to format\n    :param int max_length: Maximum length of result string (> 3)\n    :return: Formatted UUID\n    :rtype: str\n    :raises ValueError: If *max_length* is not larger than 3\n\n    This function formats a UUID so it is not longer than *max_length*\n    characters. The resulting string is returned. It does so by replacing\n    characters at the end of the *uuid* with three dots, if necessary.\n    The idea is that the start of the *uuid* is the most important part\n    to be able to identify the related entity.\n\n    The default *max_length* is 10, which will result in a string\n    containing the first 7 characters of the *uuid* passed in. Most of\n    the time, such a string is still unique within a collection of UUIDs.",
        "attempts to get next and previous on updates",
        "Notify the client of the result of handling a request\n\n    The payload contains two elements:\n\n    - client_id\n    - result\n\n    The *client_id* is the id of the client to notify. It is assumed\n    that the notifier service is able to identify the client by this id\n    and that it can pass the *result* to it.\n\n    The *result* always contains a *status_code* element. In case the\n    message passed in is not None, it will also contain a *message*\n    element.\n\n    In case the notifier service does not exist or returns an error,\n    an error message will be logged to *stderr*.",
        "Retrieves the setting value whose name is indicated by name_hyphen.\n\n        Values starting with $ are assumed to reference environment variables,\n        and the value stored in environment variables is retrieved. It's an\n        error if thes corresponding environment variable it not set.",
        "This method does the work of updating settings. Can be passed with\n        enforce_helpstring = False which you may want if allowing end users to\n        add arbitrary metadata via the settings system.\n\n        Preferable to use update_settings (without leading _) in code to do the\n        right thing and always have docstrings.",
        "Return a combined dictionary of setting values and attribute values.",
        "Detect if we get a class or a name, convert a name to a class.",
        "Asserts that the class has a docstring, returning it if successful.",
        "Get absolute path to resource, works for dev and for PyInstaller",
        "Add new block of logbook selection windows. Only 5 allowed.",
        "Remove logbook menu set.",
        "Return selected log books by type.",
        "Verify enetered user name is on accepted MCC logbook list.",
        "Parse xml elements for pretty printing",
        "Convert supplied QPixmap object to image file.",
        "Process user inputs and subit logbook entry when user clicks Submit button",
        "Process log information and push to selected logbooks.",
        "Create graphical objects for menus.",
        "Display menus and connect even signals.",
        "Add or change list of logbooks.",
        "Remove unwanted logbooks from list.",
        "Populate log program list to correspond with log type selection.",
        "Add menus to parent gui.",
        "Iteratively remove graphical objects from layout.",
        "Adds labels to a plot.",
        "Determine the URL corresponding to Python object",
        "Update the database with model schema. Shorthand for `paver manage syncdb`.",
        "Run the dev server.\n\n    Uses `django_extensions <http://pypi.python.org/pypi/django-extensions/0.5>`, if\n    available, to provide `runserver_plus`.\n\n    Set the command to use with `options.paved.django.runserver`\n    Set the port to use with `options.paved.django.runserver_port`",
        "Run South's schemamigration command.",
        "This static method validates a BioMapMapper definition.\n        It returns None on success and throws an exception otherwise.",
        "The main method of this class and the essence of the package.\n        It allows to \"map\" stuff.\n\n        Args:\n\n            ID_s: Nested lists with strings as leafs (plain strings also possible)\n            FROM (str): Origin key for the mapping (default: main key)\n            TO (str): Destination key for the mapping (default: main key)\n            target_as_set (bool): Whether to summarize the output as a set (removes duplicates)\n            no_match_sub: Object representing the status of an ID not being able to be matched\n                          (default: None)\n\n        Returns:\n\n            Mapping: a mapping object capturing the result of the mapping request",
        "Returns all data entries for a particular key. Default is the main key.\n\n        Args:\n\n            key (str): key whose values to return (default: main key)\n\n        Returns:\n\n            List of all data entries for the key",
        "Returns list of strings split by input delimeter\n\n        Argument:\n        line - Input line to cut",
        "Get Existing Message\n\n        http://dev.wheniwork.com/#get-existing-message",
        "Creates a message\n\n        http://dev.wheniwork.com/#create/update-message",
        "Modify an existing message.\n\n        http://dev.wheniwork.com/#create/update-message",
        "Delete existing messages.\n\n        http://dev.wheniwork.com/#delete-existing-message",
        "Returns site data.\n\n        http://dev.wheniwork.com/#get-existing-site",
        "Returns a list of sites.\n\n        http://dev.wheniwork.com/#listing-sites",
        "Creates a site\n\n        http://dev.wheniwork.com/#create-update-site",
        "Returns a link to a view that moves the passed in object up in rank.\n\n    :param obj:\n        Object to move\n    :param link_text:\n        Text to display in the link.  Defaults to \"up\"\n    :returns:\n        HTML link code to view for moving the object",
        "Returns a link to a view that moves the passed in object down in rank.\n\n    :param obj:\n        Object to move\n    :param link_text:\n        Text to display in the link.  Defaults to \"down\"\n    :returns:\n        HTML link code to view for moving the object",
        "Shows a figure with a typical orientation so that x and y axes are set up as expected.",
        "Shifts indicies as needed to account for one based indexing\n\n    Positive indicies need to be reduced by one to match with zero based\n    indexing.\n\n    Zero is not a valid input, and as such will throw a value error.\n\n    Arguments:\n        index -     index to shift",
        "Returns selected positions from cut input source in desired\n        arrangement.\n\n        Argument:\n            line -      input to cut",
        "Processes positions to account for ranges\n\n        Arguments:\n            positions -     list of positions and/or ranges to process",
        "Performs cut for range from start position to end\n\n        Arguments:\n            line -              input to cut\n            start -             start of range\n            current_position -  current position in main cut function",
        "Creates list of values in a range with output delimiters.\n\n        Arguments:\n            start -     range start\n            end -       range end",
        "Locks the file by writing a '.lock' file.\n       Returns True when the file is locked and\n       False when the file was locked already",
        "Unlocks the file by remove a '.lock' file.\n       Returns True when the file is unlocked and\n       False when the file was unlocked already",
        "Initiate the local catalog and push it the cloud",
        "Initiate the local catalog by downloading the cloud catalog",
        "Return nodes in the path between 'a' and 'b' going from\n        parent to child NOT including 'a'",
        "Index of the last occurrence of x in the sequence.",
        "Create and save an admin user.\n\n    :param username:\n        Admin account's username.  Defaults to 'admin'\n    :param email:\n        Admin account's email address.  Defaults to 'admin@admin.com'\n    :param password:\n        Admin account's password.  Defaults to 'admin'\n    :returns:\n        Django user with staff and superuser privileges",
        "Returns a list of the messages from the django MessageMiddleware\n    package contained within the given response.  This is to be used during\n    unit testing when trying to see if a message was set properly in a view.\n\n    :param response: HttpResponse object, likely obtained through a\n        test client.get() or client.post() call\n\n    :returns: a list of tuples (message_string, message_level), one for each\n        message in the response context",
        "Authenticates the superuser account via the web login.",
        "Does a django test client ``get`` against the given url after\n        logging in the admin first.\n\n        :param url:\n            URL to fetch\n        :param response_code:\n            Expected response code from the URL fetch.  This value is\n            asserted.  Defaults to 200\n        :param headers:\n            Optional dictionary of headers to send in the request\n        :param follow:\n            When True, the get call will follow any redirect requests.\n            Defaults to False.\n        :returns:\n            Django testing ``Response`` object",
        "Does a django test client ``post`` against the given url after\n        logging in the admin first.\n\n        :param url:\n            URL to fetch\n        :param data:\n            Dictionary to form contents to post\n        :param response_code:\n            Expected response code from the URL fetch.  This value is\n            asserted.  Defaults to 200\n        :param headers:\n            Optional dictionary of headers to send in with the request\n        :returns:\n            Django testing ``Response`` object",
        "Returns the value displayed in the column on the web interface for\n        a given instance.\n\n        :param admin_model:\n            Instance of a :class:`admin.ModelAdmin` object that is responsible\n            for displaying the change list\n        :param instance:\n            Object instance that is the row in the admin change list\n        :field_name:\n            Name of the field/column to fetch",
        "Highest value of input image.",
        "Lowest value of input image.",
        "spawns a greenlet that does not print exceptions to the screen.\n    if you use this function you MUST use this module's join or joinall otherwise the exception will be lost",
        "Returns usage string with no trailing whitespace.",
        "Setup argparser to process arguments and generate help",
        "Opens connection to S3 returning bucket and key",
        "Upload a local file to S3.",
        "Download a remote file from S3.",
        "Creates an ical .ics file for an event using python-card-me.",
        "Returns a list view of all comments for a given event.\n    Combines event comments and update comments in one list.",
        "Returns a list view of updates for a given event.\n    If the event is over, it will be in chronological order.\n    If the event is upcoming or still going,\n    it will be in reverse chronological order.",
        "Displays list of videos for given event.",
        "Public form to add an event.",
        "Adds a memory to an event.",
        "Inserts Interpreter Library of imports into sketch in a very non-consensual way",
        "Sets the beam moments directly.\n\n        Parameters\n        ----------\n        sx : float\n            Beam moment where :math:`\\\\text{sx}^2 = \\\\langle x^2 \\\\rangle`.\n        sxp : float\n            Beam moment where :math:`\\\\text{sxp}^2 = \\\\langle x'^2 \\\\rangle`.\n        sxxp : float\n            Beam moment where :math:`\\\\text{sxxp} = \\\\langle x x' \\\\rangle`.",
        "Sets the beam moments indirectly using Courant-Snyder parameters.\n\n        Parameters\n        ----------\n        beta : float\n            Courant-Snyder parameter :math:`\\\\beta`.\n        alpha : float\n            Courant-Snyder parameter :math:`\\\\alpha`.\n        emit : float\n            Beam emittance :math:`\\\\epsilon`.\n        emit_n : float\n            Normalized beam emittance :math:`\\\\gamma \\\\epsilon`.",
        "Given a slice object, return appropriate values for use in the range function\n\n    :param slice_obj: The slice object or integer provided in the `[]` notation\n    :param length: For negative indexing we need to know the max length of the object.",
        "Helper to add error to messages field. It fills placeholder with extra call parameters\n        or values from message_value map.\n\n        :param error_code: Error code to use\n        :rparam error_code: str\n        :param value: Value checked\n        :param kwargs: Map of values to use in placeholders",
        "File copy that support compress and decompress of zip files",
        "Apply to the 'catalog' the changesets in the metafile list 'changesets",
        "Validate that an event with this name on this date does not exist.",
        "When entering the context, spawns a greenlet that sleeps for `interval` seconds between `callback` executions.\n    When leaving the context stops the greenlet.\n    The yielded object is the `GeventLoop` object so the loop can be stopped from within the context.\n\n    For example:\n    ```\n    with loop_in_background(60.0, purge_cache) as purge_cache_job:\n        ...\n        ...\n        if should_stop_cache():\n            purge_cache_job.stop()\n    ```",
        "Main loop - used internally.",
        "Starts the loop. Calling a running loop is an error.",
        "Kills the running loop and waits till it gets killed.",
        "Used to plot a set of coordinates.\n\n\n    Parameters\n    ----------\n    x, y : :class:`numpy.ndarray`\n        1-D ndarrays of lengths N and M, respectively, specifying pixel centers\n    z : :class:`numpy.ndarray`\n        An (M, N) ndarray or masked array of values to be colormapped, or a (M, N, 3) RGB array, or a (M, N, 4) RGBA array.\n    ax : :class:`matplotlib.axes.Axes`, optional\n        The axis to plot to.\n    fig : :class:`matplotlib.figure.Figure`, optional\n        The figure to plot to.\n    cmap : :class:`matplotlib.colors.Colormap`, optional\n        The colormap to use.\n    alpha : float, optional\n        The transparency to use.\n    scalex : bool, optional\n        To set the x",
        "Fix common spacing errors caused by LaTeX's habit\n        of using an inter-sentence space after any full stop.",
        "Transform hyphens to various kinds of dashes",
        "Replace target with replacement",
        "Regex substitute target with replacement",
        "Call the Sphinx Makefile with the specified targets.\n\n    `options.paved.docs.path`: the path to the Sphinx folder (where the Makefile resides).",
        "Upload the docs to a remote location via rsync.\n\n    `options.paved.docs.rsync_location`: the target location to rsync files to.\n\n    `options.paved.docs.path`: the path to the Sphinx folder (where the Makefile resides).\n\n    `options.paved.docs.build_rel`: the path of the documentation\n        build folder, relative to `options.paved.docs.path`.",
        "Push Sphinx docs to github_ gh-pages branch.\n\n     1. Create file .nojekyll\n     2. Push the branch to origin/gh-pages\n        after committing using ghp-import_\n\n    Requirements:\n     - easy_install ghp-import\n\n    Options:\n     - `options.paved.docs.*` is not used\n     - `options.sphinx.docroot` is used (default=docs)\n     - `options.sphinx.builddir` is used (default=.build)\n    # end of copy\n\n    builddir=builddir / 'html'\n    if not builddir.exists():\n        raise BuildFailure(\"Sphinx build directory (%s) does not exist.\"\n                           % builddir)\n\n    nojekyll = path(bu",
        "Open your web browser and display the generated html documentation.",
        "Tries to minimize the length of CSS code passed as parameter. Returns string.",
        "Return an open file-object to the index file",
        "Create the tasks on the server",
        "Update existing tasks on the server",
        "Reconcile this collection with the server.",
        "Prompts the user for yes or no.",
        "Prompts the user with custom options.",
        "Reading the configure file and adds non-existing attributes to 'args",
        "Writing the configure file with the attributes in 'args",
        "Create a new instance of a game. Note, a mode MUST be provided and MUST be of\n        type GameMode.\n\n        :param mode: <required>",
        "Bumps the Version given a target\n\n        The target can be either MAJOR, MINOR or PATCH",
        "Returns a copy of this object",
        "Returns a Tag with a given revision",
        "Parses a string into a Tag",
        "Tiles open figures.",
        "When a Comment is added, updates the Update to set \"last_updated\" time",
        "Adds useful global items to the context for use in templates.\n\n    * *request*: the request object\n    * *HOST*: host name of server\n    * *IN_ADMIN*: True if you are in the django admin area",
        "Create the challenge on the server",
        "Update existing challenge on the server",
        "Check if a challenge exists on the server",
        "Returns position data.\n\n        http://dev.wheniwork.com/#get-existing-position",
        "Returns a list of positions.\n\n        http://dev.wheniwork.com/#listing-positions",
        "Creates a position\n\n        http://dev.wheniwork.com/#create-update-position",
        "Print \"Source Lines of Code\" and export to file.\n\n    Export is hudson_ plugin_ compatible: sloccount.sc\n\n    requirements:\n     - sloccount_ should be installed.\n     - tee and pipes are used\n\n    options.paved.pycheck.sloccount.param\n\n    .. _sloccount: http://www.dwheeler.com/sloccount/\n    .. _hudson: http://hudson-ci.org/\n    .. _plugin: http://wiki.hudson-ci.org/display/HUDSON/SLOCCount+Plugin",
        "passive check of python programs by pyflakes.\n\n    requirements:\n     - pyflakes_ should be installed. ``easy_install pyflakes``\n\n    options.paved.pycheck.pyflakes.param\n\n    .. _pyflakes: http://pypi.python.org/pypi/pyflakes",
        "Handle HTTP exception\n\n    :param werkzeug.exceptions.HTTPException exception: Raised exception\n\n    A response is returned, as formatted by the :py:func:`response` function.",
        "Returns True if the value given is a valid CSS colour, i.e. matches one\n    of the regular expressions in the module or is in the list of\n    predetefined values by the browser.",
        "Reynold number utility function that return Reynold number for vehicle at specific length and speed.\n    Optionally, it can also take account of temperature effect of sea water.\n\n        Kinematic viscosity from: http://web.mit.edu/seawater/2017_MIT_Seawater_Property_Tables_r2.pdf\n\n    :param length: metres length of the vehicle\n    :param speed: m/s speed of the vehicle\n    :param temperature: degree C \n    :return: Reynolds number of the vehicle (dimensionless)",
        "Froude number utility function that return Froude number for vehicle at specific length and speed.\n\n    :param speed: m/s speed of the vehicle\n    :param length: metres length of the vehicle\n    :return: Froude number of the vehicle (dimensionless)",
        "Residual resistance coefficient estimation from slenderness function, prismatic coefficient and Froude number.\n\n    :param slenderness: Slenderness coefficient dimensionless :math:`L/(\u2207^{1/3})` where L is length of ship, \u2207 is displacement\n    :param prismatic_coef: Prismatic coefficient dimensionless :math:`\u2207/(L\\cdot A_m)` where L is length of ship, \u2207 is displacement Am is midsection area of the ship\n    :param froude_number: Froude number of the ship dimensionless \n    :return: Residual resistance of the ship",
        "Assign values for the main dimension of a ship.\n\n        :param length: metres length of the vehicle\n        :param draught: metres draught of the vehicle\n        :param beam: metres beam of the vehicle\n        :param speed: m/s speed of the vehicle\n        :param slenderness_coefficient: Slenderness coefficient dimensionless :math:`L/(\u2207^{1/3})` where L is length of ship,\n            \u2207 is displacement\n        :param prismatic_coefficient: Prismatic coefficient dimensionless :math:`\u2207/(L\\cdot A_m)` where L is length of ship,\n            \u2207 is displacement Am is midsection area of the ship",
        "Return resistance of the vehicle.\n\n        :return: newton the resistance of the ship",
        "Return the maximum deck area of the ship\n\n        :param water_plane_coef: optional water plane coefficient\n        :return: Area of the deck",
        "Total propulsion power of the ship.\n\n        :param propulsion_eff: Shaft efficiency of the ship\n        :param sea_margin: Sea margin take account of interaction between ship and the sea, e.g. wave\n        :return: Watts shaft propulsion power of the ship",
        "Configure the api to use given url and token or to get them from the\n        Config.",
        "Send zipfile to TMC for given exercise",
        "Ensures that the request url is valid.\n        Sometimes we have URLs that the server gives that are preformatted,\n        sometimes we need to form our own.",
        "Extract json from a response.\n            Assumes response is valid otherwise.\n            Internal use only.",
        "Wrapper for gevent.joinall if the greenlet that waits for the joins is killed, it kills all the greenlets it\n    joins for.",
        "Creates an error from the given code, and args and kwargs.\n\n    :param code: The acknowledgement code\n    :param args: Exception args\n    :param kwargs: Exception kwargs\n    :return: the error for the given acknowledgement code",
        "Creates an error Acknowledgement message.\n        The message's code and message are taken from this exception.\n\n        :return: the message representing this exception",
        "Clean up extra files littering the source tree.\n\n    options.paved.clean.dirs: directories to search recursively\n    options.paved.clean.patterns: patterns to search for and remove",
        "print paver options.\n\n    Prettified by json.\n    `long_description` is removed",
        "\\\n        Parses a binary protobuf message into a Message object.",
        "Creates a new ``Node`` based on the extending class and adds it as\n        a child to this ``Node``.\n\n        :param kwargs: \n            arguments for constructing the data object associated with this\n            ``Node``\n        :returns: \n            extender of the ``Node`` class",
        "Returns a list of the ancestors of this node.",
        "Returns a list of the ancestors of this node but does not pass the\n        root node, even if the root has parents due to cycles.",
        "Returns a list of descendents of this node.",
        "Returns True if it is legal to remove this node and still leave the\n        graph as a single connected entity, not splitting it into a forest.\n        Only nodes with no children or those who cause a cycle can be deleted.",
        "Removes the node and all descendents without looping back past the\n        root.  Note this does not remove the associated data objects.\n\n        :returns:\n            list of :class:`BaseDataNode` subclassers associated with the\n            removed ``Node`` objects.",
        "Returns a list of nodes that would be removed if prune were called\n        on this element.",
        "Called to verify that the given rule can become a child of the\n        current node.  \n\n        :raises AttributeError: \n            if the child is not allowed",
        "Returns location data.\n\n        http://dev.wheniwork.com/#get-existing-location",
        "Returns a list of locations.\n\n        http://dev.wheniwork.com/#listing-locations",
        "The reduced chi-square of the linear least squares",
        "Create the task on the server",
        "Update existing task on the server",
        "Retrieve a task from the server",
        "Formats a string with color",
        "Returns user profile data.\n\n        http://dev.wheniwork.com/#get-existing-user",
        "Returns a list of users.\n\n        http://dev.wheniwork.com/#listing-users",
        "Attempt to set the virtualenv activate command, if it hasn't been specified.",
        "Recursively update the destination dict-like object with the source dict-like object.\n\n    Useful for merging options and Bunches together!\n\n    Based on:\n    http://code.activestate.com/recipes/499335-recursively-update-a-dictionary-without-hitting-py/#c1",
        "Send the given arguments to `pip install`.",
        "When I Work GET method. Return representation of the requested\n        resource.",
        "When I Work PUT method.",
        "When I Work POST method.",
        "When I Work DELETE method.",
        "Creates a shift\n\n        http://dev.wheniwork.com/#create/update-shift",
        "Delete existing shifts.\n\n        http://dev.wheniwork.com/#delete-shift",
        "Returns combined list of event and update comments.",
        "Returns chained list of event and update images.",
        "Gets count of all images from both event and updates.",
        "Gets images and videos to populate top assets.\n\n        Map is built separately.",
        "Decorated methods progress will be displayed to the user as a spinner.\n        Mostly for slower functions that do some network IO.",
        "Launches a new menu. Wraps curses nicely so exceptions won't screw with\n        the terminal too much.",
        "Overridden method that handles that re-ranking of objects and the\n        integrity of the ``rank`` field.\n\n        :param rerank:\n            Added parameter, if True will rerank other objects based on the\n            change in this save.  Defaults to True.",
        "Removes any blank ranks in the order.",
        "Returns the field names of a Django model object.\n\n    :param obj: the Django model class or object instance to get the fields\n        from\n    :param ignore_auto: ignore any fields of type AutoField. Defaults to True\n    :param ignore_relations: ignore any fields that involve relations such as\n        the ForeignKey or ManyToManyField\n    :param exclude: exclude anything in this list from the results\n\n    :returns: generator of found field names",
        "Register all HTTP error code error handlers\n\n    Currently, errors are handled by the JSON error handler.",
        "Plots but automatically resizes x axis.\n\n    .. versionadded:: 1.4\n\n    Parameters\n    ----------\n    args\n        Passed on to :meth:`matplotlib.axis.Axis.plot`.\n    ax : :class:`matplotlib.axis.Axis`, optional\n        The axis to plot to.\n    kwargs\n        Passed on to :meth:`matplotlib.axis.Axis.plot`.",
        "Create a vector of values over an interval with a specified step size.\n\n    Parameters\n    ----------\n\n    start : float\n        The beginning of the interval.\n    stop : float\n        The end of the interval.\n    step : float\n        The step size.\n\n    Returns\n    -------\n    vector : :class:`numpy.ndarray`\n        The vector of values.",
        "Passes the selected course as the first argument to func.",
        "Passes the selected exercise as the first argument to func.",
        "If func returns False the program exits immediately.",
        "Configure tmc.py to use your account.",
        "Download the exercises from the server.",
        "Go to the next exercise.",
        "Spawns a process with `command path-of-exercise`",
        "Select a course or an exercise.",
        "Submit the selected exercise to the server.",
        "Sends the selected exercise to the TMC pastebin.",
        "Update the data of courses and or exercises from server.",
        "Determine the type of x",
        "map for a directory",
        "Apply the types on the elements of the line",
        "Convert a file to a .csv file",
        "Returns a link to the django admin change list with a filter set to\n    only the object given.\n\n    :param obj:\n        Object to create the admin change list display link for\n    :param display:\n        Text to display in the link.  Defaults to string call of the object\n    :returns:\n        Text containing HTML for a link",
        "Returns string representation of an object, either the default or based\n    on the display template passed in.",
        "Adds a ``list_display`` attribute that appears as a link to the\n        django admin change page for the type of object being shown. Supports\n        double underscore attribute name dereferencing.\n\n        :param attr:\n            Name of the attribute to dereference from the corresponding\n            object, i.e. what will be lined to.  This name supports double\n            underscore object link referencing for ``models.ForeignKey``\n            members.\n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``attr``\n\n        :param display:\n            What to display as the text for the link being shown.  If not\n            given it defaults to the string representation of the object for\n            the row: ``str(obj)`` .  This parameter supports django\n            templating, the context for which contains a dictionary key named\n           ",
        "Adds a ``list_display`` attribute showing an object.  Supports\n        double underscore attribute name dereferencing.\n\n        :param attr:\n            Name of the attribute to dereference from the corresponding\n            object, i.e. what will be lined to.  This name supports double\n            underscore object link referencing for ``models.ForeignKey``\n            members.\n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``attr``\n\n        :param display:\n            What to display as the text for the link being shown.  If not\n            given it defaults to the string representation of the object for\n            the row: ``str(obj)``.  This parameter supports django templating,\n            the context for which contains a dictionary key named \"obj\" with\n            the value being the object for the row.",
        "Adds a ``list_display`` attribute showing a field in the object\n        using a python %formatted string.\n\n        :param field:\n            Name of the field in the object.\n\n        :param format_string:\n            A old-style (to remain python 2.x compatible) % string formatter\n            with a single variable reference. The named ``field`` attribute\n            will be passed to the formatter using the \"%\" operator. \n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``field``",
        "View decorator that enforces that the method was called using POST.\n    This decorator can be called with or without parameters.  As it is\n    expected to wrap a view, the first argument of the method being wrapped is\n    expected to be a ``request`` object.\n\n    .. code-block:: python\n\n        @post_required\n        def some_view(request):\n            pass\n\n\n        @post_required(['firstname', 'lastname'])\n        def some_view(request):\n            pass\n\n    The optional parameter contains a single list which specifies the names of\n    the expected fields in the POST dictionary.  The list is not exclusive,\n    you can pass in fields that are not checked by the decorator.\n\n    :param options:\n        List of the names of expected POST keys.",
        "View decorator that enforces that the method was called using POST and\n    contains a field containing a JSON dictionary. This method should\n    only be used to wrap views and assumes the first argument of the method\n    being wrapped is a ``request`` object.\n\n    .. code-block:: python\n\n        @json_post_required('data', 'json_data')\n        def some_view(request):\n            username = request.json_data['username']\n\n    :param field:\n        The name of the POST field that contains a JSON dictionary\n    :param request_name:\n        [optional] Name of the parameter on the request to put the\n        deserialized JSON data. If not given the field name is used",
        "Divergence of matched beam",
        "The plasma density in SI units.",
        "Semver tag triggered deployment helper",
        "Performs some environment checks prior to the program's execution",
        "Prints latest tag's information",
        "Prompts user before proceeding",
        "Gets an array from datasets.\n\n    .. versionadded:: 1.4",
        "Get the current directory state",
        "Add one tick to progress bar",
        "Push k to the top of the list\n\n        >>> l = DLL()\n        >>> l.push(1)\n        >>> l\n        [1]\n        >>> l.push(2)\n        >>> l\n        [2, 1]\n        >>> l.push(3)\n        >>> l\n        [3, 2, 1]",
        "Call this method to increment the named counter.  This is atomic on\n        the database.\n\n        :param name:\n            Name for a previously created ``Counter`` object",
        "print loading message on screen\n\n        .. note::\n\n            loading message only write to `sys.stdout`\n\n\n        :param int wait: seconds to wait\n        :param str message: message to print\n        :return: None",
        "print warn type message,\n        if file handle is `sys.stdout`, print color message\n\n\n        :param str message: message to print\n        :param file fh: file handle,default is `sys.stdout`\n        :param str prefix: message prefix,default is `[warn]`\n        :param str suffix: message suffix ,default is `...`\n        :return: None",
        "print error type message\n        if file handle is `sys.stderr`, print color message\n\n        :param str message: message to print\n        :param file fh: file handle, default is `sys.stdout`\n        :param str prefix: message prefix,default is `[error]`\n        :param str suffix: message suffix ,default is '...'\n        :return: None",
        "a built-in wrapper make dry-run easier.\n        you should use this instead use `os.system`\n\n        .. note::\n\n            to use it,you need add '--dry-run' option in\n            your argparser options\n\n\n        :param str cmd: command to execute\n        :param bool fake_code: only display command\n            when is True,default is False\n        :return:",
        "Returns a Corrected URL to be used for a Request\n        as per the REST API.",
        "Main method.\n\n    This method holds what you want to execute when\n    the script is run on command line.",
        "Pickle and compress.",
        "Decompress and unpickle.",
        "Displays the contact form and sends the email",
        "try use gitconfig info.\n        author,email etc.",
        "Init project.",
        "In general, you don't need to overwrite this method.\n\n        :param options:\n        :return:",
        "Return a new filename to use as the combined file name for a\n    bunch of files, based on the SHA of their contents.\n    A precondition is that they all have the same file extension\n\n    Given that the list of files can have different paths, we aim to use the\n    most common path.\n\n    Example:\n      /somewhere/else/foo.js\n      /somewhere/bar.js\n      /somewhere/different/too/foobar.js\n    The result will be\n      /somewhere/148713695b4a4b9083e506086f061f9c.js\n\n    Another thing to note, if the filenames have timestamps in them, combine\n    them all and use the highest timestamp.",
        "Extract the oritentation EXIF tag from the image, which should be a PIL Image instance,\n    and if there is an orientation tag that would rotate the image, apply that rotation to\n    the Image instance given to do an in-place rotation.\n\n    :param Image im: Image instance to inspect\n    :return: A possibly transposed image instance",
        "Start a new piece",
        "Start a new site.",
        "Publish the site",
        "Returns a list of the branches",
        "Returns the currently active branch",
        "Create a patch between tags",
        "Create a callable that applies ``func`` to a value in a sequence.\n\n    If the value is not a sequence or is an empty sequence then ``None`` is\n    returned.\n\n    :type  func: `callable`\n    :param func: Callable to be applied to each result.\n\n    :type  n: `int`\n    :param n: Index of the value to apply ``func`` to.",
        "Create a callable that applies ``func`` to every value in a sequence.\n\n    If the value is not a sequence then an empty list is returned.\n\n    :type  func: `callable`\n    :param func: Callable to be applied to the first result.",
        "Parse a value as text.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat ``bytes`` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `unicode`\n    :return: Parsed text or ``None`` if ``value`` is neither `bytes` nor\n        `unicode`.",
        "Parse a value as an integer.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse\n\n    :type  base: `unicode` or `bytes`\n    :param base: Base to assume ``value`` is specified in.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat ``bytes`` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `int`\n    :return: Parsed integer or ``None`` if ``value`` could not be parsed as an\n        integer.",
        "Parse a value as a boolean.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse.\n\n    :type  true: `tuple` of `unicode`\n    :param true: Values to compare, ignoring case, for ``True`` values.\n\n    :type  false: `tuple` of `unicode`\n    :param false: Values to compare, ignoring case, for ``False`` values.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `bool`\n    :return: Parsed boolean or ``None`` if ``value`` did not match ``true`` or\n        ``false`` values.",
        "Parse a value as a delimited list.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse.\n\n    :type  parser: `callable` taking a `unicode` parameter\n    :param parser: Callable to map over the delimited text values.\n\n    :type  delimiter: `unicode`\n    :param delimiter: Delimiter text.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `list`\n    :return: List of parsed values.",
        "Parse a value as a POSIX timestamp in seconds.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse, which should be the number of seconds\n        since the epoch.\n\n    :type  _divisor: `float`\n    :param _divisor: Number to divide the value by.\n\n    :type  tz: `tzinfo`\n    :param tz: Timezone, defaults to UTC.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `datetime.datetime`\n    :return: Parsed datetime or ``None`` if ``value`` could not be parsed.",
        "Parse query parameters.\n\n    :type  expected: `dict` mapping `bytes` to `callable`\n    :param expected: Mapping of query argument names to argument parsing\n        callables.\n\n    :type  query: `dict` mapping `bytes` to `list` of `bytes`\n    :param query: Mapping of query argument names to lists of argument values,\n        this is the form that Twisted Web's `IRequest.args\n        <twisted:twisted.web.iweb.IRequest.args>` value takes.\n\n    :rtype: `dict` mapping `bytes` to `object`\n    :return: Mapping of query argument names to parsed argument values.",
        "Put metrics to cloudwatch. Metric shoult be instance or list of\n        instances of CloudWatchMetric",
        "Render a given resource.\n\n    See `IResource.render <twisted:twisted.web.resource.IResource.render>`.",
        "Adapt a result to `IResource`.\n\n        Several adaptions are tried they are, in order: ``None``,\n        `IRenderable <twisted:twisted.web.iweb.IRenderable>`, `IResource\n        <twisted:twisted.web.resource.IResource>`, and `URLPath\n        <twisted:twisted.python.urlpath.URLPath>`. Anything else is returned as\n        is.\n\n        A `URLPath <twisted:twisted.python.urlpath.URLPath>` is treated as\n        a redirect.",
        "Handle the result from `IResource.render`.\n\n        If the result is a `Deferred` then return `NOT_DONE_YET` and add\n        a callback to write the result to the request when it arrives.",
        "Negotiate a handler based on the content types acceptable to the\n        client.\n\n        :rtype: 2-`tuple` of `twisted.web.iweb.IResource` and `bytes`\n        :return: Pair of a resource and the content type.",
        "Parse and sort an ``Accept`` header.\n\n    The header is sorted according to the ``q`` parameter for each header value.\n\n    @rtype: `OrderedDict` mapping `bytes` to `dict`\n    @return: Mapping of media types to header parameters.",
        "Split an HTTP header whose components are separated with commas.\n\n    Each component is then split on semicolons and the component arguments\n    converted into a `dict`.\n\n    @return: `list` of 2-`tuple` of `bytes`, `dict`\n    @return: List of header arguments and mapping of component argument names\n        to values.",
        "Extract an encoding from a ``Content-Type`` header.\n\n    @type  requestHeaders: `twisted.web.http_headers.Headers`\n    @param requestHeaders: Request headers.\n\n    @type  encoding: `bytes`\n    @param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one. Defaults to ``UTF-8``.\n\n    @rtype: `bytes`\n    @return: Content encoding.",
        "Create a nil-safe callable decorator.\n\n    If the wrapped callable receives ``None`` as its argument, it will return\n    ``None`` immediately.",
        "Get or set `Settings._wrapped`\n\n    :param str path: a python module file,\n        if user set it,write config to `Settings._wrapped`\n    :param str with_path: search path\n    :return: A instance of `Settings`",
        "bind user variable to `_wrapped`\n\n        .. note::\n\n            you don't need call this method by yourself.\n\n            program will call it in  `cliez.parser.parse`\n\n\n        .. expection::\n\n            if path is not correct,will cause an `ImportError`\n\n\n        :param str mod_path: module path, *use dot style,'mod.mod1'*\n        :param str with_path: add path to `sys.path`,\n            if path is file,use its parent.\n        :return: A instance of `Settings`",
        "Get the version from version module without importing more than\n    necessary.",
        "send a transaction immediately. Failed transactions are picked up by the TxBroadcaster\n\n        :param ip: specific peer IP to send tx to\n        :param port: port of specific peer\n        :param use_open_peers: use Arky's broadcast method",
        "check if a tx is confirmed, else resend it.\n\n        :param use_open_peers: select random peers fro api/peers endpoint",
        "Get sub-command list\n\n    .. note::\n\n        Don't use logger handle this function errors.\n\n        Because the error should be a code error,not runtime error.\n\n\n    :return: `list` matched sub-parser",
        "Add class options to argparser options.\n\n    :param cliez.component.Component klass: subclass of Component\n    :param Namespace sub_parsers:\n    :param str default_epilog: default_epilog\n    :param list general_arguments: global options, defined by user\n    :return: Namespace subparser",
        "parser cliez app\n\n    :param argparse.ArgumentParser parser: an instance\n        of argparse.ArgumentParser\n    :param argv: argument list,default is `sys.argv`\n    :type argv: list or tuple\n\n    :param str settings: settings option name,\n        default is settings.\n\n    :param object no_args_func: a callable object.if no sub-parser matched,\n        parser will call it.\n\n    :return:  an instance of `cliez.component.Component` or its subclass",
        "Convert Hump style to underscore\n\n    :param name: Hump Character\n    :return: str",
        "Fetches fuel prices for all stations.",
        "Gets the fuel prices for a specific fuel station.",
        "Gets all the fuel prices within the specified radius.",
        "Gets the fuel price trends for the given location and fuel types.",
        "Fetches API reference data.\n\n        :param modified_since: The response will be empty if no\n        changes have been made to the reference data since this\n        timestamp, otherwise all reference data will be returned.",
        "Called before template is applied.",
        "Match a route parameter.\n\n    `Any` is a synonym for `Text`.\n\n    :type  name: `bytes`\n    :param name: Route parameter name.\n\n    :type  encoding: `bytes`\n    :param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one.\n\n    :return: ``callable`` suitable for use with `route` or `subroute`.",
        "Match an integer route parameter.\n\n    :type  name: `bytes`\n    :param name: Route parameter name.\n\n    :type  base: `int`\n    :param base: Base to interpret the value in.\n\n    :type  encoding: `bytes`\n    :param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one.\n\n    :return: ``callable`` suitable for use with `route` or `subroute`.",
        "Match a request path against our path components.\n\n    The path components are always matched relative to their parent is in the\n    resource hierarchy, in other words it is only possible to match URIs nested\n    more deeply than the parent resource.\n\n    :type  components: ``iterable`` of `bytes` or `callable`\n    :param components: Iterable of path components, to match against the\n        request, either static strings or dynamic parameters. As a convenience,\n        a single `bytes` component containing ``/`` may be given instead of\n        manually separating the components. If no components are given the null\n        route is matched, this is the case where ``segments`` is empty.\n\n    :type  segments: ``sequence`` of `bytes`\n    :param segments: Sequence of path segments, from the request, to match\n        against.\n\n    :type  partialMatching: `bool`\n",
        "Decorate a router-producing callable to instead produce a resource.\n\n    This simply produces a new callable that invokes the original callable, and\n    calls ``resource`` on the ``routerAttribute``.\n\n    If the router producer has multiple routers the attribute can be altered to\n    choose the appropriate one, for example:\n\n    .. code-block:: python\n\n        class _ComplexRouter(object):\n            router = Router()\n            privateRouter = Router()\n\n            @router.route('/')\n            def publicRoot(self, request, params):\n                return SomethingPublic(...)\n\n            @privateRouter.route('/')\n            def privateRoot(self, request, params):\n                return SomethingPrivate(...)\n\n        PublicResource = routedResource(_ComplexRouter)\n        PrivateResource = routedResource(_ComplexRouter, 'privateRouter')\n\n    :type  f: ``callable",
        "Create a new `Router` instance, with it's own set of routes, for\n        ``obj``.",
        "Add a route handler and matcher to the collection of possible routes.",
        "See `txspinneret.route.route`.\n\n        This decorator can be stacked with itself to specify multiple routes\n        with a single handler.",
        "See `txspinneret.route.subroute`.\n\n        This decorator can be stacked with itself to specify multiple routes\n        with a single handler.",
        "Create a NamedTemporaryFile instance to be passed to atomic_writer",
        "Open a NamedTemoraryFile handle in a context manager",
        "Read entry from JSON file",
        "Save entry to JSON file",
        "Update entry by UUID in the JSON file",
        "Get the number of the shell command.",
        "Execute a shell command.",
        "Simple program that creates an temp S3 link.",
        "Poll ``self.stdout`` and return True if it is readable.\n\n        :param float timeout: seconds to wait I/O\n        :return: True if readable, else False\n        :rtype: boolean",
        "Retrieve a list of characters and escape codes where each escape\n        code uses only one index. The indexes will not match up with the\n        indexes in the original string.",
        "Strip all color codes from a string.\n        Returns empty string for \"falsey\" inputs.",
        "Called when builder group collect files\n        Resolves absolute url if relative passed\n\n        :type asset: static_bundle.builders.Asset\n        :type builder: static_bundle.builders.StandardBuilder",
        "Add single file or list of files to bundle\n\n        :type: file_path: str|unicode",
        "Add directory or directories list to bundle\n\n        :param exclusions: List of excluded paths\n\n        :type path: str|unicode\n        :type exclusions: list",
        "Add custom path objects\n\n        :type: path_object: static_bundle.paths.AbstractPath",
        "Add prepare handler to bundle\n\n        :type: prepare_handler: static_bundle.handlers.AbstractPrepareHandler",
        "Called when builder run collect files in builder group\n\n        :rtype: list[static_bundle.files.StaticFileResult]",
        "Get the number of files in the folder.",
        "Register the contents as JSON",
        "Translate the data with the translation table",
        "Get the data in JSON form",
        "Get the data as JSON tuples",
        "Issues a GET request against the API, properly formatting the params\n\n        :param url: a string, the url you are requesting\n        :param params: a dict, the key-value of all the paramaters needed\n                       in the request\n        :returns: a dict parsed of the JSON response",
        "Issues a POST request against the API, allows for multipart data uploads\n\n        :param url: a string, the url you are requesting\n        :param params: a dict, the key-value of all the parameters needed\n                       in the request\n        :param files: a list, the list of tuples of files\n\n        :returns: a dict parsed of the JSON response",
        "Go through the env var map, transferring the values to this object\n        as attributes.\n\n        :raises: RuntimeError if a required env var isn't defined.",
        "Create a temporary directory with input data for the test.\n    The directory contents is copied from a directory with the same name as the module located in the same directory of\n    the test module.",
        "Compare two files contents. If the files differ, show the diff and write a nice HTML\n        diff file into the data directory.\n\n        Searches for the filenames both inside and outside the data directory (in that order).\n\n        :param unicode obtained_fn: basename to obtained file into the data directory, or full path.\n\n        :param unicode expected_fn: basename to expected file into the data directory, or full path.\n\n        :param bool binary:\n            Thread both files as binary files.\n\n        :param unicode encoding:\n            File's encoding. If not None, contents obtained from file will be decoded using this\n            `encoding`.\n\n        :param callable fix_callback:\n            A callback to \"fix\" the contents of the obtained (first) file.\n            This callback receives a list of strings (lines) and must also return a list of lines,\n            changed as needed.\n           ",
        "Returns a nice side-by-side diff of the given files, as a string.",
        "Add a peer or multiple peers to the PEERS variable, takes a single string or a list.\n\n        :param peer(list or string)",
        "remove one or multiple peers from PEERS variable\n\n        :param peer(list or string):",
        "check the status of the network and the peers\n\n        :return: network_height, peer_status",
        "broadcasts a transaction to the peerslist using ark-js library",
        "Exposes a given service to this API.",
        "Main entry point, expects doctopt arg dict as argd.",
        "Print a message only if DEBUG is truthy.",
        "Parse a string as an integer.\n        Exit with a message on failure.",
        "If `s` is a file name, read the file and return it's content.\n        Otherwise, return the original string.\n        Returns None if the file was opened, but errored during reading.",
        "Wait for response until timeout.\n        If timeout is specified to None, ``self.timeout`` is used.\n\n        :param float timeout: seconds to wait I/O",
        "If the file-object is not seekable, return  ArchiveTemp of the fileobject,\n    otherwise return the file-object itself",
        "Setup before_request, after_request handlers for tracing.",
        "Records the starting time of this reqeust.",
        "Calculates the request duration, and adds a transaction\n        ID to the header.",
        "Insert spaces between words until it is wide enough for `width`.",
        "Prepend or append text to lines. Yields each line.",
        "Format block by splitting on individual characters.",
        "Format block by wrapping on spaces.",
        "Remove spaces in between words until it is small enough for\n            `width`.\n            This will always leave at least one space between words,\n            so it may not be able to get below `width` characters.",
        "Check IP trough the httpBL API\n\n        :param ip: ipv4 ip address\n        :return: httpBL results or None if any error is occurred",
        "Check if IP is a threat\n\n        :param result: httpBL results; if None, then results from last check_ip() used (optional)\n        :param harmless_age: harmless age for check if httpBL age is older (optional)\n        :param threat_score: threat score for check if httpBL threat is lower (optional)\n        :param threat_type:  threat type, if not equal httpBL score type, then return False (optional)\n        :return: True or False",
        "Check if IP is suspicious\n\n        :param result: httpBL results; if None, then results from last check_ip() used (optional)\n        :return: True or False",
        "Invalidate httpBL cache for IP address\n\n        :param ip: ipv4 IP address",
        "Invalidate httpBL cache",
        "Runs the consumer.",
        "Upload the next batch of items, return whether successful.",
        "Return the next batch of items to upload.",
        "Get a single item from the queue.",
        "Attempt to upload the batch and retry before raising an error",
        "Translate camelCase into underscore format.\n\n    >>> _camelcase_to_underscore('minutesBetweenSummaries')\n    'minutes_between_summaries'",
        "Creates the Trello endpoint tree.\n\n    >>> r = {'1': { \\\n                 'actions': {'METHODS': {'GET'}}, \\\n                 'boards': { \\\n                     'members': {'METHODS': {'DELETE'}}}} \\\n            }\n    >>> r == create_tree([ \\\n                 'GET /1/actions/[idAction]', \\\n                 'DELETE /1/boards/[board_id]/members/[idMember]'])\n    True",
        "Prints the complete YAML.",
        "Connect by wmi and run wql.",
        "Wrapper for the other log methods, decide which one based on the\n        URL parameter.",
        "Write to a local log file",
        "Write to a remote host via HTTP POST",
        "Helper method to store username and password",
        "Set connection parameters. Call set_connection with no arguments to clear.",
        "Set delegate parameters. Call set_delegate with no arguments to clear.",
        "Takes a single address and returns the current balance.",
        "returns a list of named tuples,  x.timestamp, x.amount including block rewards",
        "Massages the 'true' and 'false' strings to bool equivalents.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :rtype: bool\n    :return: True or False, depending on the value.",
        "If the value is ``None``, fail validation.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :raises: ValueError if the config value is None.",
        "Make sure the value evaluates to boolean True.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :raises: ValueError if the config value evaluates to boolean False.",
        "Convert an evar value into a Python logging level constant.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :return: A validated string.\n    :raises: ValueError if the log level is invalid.",
        "Register a new range type as a PostgreSQL range.\n\n        >>> register_range_type(\"int4range\", intrange, conn)\n\n    The above will make sure intrange is regarded as an int4range for queries\n    and that int4ranges will be cast into intrange when fetching rows.\n\n    pgrange should be the full name including schema for the custom range type.\n\n    Note that adaption is global, meaning if a range type is passed to a regular\n    psycopg2 connection it will adapt it to its proper range type. Parsing of\n    rows from the database however is not global and just set on a per connection\n    basis.",
        "Acquires the correct error for a given response.\n\n  :param requests.Response response: HTTP error response\n  :returns: the appropriate error for a given response\n  :rtype: APIError",
        "Converts the request parameters to Python.\n\n    :param request: <pyramid.request.Request> || <dict>\n\n    :return: <dict>",
        "Extracts ORB context information from the request.\n\n    :param request: <pyramid.request.Request>\n    :param model: <orb.Model> || None\n\n    :return: {<str> key: <variant> value} values, <orb.Context>",
        "Handles real-time updates to the order book.",
        "Used exclusively as a thread which keeps the WebSocket alive.",
        "Connects and subscribes to the WebSocket Feed.",
        "Marks a view function as being exempt from the cached httpbl view protection.",
        "Hook point for overriding how the CounterPool gets its connection to\n        AWS.",
        "Hook point for overriding how the CounterPool determines the schema\n        to be used when creating a missing table.",
        "Hook point for overriding how the CounterPool creates a new table\n        in DynamooDB",
        "Hook point for overriding how the CounterPool transforms table_name\n        into a boto DynamoDB Table object.",
        "Hook point for overriding how the CouterPool creates a DynamoDB item\n        for a given counter when an existing item can't be found.",
        "Hook point for overriding how the CouterPool fetches a DynamoDB item\n        for a given counter.",
        "Gets the DynamoDB item behind a counter and ties it to a Counter\n        instace.",
        "Use an event to build a many-to-one relationship on a class.\n\n    This makes use of the :meth:`.References._reference_table` method\n    to generate a full foreign key relationship to the remote table.",
        "Use an event to build a one-to-many relationship on a class.\n\n    This makes use of the :meth:`.References._reference_table` method\n    to generate a full foreign key relationship from the remote table.",
        "Djeffify data between tags",
        "Create a foreign key reference from the local class to the given remote\n        table.\n\n        Adds column references to the declarative class and adds a\n        ForeignKeyConstraint.",
        "Path join helper method\n    Join paths if list passed\n\n    :type path: str|unicode|list\n    :rtype: str|unicode",
        "Read helper method\n\n    :type file_path: str|unicode\n    :type encoding: str|unicode\n    :rtype: str|unicode",
        "Write helper method\n\n    :type file_path: str|unicode\n    :type contents: str|unicode\n    :type encoding: str|unicode",
        "Copy file helper method\n\n    :type src: str|unicode\n    :type dest: str|unicode",
        "Split file name and extension\n\n    :type path: str|unicode\n    :rtype: one str|unicode",
        "Helper method for absolute and relative paths resolution\n        Split passed path and return each directory parts\n\n        example: \"/usr/share/dir\"\n        return: [\"usr\", \"share\", \"dir\"]\n\n        @type path: one of (unicode, str)\n        @rtype: list",
        "Creates fully qualified endpoint URIs.\n\n    :param parts: the string parts that form the request URI",
        "Makes sure we have proper ISO 8601 time.\n\n    :param time: either already ISO 8601 a string or datetime.datetime\n    :returns: ISO 8601 time\n    :rtype: str",
        "Returns the given response or raises an APIError for non-2xx responses.\n\n    :param requests.Response response: HTTP response\n    :returns: requested data\n    :rtype: requests.Response\n    :raises APIError: for non-2xx responses",
        "Checks if a next message is possible.\n\n    :returns: True if a next message is possible, otherwise False\n    :rtype: bool",
        "Colors text with code and given format",
        "Registers the given message type in the local database.\n\n    Args:\n      message: a message.Message, to be registered.\n\n    Returns:\n      The provided message.",
        "Insert object before index.\n\n        :param int index: index to insert in\n        :param string value: path to insert",
        "Parse runtime path representation to list.\n\n        :param string string: runtime path string\n        :return: list of runtime paths\n        :rtype: list of string",
        "Add some bundle to build group\n\n        :type bundle: static_bundle.bundles.AbstractBundle\n        @rtype: BuildGroup",
        "Return collected files links\n\n        :rtype: list[static_bundle.files.StaticFileResult]",
        "Asset minifier\n        Uses default minifier in bundle if it's not defined\n\n        :rtype: static_bundle.minifiers.DefaultMinifier|None",
        "Render all includes in asset by names\n\n        :type name: str|unicode\n        :rtype: str|unicode",
        "Return links without build files",
        "Coerce everything to strings.\n    All objects representing time get output according to default_date_fmt.",
        "Initialize the zlogger.\n\n    Sets up a rotating file handler to the specified path and file with\n    the given size and backup count limits, sets the default\n    application_name, server_hostname, and default/whitelist fields.\n\n    :param path: path to write the log file\n    :param target: name of the log file\n    :param logger_name: name of the logger (defaults to root)\n    :param level: log level for this logger (defaults to logging.DEBUG)\n    :param maxBytes: size of the file before rotation (default 1MB)\n    :param application_name: app name to add to each log entry\n    :param server_hostname: hostname to add to each log entry\n    :param fields: default/whitelist fields.\n    :type path: string\n    :type target: string\n    :type logger_name: string\n    :type level",
        "formats a logging.Record into a standard json log entry\n\n        :param record: record to be formatted\n        :type record: logging.Record\n        :return: the formatted json string\n        :rtype: string",
        "Initialize the model for a Pyramid app.\n\n    Activate this setup using ``config.include('baka_model')``.",
        "Return absolute and relative path for file\n\n        :type root_path: str|unicode\n        :type file_name: str|unicode\n        :type input_dir: str|unicode\n        :rtype: tuple",
        "Adds an EnumDescriptor to the pool.\n\n    This method also registers the FileDescriptor associated with the message.\n\n    Args:\n      enum_desc: An EnumDescriptor.",
        "Gets the FileDescriptor for the file containing the specified symbol.\n\n    Args:\n      symbol: The name of the symbol to search for.\n\n    Returns:\n      A FileDescriptor that contains the specified symbol.\n\n    Raises:\n      KeyError: if the file can not be found in the pool.",
        "Loads the named descriptor from the pool.\n\n    Args:\n      full_name: The full name of the descriptor to load.\n\n    Returns:\n      The descriptor for the named type.",
        "Loads the named enum descriptor from the pool.\n\n    Args:\n      full_name: The full name of the enum descriptor to load.\n\n    Returns:\n      The enum descriptor for the named type.",
        "Loads the named extension descriptor from the pool.\n\n    Args:\n      full_name: The full name of the extension descriptor to load.\n\n    Returns:\n      A FieldDescriptor, describing the named extension.",
        "Make a protobuf EnumDescriptor given an EnumDescriptorProto protobuf.\n\n    Args:\n      enum_proto: The descriptor_pb2.EnumDescriptorProto protobuf message.\n      package: Optional package name for the new message EnumDescriptor.\n      file_desc: The file containing the enum descriptor.\n      containing_type: The type containing this enum.\n      scope: Scope containing available types.\n\n    Returns:\n      The added descriptor",
        "Creates a field descriptor from a FieldDescriptorProto.\n\n    For message and enum type fields, this method will do a look up\n    in the pool for the appropriate descriptor for that type. If it\n    is unavailable, it will fall back to the _source function to\n    create it. If this type is still unavailable, construction will\n    fail.\n\n    Args:\n      field_proto: The proto describing the field.\n      message_name: The name of the containing message.\n      index: Index of the field\n      is_extension: Indication that this field is for an extension.\n\n    Returns:\n      An initialized FieldDescriptor object",
        "Get a ``sqlalchemy.orm.Session`` instance backed by a transaction.\n\n    This function will hook the session to the transaction manager which\n    will take care of committing any changes.\n\n    - When using pyramid_tm it will automatically be committed or aborted\n      depending on whether an exception is raised.\n\n    - When using scripts you should wrap the session in a manager yourself.\n      For example::\n\n          import transaction\n\n          engine = get_engine(settings)\n          session_factory = get_session_factory(engine)\n          with transaction.manager:\n              dbsession = get_tm_session(session_factory, transaction.manager)",
        "Generate a random string of the specified length.\n\n    The returned string is composed of an alphabet that shouldn't include any\n    characters that are easily mistakeable for one another (I, 1, O, 0), and\n    hopefully won't accidentally contain any English-language curse words.",
        "Require that the named `field` has the right `data_type`",
        "Forces a flush from the internal queue to the server",
        "Use all decompressor possible to make the stream",
        "Manage a Marv site",
        "Returns a decoder for a MessageSet item.\n\n  The parameter is the _extensions_by_number map for the message class.\n\n  The message set message looks like this:\n    message MessageSet {\n      repeated group Item = 1 {\n        required int32 type_id = 2;\n        required string message = 3;\n      }\n    }",
        "Flask like implementation of getting the applicaiton name via\n    the filename of the including file",
        "Given a Python function name, return the function it refers to.",
        "Add a function to the function list, in order.",
        "Return the mapping of a document according to the function list.",
        "Reduce several mapped documents by several reduction functions.",
        "Re-reduce a set of values, with a list of rereduction functions.",
        "Validate...this function is undocumented, but still in CouchDB.",
        "The main function called to handle a request.",
        "Log an event on the CouchDB server.",
        "Generates a universally unique ID.\n    Any arguments only create more randomness.",
        "revoke_token removes the access token from the data_store",
        "_auth - internal method to ensure the client_id and client_secret passed with\n        the nonce match",
        "_validate_request_code - internal method for verifying the the given nonce.\n        also removes the nonce from the data_store, as they are intended for\n        one-time use.",
        "_generate_token - internal function for generating randomized alphanumberic\n        strings of a given length",
        "Merge multiple ordered so that within-ordered order is preserved",
        "Helps us validate the parameters for the request\n\n    :param valid_options: a list of strings of valid options for the\n                          api request\n    :param params: a dict, the key-value store which we really only care about\n                   the key which has tells us what the user is using for the\n                   API request\n\n    :returns: None or throws an exception if the validation fails",
        "Get current datetime for every file.",
        "run your main spider here\n        as for branch spider result data, you can return everything or do whatever with it\n        in your own code\n\n        :return: None",
        "Read version info from a file without importing it",
        "Make a protobuf Descriptor given a DescriptorProto protobuf.\n\n  Handles nested descriptors. Note that this is limited to the scope of defining\n  a message inside of another message. Composite fields can currently only be\n  resolved if the message is defined in the same scope as the field.\n\n  Args:\n    desc_proto: The descriptor_pb2.DescriptorProto message.\n    package: Optional package name for the new message Descriptor (string).\n    build_file_if_cpp: Update the C++ descriptor pool if api matches.\n                       Set to False on recursion, so no duplicates are created.\n    syntax: The syntax/semantics that should be used.  Set to \"proto3\" to get\n            proto3 field presence semantics.\n  Returns:\n    A Descriptor for protobuf messages.",
        "Returns the root if this is a nested type, or itself if its the root.",
        "Searches for the specified method, and returns its descriptor.",
        "Converts protobuf message to JSON format.\n\n  Args:\n    message: The protocol buffers message instance to serialize.\n    including_default_value_fields: If True, singular primitive fields,\n        repeated fields, and map fields will always be serialized.  If\n        False, only serialize non-empty fields.  Singular message fields\n        and oneof fields are not affected by this option.\n\n  Returns:\n    A string containing the JSON formatted protocol buffer message.",
        "Converts message to an object according to Proto3 JSON Specification.",
        "Converts Struct message according to Proto3 JSON Specification.",
        "Parses a JSON representation of a protocol message into a message.\n\n  Args:\n    text: Message JSON representation.\n    message: A protocol beffer message to merge into.\n\n  Returns:\n    The same message passed as argument.\n\n  Raises::\n    ParseError: On JSON parsing problems.",
        "Convert field value pairs into regular message.\n\n  Args:\n    js: A JSON object to convert the field value pairs.\n    message: A regular protocol message to record the data.\n\n  Raises:\n    ParseError: In case of problems converting.",
        "Convert a JSON object into a message.\n\n  Args:\n    value: A JSON object.\n    message: A WKT or regular protocol message to record the data.\n\n  Raises:\n    ParseError: In case of convert problems.",
        "Convert a JSON representation into Value message.",
        "Convert a JSON representation into ListValue message.",
        "Convert a JSON representation into Struct message.",
        "Update config options with the provided dictionary of options.",
        "Completes measuring time interval and updates counter.",
        "Converts Duration to string format.\n\n    Returns:\n      A string converted from self. The string format will contains\n      3, 6, or 9 fractional digits depending on the precision required to\n      represent the exact Duration value. For example: \"1s\", \"1.010s\",\n      \"1.000000100s\", \"-3.100s\"",
        "Converts a string to Duration.\n\n    Args:\n      value: A string to be converted. The string must end with 's'. Any\n          fractional digits (or none) are accepted as long as they fit into\n          precision. For example: \"1s\", \"1.01s\", \"1.0000001s\", \"-3.100s\n\n    Raises:\n      ParseError: On parsing problems.",
        "Converts string to FieldMask according to proto3 JSON spec.",
        "Return a CouchDB document, given its ID, revision and database name.",
        "Give reST format README for pypi.",
        "remove records from collection whose parameters match kwargs",
        "Resolve the URL to this point.\n\n        >>> trello = TrelloAPIV1('APIKEY')\n        >>> trello.batch._url\n        '1/batch'\n        >>> trello.boards(board_id='BOARD_ID')._url\n        '1/boards/BOARD_ID'\n        >>> trello.boards(board_id='BOARD_ID')(field='FIELD')._url\n        '1/boards/BOARD_ID/FIELD'\n        >>> trello.boards(board_id='BOARD_ID').cards(filter='FILTER')._url\n        '1/boards/BOARD_ID/cards/FILTER'",
        "Makes the HTTP request.",
        "Skips over a field value.\n\n  Args:\n    tokenizer: A tokenizer to parse the field name and values.\n\n  Raises:\n    ParseError: In case an invalid field value is found.",
        "Parses an integer.\n\n  Args:\n    text: The text to parse.\n    is_signed: True if a signed integer must be parsed.\n    is_long: True if a long integer must be parsed.\n\n  Returns:\n    The integer value.\n\n  Raises:\n    ValueError: Thrown Iff the text is not a valid integer.",
        "Convert protobuf message to text format.\n\n    Args:\n      message: The protocol buffers message.",
        "Converts an text representation of a protocol message into a message.\n\n    Args:\n      lines: Lines of a message's text representation.\n      message: A protocol buffer message to merge into.\n\n    Raises:\n      ParseError: On text parsing problems.",
        "Merges a single scalar field into a message.\n\n    Args:\n      tokenizer: A tokenizer to parse the field value.\n      message: The message of which field is a member.\n      field: The descriptor of the field to be merged.\n\n    Raises:\n      ParseError: In case of text parsing problems.",
        "Consumes protocol message field identifier.\n\n    Returns:\n      Identifier string.\n\n    Raises:\n      ParseError: If an identifier couldn't be consumed.",
        "Consumes a signed 32bit integer number.\n\n    Returns:\n      The integer parsed.\n\n    Raises:\n      ParseError: If a signed 32bit integer couldn't be consumed.",
        "Consumes an floating point number.\n\n    Returns:\n      The number parsed.\n\n    Raises:\n      ParseError: If a floating point number couldn't be consumed.",
        "Consumes a boolean value.\n\n    Returns:\n      The bool parsed.\n\n    Raises:\n      ParseError: If a boolean value couldn't be consumed.",
        "Consume one token of a string literal.\n\n    String literals (whether bytes or text) can come in multiple adjacent\n    tokens which are automatically concatenated, like in C or Python.  This\n    method only consumes one token.\n\n    Returns:\n      The token parsed.\n    Raises:\n      ParseError: When the wrong format data is found.",
        "convert ark timestamp to unix timestamp",
        "Close the connection.",
        "Replace macros with content defined in the config.\n\n        :param content: Markdown content\n\n        :returns: Markdown content without macros",
        "Return a pathname possibly with a number appended to it so that it is\r\n\tunique in the directory.",
        "Append numbers in sequential order to the filename or folder name\r\n\tNumbers should be appended before the extension on a filename.",
        "Custom version of splitext that doesn't perform splitext on directories",
        "Set the modified time of a file",
        "Get the modified time for a file as a datetime instance",
        "wrap a function that returns a dir, making sure it exists",
        "Check whether a file is presumed hidden, either because\r\n\tthe pathname starts with dot or because the platform\r\n\tindicates such.",
        "Get closer to your EOL",
        "Open a connection over the serial line and receive data lines",
        "create & start main thread\n\n        :return: None",
        "Scans through all children of node and gathers the\n    text. If node has non-text child-nodes then\n    NotTextNodeError is raised.",
        "Get the number of credits remaining at AmbientSMS",
        "Send a mesage via the AmbientSMS API server",
        "Inteface for sending web requests to the AmbientSMS API Server",
        "Called for each file\n        Must return file content\n        Can be wrapped\n\n        :type f: static_bundle.files.StaticFileResult\n        :type text: str|unicode\n        :rtype: str|unicode",
        "Return True if the class is a date type.",
        "Convert a date or time to a datetime. If when is a date then it sets the time to midnight. If\n    when is a time it sets the date to the epoch. If when is None or a datetime it returns when.\n    Otherwise a TypeError is raised. Returned datetimes have tzinfo set to None unless when is a\n    datetime with tzinfo set in which case it remains the same.",
        "Return a date, time, or datetime converted to a datetime in the given timezone. If when is a\n    datetime and has no timezone it is assumed to be local time. Date and time objects are also\n    assumed to be UTC. The tz value defaults to UTC. Raise TypeError if when cannot be converted to\n    a datetime.",
        "Return a Unix timestamp in seconds for the provided datetime. The `totz` function is called\n    on the datetime to convert it to the provided timezone. It will be converted to UTC if no\n    timezone is provided.",
        "Return a Unix timestamp in milliseconds for the provided datetime. The `totz` function is\n    called on the datetime to convert it to the provided timezone. It will be converted to UTC if\n    no timezone is provided.",
        "Return the datetime representation of the provided Unix timestamp. By defaults the timestamp is\n    interpreted as UTC. If tzin is set it will be interpreted as this timestamp instead. By default\n    the output datetime will have UTC time. If tzout is set it will be converted in this timezone\n    instead.",
        "Return the Unix timestamp in milliseconds as a datetime object. If tz is set it will be\n    converted to the requested timezone otherwise it defaults to UTC.",
        "Return the datetime truncated to the precision of the provided unit.",
        "Return the date for the day of this week.",
        "Internal function that determines EOL_STYLE_NATIVE constant with the proper value for the\n    current platform.",
        "Normalizes a path maintaining the final slashes.\n\n    Some environment variables need the final slash in order to work.\n\n    Ex. The SOURCES_DIR set by subversion must end with a slash because of the way it is used\n    in the Visual Studio projects.\n\n    :param unicode path:\n        The path to normalize.\n\n    :rtype: unicode\n    :returns:\n        Normalized path",
        "Returns a version of a path that is unique.\n\n    Given two paths path1 and path2:\n        CanonicalPath(path1) == CanonicalPath(path2) if and only if they represent the same file on\n        the host OS. Takes account of case, slashes and relative paths.\n\n    :param unicode path:\n        The original path.\n\n    :rtype: unicode\n    :returns:\n        The unique path.",
        "Replaces all slashes and backslashes with the target separator\n\n    StandardPath:\n        We are defining that the standard-path is the one with only back-slashes in it, either\n        on Windows or any other platform.\n\n    :param bool strip:\n        If True, removes additional slashes from the end of the path.",
        "Copy a file from source to target.\n\n    :param  source_filename:\n        @see _DoCopyFile\n\n    :param  target_filename:\n        @see _DoCopyFile\n\n    :param bool md5_check:\n        If True, checks md5 files (of both source and target files), if they match, skip this copy\n        and return MD5_SKIP\n\n        Md5 files are assumed to be {source, target} + '.md5'\n\n        If any file is missing (source, target or md5), the copy will always be made.\n\n    :param  copy_symlink:\n        @see _DoCopyFile\n\n    :raises FileAlreadyExistsError:\n        If target_filename already exists, and override is False\n\n    :raises NotImplementedProtocol:\n        If file protocol is not accepted\n\n        Protocols allowed are:\n            source",
        "Copy a file locally to a directory.\n\n    :param unicode source_filename:\n        The filename to copy from.\n\n    :param unicode target_filename:\n        The filename to copy to.\n\n    :param bool copy_symlink:\n        If True and source_filename is a symlink, target_filename will also be created as\n        a symlink.\n\n        If False, the file being linked will be copied instead.",
        "Copy files from the given source to the target.\n\n    :param unicode source_dir:\n        A filename, URL or a file mask.\n        Ex.\n            x:\\coilib50\n            x:\\coilib50\\*\n            http://server/directory/file\n            ftp://server/directory/file\n\n\n    :param unicode target_dir:\n        A directory or an URL\n        Ex.\n            d:\\Temp\n            ftp://server/directory\n\n    :param bool create_target_dir:\n        If True, creates the target path if it doesn't exists.\n\n    :param bool md5_check:\n        .. seealso:: CopyFile\n\n    :raises DirectoryNotFoundError:\n        If target_dir does not exist, and create_target_dir is False\n\n    .. seealso:: CopyFile for documentation on accepted protocols\n\n    .. seealso::",
        "Copies files into directories, according to a file mapping\n\n    :param list(tuple(unicode,unicode)) file_mapping:\n        A list of mappings between the directory in the target and the source.\n        For syntax, @see: ExtendedPathMask\n\n    :rtype: list(tuple(unicode,unicode))\n    :returns:\n        List of files copied. (source_filename, target_filename)\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information",
        "Recursively copy a directory tree.\n\n    :param unicode source_dir:\n        Where files will come from\n\n    :param unicode target_dir:\n        Where files will go to\n\n    :param bool override:\n        If True and target_dir already exists, it will be deleted before copying.\n\n    :raises NotImplementedForRemotePathError:\n        If trying to copy to/from remote directories",
        "Deletes the given local filename.\n\n    .. note:: If file doesn't exist this method has no effect.\n\n    :param unicode target_filename:\n        A local filename\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a non-local path\n\n    :raises FileOnlyActionError:\n        Raised when filename refers to a directory.",
        "Appends content to a local file.\n\n    :param unicode filename:\n\n    :param unicode contents:\n\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n        Replaces the EOL by the appropriate EOL depending on the eol_style value.\n        Considers that all content is using only \"\\n\" as EOL.\n\n    :param unicode encoding:\n        Target file's content encoding.\n        Defaults to sys.getfilesystemencoding()\n\n    :param bool binary:\n        If True, content is appended in binary mode. In this case, `contents` must be `bytes` and not\n        `unicode`\n\n    :raises NotImplementedForRemotePathError:\n        If trying to modify a non-local path\n\n    :raises ValueError:\n        If trying to mix unicode `contents` without `encoding`, or `encoding` without\n        unicode",
        "Moves a file.\n\n    :param unicode source_filename:\n\n    :param unicode target_filename:\n\n    :raises NotImplementedForRemotePathError:\n        If trying to operate with non-local files.",
        "Moves a directory.\n\n    :param unicode source_dir:\n\n    :param unicode target_dir:\n\n    :raises NotImplementedError:\n        If trying to move anything other than:\n            Local dir -> local dir\n            FTP dir -> FTP dir (same host)",
        "Reads a file and returns its contents. Works for both local and remote files.\n\n    :param unicode filename:\n\n    :param bool binary:\n        If True returns the file as is, ignore any EOL conversion.\n\n    :param unicode encoding:\n        File's encoding. If not None, contents obtained from file will be decoded using this\n        `encoding`.\n\n    :param None|''|'\\n'|'\\r'|'\\r\\n' newline:\n        Controls universal newlines.\n        See 'io.open' newline parameter documentation for more details.\n\n    :returns str|unicode:\n        The file's contents.\n        Returns unicode string when `encoding` is not None.\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information",
        "Reads a file and returns its contents as a list of lines. Works for both local and remote files.\n\n    :param unicode filename:\n\n    :param None|''|'\\n'|'\\r'|'\\r\\n' newline:\n        Controls universal newlines.\n        See 'io.open' newline parameter documentation for more details.\n\n    :param unicode encoding:\n        File's encoding. If not None, contents obtained from file will be decoded using this\n        `encoding`.\n\n    :returns list(unicode):\n        The file's lines\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information",
        "Lists the files in the given directory\n\n    :type directory: unicode | unicode\n    :param directory:\n        A directory or URL\n\n    :rtype: list(unicode) | list(unicode)\n    :returns:\n        List of filenames/directories found in the given directory.\n        Returns None if the given directory does not exists.\n\n        If `directory` is a unicode string, all files returned will also be unicode\n\n    :raises NotImplementedProtocol:\n        If file protocol is not local or FTP\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information",
        "Create a file with the given contents.\n\n    :param unicode filename:\n        Filename and path to be created.\n\n    :param unicode contents:\n        The file contents as a string.\n\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n        Replaces the EOL by the appropriate EOL depending on the eol_style value.\n        Considers that all content is using only \"\\n\" as EOL.\n\n    :param bool create_dir:\n        If True, also creates directories needed in filename's path\n\n    :param unicode encoding:\n        Target file's content encoding. Defaults to sys.getfilesystemencoding()\n        Ignored if `binary` = True\n\n    :param bool binary:\n        If True, file is created in binary mode. In this case, `contents` must be `bytes` and not\n        `unicode`\n\n   ",
        "Replaces all occurrences of \"old\" by \"new\" in the given file.\n\n    :param unicode filename:\n        The name of the file.\n\n    :param unicode old:\n        The string to search for.\n\n    :param unicode new:\n        Replacement string.\n\n    :return unicode:\n        The new contents of the file.",
        "Create directory including any missing intermediate directory.\n\n    :param unicode directory:\n\n    :return unicode|urlparse.ParseResult:\n        Returns the created directory or url (see urlparse).\n\n    :raises NotImplementedProtocol:\n        If protocol is not local or FTP.\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information",
        "Deletes a directory.\n\n    :param unicode directory:\n\n    :param bool skip_on_error:\n        If True, ignore any errors when trying to delete directory (for example, directory not\n        found)\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a remote directory.",
        "On Windows, returns a list of mapped network drives\n\n    :return: tuple(string, string, bool)\n        For each mapped netword drive, return 3 values tuple:\n            - the local drive\n            - the remote path-\n            - True if the mapping is enabled (warning: not reliable)",
        "Create a symbolic link at `link_path` pointing to `target_path`.\n\n    :param unicode target_path:\n        Link target\n\n    :param unicode link_path:\n        Fullpath to link name\n\n    :param bool override:\n        If True and `link_path` already exists as a link, that link is overridden.",
        "Read the target of the symbolic link at `path`.\n\n    :param unicode path:\n        Path to a symbolic link\n\n    :returns unicode:\n        Target of a symbolic link",
        "Checks if a given path is local, raise an exception if not.\n\n    This is used in filesystem functions that do not support remote operations yet.\n\n    :param unicode path:\n\n    :raises NotImplementedForRemotePathError:\n        If the given path is not local",
        "Replaces eol on each line by the given eol_style.\n\n    :param unicode contents:\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:",
        "Verifies if a filename match with given patterns.\n\n    :param str filename: The filename to match.\n    :param list(str) masks: The patterns to search in the filename.\n    :return bool:\n        True if the filename has matched with one pattern, False otherwise.",
        "Searches for files in a given directory that match with the given patterns.\n\n    :param str dir_: the directory root, to search the files.\n    :param list(str) in_filters: a list with patterns to match (default = all). E.g.: ['*.py']\n    :param list(str) out_filters: a list with patterns to ignore (default = none). E.g.: ['*.py']\n    :param bool recursive: if True search in subdirectories, otherwise, just in the root.\n    :param bool include_root_dir: if True, includes the directory being searched in the returned paths\n    :param bool standard_paths: if True, always uses unix path separators \"/\"\n    :return list(str):\n        A list of strings with the files that matched (with the full path in the filesystem).",
        "os.path.expanduser wrapper, necessary because it cannot handle unicode strings properly.\n\n    This is not necessary in Python 3.\n\n    :param path:\n        .. seealso:: os.path.expanduser",
        "Helper to iterate over the files in a directory putting those in the passed StringIO in ini\n    format.\n\n    :param unicode directory:\n        The directory for which the hash should be done.\n\n    :param StringIO stringio:\n        The string to which the dump should be put.\n\n    :param unicode base:\n        If provided should be added (along with a '/') before the name=hash of file.\n\n    :param unicode exclude:\n        Pattern to match files to exclude from the hashing. E.g.: *.gz\n\n    :param unicode include:\n        Pattern to match files to include in the hashing. E.g.: *.zip",
        "Iterator for random hexadecimal hashes\n\n    :param iterator_size:\n        Amount of hashes return before this iterator stops.\n        Goes on forever if `iterator_size` is negative.\n\n    :param int hash_length:\n        Size of each hash returned.\n\n    :return generator(unicode):",
        "A context manager to replace and restore a value using a getter and setter.\n\n    :param object obj: The object to replace/restore.\n    :param object key: The key to replace/restore in the object.\n    :param object value: The value to replace.\n\n    Example::\n\n      with PushPop2(sys.modules, 'alpha', None):\n        pytest.raises(ImportError):\n          import alpha",
        "Return the database specifier for a database string.\n    \n    This accepts a database name or URL, and returns a database specifier in the\n    format accepted by ``specifier_to_db``. It is recommended that you consult\n    the documentation for that function for an explanation of the format.",
        "Return a CouchDB database instance from a database string.",
        "Make sure a DB specifier exists, creating it if necessary.",
        "Exclude NoSet objec\n\n    .. code-block::\n\n        >>> coerce(NoSet, 'value')\n        'value'",
        "Parse a hub key into a dictionary of component parts\n\n    :param key: str, a hub key\n    :returns: dict, hub key split into parts\n    :raises: ValueError",
        "Raise an exception if string doesn't match a part's regex\n\n    :param string: str\n    :param part: a key in the PARTS dict\n    :raises: ValueError, TypeError",
        "apply default settings to commands\n            not static, shadow \"self\" in eval",
        "add commands to parser",
        "get config for subparser and create commands",
        "custom command line  action to show version",
        "custom command line action to check file exist",
        "Return the consumer and oauth tokens with three-legged OAuth process and\n    save in a yaml file in the user's home directory.",
        "Adds properties for all fields in this protocol message type.",
        "Unpacks Any message and returns the unpacked message.\n\n  This internal method is differnt from public Any Unpack method which takes\n  the target message as argument. _InternalUnpackAny method does not have\n  target message type and need to find the message type in descriptor pool.\n\n  Args:\n    msg: An Any message to be unpacked.\n\n  Returns:\n    The unpacked message."
    ],
    "references": [
        [
            "Stops monitoring the predefined directory."
        ],
        [
            "Called when a file in the monitored directory has been moved.\n\n        Breaks move down into a delete and a create (which it is sometimes detected as!).\n        :param event: the file system event"
        ],
        [
            "Tears down all temp files and directories."
        ],
        [
            "Test whether a file target is not exists or it exists but allow\n        overwrite."
        ],
        [
            "Copy this file to other place."
        ],
        [
            "Clients a Docker client.\n\n    Will raise a `ConnectionError` if the Docker daemon is not accessible.\n    :return: the Docker client"
        ],
        [
            "Decorate methods when repository path is required."
        ],
        [
            "clean repository given before and after states"
        ],
        [
            "Get repository descriptive stats\n\n        :Returns:\n            #. numberOfDirectories (integer): Number of diretories in repository\n            #. numberOfFiles (integer): Number of files in repository"
        ],
        [
            "Reset repository instance."
        ],
        [
            "Load repository from a directory path and update the current instance.\n        First, new repository still will be loaded. If failed, then old\n        style repository load will be tried.\n\n        :Parameters:\n            #. path (string): The path of the directory from where to load\n               the repository from. If '.' or an empty string is passed,\n               the current working directory will be used.\n            #. verbose (boolean): Whether to be verbose about abnormalities\n            #. ntrials (int): After aquiring all locks, ntrials is the maximum\n               number of trials allowed before failing.\n               In rare cases, when multiple processes\n               are accessing the same repository components, different processes\n               can alter repository components between successive lock releases\n               of some other process. Bigger number of trials lowers the\n               likelyhood of failure due to multiple processes same time\n               alteration.\n\n        :Returns:\n             #. repository (pyrep.Repository): returns self repository with loaded data."
        ],
        [
            "Remove all repository from path along with all repository tracked files.\n\n        :Parameters:\n            #. path (None, string): The path the repository to remove.\n            #. removeEmptyDirs (boolean): Whether to remove remaining empty\n               directories."
        ],
        [
            "Get whether creating a file or a directory from the basenane of the given\n        path is allowed\n\n        :Parameters:\n            #. path (str): The absolute or relative path or simply the file\n               or directory name.\n\n        :Returns:\n            #. allowed (bool): Whether name is allowed.\n            #. message (None, str): Reason for the name to be forbidden."
        ],
        [
            "Given a path, return relative path to diretory\n\n        :Parameters:\n            #. path (str): Path as a string\n            #. split (boolean): Whether to split path to its components\n\n        :Returns:\n            #. relativePath (str, list): Relative path as a string or as a list\n               of components if split is True"
        ],
        [
            "Get a list representation of repository state along with useful\n        information. List state is ordered relativeley to directories level\n\n        :Parameters:\n            #. relaPath (None, str): relative directory path from where to\n               start. If None all repository representation is returned.\n\n        :Returns:\n            #. state (list): List representation of the repository.\n               List items are all dictionaries. Every dictionary has a single\n               key which is the file or the directory name and the value is a\n               dictionary of information including:\n\n                   * 'type': the type of the tracked whether it's file, dir, or objectdir\n                   * 'exists': whether file or directory actually exists on disk\n                   * 'pyrepfileinfo': In case of a file or an objectdir whether .%s_pyrepfileinfo exists\n                   * 'pyrepdirinfo': In case of a directory whether .pyrepdirinfo exists"
        ],
        [
            "Get file information dict from the repository given its relative path.\n\n        :Parameters:\n            #. relativePath (string): The relative to the repository path of\n               the file.\n\n        :Returns:\n            #. info (None, dictionary): The file information dictionary.\n               If None, it means an error has occurred.\n            #. errorMessage (string): The error message if any error occurred."
        ],
        [
            "Check whether a given relative path is a repository file path\n\n        :Parameters:\n            #. relativePath (string): File relative path\n\n        :Returns:\n            #. isRepoFile (boolean): Whether file is a repository file.\n            #. isFileOnDisk (boolean): Whether file is found on disk.\n            #. isFileInfoOnDisk (boolean): Whether file info is found on disk.\n            #. isFileClassOnDisk (boolean): Whether file class is found on disk."
        ],
        [
            "Create a tar file package of all the repository files and directories.\n        Only files and directories that are tracked in the repository\n        are stored in the package tar file.\n\n        **N.B. On some systems packaging requires root permissions.**\n\n        :Parameters:\n            #. path (None, string): The real absolute path where to create the\n               package. If None, it will be created in the same directory as\n               the repository. If '.' or an empty string is passed, the current\n               working directory will be used.\n            #. name (None, string): The name to give to the package file\n               If None, the package directory name will be used with the\n               appropriate extension added.\n            #. mode (None, string): The writing mode of the tarfile.\n               If None, automatically the best compression mode will be chose.\n               Available modes are ('w', 'w:', 'w:gz', 'w:bz2')"
        ],
        [
            "Renames an item in this collection as a transaction.\n\n        Will override if new key name already exists.\n        :param key: the current name of the item\n        :param new_key: the new name that the item should have"
        ],
        [
            "Use default hash method to return hash value of a piece of string\n    default setting use 'utf-8' encoding."
        ],
        [
            "Return md5 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file\n\n    CPU = i7-4600U 2.10GHz - 2.70GHz, RAM = 8.00 GB\n    1 second can process 0.25GB data\n\n    - 0.59G - 2.43 sec\n    - 1.3G - 5.68 sec\n    - 1.9G - 7.72 sec\n    - 2.5G - 10.32 sec\n    - 3.9G - 16.0 sec"
        ],
        [
            "Return sha256 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file"
        ],
        [
            "Return sha512 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file"
        ],
        [
            "A command line auto complete similar behavior. Find all item with same\n        prefix of this one.\n\n        :param case_sensitive: toggle if it is case sensitive.\n        :return: list of :class:`pathlib_mate.pathlib2.Path`."
        ],
        [
            "Print ``top_n`` big dir in this dir."
        ],
        [
            "Print ``top_n`` big file in this dir."
        ],
        [
            "Print ``top_n`` big dir and ``top_n`` big file in each dir."
        ],
        [
            "Create a new folder having exactly same structure with this directory.\n        However, all files are just empty file with same file name.\n\n        :param dst: destination directory. The directory can't exists before\n        you execute this.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u521b\u5efa\u4e00\u4e2a\u76ee\u5f55\u7684\u955c\u50cf\u62f7\u8d1d, \u4e0e\u62f7\u8d1d\u64cd\u4f5c\u4e0d\u540c\u7684\u662f, \u6587\u4ef6\u7684\u526f\u672c\u53ea\u662f\u5728\u6587\u4ef6\u540d\u4e0a\n        \u4e0e\u539f\u4ef6\u4e00\u81f4, \u4f46\u662f\u662f\u7a7a\u6587\u4ef6, \u5b8c\u5168\u6ca1\u6709\u5185\u5bb9, \u6587\u4ef6\u5927\u5c0f\u4e3a0\u3002"
        ],
        [
            "Execute every ``.py`` file as main script.\n\n        :param py_exe: str, python command or python executable path.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709Python\u6587\u4ef6\u4f5c\u4e3a\u4e3b\u811a\u672c\u7528\u5f53\u524d\u89e3\u91ca\u5668\u8fd0\u884c\u3002"
        ],
        [
            "Trail white space at end of each line for every ``.py`` file.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u88ab\u9009\u62e9\u7684\u6587\u4ef6\u4e2d\u884c\u672b\u7684\u7a7a\u683c\u5220\u9664\u3002"
        ],
        [
            "Auto convert your python code in a directory to pep8 styled code.\n\n        :param kwargs: arguments for ``autopep8.fix_code`` method.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709Python\u6587\u4ef6\u7528pep8\u98ce\u683c\u683c\u5f0f\u5316\u3002\u589e\u52a0\u5176\u53ef\u8bfb\u6027\u548c\u89c4\u8303\u6027\u3002"
        ],
        [
            "File size in bytes."
        ],
        [
            "Get most recent modify time in timestamp."
        ],
        [
            "Get most recent access time in timestamp."
        ],
        [
            "Get most recent create time in timestamp."
        ],
        [
            "Lists options that have not been used to format other values in \n        their sections. \n        \n        Good for finding out if the user has misspelled any of the options."
        ],
        [
            "List names of options and positional arguments."
        ],
        [
            "Add an Option object to the user interface."
        ],
        [
            "Append a positional argument to the user interface.\n\n        Optional positional arguments must be added after the required ones. \n        The user interface can have at most one recurring positional argument, \n        and if present, that argument must be the last one."
        ],
        [
            "Read program documentation from a DocParser compatible file.\n\n        docsfiles is a list of paths to potential docsfiles: parse if present.\n        A string is taken as a list of one item."
        ],
        [
            "Return user friendly help on program options."
        ],
        [
            "Return user friendly help on positional arguments in the program."
        ],
        [
            "Return user friendly help on positional arguments.        \n\n        indent is the number of spaces preceeding the text on each line. \n        \n        The indent of the documentation is dependent on the length of the \n        longest label that is shorter than maxindent. A label longer than \n        maxindent will be printed on its own line.\n        \n        width is maximum allowed page width, use self.width if 0."
        ],
        [
            "Return a summary of program options, their values and origins.\n        \n        width is maximum allowed page width, use self.width if 0."
        ],
        [
            "Parse text blocks from a file."
        ],
        [
            "Pop, parse and return the first self.nargs items from args.\n\n        if self.nargs > 1 a list of parsed values will be returned.\n        \n        Raise BadNumberOfArguments or BadArgument on errors.\n         \n        NOTE: argv may be modified in place by this method."
        ],
        [
            "Parse arguments found in settings files.\n        \n        Use the values in self.true for True in settings files, or those in \n        self.false for False, case insensitive."
        ],
        [
            "Return the separator that preceding format i, or '' for i == 0."
        ],
        [
            "Return a URL to redirect the user to for OAuth authentication."
        ],
        [
            "Exchange the authorization code for an access token."
        ],
        [
            "Wraps Lock.acquire"
        ],
        [
            "Wraps Lock.release"
        ],
        [
            "Handle a dict that might contain a wrapped state for a custom type."
        ],
        [
            "Wrap the marshalled state in a dictionary.\n\n        The returned dictionary has two keys, corresponding to the ``type_key`` and ``state_key``\n        options. The former holds the type name and the latter holds the marshalled state.\n\n        :param typename: registered name of the custom type\n        :param state: the marshalled state of the object\n        :return: an object serializable by the serializer"
        ],
        [
            "Enable HTTP access to a dataset.\n\n    This only works on datasets in some systems. For example, datasets stored\n    in AWS S3 object storage and Microsoft Azure Storage can be published as\n    datasets accessible over HTTP. A published dataset is world readable."
        ],
        [
            "Update the descriptive metadata interactively.\n\n    Uses values entered by the user. Note that the function keeps recursing\n    whenever a value is another ``CommentedMap`` or a ``list``. The\n    function works as passing dictionaries and lists into a function edits\n    the values in place."
        ],
        [
            "Create a proto dataset."
        ],
        [
            "Interactive prompting to populate the readme."
        ],
        [
            "Default editor updating of readme content."
        ],
        [
            "Show the descriptive metadata in the readme."
        ],
        [
            "Use YAML from a file or stdin to populate the readme.\n\n    To stream content from stdin use \"-\", e.g.\n\n    echo \"desc: my data\" | dtool readme write <DS_URI> -"
        ],
        [
            "Add a file to the proto dataset."
        ],
        [
            "Add metadata to a file in the proto dataset."
        ],
        [
            "Convert a proto dataset into a dataset.\n\n    This step is carried out after all files have been added to the dataset.\n    Freezing a dataset finalizes it with a stamp marking it as frozen."
        ],
        [
            "Copy a dataset to a different location."
        ],
        [
            "Compress anything to bytes or string.\n\n    :params obj: \n    :params level: \n    :params return_type: if bytes, then return bytes; if str, then return\n      base64.b64encode bytes in utf-8 string."
        ],
        [
            "attempt to deduce if a pre 100 year was lost\n         due to padded zeros being taken off"
        ],
        [
            "Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings"
        ],
        [
            "The CPython version of ``fromutc`` checks that the input is a ``datetime``\n    object and that ``self`` is attached as its ``tzinfo``."
        ],
        [
            "Given a datetime in UTC, return local time"
        ],
        [
            "Strip comments from line string."
        ],
        [
            "Strip comments from json string.\n\n    :param string: A string containing json with comments started by comment_symbols.\n    :param comment_symbols: Iterable of symbols that start a line comment (default # or //).\n    :return: The string with the comments removed."
        ],
        [
            "dayofweek == 0 means Sunday, whichweek 5 means last instance"
        ],
        [
            "Convert a registry key's values to a dictionary."
        ],
        [
            "Parse strings as returned from the Windows registry into the time zone\n        name as defined in the registry.\n\n        >>> from dateutil.tzwin import tzres\n        >>> tzr = tzres()\n        >>> print(tzr.name_from_string('@tzres.dll,-251'))\n        'Dateline Daylight Time'\n        >>> print(tzr.name_from_string('Eastern Standard Time'))\n        'Eastern Standard Time'\n\n        :param tzname_str:\n            A timezone name string as returned from a Windows registry key.\n\n        :return:\n            Returns the localized timezone string from tzres.dll if the string\n            is of the form `@tzres.dll,-offset`, else returns the input string."
        ],
        [
            "This retrieves a time zone from the local zoneinfo tarball that is packaged\n    with dateutil.\n\n    :param name:\n        An IANA-style time zone name, as found in the zoneinfo file.\n\n    :return:\n        Returns a :class:`dateutil.tz.tzfile` time zone object.\n\n    .. warning::\n        It is generally inadvisable to use this function, and it is only\n        provided for API compatibility with earlier versions. This is *not*\n        equivalent to ``dateutil.tz.gettz()``, which selects an appropriate\n        time zone based on the inputs, favoring system zoneinfo. This is ONLY\n        for accessing the dateutil-specific zoneinfo (which may be out of\n        date compared to the system zoneinfo).\n\n    .. deprecated:: 2.6\n        If you need to use a specific zoneinfofile over the system zoneinfo,\n        instantiate a :class:`dateutil.zoneinfo.ZoneInfoFile` object and call\n        :func:`dateutil.zoneinfo.ZoneInfoFile.get(name)` instead.\n\n        Use :func:`get_zonefile_instance` to retrieve an instance of the\n        dateutil-provided zoneinfo."
        ],
        [
            "Get the zonefile metadata\n\n    See `zonefile_metadata`_\n\n    :returns:\n        A dictionary with the database metadata\n\n    .. deprecated:: 2.6\n        See deprecation warning in :func:`zoneinfo.gettz`. To get metadata,\n        query the attribute ``zoneinfo.ZoneInfoFile.metadata``."
        ],
        [
            "Get the configuration for the given JID based on XMPP_HTTP_UPLOAD_ACCESS.\n\n    If the JID does not match any rule, ``False`` is returned."
        ],
        [
            "Given a datetime and a time zone, determine whether or not a given datetime\n    would fall in a gap.\n\n    :param dt:\n        A :class:`datetime.datetime` (whose time zone will be ignored if ``tz``\n        is provided.)\n\n    :param tz:\n        A :class:`datetime.tzinfo` with support for the ``fold`` attribute. If\n        ``None`` or not provided, the datetime's own time zone will be used.\n\n    :return:\n        Returns a boolean value whether or not the \"wall time\" exists in ``tz``."
        ],
        [
            "Set the time zone data of this object from a _tzfile object"
        ],
        [
            "Return a version of this object represented entirely using integer\n        values for the relative attributes.\n\n        >>> relativedelta(days=1.5, hours=2).normalized()\n        relativedelta(days=1, hours=14)\n\n        :return:\n            Returns a :class:`dateutil.relativedelta.relativedelta` object."
        ],
        [
            "Create a new HMAC hash.\n\n    :param secret: The secret used when hashing data.\n    :type secret: bytes\n    :param data: The data to hash.\n    :type data: bytes\n    :param alg: The algorithm to use when hashing `data`.\n    :type alg: str\n    :return: New HMAC hash.\n    :rtype: bytes"
        ],
        [
            "Decodes the given token's header and payload and validates the signature.\n\n    :param secret: The secret used to decode the token. Must match the\n        secret used when creating the token.\n    :type secret: Union[str, bytes]\n    :param token: The token to decode.\n    :type token: Union[str, bytes]\n    :param alg: The algorithm used to decode the token. Must match the\n        algorithm used when creating the token.\n    :type alg: str\n    :return: The decoded header and payload.\n    :rtype: Tuple[dict, dict]"
        ],
        [
            "Compares the given signatures.\n\n    :param expected: The expected signature.\n    :type expected: Union[str, bytes]\n    :param actual: The actual signature.\n    :type actual: Union[str, bytes]\n    :return: Do the signatures match?\n    :rtype: bool"
        ],
        [
            "Compares the given tokens.\n\n    :param expected: The expected token.\n    :type expected: Union[str, bytes]\n    :param actual: The actual token.\n    :type actual: Union[str, bytes]\n    :return: Do the tokens match?\n    :rtype: bool"
        ],
        [
            "Is the token valid? This method only checks the timestamps within the\n        token and compares them against the current time if none is provided.\n\n        :param time: The timestamp to validate against\n        :type time: Union[int, None]\n        :return: The validity of the token.\n        :rtype: bool"
        ],
        [
            "Check for registered claims in the payload and move them to the\n        registered_claims property, overwriting any extant claims."
        ],
        [
            "Create a token based on the data held in the class.\n\n        :return: A new token\n        :rtype: str"
        ],
        [
            "Decodes the given token into an instance of `Jwt`.\n\n        :param secret: The secret used to decode the token. Must match the\n            secret used when creating the token.\n        :type secret: Union[str, bytes]\n        :param token: The token to decode.\n        :type token: Union[str, bytes]\n        :param alg: The algorithm used to decode the token. Must match the\n            algorithm used when creating the token.\n        :type alg: str\n        :return: The decoded token.\n        :rtype: `Jwt`"
        ],
        [
            "Compare against another `Jwt`.\n\n        :param jwt: The token to compare against.\n        :type jwt: Jwt\n        :param compare_dates: Should the comparision take dates into account?\n        :type compare_dates: bool\n        :return: Are the two Jwt's the same?\n        :rtype: bool"
        ],
        [
            "Download a file."
        ],
        [
            "Test a file is a valid json file.\n\n    - *.json: uncompressed, utf-8 encode json file\n    - *.js: uncompressed, utf-8 encode json file\n    - *.gz: compressed, utf-8 encode json file"
        ],
        [
            "``set`` dumper."
        ],
        [
            "``collections.deque`` dumper."
        ],
        [
            "``collections.OrderedDict`` dumper."
        ],
        [
            "``numpy.ndarray`` dumper."
        ],
        [
            "Decorator for rruleset methods which may invalidate the\n    cached length."
        ],
        [
            "Returns the last recurrence before the given datetime instance. The\n            inc keyword defines what happens if dt is an occurrence. With\n            inc=True, if dt itself is an occurrence, it will be returned."
        ],
        [
            "Returns the first recurrence after the given datetime instance. The\n            inc keyword defines what happens if dt is an occurrence. With\n            inc=True, if dt itself is an occurrence, it will be returned."
        ],
        [
            "Generator which yields up to `count` recurrences after the given\n        datetime instance, equivalent to `after`.\n\n        :param dt:\n            The datetime at which to start generating recurrences.\n\n        :param count:\n            The maximum number of recurrences to generate. If `None` (default),\n            dates are generated until the recurrence rule is exhausted.\n\n        :param inc:\n            If `dt` is an instance of the rule and `inc` is `True`, it is\n            included in the output.\n\n        :yields: Yields a sequence of `datetime` objects."
        ],
        [
            "Return new rrule with same attributes except for those attributes given new\n           values by whichever keyword arguments are specified."
        ],
        [
            "Run the excel_to_html function from the\n    command-line.\n\n    Args:\n        -p path to file\n        -s name of the sheet to convert\n        -css classes to apply\n        -m attempt to combine merged cells\n        -c caption for accessibility\n        -su summary for accessibility\n        -d details for accessibility\n\n    Example use:\n\n        excel_to_html -p myfile.xlsx -s SheetName -css diablo-python -m true"
        ],
        [
            "Gets the requested template for the given language.\n\n        Args:\n            language: string, the language of the template to look for.\n\n            template_type: string, 'iterable' or 'singular'. \n            An iterable template is needed when the value is an iterable\n            and needs more unpacking, e.g. list, tuple. A singular template \n            is needed when unpacking is complete and the value is singular, \n            e.g. string, int, float.\n\n            indentation: int, the indentation level.\n    \n            key: multiple types, the array key.\n\n            val: multiple types, the array values\n\n        Returns:\n            string, template formatting for arrays by language."
        ],
        [
            "Unserializes a serialized php array and prints it to\n        the console as a data structure in the specified language.\n        Used to translate or convert a php array into a data structure \n        in another language. Currently supports, PHP, Python, Javascript,\n        and JSON. \n\n        Args:\n            string: a string of serialized php\n        \n            language: a string representing the desired output \n            format for the array.\n\n            level: integer, indentation level in spaces. \n            Defaults to 3.\n\n            retdata: boolean, the method will return the string\n            in addition to printing it if set to True. Defaults \n            to false.\n\n        Returns:\n            None but prints a string to the console if retdata is \n            False, otherwise returns a string."
        ],
        [
            "Only API function for the config module.\n\n    :return: {dict}     loaded validated configuration."
        ],
        [
            "Create a reusable class from a generator function\n\n    Parameters\n    ----------\n    func: GeneratorCallable[T_yield, T_send, T_return]\n        the function to wrap\n\n    Note\n    ----\n    * the callable must have an inspectable signature\n    * If bound to a class, the new reusable generator is callable as a method.\n      To opt out of this, add a :func:`staticmethod` decorator above\n      this decorator."
        ],
        [
            "Send an item into a generator expecting a final return value\n\n    Parameters\n    ----------\n    gen: ~typing.Generator[T_yield, T_send, T_return]\n        the generator to send the value to\n    value: T_send\n        the value to send\n\n    Raises\n    ------\n    RuntimeError\n        if the generator did not return as expected\n\n    Returns\n    -------\n    T_return\n        the generator's return value"
        ],
        [
            "Apply a function to all ``send`` values of a generator\n\n    Parameters\n    ----------\n    func: ~typing.Callable[[T_send], T_mapped]\n        the function to apply\n    gen: Generable[T_yield, T_mapped, T_return]\n        the generator iterable.\n\n    Returns\n    -------\n    ~typing.Generator[T_yield, T_send, T_return]\n        the mapped generator"
        ],
        [
            "Prints the traceback and invokes the ipython debugger on any exception\n\n    Only invokes ipydb if you are outside ipython or python interactive session.\n    So scripts must be called from OS shell in order for exceptions to ipy-shell-out.\n\n    Dependencies:\n      Needs `pip install ipdb`\n\n    Arguments:\n      exc_type (type): The exception type/class (e.g. RuntimeError)\n      exc_value (Exception): The exception instance (e.g. the error message passed to the Exception constructor)\n      exc_trace (Traceback): The traceback instance\n    \n    References:\n      http://stackoverflow.com/a/242531/623735\n\n    Example Usage:\n      $  python -c 'from pug import debug;x=[];x[0]'\n      Traceback (most recent call last):\n        File \"<string>\", line 1, in <module>\n      IndexError: list index out of range\n\n      > <string>(1)<module>()\n\n      ipdb> x\n      []\n      ipdb> locals()\n      {'__builtins__': <module '__builtin__' (built-in)>, '__package__': None, 'x': [], 'debug': <module 'pug.debug' from 'pug/debug.py'>, '__name__': '__main__', '__doc__': None}\n      ipdb>"
        ],
        [
            "Copies a file from its location on the web to a designated \n    place on the local machine.\n\n    Args:\n        file_path: Complete url of the file to copy, string (e.g. http://fool.com/input.css).\n\n        target_path: Path and name of file on the local machine, string. (e.g. /directory/output.css)\n\n    Returns:\n        None."
        ],
        [
            "Counts the number of lines in a file.\n\n    Args:\n        fname: string, name of the file.\n\n    Returns:\n        integer, the number of lines in the file."
        ],
        [
            "Indentes css that has not been indented and saves it to a new file.\n    A new file is created if the output destination does not already exist.\n\n    Args:\n        f: string, path to file.\n\n        output: string, path/name of the output file (e.g. /directory/output.css).\n    print type(response.read())\n\n    Returns:\n        None."
        ],
        [
            "Adds line breaks after every occurance of a given character in a file.\n\n    Args:\n        f: string, path to input file.\n\n        output: string, path to output file.\n\n    Returns:\n        None."
        ],
        [
            "Reformats poorly written css. This function does not validate or fix errors in the code.\n    It only gives code the proper indentation. \n\n    Args:\n        input_file: string, path to the input file.\n\n        output_file: string, path to where the reformatted css should be saved. If the target file\n        doesn't exist, a new file is created.\n\n    Returns:\n        None."
        ],
        [
            "Take a list of strings and clear whitespace \n    on each one. If a value in the list is not a \n    string pass it through untouched.\n\n    Args:\n        iterable: mixed list\n\n    Returns: \n        mixed list"
        ],
        [
            "Calculates the future value of money invested at an anual interest rate,\n    x times per year, for a given number of years.\n\n    Args:\n        present_value: int or float, the current value of the money (principal).\n\n        annual_rate: float 0 to 1 e.g., .5 = 50%), the interest rate paid out.\n\n        periods_per_year: int, the number of times money is invested per year.\n\n        years: int, the number of years invested.\n\n    Returns:\n        Float, the future value of the money invested with compound interest."
        ],
        [
            "Uses Heron's formula to find the area of a triangle\n    based on the coordinates of three points.\n\n    Args:\n        point1: list or tuple, the x y coordinate of point one.\n\n        point2: list or tuple, the x y coordinate of point two.\n\n        point3: list or tuple, the x y coordinate of point three.\n\n    Returns:\n        The area of a triangle as a floating point number.\n\n    Requires:\n        The math module, point_distance()."
        ],
        [
            "Calculates  the median of a list of integers or floating point numbers.\n\n    Args:\n        data: A list of integers or floating point numbers\n\n    Returns:\n        Sorts the list numerically and returns the middle number if the list has an odd number\n        of items. If the list contains an even number of items the mean of the two middle numbers\n        is returned."
        ],
        [
            "Calculates the average or mean of a list of numbers\n\n    Args:\n        numbers: a list of integers or floating point numbers.\n\n        numtype: string, 'decimal' or 'float'; the type of number to return.\n\n    Returns:\n        The average (mean) of the numbers as a floating point number\n        or a Decimal object.\n\n    Requires:\n        The math module"
        ],
        [
            "Calculates the population or sample variance of a list of numbers.\n    A large number means the results are all over the place, while a\n    small number means the results are comparatively close to the average.\n\n    Args:\n        numbers: a list  of integers or floating point numbers to compare.\n\n        type: string, 'population' or 'sample', the kind of variance to be computed.\n\n    Returns:\n        The computed population or sample variance.\n        Defaults to population variance.\n\n    Requires:\n        The math module, average()"
        ],
        [
            "Finds the percentage of one number over another.\n\n    Args:\n        a: The number that is a percent, int or float.\n\n        b: The base number that a is a percent of, int or float.\n\n        i: Optional boolean integer. True if the user wants the result returned as\n        a whole number. Assumes False.\n\n        r: Optional boolean round. True if the user wants the result rounded.\n        Rounds to the second decimal point on floating point numbers. Assumes False.\n\n    Returns:\n        The argument a as a percentage of b. Throws a warning if integer is set to True\n        and round is set to False."
        ],
        [
            "Get datetime string from datetime object\n\n        :param datetime datetime_obj: datetime object\n        :return: datetime string\n        :rtype: str"
        ],
        [
            "attr pipe can extract attribute value of object.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_name: The name of attribute\n    :type attr_name: str\n    :returns: generator"
        ],
        [
            "attrs pipe can extract attribute values of object.\n\n    If attr_names is a list and its item is not a valid attribute of\n    prev's object. It will be excluded from yielded dict.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_names: The list of attribute names\n    :type attr_names: str of list\n    :returns: generator"
        ],
        [
            "attrdict pipe can extract attribute values of object into a dict.\n\n    The argument attr_names can be a list or a dict.\n\n    If attr_names is a list and its item is not a valid attribute of\n    prev's object. It will be excluded from yielded dict.\n\n    If attr_names is dict and the key doesn't exist in prev's object.\n    the value of corresponding attr_names key will be copy to yielded dict.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_names: The list or dict of attribute names\n    :type attr_names: str of list or dict\n    :returns: generator"
        ],
        [
            "flatten pipe extracts nested item from previous pipe.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param depth: The deepest nested level to be extracted. 0 means no extraction.\n    :type depth: integer\n    :returns: generator"
        ],
        [
            "values pipe extract value from previous pipe.\n\n    If previous pipe send a dictionary to values pipe, keys should contains\n    the key of dictionary which you want to get. If previous pipe send list or\n    tuple,\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :returns: generator"
        ],
        [
            "pack pipe takes n elements from previous generator and yield one\n    list to next.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param rest: Set True to allow to output the rest part of last elements.\n    :type prev: boolean\n    :param padding: Specify the padding element for the rest part of last elements.\n    :type prev: boolean\n    :returns: generator\n\n    :Example:\n    >>> result([1,2,3,4,5,6,7] | pack(3))\n    [[1, 2, 3], [4, 5, 6]]\n\n    >>> result([1,2,3,4,5,6,7] | pack(3, rest=True))\n    [[1, 2, 3], [4, 5, 6], [7,]]\n\n    >>> result([1,2,3,4,5,6,7] | pack(3, padding=None))\n    [[1, 2, 3], [4, 5, 6], [7, None, None]]"
        ],
        [
            "The pipe greps the data passed from previous generator according to\n    given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter out data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :param kw:\n    :type kw: dict\n    :returns: generator"
        ],
        [
            "The pipe greps the data passed from previous generator according to\n    given regular expression. The data passed to next pipe is MatchObject\n    , dict or tuple which determined by 'to' in keyword argument.\n\n    By default, match pipe yields MatchObject. Use 'to' in keyword argument\n    to change the type of match result.\n\n    If 'to' is dict, yield MatchObject.groupdict().\n    If 'to' is tuple, yield MatchObject.groups().\n    If 'to' is list, yield list(MatchObject.groups()).\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter data.\n    :type pattern: str|unicode\n    :param to: What data type the result should be stored. dict|tuple|list\n    :type to: type\n    :returns: generator"
        ],
        [
            "The resplit pipe split previous pipe input by regular expression.\n\n    Use 'maxsplit' keyword argument to limit the number of split.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to split string.\n    :type pattern: str|unicode"
        ],
        [
            "sub pipe is a wrapper of re.sub method.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern string.\n    :type pattern: str|unicode\n    :param repl: Check repl argument in re.sub method.\n    :type repl: str|unicode|callable"
        ],
        [
            "wildcard pipe greps data passed from previous generator\n    according to given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The wildcard string which used to filter data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :returns: generator"
        ],
        [
            "This pipe read data from previous iterator and write it to stdout.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param endl: The end-of-line symbol for each output.\n    :type endl: str\n    :param thru: If true, data will passed to next generator. If false, data\n                 will be dropped.\n    :type thru: bool\n    :returns: generator"
        ],
        [
            "This pipe get filenames or file object from previous pipe and read the\n    content of file. Then, send the content of file line by line to next pipe.\n\n    The start and end parameters are used to limit the range of reading from file.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param filename: The files to be read. If None, use previous pipe input as filenames.\n    :type filename: None|str|unicode|list|tuple\n    :param mode: The mode to open file. default is 'r'\n    :type mode: str\n    :param trim: The function to trim the line before send to next pipe.\n    :type trim: function object.\n    :param start: if star is specified, only line number larger or equal to start will be sent.\n    :type start: integer\n    :param end: The last line number to read.\n    :type end: integer\n    :returns: generator"
        ],
        [
            "sh pipe execute shell command specified by args. If previous pipe exists,\n    read data from it and write it to stdin of shell process. The stdout of\n    shell process will be passed to next pipe object line by line.\n\n    A optional keyword argument 'trim' can pass a function into sh pipe. It is\n    used to trim the output from shell process. The default trim function is\n    str.rstrip. Therefore, any space characters in tail of\n    shell process output line will be removed.\n\n    For example:\n\n    py_files = result(sh('ls') | strip | wildcard('*.py'))\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param args: The command line arguments. It will be joined by space character.\n    :type args: list of string.\n    :param kw: arguments for subprocess.Popen.\n    :type kw: dictionary of options.\n    :returns: generator"
        ],
        [
            "This pipe wrap os.walk and yield absolute path one by one.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param args: The end-of-line symbol for each output.\n    :type args: list of string.\n    :param kw: The end-of-line symbol for each output.\n    :type kw: dictionary of options. Add 'endl' in kw to specify end-of-line symbol.\n    :returns: generator"
        ],
        [
            "alias of str.join"
        ],
        [
            "alias of string.Template.substitute"
        ],
        [
            "alias of string.Template.safe_substitute"
        ],
        [
            "Convert data from previous pipe with specified encoding."
        ],
        [
            "Regiser all default type-to-pipe convertors."
        ],
        [
            "Convert Paginator instance to dict\n\n        :return: Paging data\n        :rtype: dict"
        ],
        [
            "Check that a process is not running more than once, using PIDFILE"
        ],
        [
            "This function will check whether a PID is currently running"
        ],
        [
            "This function will disown, so the Ardexa service can be restarted"
        ],
        [
            "Run a  program and check program return code Note that some commands don't work\n    well with Popen.  So if this function is specifically called with 'shell=True',\n    then it will run the old 'os.system'. In which case, there is no program output"
        ],
        [
            "Yield each integer from a complex range string like \"1-9,12,15-20,23\"\n\n    >>> list(parse_address_list('1-9,12,15-20,23'))\n    [1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 15, 16, 17, 18, 19, 20, 23]\n\n    >>> list(parse_address_list('1-9,12,15-20,2-3-4'))\n    Traceback (most recent call last):\n        ...\n    ValueError: format error in 2-3-4"
        ],
        [
            "Do url-encode resource ids"
        ],
        [
            "Get item creator according registered item type.\n\n    :param item_type: The type of item to be checed.\n    :type item_type: types.TypeType.\n    :returns: Creator function. None if type not found."
        ],
        [
            "Self-cloning. All its next Pipe objects are cloned too.\n\n        :returns: cloned object"
        ],
        [
            "Append next object to pipe tail.\n\n        :param next: The Pipe object to be appended to tail.\n        :type next: Pipe object."
        ],
        [
            "Return an generator as iterator object.\n\n        :param prev: Previous Pipe object which used for data input.\n        :returns: A generator for iteration."
        ],
        [
            "Wrap a reduce function to Pipe object. Reduce function is a function\n        with at least two arguments. It works like built-in reduce function.\n        It takes first argument for accumulated result, second argument for\n        the new data to process. A keyword-based argument named 'init' is\n        optional. If init is provided, it is used for the initial value of\n        accumulated result. Or, the initial value is None.\n\n        The first argument is the data to be converted. The return data from\n        filter function should be a boolean value. If true, data can pass.\n        Otherwise, data is omitted.\n\n        :param func: The filter function to be wrapped.\n        :type func: function object\n        :param args: The default arguments to be used for filter function.\n        :param kw: The default keyword arguments to be used for filter function.\n        :returns: Pipe object"
        ],
        [
            "Return a dictionary of network name to active status bools.\n\n        Sample virsh net-list output::\n\n    Name                 State      Autostart\n    -----------------------------------------\n    default              active     yes\n    juju-test            inactive   no\n    foobar               inactive   no\n\n    Parsing the above would return::\n    {\"default\": True, \"juju-test\": False, \"foobar\": False}\n\n    See: http://goo.gl/kXwfC"
        ],
        [
            "flush the line to stdout"
        ],
        [
            "runs the passed in arguments and returns an iterator on the output of\n        running command"
        ],
        [
            "Build a basic 035 subfield with basic information from the OAI-PMH request.\n\n    :param root: ElementTree root node\n\n    :return: list of subfield tuples [(..),(..)]"
        ],
        [
            "Strip out namespace data from an ElementTree.\n\n    This function is recursive and will traverse all\n    subnodes to the root element\n\n    @param root: the root element\n\n    @return: the same root element, minus namespace"
        ],
        [
            "Load values from a dictionary structure. Nesting can be used to\n            represent namespaces.\n\n            >>> c = ConfigDict()\n            >>> c.load_dict({'some': {'namespace': {'key': 'value'} } })\n            {'some.namespace.key': 'value'}"
        ],
        [
            "The oembed endpoint, or the url to which requests for metadata are passed.\n    Third parties will want to access this view with URLs for your site's\n    content and be returned OEmbed metadata."
        ],
        [
            "Extract and return oembed content for given urls.\n\n    Required GET params:\n        urls - list of urls to consume\n\n    Optional GET params:\n        width - maxwidth attribute for oembed content\n        height - maxheight attribute for oembed content\n        template_dir - template_dir to use when rendering oembed\n\n    Returns:\n        list of dictionaries with oembed metadata and renderings, json encoded"
        ],
        [
            "A site profile detailing valid endpoints for a given domain.  Allows for\n    better auto-discovery of embeddable content.\n\n    OEmbed-able content lives at a URL that maps to a provider."
        ],
        [
            "scan path directory and any subdirectories for valid captain scripts"
        ],
        [
            "Make the request params given location data"
        ],
        [
            "Get the tax rate from the ZipTax response"
        ],
        [
            "Check if there are exceptions that should be raised"
        ],
        [
            "Recursively extract all text from node."
        ],
        [
            "Registers a provider with the site."
        ],
        [
            "Unregisters a provider from the site."
        ],
        [
            "Populate the internal registry's dictionary with the regexes for each\n        provider instance"
        ],
        [
            "Find the right provider for a URL"
        ],
        [
            "A hook for django-based oembed providers to delete any stored oembeds"
        ],
        [
            "The heart of the matter"
        ],
        [
            "Load up StoredProviders from url if it is an oembed scheme"
        ],
        [
            "Iterate over the returned json and try to sort out any new providers"
        ],
        [
            "A kind of cheesy method that allows for callables or attributes to\n        be used interchangably"
        ],
        [
            "Return an ImageFileField instance"
        ],
        [
            "Build a dictionary of metadata for the requested object."
        ],
        [
            "Parses the date from a url and uses it in the query.  For objects which\n        are unique for date."
        ],
        [
            "Override the base."
        ],
        [
            "Add the 909 OAI info to 035."
        ],
        [
            "Check if we shall add cnum in 035."
        ],
        [
            "Remove hidden notes and tag a CERN if detected."
        ],
        [
            "Remove INSPIRE specific notes."
        ],
        [
            "Move title info from 245 to 111 proceeding style."
        ],
        [
            "Update reportnumbers."
        ],
        [
            "Remove dashes from ISBN."
        ],
        [
            "Remove duplicate BibMatch DOIs."
        ],
        [
            "260 Date normalization."
        ],
        [
            "041 Language."
        ],
        [
            "Generate directory listing HTML\n\n    Arguments:\n        FS (FS): filesystem object to read files from\n        filepath (str): path to generate directory listings for\n\n    Keyword Arguments:\n        list_dir (callable: list[str]): list file names in a directory\n        isdir (callable: bool): os.path.isdir\n\n    Yields:\n        str: lines of an HTML table"
        ],
        [
            "Checks if files are not being uploaded to server.\n    @timeout - time after which the script will register an error."
        ],
        [
            "Converts capital letters to lower keeps first letter capital."
        ],
        [
            "Scans a block of text and extracts oembed data on any urls,\n        returning it in a list of dictionaries"
        ],
        [
            "Try to maintain parity with what is extracted by extract since strip\n        will most likely be used in conjunction with extract"
        ],
        [
            "Automatically build the provider index."
        ],
        [
            "pass in a list of options, promt the user to select one, and return the selected option or None"
        ],
        [
            "Transforms the argparse arguments from Namespace to dict and then to Bunch\n    Therefore it is not necessary to access the arguments using the dict syntax\n    The settings can be called like regular vars on the settings object"
        ],
        [
            "Reads a dom xml element in oaidc format and\n            returns the bibrecord object"
        ],
        [
            "display a progress that can update in place\n\n    example -- \n        total_length = 1000\n        with echo.progress(total_length) as p:\n            for x in range(total_length):\n                # do something crazy\n                p.update(x)\n\n    length -- int -- the total size of what you will be updating progress on"
        ],
        [
            "print format_msg to stderr"
        ],
        [
            "prints a banner\n\n    sep -- string -- the character that will be on the line on the top and bottom\n        and before any of the lines, defaults to *\n    count -- integer -- the line width, defaults to 80"
        ],
        [
            "format columned data so we can easily print it out on a console, this just takes\n    columns of data and it will format it into properly aligned columns, it's not\n    fancy, but it works for most type of strings that I need it for, like server name\n    lists.\n\n    other formatting options:\n        http://stackoverflow.com/a/8234511/5006\n\n    other packages that probably do this way better:\n        https://stackoverflow.com/a/26937531/5006\n\n    :Example:\n        >>> echo.table([(1, 2), (3, 4), (5, 6), (7, 8), (9, 0)])\n        1  2\n        3  4\n        5  6\n        7  8\n        9  0\n        >>> echo.table([1, 3, 5, 7, 9], [2, 4, 6, 8, 0])\n        1  2\n        3  4\n        5  6\n        7  8\n        9  0\n\n    :param *columns: can either be a list of rows or multiple lists representing each\n        column in the table\n    :param **kwargs: dict\n        prefix -- string -- what you want before each row (eg, a tab)\n        buf_count -- integer -- how many spaces between longest col value and its neighbor\n        headers -- list -- the headers you want, must match column count\n        widths -- list -- the widths of each column you want to use, this doesn't have\n            to match column count, so you can do something like [0, 5] to set the\n            width of the second column\n        width -- int -- similar to widths except it will set this value for all columns"
        ],
        [
            "echo a prompt to the user and wait for an answer\n\n    question -- string -- the prompt for the user\n    choices -- list -- if given, only exit when prompt matches one of the choices\n    return -- string -- the answer that was given by the user"
        ],
        [
            "Returns the records listed in the webpage given as\n        parameter as a xml String.\n\n        @param url: the url of the Journal, Book, Protocol or Reference work"
        ],
        [
            "Logs into the specified ftp server and returns connector."
        ],
        [
            "Set the thermostat mode\n\n        :param mode: The desired mode integer value.\n                     Auto = 1\n                     Temporary hold = 2\n                     Permanent hold = 3"
        ],
        [
            "Set the target temperature to the desired fahrenheit, with more granular control of the\n        hold mode\n\n        :param fahrenheit: The desired temperature in F\n        :param mode: The desired mode to operate in"
        ],
        [
            "Set the target temperature to the desired celsius, with more granular control of the hold\n        mode\n\n        :param celsius: The desired temperature in C\n        :param mode: The desired mode to operate in"
        ],
        [
            "Updates the target temperature on the NuHeat API\n\n        :param temperature: The desired temperature in NuHeat format\n        :param permanent: Permanently hold the temperature. If set to False, the schedule will\n                          resume at the next programmed event"
        ],
        [
            "This function returns a Bunch object from the stated config file.\n\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    NOTE:\n        The values are not evaluated by default.\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    filename:\n        The desired config file to read.\n        The config file must be written in a syntax readable to the\n        ConfigParser module -> INI syntax\n\n        [sectionA]\n        optionA1 = ...\n        optionA2 = ...\n\n    section_option_dict:\n        A dictionary that contains keys, which are associated to the sections\n        in the config file, and values, which are a list of the desired\n        options.\n        If empty, everything will be loaded.\n        If the lists are empty, everything from the sections will be loaded.\n\n    Example:\n        dict = {'sectionA': ['optionA1', 'optionA2', ...],\n                'sectionB': ['optionB1', 'optionB2', ...]}\n\n        config = get_config('config.cfg', dict)\n        config.sectionA.optionA1\n\n    Other:\n        Bunch can be found in configparser.py"
        ],
        [
            "Authenticate against the NuHeat API"
        ],
        [
            "Make a request to the NuHeat API\n\n        :param url: The URL to request\n        :param method: The type of request to make (GET, POST)\n        :param data: Data to be sent along with POST requests\n        :param params: Querystring parameters\n        :param retry: Attempt to re-authenticate and retry request if necessary"
        ],
        [
            "Return representation of html start tag and attributes."
        ],
        [
            "Return representation of html end tag."
        ],
        [
            "Return stripped HTML, keeping only MathML."
        ],
        [
            "return True if callback is an instance of a class"
        ],
        [
            "return True if callback is a vanilla plain jane function"
        ],
        [
            "these kwargs come from the @arg decorator, they are then merged into any\n        keyword arguments that were automatically generated from the main function\n        introspection"
        ],
        [
            "find any matching parser_args from list_args and merge them into this\n        instance\n\n        list_args -- list -- an array of (args, kwargs) tuples"
        ],
        [
            "Overridden to not get rid of newlines\n\n        https://github.com/python/cpython/blob/2.7/Lib/argparse.py#L620"
        ],
        [
            "create string suitable for HTTP User-Agent header"
        ],
        [
            "Add a MARCXML datafield as a new child to a XML document."
        ],
        [
            "Given a document, return XML prettified."
        ],
        [
            "Transform & and < to XML valid &amp; and &lt.\n\n    Pass a list of tags as string to enable replacement of\n    '<' globally but keep any XML tags in the list."
        ],
        [
            "Properly format arXiv IDs."
        ],
        [
            "Convert journal name to Inspire's short form."
        ],
        [
            "Add correct nations field according to mapping in NATIONS_DEFAULT_MAP."
        ],
        [
            "Fix bad Unicode special dashes in string."
        ],
        [
            "Try to capitalize properly a title string."
        ],
        [
            "Convert some HTML tags to latex equivalents."
        ],
        [
            "Download URL to a file."
        ],
        [
            "Run a shell command."
        ],
        [
            "Create a logger object."
        ],
        [
            "Perform the actual uncompression."
        ],
        [
            "Locate all files matching supplied filename pattern recursively."
        ],
        [
            "Punctuate author names properly.\n\n    Expects input in the form 'Bloggs, J K' and will return 'Bloggs, J. K.'."
        ],
        [
            "Convert a date-value to the ISO date standard."
        ],
        [
            "Convert a date-value to the ISO date standard for humans."
        ],
        [
            "Convert list of images to PNG format.\n\n    @param: image_list ([string, string, ...]): the list of image files\n        extracted from the tarball in step 1\n\n    @return: image_list ([str, str, ...]): The list of image files when all\n        have been converted to PNG format."
        ],
        [
            "Generate a safe and closed filepath."
        ],
        [
            "Get letters from string only."
        ],
        [
            "Return True if license is compatible with Open Access"
        ],
        [
            "Information about the current volume, issue, etc. is available\n        in a file called issue.xml that is available in a higher directory."
        ],
        [
            "issue.xml from Elsevier assume the existence of a local DTD.\n        This procedure install the DTDs next to the issue.xml file\n        and normalize it using xmllint in order to resolve all namespaces\n        and references."
        ],
        [
            "main.xml from Elsevier assume the existence of a local DTD.\n        This procedure install the DTDs next to the main.xml file\n        and normalize it using xmllint in order to resolve all namespaces\n        and references."
        ],
        [
            "Return the best effort start_date."
        ],
        [
            "Extract oembed resources from a block of text.  Returns a list\n    of dictionaries.\n\n    Max width & height can be specified:\n    {% for embed in block_of_text|extract_oembeds:\"400x300\" %}\n\n    Resource type can be specified:\n    {% for photo_embed in block_of_text|extract_oembeds:\"photo\" %}\n\n    Or both:\n    {% for embed in block_of_text|extract_oembeds:\"400x300xphoto\" %}"
        ],
        [
            "A node which parses everything between its two nodes, and replaces any links\n    with OEmbed-provided objects, if possible.\n\n    Supports two optional argument, which is the maximum width and height,\n    specified like so:\n\n    {% oembed 640x480 %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    and or the name of a sub tempalte directory to render templates from:\n\n    {% oembed 320x240 in \"comments\" %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    or:\n\n    {% oembed in \"comments\" %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    either of those will render templates in oembed/comments/oembedtype.html\n\n    Additionally, you can specify a context variable to drop the rendered text in:\n\n    {% oembed 600x400 in \"comments\" as var_name %}...{% endoembed %}\n    {% oembed as var_name %}...{% endoembed %}"
        ],
        [
            "Generates a &lt;link&gt; tag with oembed autodiscovery bits for an object.\n\n    {% oembed_autodiscover video %}"
        ],
        [
            "Generates a &lt;link&gt; tag with oembed autodiscovery bits.\n\n    {% oembed_url_scheme %}"
        ],
        [
            "return the parser for the current name"
        ],
        [
            "load the module so we can actually run the script's function"
        ],
        [
            "get the contents of the script"
        ],
        [
            "parse and import the script, and then run the script's main function"
        ],
        [
            "return that path to be able to call this script from the passed in\n        basename\n\n        example -- \n            basepath = /foo/bar\n            self.path = /foo/bar/che/baz.py\n            self.call_path(basepath) # che/baz.py\n\n        basepath -- string -- the directory you would be calling this script in\n        return -- string -- the minimum path that you could use to execute this script\n            in basepath"
        ],
        [
            "load the script and set the parser and argument info\n\n        I feel that this is way too brittle to be used long term, I think it just\n        might be best to import the stupid module, the thing I don't like about that\n        is then we import basically everything, which seems bad?"
        ],
        [
            "return True if this script can be run from the command line"
        ],
        [
            "Handles registering the fields with the FieldRegistry and creating a \n    post-save signal for the model."
        ],
        [
            "I need a way to ensure that this signal gets created for all child\n        models, and since model inheritance doesn't have a 'contrubite_to_class'\n        style hook, I am creating a fake virtual field which will be added to\n        all subclasses and handles creating the signal"
        ],
        [
            "Fetch response headers and data from a URL, raising a generic exception\n    for any kind of failure."
        ],
        [
            "Given a url which may or may not be a relative url, convert it to a full\n    url path given another full url as an example"
        ],
        [
            "Generate a fake request object to allow oEmbeds to use context processors."
        ],
        [
            "dynamically load a class given a string of the format\n    \n    package.Class"
        ],
        [
            "Override the base get_record."
        ],
        [
            "Special handling if record is a CMS NOTE."
        ],
        [
            "Handle reportnumbers."
        ],
        [
            "653 Free Keywords."
        ],
        [
            "710 Collaboration."
        ],
        [
            "Return a field created with the provided elements.\n\n    Global position is set arbitrary to -1."
        ],
        [
            "Create a list of records from the marcxml description.\n\n    :returns: a list of objects initiated by the function create_record().\n              Please see that function's docstring."
        ],
        [
            "Create a record object from the marcxml description.\n\n    Uses the lxml parser.\n\n    The returned object is a tuple (record, status_code, list_of_errors),\n    where status_code is 0 when there are errors, 1 when no errors.\n\n    The return record structure is as follows::\n\n        Record := {tag : [Field]}\n        Field := (Subfields, ind1, ind2, value)\n        Subfields := [(code, value)]\n\n    .. code-block:: none\n\n                                    .--------.\n                                    | record |\n                                    '---+----'\n                                        |\n               .------------------------+------------------------------------.\n               |record['001']           |record['909']        |record['520'] |\n               |                        |                     |              |\n        [list of fields]           [list of fields]     [list of fields]    ...\n               |                        |                     |\n               |               .--------+--+-----------.      |\n               |               |           |           |      |\n               |[0]            |[0]        |[1]       ...     |[0]\n          .----+------.  .-----+-----.  .--+--------.     .---+-------.\n          | Field 001 |  | Field 909 |  | Field 909 |     | Field 520 |\n          '-----------'  '-----+-----'  '--+--------'     '---+-------'\n               |               |           |                  |\n              ...              |          ...                ...\n                               |\n                    .----------+-+--------+------------.\n                    |            |        |            |\n                    |[0]         |[1]     |[2]         |\n          [list of subfields]   'C'      '4'          ...\n                    |\n               .----+---------------+------------------------+\n               |                    |                        |\n        ('a', 'value')              |            ('a', 'value for another a')\n                     ('b', 'value for subfield b')\n\n    :param marcxml: an XML string representation of the record to create\n    :param verbose: the level of verbosity: 0 (silent), 1-2 (warnings),\n                    3(strict:stop when errors)\n    :param correct: 1 to enable correction of marcxml syntax. Else 0.\n    :return: a tuple (record, status_code, list_of_errors), where status\n             code is 0 where there are errors, 1 when no errors"
        ],
        [
            "Filter the given field.\n\n    Filters given field and returns only that field instances that contain\n    filter_subcode with given filter_value. As an input for search function\n    accepts output from record_get_field_instances function. Function can be\n    run in three modes:\n\n    - 'e' - looking for exact match in subfield value\n    - 's' - looking for substring in subfield value\n    - 'r' - looking for regular expression in subfield value\n\n    Example:\n\n    record_filter_field(record_get_field_instances(rec, '999', '%', '%'),\n                        'y', '2001')\n\n    In this case filter_subcode is 'y' and filter_value is '2001'.\n\n    :param field_instances: output from record_get_field_instances\n    :param filter_subcode: name of the subfield\n    :type filter_subcode: string\n    :param filter_value: value of the subfield\n    :type filter_value: string\n    :param filter_mode: 'e','s' or 'r'"
        ],
        [
            "Return a record where all the duplicate fields have been removed.\n\n    Fields are considered identical considering also the order of their\n    subfields."
        ],
        [
            "Return True if rec1 is identical to rec2.\n\n    It does so regardless of a difference in the 005 tag (i.e. the timestamp)."
        ],
        [
            "Return the list of field instances for the specified tag and indications.\n\n    Return empty list if not found.\n    If tag is empty string, returns all fields\n\n    Parameters (tag, ind1, ind2) can contain wildcard %.\n\n    :param rec: a record structure as returned by create_record()\n    :param tag: a 3 characters long string\n    :param ind1: a 1 character long string\n    :param ind2: a 1 character long string\n    :param code: a 1 character long string\n    :return: a list of field tuples (Subfields, ind1, ind2, value,\n             field_position_global) where subfields is list of (code, value)"
        ],
        [
            "Delete the field with the given position.\n\n    If global field position is specified, deletes the field with the\n    corresponding global field position.\n    If field_position_local is specified, deletes the field with the\n    corresponding local field position and tag.\n    Else deletes all the fields matching tag and optionally ind1 and\n    ind2.\n\n    If both field_position_global and field_position_local are present,\n    then field_position_local takes precedence.\n\n    :param rec: the record data structure\n    :param tag: the tag of the field to be deleted\n    :param ind1: the first indicator of the field to be deleted\n    :param ind2: the second indicator of the field to be deleted\n    :param field_position_global: the global field position (record wise)\n    :param field_position_local: the local field position (tag wise)\n    :return: the list of deleted fields"
        ],
        [
            "Add the fields into the record at the required position.\n\n    The position is specified by the tag and the field_position_local in the\n    list of fields.\n\n    :param rec: a record structure\n    :param tag: the tag of the fields to be moved\n    :param field_position_local: the field_position_local to which the field\n                                 will be inserted. If not specified, appends\n                                 the fields to the tag.\n    :param a: list of fields to be added\n    :return: -1 if the operation failed, or the field_position_local if it was\n             successful"
        ],
        [
            "Move some fields to the position specified by 'field_position_local'.\n\n    :param rec: a record structure as returned by create_record()\n    :param tag: the tag of the fields to be moved\n    :param field_positions_local: the positions of the fields to move\n    :param field_position_local: insert the field before that\n                                 field_position_local. If unspecified, appends\n                                 the fields :return: the field_position_local\n                                 is the operation was successful"
        ],
        [
            "Delete all subfields with subfield_code in the record."
        ],
        [
            "Return the the matching field.\n\n    One has to enter either a global field position or a local field position.\n\n    :return: a list of subfield tuples (subfield code, value).\n    :rtype: list"
        ],
        [
            "Replace a field with a new field."
        ],
        [
            "Return the subfield of the matching field.\n\n    One has to enter either a global field position or a local field position.\n\n    :return: a list of subfield tuples (subfield code, value).\n    :rtype:  list"
        ],
        [
            "Delete subfield from position specified.\n\n    Specify the subfield by tag, field number and subfield position."
        ],
        [
            "Add subfield into specified position.\n\n    Specify the subfield by tag, field number and optionally by subfield\n    position."
        ],
        [
            "Modify controlfield at position specified by tag and field number."
        ],
        [
            "Modify subfield at specified position.\n\n    Specify the subfield by tag, field number and subfield position."
        ],
        [
            "Move subfield at specified position.\n\n    Sspecify the subfield by tag, field number and subfield position to new\n    subfield position."
        ],
        [
            "Generate the XML for record 'rec'.\n\n    :param rec: record\n    :param tags: list of tags to be printed\n    :return: string"
        ],
        [
            "Generate the XML for field 'field' and returns it as a string."
        ],
        [
            "Print a record.\n\n    :param format: 1 XML, 2 HTML (not implemented)\n    :param tags: list of tags to be printed"
        ],
        [
            "Print a list of records.\n\n    :param format: 1 XML, 2 HTML (not implemented)\n    :param tags: list of tags to be printed\n           if 'listofrec' is not a list it returns empty string"
        ],
        [
            "Return the global and local positions of the first occurrence of the field.\n\n    :param rec:    A record dictionary structure\n    :type  rec:    dictionary\n    :param tag:    The tag of the field to search for\n    :type  tag:    string\n    :param field:  A field tuple as returned by create_field()\n    :type  field:  tuple\n    :param strict: A boolean describing the search method. If strict\n                   is False, then the order of the subfields doesn't\n                   matter. Default search method is strict.\n    :type  strict: boolean\n    :return:       A tuple of (global_position, local_position) or a\n                   tuple (None, None) if the field is not present.\n    :rtype:        tuple\n    :raise InvenioBibRecordFieldError: If the provided field is invalid."
        ],
        [
            "Find subfield instances in a particular field.\n\n    It tests values in 1 of 3 possible ways:\n     - Does a subfield code exist? (ie does 773__a exist?)\n     - Does a subfield have a particular value? (ie 773__a == 'PhysX')\n     - Do a pair of subfields have particular values?\n        (ie 035__2 == 'CDS' and 035__a == '123456')\n\n    Parameters:\n     * rec - dictionary: a bibrecord structure\n     * tag - string: the tag of the field (ie '773')\n     * ind1, ind2 - char: a single characters for the MARC indicators\n     * sub_key - char: subfield key to find\n     * sub_value - string: subfield value of that key\n     * sub_key2 - char: key of subfield to compare against\n     * sub_value2 - string: expected value of second subfield\n     * case_sensitive - bool: be case sensitive when matching values\n\n    :return: false if no match found, else provides the field position (int)"
        ],
        [
            "Remove unchanged volatile subfields from the record."
        ],
        [
            "Turns all subfields to volatile"
        ],
        [
            "Remove empty subfields and fields from the record.\n\n    If 'tag' is not None, only a specific tag of the record will be stripped,\n    otherwise the whole record.\n\n    :param rec:  A record dictionary structure\n    :type  rec:  dictionary\n    :param tag:  The tag of the field to strip empty fields from\n    :type  tag:  string"
        ],
        [
            "Remove all non-empty controlfields from the record.\n\n    :param rec:  A record dictionary structure\n    :type  rec:  dictionary"
        ],
        [
            "Order subfields from a record alphabetically based on subfield code.\n\n    If 'tag' is not None, only a specific tag of the record will be reordered,\n    otherwise the whole record.\n\n    :param rec: bibrecord\n    :type rec: bibrec\n    :param tag: tag where the subfields will be ordered\n    :type tag: str"
        ],
        [
            "Compare 2 fields.\n\n    If strict is True, then the order of the subfield will be taken care of, if\n    not then the order of the subfields doesn't matter.\n\n    :return: True if the field are equivalent, False otherwise."
        ],
        [
            "Check if a field is well-formed.\n\n    :param field: A field tuple as returned by create_field()\n    :type field:  tuple\n    :raise InvenioBibRecordFieldError: If the field is invalid."
        ],
        [
            "Shift all global field positions.\n\n    Shift all global field positions with global field positions\n    higher or equal to 'start' from the value 'delta'."
        ],
        [
            "Return true if MARC 'tag' matches a 'pattern'.\n\n    'pattern' is plain text, with % as wildcard\n\n    Both parameters must be 3 characters long strings.\n\n    .. doctest::\n\n        >>> _tag_matches_pattern(\"909\", \"909\")\n        True\n        >>> _tag_matches_pattern(\"909\", \"9%9\")\n        True\n        >>> _tag_matches_pattern(\"909\", \"9%8\")\n        False\n\n    :param tag: a 3 characters long string\n    :param pattern: a 3 characters long string\n    :return: False or True"
        ],
        [
            "Check if the global field positions in the record are valid.\n\n    I.e., no duplicate global field positions and local field positions in the\n    list of fields are ascending.\n\n    :param record: the record data structure\n    :return: the first error found as a string or None if no error was found"
        ],
        [
            "Sort the fields inside the record by indicators."
        ],
        [
            "Sort a set of fields by their indicators.\n\n    Return a sorted list with correct global field positions."
        ],
        [
            "Create a record object using the LXML parser.\n\n    If correct == 1, then perform DTD validation\n    If correct == 0, then do not perform DTD validation\n\n    If verbose == 0, the parser will not give warnings.\n    If 1 <= verbose <= 3, the parser will not give errors, but will warn\n        the user about possible mistakes (implement me!)\n    If verbose > 3 then the parser will be strict and will stop in case of\n        well-formedness errors or DTD errors."
        ],
        [
            "Retrieve all children from node 'node' with name 'name'."
        ],
        [
            "Iterate through all the children of a node.\n\n    Returns one string containing the values from all the text-nodes\n    recursively."
        ],
        [
            "Check and correct the structure of the record.\n\n    :param record: the record data structure\n    :return: a list of errors found"
        ],
        [
            "Return a warning message of code 'code'.\n\n    If code = (cd, str) it returns the warning message of code 'cd' and appends\n    str at the end"
        ],
        [
            "Compare twolists using given comparing function.\n\n    :param list1: first list to compare\n    :param list2: second list to compare\n    :param custom_cmp: a function taking two arguments (element of\n        list 1, element of list 2) and\n    :return: True or False depending if the values are the same"
        ],
        [
            "Parse an XML document and clean any namespaces."
        ],
        [
            "Clean MARCXML harvested from OAI.\n\n        Allows the xml to be used with BibUpload or BibRecord.\n\n        :param xml: either XML as a string or path to an XML file\n\n        :return: ElementTree of clean data"
        ],
        [
            "Generate the record deletion if deleted form OAI-PMH."
        ],
        [
            "Return a session for yesss.at."
        ],
        [
            "Check for working login data."
        ],
        [
            "Send an SMS."
        ],
        [
            "Return the date of the article in file."
        ],
        [
            "Return this articles' collection."
        ],
        [
            "Attach fulltext FFT."
        ],
        [
            "Convert the list of bibrecs into one MARCXML.\n\n        >>> from harvestingkit.bibrecord import BibRecordPackage\n        >>> from harvestingkit.inspire_cds_package import Inspire2CDS\n        >>> bibrecs = BibRecordPackage(\"inspire.xml\")\n        >>> bibrecs.parse()\n        >>> xml = Inspire2CDS.convert_all(bibrecs.get_records())\n\n        :param records: list of BibRecord dicts\n        :type records: list\n\n        :returns: MARCXML as string"
        ],
        [
            "Yield single conversion objects from a MARCXML file or string.\n\n        >>> from harvestingkit.inspire_cds_package import Inspire2CDS\n        >>> for record in Inspire2CDS.from_source(\"inspire.xml\"):\n        >>>     xml = record.convert()"
        ],
        [
            "Return the opposite mapping by searching the imported KB."
        ],
        [
            "Load configuration from config.\n\n        Meant to run only once per system process as\n        class variable in subclasses."
        ],
        [
            "Try to match the current record to the database."
        ],
        [
            "Keep only fields listed in field_list."
        ],
        [
            "Clear any fields listed in field_list."
        ],
        [
            "Add 035 number from 001 recid with given source."
        ],
        [
            "Add a control-number 00x for given tag with value."
        ],
        [
            "650 Translate Categories."
        ],
        [
            "Connects and logins to the server."
        ],
        [
            "Downloads a file from the FTP server to target folder\n\n        :param source_file: the absolute path for the file on the server\n                   it can be the one of the files coming from\n                   FtpHandler.dir().\n        :type source_file: string\n        :param target_folder: relative or absolute path of the\n                              destination folder default is the\n                              working directory.\n        :type target_folder: string"
        ],
        [
            "Changes the working directory on the server.\n\n        :param folder: the desired directory.\n        :type folder: string"
        ],
        [
            "Lists the files and folders of a specific directory\n        default is the current working directory.\n\n        :param folder: the folder to be listed.\n        :type folder: string\n\n        :returns: a tuple with the list of files in the folder\n                  and the list of subfolders in the folder."
        ],
        [
            "Creates a folder in the server\n\n        :param folder: the folder to be created.\n        :type folder: string"
        ],
        [
            "Delete a file from the server.\n\n        :param filename: the file to be deleted.\n        :type filename: string"
        ],
        [
            "Delete a folder from the server.\n\n        :param foldername: the folder to be deleted.\n        :type foldername: string"
        ],
        [
            "Returns the filesize of a file\n\n        :param filename: the full path to the file on the server.\n        :type filename: string\n\n        :returns: string representation of the filesize."
        ],
        [
            "Uploads a file on the server to the desired location\n\n        :param filename: the name of the file to be uploaded.\n        :type filename: string\n        :param location: the directory in which the file will\n                         be stored.\n        :type location: string"
        ],
        [
            "Parses a block of text indiscriminately"
        ],
        [
            "Parses a block of text rendering links that occur on their own line\n        normally but rendering inline links using a special template dir"
        ],
        [
            "Do the legwork of logging into the Midas Server instance, storing the API\n    key and token.\n\n    :param email: (optional) Email address to login with. If not set, the\n        console will be prompted.\n    :type email: None | string\n    :param password: (optional) User password to login with. If not set and no\n        'api_key' is set, the console will be prompted.\n    :type password: None | string\n    :param api_key: (optional) API key to login with. If not set, password\n        login with be used.\n    :type api_key: None | string\n    :param application: (optional) Application name to be used with 'api_key'.\n    :type application: string\n    :param url: (optional) URL address of the Midas Server instance to login\n        to. If not set, the console will be prompted.\n    :type url: None | string\n    :param verify_ssl_certificate: (optional) If True, the SSL certificate will\n        be verified\n    :type verify_ssl_certificate: bool\n    :returns: API token.\n    :rtype: string"
        ],
        [
            "Renew or get a token to use for transactions with the Midas Server\n    instance.\n\n    :returns: API token.\n    :rtype: string"
        ],
        [
            "Create an item from the local file in the Midas Server folder corresponding\n    to the parent folder id.\n\n    :param local_file: full path to a file on the local file system\n    :type local_file: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the item will be added\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool"
        ],
        [
            "Create a folder from the local file in the midas folder corresponding to\n    the parent folder id.\n\n    :param local_folder: full path to a directory on the local file system\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the folder will be added\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing folder of\n       the same name in the same location, or create a new one instead\n    :type reuse_existing: bool"
        ],
        [
            "Create and return a hex checksum using the MD5 sum of the passed in file.\n    This will stream the file, rather than load it all into memory.\n\n    :param file_path: full path to the file\n    :type file_path: string\n    :returns: a hex checksum\n    :rtype: string"
        ],
        [
            "Create a bitstream in the given item.\n\n    :param file_path: full path to the local file\n    :type file_path: string\n    :param local_file: name of the local file\n    :type local_file: string\n    :param log_ind: (optional) any additional message to log upon creation of\n        the bitstream\n    :type log_ind: None | string"
        ],
        [
            "Function for doing an upload of a file as an item. This should be a\n    building block for user-level functions.\n\n    :param local_file: name of local file to upload\n    :type local_file: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the item will be added\n    :type parent_folder_id: int | long\n    :param file_path: full path to the file\n    :type file_path: string\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool"
        ],
        [
            "Function for creating a remote folder and returning the id. This should be\n    a building block for user-level functions.\n\n    :param local_folder: full path to a local folder\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the new folder will be added\n    :type parent_folder_id: int | long\n    :returns: id of the remote folder that was created\n    :rtype: int | long"
        ],
        [
            "Function to recursively upload a folder and all of its descendants.\n\n    :param local_folder: full path to local folder to be uploaded\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the new folder will be added\n    :type parent_folder_id: int | long\n    :param leaf_folders_as_items: (optional) whether leaf folders should have\n        all files uploaded as single items\n    :type leaf_folders_as_items: bool\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool"
        ],
        [
            "Return whether a folder contains only files. This will be False if the\n    folder contains any subdirectories.\n\n    :param local_folder: full path to the local folder\n    :type local_folder: string\n    :returns: True if the folder contains only files\n    :rtype: bool"
        ],
        [
            "Upload a folder as a new item. Take a folder and use its base name as the\n    name of a new item. Then, upload its containing files into the new item as\n    bitstreams.\n\n    :param local_folder: The path to the folder to be uploaded\n    :type local_folder: string\n    :param parent_folder_id: The id of the destination folder for the new item.\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool"
        ],
        [
            "Upload a pattern of files. This will recursively walk down every tree in\n    the file pattern to create a hierarchy on the server. As of right now, this\n    places the file into the currently logged in user's home directory.\n\n    :param file_pattern: a glob type pattern for files\n    :type file_pattern: string\n    :param destination: (optional) name of the midas destination folder,\n        defaults to Private\n    :type destination: string\n    :param leaf_folders_as_items: (optional) whether leaf folders should have\n        all files uploaded as single items\n    :type leaf_folders_as_items: bool\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool"
        ],
        [
            "Descend a path to return a folder id starting from the given folder id.\n\n    :param parsed_path: a list of folders from top to bottom of a hierarchy\n    :type parsed_path: list[string]\n    :param folder_id: The id of the folder from which to start the descent\n    :type folder_id: int | long\n    :returns: The id of the found folder or -1\n    :rtype: int | long"
        ],
        [
            "Find an item or folder matching the name. A folder will be found first if\n    both are present.\n\n    :param name: The name of the resource\n    :type name: string\n    :param folder_id: The folder to search within\n    :type folder_id: int | long\n    :returns: A tuple indicating whether the resource is an item an the id of\n        said resource. i.e. (True, item_id) or (False, folder_id). Note that in\n        the event that we do not find a result return (False, -1)\n    :rtype: (bool, int | long)"
        ],
        [
            "Get a folder id from a path on the server.\n\n    Warning: This is NOT efficient at all.\n\n    The schema for this path is:\n    path := \"/users/<name>/\" | \"/communities/<name>\" , {<subfolder>/}\n    name := <firstname> , \"_\" , <lastname>\n\n    :param path: The virtual path on the server.\n    :type path: string\n    :returns: a tuple indicating True or False about whether the resource is an\n        item and id of the resource i.e. (True, item_id) or (False, folder_id)\n    :rtype: (bool, int | long)"
        ],
        [
            "Download a folder to the specified path along with any children.\n\n    :param folder_id: The id of the target folder\n    :type folder_id: int | long\n    :param path: (optional) the location to download the folder\n    :type path: string"
        ],
        [
            "Download the requested item to the specified path.\n\n    :param item_id: The id of the item to be downloaded\n    :type item_id: int | long\n    :param path: (optional) the location to download the item\n    :type path: string\n    :param item: The dict of item info\n    :type item: dict | None"
        ],
        [
            "Recursively download a file or item from the Midas Server instance.\n\n    :param server_path: The location on the server to find the resource to\n        download\n    :type server_path: string\n    :param local_path: The location on the client to store the downloaded data\n    :type local_path: string"
        ],
        [
            "Login and get a token. If you do not specify a specific application,\n        'Default' will be used.\n\n        :param email: Email address of the user\n        :type email: string\n        :param api_key: API key assigned to the user\n        :type api_key: string\n        :param application: (optional) Application designated for this API key\n        :type application: string\n        :returns: Token to be used for interaction with the API until\n            expiration\n        :rtype: string"
        ],
        [
            "List the folders in the users home area.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :returns: List of dictionaries containing folder information.\n        :rtype: list[dict]"
        ],
        [
            "Get the default API key for a user.\n\n        :param email: The email of the user.\n        :type email: string\n        :param password: The user's password.\n        :type password: string\n        :returns: API key to confirm that it was fetched successfully.\n        :rtype: string"
        ],
        [
            "List the public users in the system.\n\n        :param limit: (optional) The number of users to fetch.\n        :type limit: int | long\n        :returns: The list of users.\n        :rtype: list[dict]"
        ],
        [
            "Get a user by the email of that user.\n\n        :param email: The email of the desired user.\n        :type email: string\n        :returns: The user requested.\n        :rtype: dict"
        ],
        [
            "Create a new community or update an existing one using the uuid.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param name: The community name.\n        :type name: string\n        :param description: (optional) The community description.\n        :type description: string\n        :param uuid: (optional) uuid of the community. If none is passed, will\n            generate one.\n        :type uuid: string\n        :param privacy: (optional) Default 'Public', possible values\n            [Public|Private].\n        :type privacy: string\n        :param can_join: (optional) Default 'Everyone', possible values\n            [Everyone|Invitation].\n        :type can_join: string\n        :returns: The community dao that was created.\n        :rtype: dict"
        ],
        [
            "Get a community based on its name.\n\n        :param name: The name of the target community.\n        :type name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The requested community.\n        :rtype: dict"
        ],
        [
            "Get a community based on its id.\n\n        :param community_id: The id of the target community.\n        :type community_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The requested community.\n        :rtype: dict"
        ],
        [
            "Get the non-recursive children of the passed in community_id.\n\n        :param community_id: The id of the requested community.\n        :type community_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: List of the folders in the community.\n        :rtype: dict[string, list]"
        ],
        [
            "List all communities visible to a user.\n\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The list of communities.\n        :rtype: list[dict]"
        ],
        [
            "Get the attributes of the specified folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the requested folder.\n        :type folder_id: int | long\n        :returns: Dictionary of the folder attributes.\n        :rtype: dict"
        ],
        [
            "Get the non-recursive children of the passed in folder_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the requested folder.\n        :type folder_id: int | long\n        :returns: Dictionary of two lists: 'folders' and 'items'.\n        :rtype: dict[string, list]"
        ],
        [
            "Delete the folder with the passed in folder_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder to be deleted.\n        :type folder_id: int | long\n        :returns: None.\n        :rtype: None"
        ],
        [
            "Move a folder to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder to be moved.\n        :type folder_id: int | long\n        :param dest_folder_id: The id of destination (new parent) folder.\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the moved folder.\n        :rtype: dict"
        ],
        [
            "Create an item to the server.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param name: The name of the item to be created.\n        :type name: string\n        :param parent_id: The id of the destination folder.\n        :type parent_id: int | long\n        :param description: (optional) The description text of the item.\n        :type description: string\n        :param uuid: (optional) The UUID for the item. It will be generated if\n            not given.\n        :type uuid: string\n        :param privacy: (optional) The privacy state of the item\n            ('Public' or 'Private').\n        :type privacy: string\n        :returns: Dictionary containing the details of the created item.\n        :rtype: dict"
        ],
        [
            "Get the attributes of the specified item.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the requested item.\n        :type item_id: int | string\n        :returns: Dictionary of the item attributes.\n        :rtype: dict"
        ],
        [
            "Download an item to disk.\n\n        :param item_id: The id of the item to be downloaded.\n        :type item_id: int | long\n        :param token: (optional) The authentication token of the user\n            requesting the download.\n        :type token: None | string\n        :param revision: (optional) The revision of the item to download, this\n            defaults to HEAD.\n        :type revision: None | int | long\n        :returns: A tuple of the filename and the content iterator.\n        :rtype: (string, unknown)"
        ],
        [
            "Delete the item with the passed in item_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be deleted.\n        :type item_id: int | long\n        :returns: None.\n        :rtype: None"
        ],
        [
            "Get the metadata associated with an item.\n\n        :param item_id: The id of the item for which metadata will be returned\n        :type item_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :param revision: (optional) Revision of the item. Defaults to latest\n            revision.\n        :type revision: int | long\n        :returns: List of dictionaries containing item metadata.\n        :rtype: list[dict]"
        ],
        [
            "Set the metadata associated with an item.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item for which metadata will be set.\n        :type item_id: int | long\n        :param element: The metadata element name.\n        :type element: string\n        :param value: The metadata value for the field.\n        :type value: string\n        :param qualifier: (optional) The metadata qualifier. Defaults to empty\n            string.\n        :type qualifier: None | string\n        :returns: None.\n        :rtype: None"
        ],
        [
            "Share an item to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be shared.\n        :type item_id: int | long\n        :param dest_folder_id: The id of destination folder where the item is\n            shared to.\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the shared item.\n        :rtype: dict"
        ],
        [
            "Move an item from the source folder to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be moved\n        :type item_id: int | long\n        :param src_folder_id: The id of source folder where the item is located\n        :type src_folder_id: int | long\n        :param dest_folder_id: The id of destination folder where the item is\n            moved to\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the moved item\n        :rtype: dict"
        ],
        [
            "Return all items.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name.\n        :rtype: list[dict]"
        ],
        [
            "Return all items with a given name and parent folder id.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param folder_id: The id of the parent folder to search by.\n        :type folder_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name and parent folder id.\n        :rtype: list[dict]"
        ],
        [
            "Return all items with a given name and parent folder name.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param folder_name: The name of the parent folder to search by.\n        :type folder_name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name and parent folder\n            name.\n        :rtype: list[dict]"
        ],
        [
            "Create a link bitstream.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder in which to create a new item\n            that will contain the link. The new item will have the same name as\n            the URL unless an item name is supplied.\n        :type folder_id: int | long\n        :param url: The URL of the link you will create, will be used as the\n            name of the bitstream and of the item unless an item name is\n            supplied.\n        :type url: string\n        :param item_name: (optional)  The name of the newly created item, if\n            not supplied, the item will have the same name as the URL.\n        :type item_name: string\n        :param length: (optional) The length in bytes of the file to which the\n            link points.\n        :type length: int | long\n        :param checksum: (optional) The MD5 checksum of the file to which the\n            link points.\n        :type checksum: string\n        :returns: The item information of the item created.\n        :rtype: dict"
        ],
        [
            "Generate a token to use for upload.\n\n        Midas Server uses a individual token for each upload. The token\n        corresponds to the file specified and that file only. Passing the MD5\n        checksum allows the server to determine if the file is already in the\n        asset store.\n\n        If :param:`checksum` is passed and the token returned is blank, the\n        server already has this file and there is no need to follow this\n        call with a call to `perform_upload`, as the passed in file will have\n        been added as a bitstream to the item's latest revision, creating a\n        new revision if one doesn't exist.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item in which to upload the file as a\n            bitstream.\n        :type item_id: int | long\n        :param filename: The name of the file to generate the upload token for.\n        :type filename: string\n        :param checksum: (optional) The checksum of the file to upload.\n        :type checksum: None | string\n        :returns: String of the upload token.\n        :rtype: string"
        ],
        [
            "Upload a file into a given item (or just to the public folder if the\n        item is not specified.\n\n        :param upload_token: The upload token (returned by\n            generate_upload_token)\n        :type upload_token: string\n        :param filename: The upload filename. Also used as the path to the\n            file, if 'filepath' is not set.\n        :type filename: string\n        :param mode: (optional) Stream or multipart. Default is stream.\n        :type mode: string\n        :param folder_id: (optional) The id of the folder to upload into.\n        :type folder_id: int | long\n        :param item_id: (optional) If set, will append item ``bitstreams`` to\n            the latest revision (or the one set using :param:`revision` ) of\n            the existing item.\n        :type item_id: int | long\n        :param revision: (optional) If set, will add a new file into an\n            existing revision. Set this to 'head' to add to the most recent\n            revision.\n        :type revision: string | int | long\n        :param filepath: (optional) The path to the file.\n        :type filepath: string\n        :param create_additional_revision: (optional) If set, will create a\n            new revision in the existing item.\n        :type create_additional_revision: bool\n        :returns: Dictionary containing the details of the item created or\n            changed.\n        :rtype: dict"
        ],
        [
            "Get the resources corresponding to a given query.\n\n        :param search: The search criterion.\n        :type search: string\n        :param token: (optional) The credentials to use when searching.\n        :type token: None | string\n        :returns: Dictionary containing the search result. Notable is the\n            dictionary item 'results', which is a list of item details.\n        :rtype: dict"
        ],
        [
            "Add a Condor DAG to the given Batchmake task.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param batchmaketaskid: id of the Batchmake task for this DAG\n        :type batchmaketaskid: int | long\n        :param dagfilename: Filename of the DAG file\n        :type dagfilename: string\n        :param dagmanoutfilename: Filename of the DAG processing output\n        :type dagmanoutfilename: string\n        :returns: The created Condor DAG DAO\n        :rtype: dict"
        ],
        [
            "Add a Condor DAG job to the Condor DAG associated with this\n        Batchmake task\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param batchmaketaskid: id of the Batchmake task for this DAG\n        :type batchmaketaskid: int | long\n        :param jobdefinitionfilename: Filename of the definition file for the\n            job\n        :type jobdefinitionfilename: string\n        :param outputfilename: Filename of the output file for the job\n        :type outputfilename: string\n        :param errorfilename: Filename of the error file for the job\n        :type errorfilename: string\n        :param logfilename: Filename of the log file for the job\n        :type logfilename: string\n        :param postfilename: Filename of the post script log file for the job\n        :type postfilename: string\n        :return: The created Condor job DAO.\n        :rtype: dict"
        ],
        [
            "Extract DICOM metadata from the given item\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: id of the item to be extracted\n        :type item_id: int | long\n        :return: the item revision DAO\n        :rtype: dict"
        ],
        [
            "Log in to get the real token using the temporary token and otp.\n\n        :param temp_token: The temporary token or id returned from normal login\n        :type temp_token: string\n        :param one_time_pass: The one-time pass to be sent to the underlying\n            multi-factor engine.\n        :type one_time_pass: string\n        :returns: A standard token for interacting with the web api.\n        :rtype: string"
        ],
        [
            "Create a big thumbnail for the given bitstream with the given width.\n        It is used as the main image of the given item and shown in the item\n        view page.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param bitstream_id: The bitstream from which to create the thumbnail.\n        :type bitstream_id: int | long\n        :param item_id: The item on which to set the thumbnail.\n        :type item_id: int | long\n        :param width: (optional) The width in pixels to which to resize (aspect\n            ratio will be preserved). Defaults to 575.\n        :type width: int | long\n        :returns: The ItemthumbnailDao object that was created.\n        :rtype: dict"
        ],
        [
            "Create a 100x100 small thumbnail for the given item. It is used for\n        preview purpose and displayed in the 'preview' and 'thumbnails'\n        sidebar sections.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The item on which to set the thumbnail.\n        :type item_id: int | long\n        :returns: The item object (with the new thumbnail id) and the path\n            where the newly created thumbnail is stored.\n        :rtype: dict"
        ],
        [
            "Search item metadata using Apache Solr.\n\n        :param query: The Apache Lucene search query.\n        :type query: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :param limit: (optional) The limit of the search.\n        :type limit: int | long\n        :returns: The list of items that match the search query.\n        :rtype: list[dict]"
        ],
        [
            "Create a new scalar data point.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param community_id: The id of the community that owns the producer.\n        :type community_id: int | long\n        :param producer_display_name: The display name of the producer.\n        :type producer_display_name: string\n        :param metric_name: The metric name that identifies which trend this\n            point belongs to.\n        :type metric_name: string\n        :param producer_revision: The repository revision of the producer that\n            produced this value.\n        :type producer_revision: int | long | string\n        :param submit_time: The submit timestamp. Must be parsable with PHP\n            strtotime().\n        :type submit_time: string\n        :param value: The value of the scalar.\n        :type value: float\n        :param config_item_id: (optional) If this value pertains to a specific\n            configuration item, pass its id here.\n        :type config_item_id: int | long\n        :param test_dataset_id: (optional) If this value pertains to a\n            specific test dataset, pass its id here.\n        :type test_dataset_id: int | long\n        :param truth_dataset_id: (optional) If this value pertains to a\n            specific ground truth dataset, pass its id here.\n        :type truth_dataset_id: int | long\n        :param silent: (optional) If true, do not perform threshold-based email\n            notifications for this scalar.\n        :type silent: bool\n        :param unofficial: (optional) If true, creates an unofficial scalar\n            visible only to the user performing the submission.\n        :type unofficial: bool\n        :param build_results_url: (optional) A URL for linking to build results\n            for this submission.\n        :type build_results_url: string\n        :param branch: (optional) The branch name in the source repository for\n            this submission.\n        :type branch: string\n        :param submission_id: (optional) The id of the submission.\n        :type submission_id: int | long\n        :param submission_uuid: (optional) The uuid of the submission. If one\n            does not exist, it will be created.\n        :type submission_uuid: string\n        :type branch: string\n        :param params: (optional) Any key/value pairs that should be displayed\n            with this scalar result.\n        :type params: dict\n        :param extra_urls: (optional) Other URL's that should be displayed with\n            with this scalar result. Each element of the list should be a dict\n            with the following keys: label, text, href\n        :type extra_urls: list[dict]\n        :param unit: (optional) The unit of the scalar value.\n        :type unit: string\n        :param reproduction_command: (optional) The command to reproduce this\n            scalar.\n        :type reproduction_command: string\n        :returns: The scalar object that was created.\n        :rtype: dict"
        ],
        [
            "Upload a JSON file containing numeric scoring results to be added as\n        scalars. File is parsed and then deleted from the server.\n\n        :param token: A valid token for the user in question.\n        :param filepath: The path to the JSON file.\n        :param community_id: The id of the community that owns the producer.\n        :param producer_display_name: The display name of the producer.\n        :param producer_revision: The repository revision of the producer\n            that produced this value.\n        :param submit_time: The submit timestamp. Must be parsable with PHP\n            strtotime().\n        :param config_item_id: (optional) If this value pertains to a specific\n            configuration item, pass its id here.\n        :param test_dataset_id: (optional) If this value pertains to a\n            specific test dataset, pass its id here.\n        :param truth_dataset_id: (optional) If this value pertains to a\n            specific ground truth dataset, pass its id here.\n        :param parent_keys: (optional) Semicolon-separated list of parent keys\n            to look for numeric results under. Use '.' to denote nesting, like\n            in normal javascript syntax.\n        :param silent: (optional) If true, do not perform threshold-based email\n            notifications for this scalar.\n        :param unofficial: (optional) If true, creates an unofficial scalar\n            visible only to the user performing the submission.\n        :param build_results_url: (optional) A URL for linking to build results\n            for this submission.\n        :param branch: (optional) The branch name in the source repository for\n            this submission.\n        :param params: (optional) Any key/value pairs that should be displayed\n            with this scalar result.\n        :type params: dict\n        :param extra_urls: (optional) Other URL's that should be displayed with\n            with this scalar result. Each element of the list should be a dict\n            with the following keys: label, text, href\n        :type extra_urls: list of dicts\n        :returns: The list of scalars that were created."
        ],
        [
            "Obtain particular version of the doc at key."
        ],
        [
            "Find a hash value for the linear combination of invocation methods."
        ],
        [
            "Connects to a Siemens S7 PLC.\n\n        Connects to a Siemens S7 using the Snap7 library.\n        See [the snap7 documentation](http://snap7.sourceforge.net/) for\n        supported models and more details.\n\n        It's not currently possible to query the device for available pins,\n        so `available_pins()` returns an empty list. Instead, you should use\n        `map_pin()` to map to a Merker, Input or Output in the PLC. The\n        internal id you should use is a string following this format:\n        '[DMQI][XBWD][0-9]+.?[0-9]*' where:\n\n        * [DMQI]: D for DB, M for Merker, Q for Output, I for Input\n        * [XBWD]: X for bit, B for byte, W for word, D for dword\n        * [0-9]+: Address of the resource\n        * [0-9]*: Bit of the address (type X only, ignored in others)\n\n        For example: 'IB100' will read a byte from an input at address 100 and\n        'MX50.2' will read/write bit 2 of the Merker at address 50. It's not\n        allowed to write to inputs (I), but you can read/write Outpus, DBs and\n        Merkers. If it's disallowed by the PLC, an exception will be thrown by\n        python-snap7 library.\n\n        For this library to work, it might be needed to change some settings\n        in the PLC itself. See\n        [the snap7 documentation](http://snap7.sourceforge.net/) for more\n        information. You also need to put the PLC in RUN mode. Not however that\n        having a Ladder program downloaded, running and modifying variables\n        will probably interfere with inputs and outputs, so put it in RUN mode,\n        but preferably without a downloaded program.\n\n        @arg address IP address of the module.\n        @arg rack rack where the module is installed.\n        @arg slot slot in the rack where the module is installed.\n        @arg port port the PLC is listenning to.\n\n        @throw RuntimeError if something went wrong\n        @throw any exception thrown by `snap7`'s methods."
        ],
        [
            "Connects to an Arduino UNO on serial port `port`.\n\n        @throw RuntimeError can't connect to Arduino"
        ],
        [
            "Returns a map of nodename to average fitness value for this block.\n        Assumes that required resources have been checked on all nodes."
        ],
        [
            "Returns a list of available drivers names."
        ],
        [
            "Maps a pin number to a physical device pin.\n\n        To make it easy to change drivers without having to refactor a lot of\n        code, this library does not use the names set by the driver to identify\n        a pin. This function will map a number, that will be used by other\n        functions, to a physical pin represented by the drivers pin id. That\n        way, if you need to use another pin or change the underlying driver\n        completly, you only need to redo the mapping.\n\n        If you're developing a driver, keep in mind that your driver will not\n        know about this. The other functions will translate the mapped pin to\n        your id before calling your function.\n\n        @arg abstract_pin_id the id that will identify this pin in the\n        other function calls. You can choose what you want.\n\n        @arg physical_pin_id the id returned in the driver.\n            See `AbstractDriver.available_pins`. Setting it to None removes the\n            mapping."
        ],
        [
            "Sets pin `pin` to `direction`.\n\n        The pin should support the requested mode. Calling this function\n        on a unmapped pin does nothing. Calling it with a unsupported direction\n        throws RuntimeError.\n\n        If you're developing a driver, you should implement\n        _set_pin_direction(self, pin, direction) where `pin` will be one of\n        your internal IDs. If a pin is set to OUTPUT, put it on LOW state.\n\n        @arg pin pin id you've set using `AbstractDriver.map_pin`\n        @arg mode a value from `AbstractDriver.Direction`\n\n        @throw KeyError if pin isn't mapped.\n        @throw RuntimeError if direction is not supported by pin."
        ],
        [
            "Gets the `ahio.Direction` this pin was set to.\n\n        If you're developing a driver, implement _pin_direction(self, pin)\n\n        @arg pin the pin you want to see the mode\n        @returns the `ahio.Direction` the pin is set to\n\n        @throw KeyError if pin isn't mapped."
        ],
        [
            "Sets pin `pin` to `type`.\n\n        The pin should support the requested mode. Calling this function\n        on a unmapped pin does nothing. Calling it with a unsupported mode\n        throws RuntimeError.\n\n        If you're developing a driver, you should implement\n        _set_pin_type(self, pin, ptype) where `pin` will be one of your\n        internal IDs. If a pin is set to OUTPUT, put it on LOW state.\n\n        @arg pin pin id you've set using `AbstractDriver.map_pin`\n        @arg mode a value from `AbstractDriver.PortType`\n\n        @throw KeyError if pin isn't mapped.\n        @throw RuntimeError if type is not supported by pin."
        ],
        [
            "Gets the `ahio.PortType` this pin was set to.\n\n        If you're developing a driver, implement _pin_type(self, pin)\n\n        @arg pin the pin you want to see the mode\n        @returns the `ahio.PortType` the pin is set to\n\n        @throw KeyError if pin isn't mapped."
        ],
        [
            "Sets the output to the given value.\n\n        Sets `pin` output to given value. If the pin is in INPUT mode, do\n        nothing. If it's an analog pin, value should be in write_range.\n        If it's not in the allowed range, it will be clamped. If pin is in\n        digital mode, value can be `ahio.LogicValue` if `pwm` = False, or a\n        number between 0 and 1 if `pwm` = True. If PWM is False, the pin will\n        be set to HIGH or LOW, if `pwm` is True, a PWM wave with the given\n        cycle will be created. If the pin does not support PWM and `pwm` is\n        True, raise RuntimeError. The `pwm` argument should be ignored in case\n        the pin is analog. If value is not valid for the given\n        pwm/analog|digital combination, raise TypeError.\n\n        If you're developing a driver, implement _write(self, pin, value, pwm)\n\n        @arg pin the pin to write to\n        @arg value the value to write on the pin\n        @arg pwm wether the output should be a pwm wave\n\n        @throw RuntimeError if the pin does not support PWM and `pwm` is True.\n        @throw TypeError if value is not valid for this pin's mode and pwm\n               value.\n        @throw KeyError if pin isn't mapped."
        ],
        [
            "Reads value from pin `pin`.\n\n        Returns the value read from pin `pin`. If it's an analog pin, returns\n        a number in analog.input_range. If it's digital, returns\n        `ahio.LogicValue`.\n\n        If you're developing a driver, implement _read(self, pin)\n\n        @arg pin the pin to read from\n        @returns the value read from the pin\n\n        @throw KeyError if pin isn't mapped."
        ],
        [
            "Sets the analog reference to `reference`\n\n        If the driver supports per pin reference setting, set pin to the\n        desired reference. If not, passing None means set to all, which is the\n        default in most hardware. If only per pin reference is supported and\n        pin is None, raise RuntimeError.\n\n        If you're developing a driver, implement\n        _set_analog_reference(self, reference, pin). Raise RuntimeError if pin\n        was set but is not supported by the platform.\n\n        @arg reference the value that describes the analog reference. See\n            `AbstractDriver.analog_references`\n        @arg pin if the the driver supports it, the pin that will use\n            `reference` as reference. None for all.\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only analog reference hardware.\n        @throw KeyError if pin isn't mapped."
        ],
        [
            "Returns the analog reference.\n\n        If the driver supports per pin analog reference setting, returns the\n        reference for pin `pin`. If pin is None, returns the global analog\n        reference. If only per pin reference is supported and pin is None,\n        raise RuntimeError.\n\n        If you're developing a driver, implement _analog_reference(self, pin)\n\n        @arg pin if the the driver supports it, the pin that will use\n            `reference` as reference. None for all.\n\n        @returns the reference used for pin\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only analog reference hardware.\n        @throw KeyError if pin isn't mapped."
        ],
        [
            "Sets PWM frequency, if supported by hardware\n\n        If the driver supports per pin frequency setting, set pin to the\n        desired frequency. If not, passing None means set to all. If only per\n        pin frequency is supported and pin is None, raise RuntimeError.\n\n        If you're developing a driver, implement\n        _set_pwm_frequency(self, frequency, pin). Raise RuntimeError if pin\n        was set but is not supported by the platform.\n\n        @arg frequency pwm frequency to be set, in Hz\n        @arg pin if the the driver supports it, the pin that will use\n            `frequency` as pwm frequency. None for all/global.\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only hardware.\n        @throw KeyError if pin isn't mapped."
        ],
        [
            "Integrate SIR epidemic model\n\n    Simulate a very basic deterministic SIR system.\n\n    :param 2x1 numpy array y0: initial conditions\n    :param Ntimestep length numpy array time: Vector of time points that \\\n    solution is returned at\n    :param float beta: transmission rate\n    :param float gamma: recovery rate\n\n    :returns: (2)x(Ntimestep) numpy array Xsim: first row S(t), second row I(t)"
        ],
        [
            "Return the URL of the server.\n\n        :returns: URL of the server\n        :rtype: string"
        ],
        [
            "Returns an estimate for the maximum amount of memory to be consumed by numpy arrays."
        ],
        [
            "Create coverage reports and open them in the browser."
        ],
        [
            "Start a Modbus server.\n\n        The following classes are available with their respective named\n        parameters:\n        \n        ModbusTcpClient\n            host: The host to connect to (default 127.0.0.1)\n            port: The modbus port to connect to (default 502)\n            source_address: The source address tuple to bind to (default ('', 0))\n            timeout: The timeout to use for this socket (default Defaults.Timeout)\n\n        ModbusUdpClient\n            host: The host to connect to (default 127.0.0.1)\n            port: The modbus port to connect to (default 502)\n            timeout: The timeout to use for this socket (default None)\n\n        ModbusSerialClient\n            method: The method to use for connection (asii, rtu, binary)\n            port: The serial port to attach to\n            stopbits: The number of stop bits to use (default 1)\n            bytesize: The bytesize of the serial messages (default 8 bits)\n            parity: Which kind of parity to use (default None)\n            baudrate: The baud rate to use for the serial device\n            timeout: The timeout between serial requests (default 3s)\n\n        When configuring the ports, the following convention should be\n        respected:\n        \n        portname: C1:13 -> Coil on device 1, address 13\n\n        The letters can be:\n\n        C = Coil\n        I = Input\n        R = Register\n        H = Holding\n\n        @arg configuration a string that instantiates one of those classes.\n\n        @throw RuntimeError can't connect to Arduino"
        ],
        [
            "Return an exception given status and error codes.\n\n    :param status_code: HTTP status code.\n    :type status_code: None | int\n    :param error_code: Midas Server error code.\n    :type error_code: None | int\n    :param value: Message to display.\n    :type value: string\n    :returns: Exception.\n    :rtype : pydas.exceptions.ResponseError"
        ],
        [
            "Retrieve the last analog data value received for the specified pin.\n\n        :param pin: Selected pin\n\n        :return: The last value entered into the analog response table."
        ],
        [
            "Disables analog reporting for a single analog pin.\n\n        :param pin: Analog pin number. For example for A0, the number is 0.\n\n        :return: No return value"
        ],
        [
            "Disables digital reporting. By turning reporting off for this pin, reporting\n        is disabled for all 8 bits in the \"port\" -\n\n        :param pin: Pin and all pins for this port\n\n        :return: No return value"
        ],
        [
            "Enables analog reporting. By turning reporting on for a single pin.\n\n        :param pin: Analog pin number. For example for A0, the number is 0.\n\n        :return: No return value"
        ],
        [
            "Enables digital reporting. By turning reporting on for all 8 bits in the \"port\" -\n        this is part of Firmata's protocol specification.\n\n        :param pin: Pin and all pins for this port\n\n        :return: No return value"
        ],
        [
            "This method will send an extended data analog output command to the selected pin\n\n        :param pin: 0 - 127\n\n        :param data: 0 - 0xfffff"
        ],
        [
            "Get the stepper library version number.\n\n        :param timeout: specify a time to allow arduino to process and return a version\n\n        :return: the stepper version number if it was set."
        ],
        [
            "Write data to an i2c device.\n\n        :param address: i2c device address\n\n        :param args: A variable number of bytes to be sent to the device"
        ],
        [
            "This method stops an I2C_READ_CONTINUOUSLY operation for the i2c device address specified.\n\n        :param address: address of i2c device"
        ],
        [
            "This method will call the Tone library for the selected pin.\n        If the tone command is set to TONE_TONE, then the specified tone will be played.\n        Else, if the tone command is TONE_NO_TONE, then any currently playing tone will be disabled.\n        It is intended for a future release of Arduino Firmata\n\n        :param pin: Pin number\n\n        :param tone_command: Either TONE_TONE, or TONE_NO_TONE\n\n        :param frequency: Frequency of tone in hz\n\n        :param duration: Duration of tone in milliseconds\n\n        :return: No return value"
        ],
        [
            "This method \"arms\" an analog pin for its data to be latched and saved in the latching table\n        If a callback method is provided, when latching criteria is achieved, the callback function is called\n        with latching data notification. In that case, the latching table is not updated.\n\n        :param pin: Analog pin number (value following an 'A' designator, i.e. A5 = 5\n\n        :param threshold_type: ANALOG_LATCH_GT | ANALOG_LATCH_LT  | ANALOG_LATCH_GTE | ANALOG_LATCH_LTE\n\n        :param threshold_value: numerical value - between 0 and 1023\n\n        :param cb: callback method\n\n        :return: True if successful, False if parameter data is invalid"
        ],
        [
            "This method \"arms\" a digital pin for its data to be latched and saved in the latching table\n        If a callback method is provided, when latching criteria is achieved, the callback function is called\n        with latching data notification. In that case, the latching table is not updated.\n\n        :param pin: Digital pin number\n\n        :param threshold_type: DIGITAL_LATCH_HIGH | DIGITAL_LATCH_LOW\n\n        :param cb: callback function\n\n        :return: True if successful, False if parameter data is invalid"
        ],
        [
            "Configure a pin as a servo pin. Set pulse min, max in ms.\n\n        :param pin: Servo Pin.\n\n        :param min_pulse: Min pulse width in ms.\n\n        :param max_pulse: Max pulse width in ms.\n\n        :return: No return value"
        ],
        [
            "Configure stepper motor prior to operation.\n\n        :param steps_per_revolution: number of steps per motor revolution\n\n        :param stepper_pins: a list of control pin numbers - either 4 or 2"
        ],
        [
            "Move a stepper motor for the number of steps at the specified speed\n\n        :param motor_speed: 21 bits of data to set motor speed\n\n        :param number_of_steps: 14 bits for number of steps & direction\n                                positive is forward, negative is reverse"
        ],
        [
            "Request the stepper library version from the Arduino.\n        To retrieve the version after this command is called, call\n        get_stepper_version"
        ],
        [
            "open the serial port using the configuration data\n        returns a reference to this instance"
        ],
        [
            "This method continually runs. If an incoming character is available on the serial port\n        it is read and placed on the _command_deque\n        @return: Never Returns"
        ],
        [
            "Set the brightness level for the entire display\n        @param brightness: brightness level (0 -15)"
        ],
        [
            "Populate the bit map with the supplied \"shape\" and color\n        and then write the entire bitmap to the display\n        @param shape: pattern to display\n        @param color: color for the pattern"
        ],
        [
            "Write the entire buffer to the display"
        ],
        [
            "Set all led's to off."
        ],
        [
            "This method handles the incoming digital message.\n        It stores the data values in the digital response table.\n        Data is stored for all 8 bits of a  digital port\n\n        :param data: Message data from Firmata\n\n        :return: No return value."
        ],
        [
            "This method handles the incoming encoder data message and stores\n        the data in the digital response table.\n\n        :param data: Message data from Firmata\n\n        :return: No return value."
        ],
        [
            "This method handles the incoming sonar data message and stores\n        the data in the response table.\n\n        :param data: Message data from Firmata\n\n        :return: No return value."
        ],
        [
            "This method will send a Sysex command to Firmata with any accompanying data\n\n        :param sysex_command: sysex command\n\n        :param sysex_data: data for command\n\n        :return : No return value."
        ],
        [
            "This method is used to transmit a non-sysex command.\n\n        :param command: Command to send to firmata includes command + data formatted by caller\n\n        :return : No return value."
        ],
        [
            "Send the reset command to the Arduino.\n        It resets the response tables to their initial values\n\n        :return: No return value"
        ],
        [
            "This method handles the incoming string data message from Firmata.\n        The string is printed to the console\n\n        :param data: Message data from Firmata\n\n        :return: No return value.s"
        ],
        [
            "This method starts the thread that continuously runs to receive and interpret\n        messages coming from Firmata. This must be the last method in this file\n        It also checks the deque for messages to be sent to Firmata."
        ],
        [
            "Use requests to fetch remote content"
        ],
        [
            "Combine finder_image_urls and extender_image_urls,\n        remove duplicate but keep order"
        ],
        [
            "Find image URL in background-image\n\n    Example:\n    <div style=\"width: 100%; height: 100%; background-image: url(http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg);\" class=\"Image iLoaded iWithTransition Frame\" src=\"http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg\"></div>\n    to\n    http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg"
        ],
        [
            "Return the node name where the ``name`` would land to"
        ],
        [
            "Return the node where the ``name`` would land to"
        ],
        [
            "Return the encoding, idletime, or refcount about the key"
        ],
        [
            "Pop a value off the tail of ``src``, push it on the head of ``dst``\n        and then return it.\n\n        This command blocks until a value is in ``src`` or until ``timeout``\n        seconds elapse, whichever is first. A ``timeout`` value of 0 blocks\n        forever.\n        Not atomic"
        ],
        [
            "RPOP a value off of the ``src`` list and LPUSH it\n        on to the ``dst`` list.  Returns the value."
        ],
        [
            "Move ``value`` from set ``src`` to set ``dst``\n        not atomic"
        ],
        [
            "Returns the members of the set resulting from the union between\n        the first set and all the successive sets."
        ],
        [
            "Store the union of sets ``src``,  ``args`` into a new\n        set named ``dest``.  Returns the number of keys in the new set."
        ],
        [
            "Sets each key in the ``mapping`` dict to its corresponding value if\n        none of the keys are already set"
        ],
        [
            "Rename key ``src`` to ``dst``"
        ],
        [
            "Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist"
        ],
        [
            "Returns a list of keys matching ``pattern``"
        ],
        [
            "Returns the number of keys in the current database"
        ],
        [
            "Prepare the date in the instance state for serialization."
        ],
        [
            "Verify the signaure of an XML document with the given certificate.\n    Returns `True` if the document is signed with a valid signature.\n    Returns `False` if the document is not signed or if the signature is\n    invalid.\n\n    :param lxml.etree._Element xml: The document to sign\n    :param file stream: The private key to sign the document with\n\n    :rtype: Boolean"
        ],
        [
            "Add number of photos to each gallery."
        ],
        [
            "Set currently authenticated user as the author of the gallery."
        ],
        [
            "For each photo set it's author to currently authenticated user."
        ],
        [
            "Outputs a list of tuples with ranges or the empty list\n        According to the rfc, start or end values can be omitted"
        ],
        [
            "Removes errored ranges"
        ],
        [
            "Converts to valid byte ranges"
        ],
        [
            "Sorts and removes overlaps"
        ],
        [
            "Renders the selected social widget. You can specify optional settings\n    that will be passed  to widget template.\n\n    Sample usage:\n    {% social_widget_render widget_template ke1=val1 key2=val2 %}\n\n    For example to render Twitter follow button you can use code like this:\n    {% social_widget_render 'twitter/follow_button.html' username=\"ev\" %}"
        ],
        [
            "In-place addition\n\n        :param addend_mat: A matrix to be added on the Sparse3DMatrix object\n        :param axis: The dimension along the addend_mat is added\n        :return: Nothing (as it performs in-place operations)"
        ],
        [
            "In-place multiplication\n\n        :param multiplier: A matrix or vector to be multiplied\n        :param axis: The dim along which 'multiplier' is multiplied\n        :return: Nothing (as it performs in-place operations)"
        ],
        [
            "Updates the probability of read origin at read level\n\n        :param model: Normalization model (1: Gene->Allele->Isoform, 2: Gene->Isoform->Allele, 3: Gene->Isoform*Allele, 4: Gene*Isoform*Allele)\n        :return: Nothing (as it performs in-place operations)"
        ],
        [
            "Runs EM iterations\n\n        :param model: Normalization model (1: Gene->Allele->Isoform, 2: Gene->Isoform->Allele, 3: Gene->Isoform*Allele, 4: Gene*Isoform*Allele)\n        :param tol: Tolerance for termination\n        :param max_iters: Maximum number of iterations until termination\n        :param verbose: Display information on how EM is running\n        :return: Nothing (as it performs in-place operations)"
        ],
        [
            "Exports expected read counts\n\n        :param filename: File name for output\n        :param grp_wise: whether the report is at isoform level or gene level\n        :param reorder: whether the report should be either 'decreasing' or 'increasing' order or just 'as-is'\n        :return: Nothing but the method writes a file"
        ],
        [
            "Exports expected depths\n\n        :param filename: File name for output\n        :param grp_wise: whether the report is at isoform level or gene level\n        :param reorder: whether the report should be either 'decreasing' or 'increasing' order or just 'as-is'\n        :return: Nothing but the method writes a file"
        ],
        [
            "Writes the posterior probability of read origin\n\n        :param filename: File name for output\n        :param title: The title of the posterior probability matrix\n        :return: Nothing but the method writes a file in EMASE format (PyTables)"
        ],
        [
            "Prints nonzero rows of the read wanted"
        ],
        [
            "Transliterate `data` with the given `scheme_map`. This function is used\n  when the source scheme is a Roman scheme.\n\n  :param data: the data to transliterate\n  :param scheme_map: a dict that maps between characters in the old scheme\n                     and characters in the new scheme"
        ],
        [
            "Transliterate `data` with the given `scheme_map`. This function is used\n  when the source scheme is a Brahmic scheme.\n\n  :param data: the data to transliterate\n  :param scheme_map: a dict that maps between characters in the old scheme\n                     and characters in the new scheme"
        ],
        [
            "Detect the input's transliteration scheme.\n\n    :param text: some text data, either a `unicode` or a `str` encoded\n                 in UTF-8."
        ],
        [
            "Add a variety of default schemes."
        ],
        [
            "converts an array of integers to utf8 string"
        ],
        [
            "set the value of delta to reflect the current codepage"
        ],
        [
            "Handle unrecognised characters."
        ],
        [
            "Transliterate a Latin character equivalent to Devanagari.\n        \n        Add VIRAMA for ligatures.\n        Convert standalone to dependent vowels."
        ],
        [
            "A convenience method"
        ],
        [
            "Load and generate ``num`` number of top-level rules from the specified grammar.\n\n    :param list grammar: The grammar file to load and generate data from\n    :param int num: The number of times to generate data\n    :param output: The output destination (an open, writable stream-type object. default=``sys.stdout``)\n    :param int max_recursion: The maximum reference-recursion when generating data (default=``10``)\n    :param int seed: The seed to initialize the PRNG with. If None, will not initialize it."
        ],
        [
            "Build the ``Quote`` instance\n\n        :param list pre: The prerequisites list\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated."
        ],
        [
            "Make the list of verbs into present participles\n\n    E.g.:\n\n        empower -> empowering\n        drive -> driving"
        ],
        [
            "Deletes sent MailerMessage records"
        ],
        [
            "Load the includes of an encoding Namelist files.\n\n  This is an implementation detail of readNamelist."
        ],
        [
            "Return a dict with the data of an encoding Namelist file.\n\n  This is an implementation detail of readNamelist."
        ],
        [
            "Detect infinite recursion and prevent it.\n\n  This is an implementation detail of readNamelist.\n\n  Raises NamelistRecursionError if namFilename is in the process of being included"
        ],
        [
            "Returns the set of codepoints contained in a given Namelist file.\n\n  This is a replacement CodepointsInSubset and implements the \"#$ include\"\n  header format.\n\n  Args:\n    namFilename: The path to the  Namelist file.\n    unique_glyphs: Optional, whether to only include glyphs unique to subset.\n  Returns:\n    A set containing the glyphs in the subset."
        ],
        [
            "Returns list of CharsetInfo about supported orthographies"
        ],
        [
            "Generates header for oauth2"
        ],
        [
            "Parse oauth2 access"
        ],
        [
            "Refresh access token"
        ],
        [
            "Calls right function according to file extension"
        ],
        [
            "Call right func to save data according to file extension"
        ],
        [
            "Write json data into a file"
        ],
        [
            "Get data from json file"
        ],
        [
            "Get data from .yml file"
        ],
        [
            "Write data into a .yml file"
        ],
        [
            "Turns distances into RBF values.\n\n        Parameters\n        ----------\n        X : array\n            The raw pairwise distances.\n\n        Returns\n        -------\n        X_rbf : array of same shape as X\n            The distances in X passed through the RBF kernel."
        ],
        [
            "Learn the linear transformation to clipped eigenvalues.\n\n        Note that if min_eig isn't zero and any of the original eigenvalues\n        were exactly zero, this will leave those eigenvalues as zero.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part."
        ],
        [
            "Learn the linear transformation to flipped eigenvalues.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part."
        ],
        [
            "Transforms X according to the linear transformation corresponding to\n        flipping the input eigenvalues.\n\n        Parameters\n        ----------\n        X : array, shape [n_test, n]\n            The test similarities to training points.\n\n        Returns\n        -------\n        Xt : array, shape [n_test, n]\n            The transformed test similarites to training points."
        ],
        [
            "Flips the negative eigenvalues of X.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.\n\n        Returns\n        -------\n        Xt : array, shape [n, n]\n            The transformed training similarities."
        ],
        [
            "Learn the transformation to shifted eigenvalues. Only depends\n        on the input dimension.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities."
        ],
        [
            "Transforms X according to the linear transformation corresponding to\n        shifting the input eigenvalues to all be at least ``self.min_eig``.\n\n        Parameters\n        ----------\n        X : array, shape [n_test, n]\n            The test similarities to training points.\n\n        Returns\n        -------\n        Xt : array, shape [n_test, n]\n            The transformed test similarites to training points. Only different\n            from X if X is the training data."
        ],
        [
            "Picks the elements of the basis to use for the given data.\n\n        Only depends on the dimension of X. If it's more convenient, you can\n        pass a single integer for X, which is the dimension to use.\n\n        Parameters\n        ----------\n        X : an integer, a :class:`Features` instance, or a list of bag features\n            The input data, or just its dimension, since only the dimension is\n            needed here."
        ],
        [
            "Transform a list of bag features into its projection series\n        representation.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            New data to transform. The data should all lie in [0, 1];\n            use :class:`skl_groups.preprocessing.BagMinMaxScaler` if not.\n\n        Returns\n        -------\n        X_new : integer array, shape ``[len(X), dim_]``\n            X transformed into the new space."
        ],
        [
            "Get distribution version.\n\n        This method is enhanced compared to original distutils implementation.\n        If the version string is set to a special value then instead of using\n        the actual value the real version is obtained by querying versiontools.\n\n        If versiontools package is not installed then the version is obtained\n        from the standard section of the ``PKG-INFO`` file. This file is\n        automatically created by any source distribution. This method is less\n        useful as it cannot take advantage of version control information that\n        is automatically loaded by versiontools. It has the advantage of not\n        requiring versiontools installation and that it does not depend on\n        ``setup_requires`` feature of ``setuptools``."
        ],
        [
            "Get a live version string using versiontools"
        ],
        [
            "Fit the transformer on the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of arrays of shape ``[n_samples[i], n_features]``\n            Training set. If a Features object, it will be stacked.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``fit()``."
        ],
        [
            "Transform the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of bag feature arrays\n            New data to transform.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``transform()``.\n\n        Returns\n        -------\n        X_new : :class:`Features`\n            Transformed features."
        ],
        [
            "Fit and transform the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of bag feature arrays\n            Data to train on and transform.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``transform()``.\n\n        Returns\n        -------\n        X_new : :class:`Features`\n            Transformed features."
        ],
        [
            "Compute the minimum and maximum to be used for later scaling.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis."
        ],
        [
            "Scaling features of X according to feature_range.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed."
        ],
        [
            "Undo the scaling of X according to feature_range.\n\n        Note that if truncate is true, any truncated points will not\n        be restored exactly.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed."
        ],
        [
            "Choose the codewords based on a training set.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of arrays of shape ``[n_samples[i], n_features]``\n            Training set. If a Features object, it will be stacked."
        ],
        [
            "Transform a list of bag features into its bag-of-words representation.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            New data to transform.\n\n        Returns\n        -------\n        X_new : integer array, shape [len(X), kmeans.n_clusters]\n            X transformed into the new space."
        ],
        [
            "Checks whether the array is either integral or boolean."
        ],
        [
            "Returns argument as an integer array, converting floats if convertable.\n    Raises ValueError if it's a float array with nonintegral values."
        ],
        [
            "Signal the start of the process.\n\n        Parameters\n        ----------\n        total : int\n            The total number of steps in the process, or None if unknown."
        ],
        [
            "Builds FLANN indices for each bag."
        ],
        [
            "Gets within-bag distances for each bag."
        ],
        [
            "r'''\n    Estimates the linear inner product \\int p q between two distributions,\n    based on kNN distances."
        ],
        [
            "r'''\n    Estimates \\int p^2 based on kNN distances.\n\n    In here because it's used in the l2 distance, above.\n\n    Returns array of shape (num_Ks,)."
        ],
        [
            "Topologically sort a DAG, represented by a dict of child => set of parents.\n    The dependency dict is destroyed during operation.\n\n    Uses the Kahn algorithm: http://en.wikipedia.org/wiki/Topological_sorting\n    Not a particularly good implementation, but we're just running it on tiny\n    graphs."
        ],
        [
            "Ks as an array and type-checked."
        ],
        [
            "The dictionary of arguments to give to FLANN."
        ],
        [
            "Sets up for divergence estimation \"from\" new data \"to\" X.\n        Builds FLANN indices for each bag, and maybe gets within-bag distances.\n\n        Parameters\n        ----------\n        X : list of arrays or :class:`skl_groups.features.Features`\n            The bags to search \"to\".\n\n        get_rhos : boolean, optional, default False\n            Compute within-bag distances :attr:`rhos_`. These are only needed\n            for some divergence functions or if do_sym is passed, and they'll\n            be computed (and saved) during :meth:`transform` if they're not\n            computed here.\n\n            If you're using Jensen-Shannon divergence, a higher max_K may\n            be needed once it sees the number of points in the transformed bags,\n            so the computation here might be wasted."
        ],
        [
            "If unstacked, convert to stacked. If stacked, do nothing."
        ],
        [
            "Copies the Feature object. Makes a copy of the features array.\n\n        Parameters\n        ----------\n        stack : boolean, optional, default False\n            Whether to stack the copy if this one is unstacked.\n\n        copy_meta : boolean, optional, default False\n            Also copy the metadata. If False, metadata in both points to the\n            same object."
        ],
        [
            "Make a Features object with no metadata; points to the same features."
        ],
        [
            "Specify the data to which kernel values should be computed.\n\n        Parameters\n        ----------\n        X : list of arrays or :class:`skl_groups.features.Features`\n            The bags to compute \"to\"."
        ],
        [
            "Transform a list of bag features into a matrix of its mean features.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            Data to transform.\n\n        Returns\n        -------\n        X_new : array, shape ``[len(X), X.dim]``\n            X transformed into its means."
        ],
        [
            "Start listening to the server"
        ],
        [
            "Connect to the server\n\n        :raise ConnectionError: If socket cannot establish a connection"
        ],
        [
            "Disconnect from the server"
        ],
        [
            "Send a command to the server\n\n        :param string command: command to send"
        ],
        [
            "Read a line from the server. Data is read from the socket until a character ``\\n`` is found\n\n        :return: the read line\n        :rtype: string"
        ],
        [
            "Read a block from the server. Lines are read until a character ``.`` is found\n\n        :return: the read block\n        :rtype: string"
        ],
        [
            "Read a block and return the result as XML\n\n        :return: block as xml\n        :rtype: xml.etree.ElementTree"
        ],
        [
            "Analyse an OpenStreetMap changeset."
        ],
        [
            "Get information about number of changesets, blocks and mapping days of a\n    user, using both the OSM API and the Mapbox comments APIself."
        ],
        [
            "Return a dictionary with id, user, user_id, bounds, date of creation\n    and all the tags of the changeset.\n\n    Args:\n        changeset: the XML string of the changeset."
        ],
        [
            "Get the changeset using the OSM API and return the content as a XML\n    ElementTree.\n\n    Args:\n        changeset: the id of the changeset."
        ],
        [
            "Get the metadata of a changeset using the OSM API and return it as a XML\n    ElementTree.\n\n    Args:\n        changeset: the id of the changeset."
        ],
        [
            "Read the first feature from the geojson and return it as a Polygon\n        object."
        ],
        [
            "Filter the changesets that intersects with the geojson geometry."
        ],
        [
            "Set the fields of this class with the metadata of the analysed\n        changeset."
        ],
        [
            "Add suspicion reason and set the suspicious flag."
        ],
        [
            "Execute the count and verify_words methods."
        ],
        [
            "Verify the fields source, imagery_used and comment of the changeset\n        for some suspect words."
        ],
        [
            "Verify if the software used in the changeset is a powerfull_editor."
        ],
        [
            "Count the number of elements created, modified and deleted by the\n        changeset and analyses if it is a possible import, mass modification or\n        a mass deletion."
        ],
        [
            "Get a stream URI from a playlist URI, ``uri``.\n    Unwraps nested playlists until something that's not a playlist is found or\n    the ``timeout`` is reached."
        ],
        [
            "Start asynchronous HTTP Server on an individual process.\n\n        :param request_handler: Sanic request handler with middleware\n        :param error_handler: Sanic error handler with middleware\n        :param debug: enables debug output (slows server)\n        :param request_timeout: time in seconds\n        :param ssl: SSLContext\n        :param sock: Socket for the server to accept connections from\n        :param request_max_size: size in bytes, `None` for no limit\n        :param reuse_port: `True` for multiple workers\n        :param loop: asyncio compatible event loop\n        :param protocol: subclass of asyncio protocol class\n        :return: Nothing"
        ],
        [
            "Grow this Pantheon by multiplying Gods."
        ],
        [
            "Get it on."
        ],
        [
            "Compare vectors. Borrowed from A. Parish."
        ],
        [
            "This model recognizes that sex chromosomes don't always line up with\n        gender. Assign M, F, or NB according to the probabilities in p_gender."
        ],
        [
            "Accept either strings or Gods as inputs."
        ],
        [
            "Produce two gametes, an egg and a sperm, from the input strings.\n        Combine them to produce a genome a la sexual reproduction."
        ],
        [
            "Produce two gametes, an egg and a sperm, from input Gods. Combine\n        them to produce a genome a la sexual reproduction. Assign divinity\n        according to probabilities in p_divinity. The more divine the parents,\n        the more divine their offspring."
        ],
        [
            "Extract 23 'chromosomes' aka words from 'gene pool' aka list of tokens\n        by searching the list of tokens for words that are related to the given\n        egg_or_sperm_word."
        ],
        [
            "Print parents' names and epithets."
        ],
        [
            "Returns all the information regarding a specific stage run\n\n        See the `Go stage instance documentation`__ for examples.\n\n        .. __: http://api.go.cd/current/#get-stage-instance\n\n        Args:\n          counter (int): The stage instance to fetch.\n            If falsey returns the latest stage instance from :meth:`history`.\n          pipeline_counter (int): The pipeline instance for which to fetch\n            the stage. If falsey returns the latest pipeline instance.\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object"
        ],
        [
            "Performs a HTTP request to the Go server\n\n        Args:\n          path (str): The full path on the Go server to request.\n            This includes any query string attributes.\n          data (str, dict, bool, optional): If any data is present this\n            request will become a POST request.\n          headers (dict, optional): Headers to set for this particular\n            request\n\n        Raises:\n          HTTPError: when the HTTP request fails.\n\n        Returns:\n          file like object: The response from a\n            :func:`urllib2.urlopen` call"
        ],
        [
            "Make the request appear to be coming from a browser\n\n        This is to interact with older parts of Go that doesn't have a\n        proper API call to be made. What will be done:\n\n        1. If no response passed in a call to `go/api/pipelines.xml` is\n           made to get a valid session\n        2. `JSESSIONID` will be populated from this request\n        3. A request to `go/pipelines` will be so the\n           `authenticity_token` (CSRF) can be extracted. It will then\n           silently be injected into `post_args` on any POST calls that\n           doesn't start with `go/api` from this point.\n\n        Args:\n          response: a :class:`Response` object from a previously successful\n            API call. So we won't have to query `go/api/pipelines.xml`\n            unnecessarily.\n\n        Raises:\n          HTTPError: when the HTTP request fails.\n          AuthenticationFailed: when failing to get the `session_id`\n            or the `authenticity_token`."
        ],
        [
            "Return a dict as a list of lists.\n\n    >>> flatten({\"a\": \"b\"})\n    [['a', 'b']]\n    >>> flatten({\"a\": [1, 2, 3]})\n    [['a', [1, 2, 3]]]\n    >>> flatten({\"a\": {\"b\": \"c\"}})\n    [['a', 'b', 'c']]\n    >>> flatten({\"a\": {\"b\": {\"c\": \"e\"}}})\n    [['a', 'b', 'c', 'e']]\n    >>> flatten({\"a\": {\"b\": \"c\", \"d\": \"e\"}})\n    [['a', 'b', 'c'], ['a', 'd', 'e']]\n    >>> flatten({\"a\": {\"b\": \"c\", \"d\": \"e\"}, \"b\": {\"c\": \"d\"}})\n    [['a', 'b', 'c'], ['a', 'd', 'e'], ['b', 'c', 'd']]"
        ],
        [
            "Returns all the information regarding a specific pipeline run\n\n        See the `Go pipeline instance documentation`__ for examples.\n\n        .. __: http://api.go.cd/current/#get-pipeline-instance\n\n        Args:\n          counter (int): The pipeline instance to fetch.\n            If falsey returns the latest pipeline instance from :meth:`history`.\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object"
        ],
        [
            "Schedule a pipeline run\n\n        Aliased as :meth:`run`, :meth:`schedule`, and :meth:`trigger`.\n\n        Args:\n          variables (dict, optional): Variables to set/override\n          secure_variables (dict, optional): Secure variables to set/override\n          materials (dict, optional): Material revisions to be used for\n            this pipeline run. The exact format for this is a bit iffy,\n            have a look at the official\n            `Go pipeline scheduling documentation`__ or inspect a call\n            from triggering manually in the UI.\n          return_new_instance (bool): Returns a :meth:`history` compatible\n            response for the newly scheduled instance. This is primarily so\n            users easily can get the new instance number. **Note:** This is done\n            in a very naive way, it just checks that the instance number is\n            higher than before the pipeline was triggered.\n          backoff_time (float): How long between each check for\n            :arg:`return_new_instance`.\n\n         .. __: http://api.go.cd/current/#scheduling-pipelines\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object"
        ],
        [
            "Yields the output and metadata from all jobs in the pipeline\n\n        Args:\n          instance: The result of a :meth:`instance` call, if not supplied\n            the latest of the pipeline will be used.\n\n        Yields:\n          tuple: (metadata (dict), output (str)).\n\n          metadata contains:\n            - pipeline\n            - pipeline_counter\n            - stage\n            - stage_counter\n            - job\n            - job_result"
        ],
        [
            "Update template config for specified template name.\n\n        .. __: https://api.go.cd/current/#edit-template-config\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object"
        ],
        [
            "Create template config for specified template name.\n\n        .. __: https://api.go.cd/current/#create-template-config\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object"
        ],
        [
            "Delete template config for specified template name.\n\n        .. __: https://api.go.cd/current/#delete-a-template\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object"
        ],
        [
            "Returns a set of all pipelines from the last response\n\n        Returns:\n          set: Response success: all the pipelines available in the response\n               Response failure: an empty set"
        ],
        [
            "Gets an artifact directory by its path.\n\n        See the `Go artifact directory documentation`__ for example responses.\n\n        .. __: http://api.go.cd/current/#get-artifact-directory\n\n        .. note::\n          Getting a directory relies on Go creating a zip file of the\n          directory in question. Because of this Go will zip the file in\n          the background and return a 202 Accepted response. It's then up\n          to the client to check again later and get the final file.\n\n          To work with normal assumptions this :meth:`get_directory` will\n          retry itself up to ``timeout`` seconds to get a 200 response to\n          return. At that point it will then return the response as is, no\n          matter whether it's still 202 or 200. The retry is done with an\n          exponential backoff with a max value between retries. See the\n          ``backoff`` and ``max_wait`` variables.\n\n          If you want to handle the retry logic yourself then use :meth:`get`\n          and add '.zip' as a suffix on the directory.\n\n        Args:\n          path_to_directory (str): The path to the directory to get.\n            It can be nested eg ``target/dist.zip``\n          timeout (int): How many seconds we will wait in total for a\n            successful response from Go when we're receiving 202\n          backoff (float): The initial value used for backoff, raises\n            exponentially until it reaches ``max_wait``\n          max_wait (int): The max time between retries\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n            A successful response is a zip-file."
        ],
        [
            "Configuration loader.\n\n    Adds support for loading templates from the Flask application's instance\n    folder (``<instance_folder>/templates``)."
        ],
        [
            "Create Flask application class.\n\n    Invenio-Files-REST needs to patch the Werkzeug form parsing in order to\n    support streaming large file uploads. This is done by subclassing the Flask\n    application class."
        ],
        [
            "Initialize application object.\n\n        :param app: An instance of :class:`~flask.Flask`."
        ],
        [
            "Initialize configuration.\n\n        :param app: An instance of :class:`~flask.Flask`."
        ],
        [
            "Covert name from CamelCase to \"Normal case\".\n\n    >>> camel2word('CamelCase')\n    'Camel case'\n    >>> camel2word('CaseWithSpec')\n    'Case with spec'"
        ],
        [
            "Format a time in seconds."
        ],
        [
            "Indent representation of a dict"
        ],
        [
            "Test for existence of ``needle`` regex within ``haystack``.\n\n    Say ``escape`` to escape the ``needle`` if you aren't really using the\n    regex feature & have special characters in it."
        ],
        [
            "Mutates any attributes on ``obj`` which are classes, with link to ``obj``.\n\n    Adds a convenience accessor which instantiates ``obj`` and then calls its\n    ``setup`` method.\n\n    Recurses on those objects as well."
        ],
        [
            "Procesa TCU, CP, FEU diario.\n\n    :param df:\n    :param verbose:\n    :param convert_kwh:\n    :return:"
        ],
        [
            "Compress the log message in order to send less bytes to the wire."
        ],
        [
            "Internal bookkeeping to handle nested classes"
        ],
        [
            "Needs to be its own method so it can be called from both wantClass and\n        registerGoodClass."
        ],
        [
            "Obtiene los dataframes de los datos de PVPC con resampling diario y mensual."
        ],
        [
            "Performs sanitation of the path after validating\n\n    :param path: path to sanitize\n    :return: path\n    :raises:\n        - InvalidPath if the path doesn't start with a slash"
        ],
        [
            "Ensures the passed schema instance is compatible\n\n    :param obj: object to validate\n    :return: obj\n    :raises:\n        - IncompatibleSchema if the passed schema is of an incompatible type"
        ],
        [
            "Journey route decorator\n\n    Enables simple serialization, deserialization and validation of Flask routes with the help of Marshmallow.\n\n    :param bp: :class:`flask.Blueprint` object\n    :param args: args to pass along to `Blueprint.route`\n    :param kwargs:\n        - :strict_slashes: Enable / disable strict slashes (default False)\n        - :validate: Enable / disable body/query validation (default True)\n        - :_query: Unmarshal Query string into this schema\n        - :_body: Unmarshal JSON body into this schema\n        - :marshal_with: Serialize the output with this schema\n    :raises:\n        - ValidationError if the query parameters or JSON body fails validation"
        ],
        [
            "Attaches a flask.Blueprint to the bundle\n\n        :param bp: :class:`flask.Blueprint` object\n        :param description: Optional description string\n        :raises:\n            - InvalidBlueprint if the Blueprint is not of type `flask.Blueprint`"
        ],
        [
            "Returns the DottedRule that results from moving the dot."
        ],
        [
            "Computes the intermediate FIRST set using symbols."
        ],
        [
            "Computes the FIRST set for every symbol in the grammar.\n\n        Tenatively based on _compute_first in PLY."
        ],
        [
            "Computes the FOLLOW set for every non-terminal in the grammar.\n\n        Tenatively based on _compute_follow in PLY."
        ],
        [
            "Computes the initial closure using the START_foo production."
        ],
        [
            "Computes the next closure for rules based on the symbol we got.\n\n        Args:\n            rules - an iterable of DottedRules\n            symbol - a string denoting the symbol we've just seen\n\n        Returns: frozenset of DottedRules"
        ],
        [
            "Fills out the entire closure based on some initial dotted rules.\n\n        Args:\n            rules - an iterable of DottedRules\n\n        Returns: frozenset of DottedRules"
        ],
        [
            "Initializes Journey extension\n\n        :param app: App passed from constructor or directly to init_app\n        :raises:\n            - NoBundlesAttached if no bundles has been attached attached"
        ],
        [
            "Returns simple info about registered blueprints\n\n        :return: Tuple containing endpoint, path and allowed methods for each route"
        ],
        [
            "Checks if a bundle exists at the provided path\n\n        :param path: Bundle path\n        :return: bool"
        ],
        [
            "Attaches a bundle object\n\n        :param bundle: :class:`flask_journey.BlueprintBundle` object\n        :raises:\n            - IncompatibleBundle if the bundle is not of type `BlueprintBundle`\n            - ConflictingPath if a bundle already exists at bundle.path\n            - MissingBlueprints if the bundle doesn't contain any blueprints"
        ],
        [
            "Register and return info about the registered blueprint\n\n        :param bp: :class:`flask.Blueprint` object\n        :param bundle_path: the URL prefix of the bundle\n        :param child_path: blueprint relative to the bundle path\n        :return: Dict with info about the blueprint"
        ],
        [
            "Returns detailed information about registered blueprint routes matching the `BlueprintBundle` path\n\n        :param app: App instance to obtain rules from\n        :param base_path: Base path to return detailed route info for\n        :return: List of route detail dicts"
        ],
        [
            "Computes the precedence of terminal and production.\n\n        The precedence of a terminal is it's level in the PRECEDENCE tuple. For\n        a production, the precedence is the right-most terminal (if it exists).\n        The default precedence is DEFAULT_PREC - (LEFT, 0).\n\n        Returns:\n            precedence - dict[terminal | production] = (assoc, level)"
        ],
        [
            "Generates the ACTION and GOTO tables for the grammar.\n\n        Returns:\n            action - dict[state][lookahead] = (action, ...)\n            goto - dict[state][just_reduced] = new_state"
        ],
        [
            "Return the antecedents and the consequent of a definite clause."
        ],
        [
            "Auxiliary routine to implement tt_entails."
        ],
        [
            "Return a list of all propositional symbols in x."
        ],
        [
            "Return True if the propositional logic expression is true in the model,\n    and False if it is false. If the model does not specify the value for\n    every proposition, this may return None to indicate 'not obvious';\n    this may happen even when the expression is tautological."
        ],
        [
            "See if the clauses are true in a partial model."
        ],
        [
            "A variable is an Expr with no args and a lowercase symbol as the op."
        ],
        [
            "Remove the sentence's clauses from the KB."
        ],
        [
            "Updates the cache with setting values from the database."
        ],
        [
            "Search game to determine best action; use alpha-beta pruning.\n    This version cuts off search and uses an evaluation function."
        ],
        [
            "Return the value to player; 1 for win, -1 for loss, 0 otherwise."
        ],
        [
            "If X wins with this move, return 1; if O return -1; else return 0."
        ],
        [
            "Return true if there is a line through move on board for player."
        ],
        [
            "Update a dict, or an object with slots, according to `entries` dict.\n\n    >>> update({'a': 1}, a=10, b=20)\n    {'a': 10, 'b': 20}\n    >>> update(Struct(a=1), a=10, b=20)\n    Struct(a=10, b=20)"
        ],
        [
            "Pick n samples from seq at random, with replacement, with the\n    probability of each element in proportion to its corresponding\n    weight."
        ],
        [
            "Return a random-sample function that picks from seq weighted by weights."
        ],
        [
            "Format args with the first argument as format string, and write.\n    Return the last arg, or format itself if there are no args."
        ],
        [
            "Try to find some reasonable name for the object."
        ],
        [
            "Open a file based at the AIMA root directory."
        ],
        [
            "Just count how many times each value of each input attribute\n    occurs, conditional on the target value. Count the different\n    target values too."
        ],
        [
            "Number of bits to represent the probability distribution in values."
        ],
        [
            "Layered feed-forward network."
        ],
        [
            "Given a list of learning algorithms, have them vote."
        ],
        [
            "Return a predictor that takes a weighted vote."
        ],
        [
            "Copy dataset, replicating each example in proportion to its weight."
        ],
        [
            "Leave one out cross-validation over the dataset."
        ],
        [
            "Generate a DataSet with n examples."
        ],
        [
            "2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints."
        ],
        [
            "Compare various learners on various datasets using cross-validation.\n    Print results as a table."
        ],
        [
            "Check that my fields make sense."
        ],
        [
            "Add an example to the list of examples, checking it first."
        ],
        [
            "Raise ValueError if example has any invalid values."
        ],
        [
            "Returns the number used for attr, which can be a name, or -n .. n-1."
        ],
        [
            "Return a copy of example, with non-input attributes replaced by None."
        ],
        [
            "Add an observation o to the distribution."
        ],
        [
            "Include o among the possible observations, whether or not\n        it's been observed yet."
        ],
        [
            "Return a random sample from the distribution."
        ],
        [
            "Return true if we remove a value."
        ],
        [
            "Minimum-remaining-values heuristic."
        ],
        [
            "Least-constraining-values heuristic."
        ],
        [
            "Prune neighbor values inconsistent with var=value."
        ],
        [
            "Maintain arc consistency."
        ],
        [
            "Solve a CSP by stochastic hillclimbing on the number of conflicts."
        ],
        [
            "Return the value that will give var the least number of conflicts.\n    If there is a tie, choose at random."
        ],
        [
            "Return the number of conflicts var=val has with other variables."
        ],
        [
            "Start accumulating inferences from assuming var=value."
        ],
        [
            "Rule out var=value."
        ],
        [
            "Return the partial assignment implied by the current inferences."
        ],
        [
            "Undo a supposition and all inferences from it."
        ],
        [
            "Return a list of variables in current assignment that are in conflict"
        ],
        [
            "The number of conflicts, as recorded with each assignment.\n        Count conflicts in row and in up, down diagonals. If there\n        is a queen there, it can't conflict with itself, so subtract 3."
        ],
        [
            "Assign var, and keep track of conflicts."
        ],
        [
            "Record conflicts caused by addition or deletion of a Queen."
        ],
        [
            "Find the best segmentation of the string of characters, given the\n    UnigramTextModel P."
        ],
        [
            "Encodes text, using a code which is a permutation of the alphabet."
        ],
        [
            "Build up a random sample of text nwords words long, using\n        the conditional probability given the n-1 preceding words."
        ],
        [
            "Index a whole collection of files."
        ],
        [
            "Index the text of a document."
        ],
        [
            "Compute a score for this word on this docid."
        ],
        [
            "Present the results as a list."
        ],
        [
            "Get results for the query and present them."
        ],
        [
            "Return a score for text based on how common letters pairs are."
        ],
        [
            "Search for a decoding of the ciphertext."
        ],
        [
            "Score is product of word scores, unigram scores, and bigram scores.\n        This can get very small, so we use logs and exp."
        ],
        [
            "Returns a ``SettingDict`` object."
        ],
        [
            "The expected utility of doing a in state s, according to the MDP and U."
        ],
        [
            "Return the state that results from going in this direction."
        ],
        [
            "Returns a ``SettingDict`` object for this queryset."
        ],
        [
            "Creates and returns an object of the appropriate type for ``value``."
        ],
        [
            "Returns ``True`` if this model should be used to store ``value``.\n\n        Checks if ``value`` is an instance of ``value_type``. Override this\n        method if you need more advanced behaviour. For example, to distinguish\n        between single and multi-line text."
        ],
        [
            "One possible schedule function for simulated annealing"
        ],
        [
            "Call genetic_algorithm on the appropriate parts of a problem.\n    This requires the problem to have states that can mate and mutate,\n    plus a value method that scores states."
        ],
        [
            "Return a random Boggle board of size n x n.\n    We represent a board as a linear list of letters."
        ],
        [
            "Print the board in a 2-d array."
        ],
        [
            "Return a list of lists, where the i-th element is the list of indexes\n    for the neighbors of square i."
        ],
        [
            "If n2 is a perfect square, return its square root, else raise error."
        ],
        [
            "List the nodes reachable in one step from this node."
        ],
        [
            "Fig. 3.10"
        ],
        [
            "Return a list of nodes forming the path from the root to this node."
        ],
        [
            "Return a new individual crossing self and other."
        ],
        [
            "Make a digraph into an undirected graph by adding symmetric edges."
        ],
        [
            "Add a link from A and B of given distance, and also add the inverse\n        link if the graph is undirected."
        ],
        [
            "Add a link from A to B of given distance, in one direction only."
        ],
        [
            "h function is straight-line distance from a node's state to goal."
        ],
        [
            "In the leftmost empty column, try all non-conflicting rows."
        ],
        [
            "Place the next queen at the given row."
        ],
        [
            "Set the board, and find all the words in it."
        ],
        [
            "The total score for the words found, according to the rules."
        ],
        [
            "Wrap the agent's program to print its input and output. This will let\n    you see what the agent is doing in the environment."
        ],
        [
            "An agent that keeps track of what locations are clean or dirty."
        ],
        [
            "Run the environment for one time step. If the\n        actions and exogenous changes are independent, this method will\n        do.  If there are interactions between them, you'll need to\n        override this method."
        ],
        [
            "Run the Environment for given number of time steps."
        ],
        [
            "Return all things exactly at a given location."
        ],
        [
            "Add a thing to the environment, setting its location. For\n        convenience, if thing is an agent program we make a new agent\n        for it. (Shouldn't need to override this."
        ],
        [
            "Remove a thing from the environment."
        ],
        [
            "Return all things within radius of location."
        ],
        [
            "By default, agent perceives things within a default radius."
        ],
        [
            "Move a thing to a new location."
        ],
        [
            "Put walls around the entire perimeter of the grid."
        ],
        [
            "Parse a list of words; according to the grammar.\n        Leave results in the chart."
        ],
        [
            "Add edge to chart, and see if it extends or predicts another edge."
        ],
        [
            "For each edge expecting a word of this category here, extend the edge."
        ],
        [
            "Add to chart any rules for B that could help extend this edge."
        ],
        [
            "See what edges can be extended by this edge."
        ],
        [
            "Adds a ``SettingDict`` object for the ``Setting`` model to the context as\n    ``SETTINGS``. Automatically creates non-existent settings with an empty\n    string as the default value."
        ],
        [
            "Return the factor for var in bn's joint distribution given e.\n    That is, bn's full joint distribution, projected to accord with e,\n    is the pointwise product of these factors for bn's variables."
        ],
        [
            "Eliminate var from all factors by summing over its values."
        ],
        [
            "Yield every way of extending e with values for all vars."
        ],
        [
            "Is event consistent with the given evidence?"
        ],
        [
            "Sample an event from bn that's consistent with the evidence e;\n    return the event and its weight, the likelihood that the event\n    accords to the evidence."
        ],
        [
            "Show the probabilities rounded and sorted by key, for the\n        sake of portable doctests."
        ],
        [
            "Add a node to the net. Its parents must already be in the\n        net, and its variable must not."
        ],
        [
            "Multiply two factors, combining their variables."
        ],
        [
            "Make a factor eliminating var by summing over its values."
        ],
        [
            "Return my probabilities; must be down to one variable."
        ],
        [
            "Strips all whitespace from a minidom XML node and its children\n\n    This operation is made in-place."
        ],
        [
            "Takes a hls color and converts to proper hue \n        Bulbs use a BGR order instead of RGB"
        ],
        [
            "Takes your standard rgb color \n        and converts it to a proper hue value"
        ],
        [
            "Takes an HTML hex code\n        and converts it to a proper hue value"
        ],
        [
            "Wait for x seconds\n            each wait command is 100ms"
        ],
        [
            "Return json from querying Web Api\n\n\t\tArgs:\n\t\t\tview: django view function.\n\t\t\trequest: http request object got from django.\n\t\t\t\t\n\t\tReturns: json format dictionary"
        ],
        [
            "put text on on screen\n    a tuple as first argument tells absolute position for the text\n    does not change TermCursor position\n    args = list of optional position, formatting tokens and strings"
        ],
        [
            "get user input without echo"
        ],
        [
            "get character. waiting for key"
        ],
        [
            "tweaked from source of base"
        ],
        [
            "getProcessOwner - Get the process owner of a pid\n\n        @param pid <int> - process id\n\n        @return - None if process not found or can't be determined. Otherwise, a dict: \n            {\n                uid  - Owner UID\n                name - Owner name, or None if one cannot be determined\n            }"
        ],
        [
            "scanProcessForCwd - Searches a given pid's cwd for a given pattern\n\n            @param pid <int> - A running process ID on this system\n            @param searchPortion <str> - Any portion of directory to search\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n\n            @return <dict> - If result is found, the following dict is returned. If no match found on the given pid, or pid is not found running, None is returned.\n                {\n                    'searchPortion' : The passed search pattern\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or uid if no mapping can be found, or \"unknown\" if neither could be determined.\n                    'cmdline'       : Commandline string\n                    'cwd'           : The exact cwd of matched process\n                }"
        ],
        [
            "scanAllProcessesForCwd - Scans all processes on the system for a given search pattern.\n\n            @param searchPortion <str> - Any portion of directory to search\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n\n        @return - <dict> - A dictionary of pid -> cwdResults for each pid that matched the search pattern. For format of \"cwdResults\", @see scanProcessForCwd"
        ],
        [
            "scanProcessForMapping - Searches a given pid's mappings for a certain pattern.\n\n            @param pid <int> - A running process ID on this system\n            @param searchPortion <str> - A mapping for which to search, example: libc or python or libz.so.1. Give empty string to return all mappings.\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n            @return <dict> - If result is found, the following dict is returned. If no match found on the given pid, or pid is not found running, None is returned.\n                {\n                    'searchPortion' : The passed search pattern\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or uid if no mapping can be found, or \"unknown\" if neither could be determined.\n                    'cmdline'       : Commandline string\n                    'matchedMappings' : All mappings likes that matched the given search pattern\n                }"
        ],
        [
            "scanAllProcessesForMapping - Scans all processes on the system for a given search pattern.\n\n            @param searchPortion <str> - A mapping for which to search, example: libc or python or libz.so.1. Give empty string to return all mappings.\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return - <dict> - A dictionary of pid -> mappingResults for each pid that matched the search pattern. For format of \"mappingResults\", @see scanProcessForMapping"
        ],
        [
            "scanProcessForOpenFile - Scans open FDs for a given pid to see if any are the provided searchPortion\n\n            @param searchPortion <str> - Filename to check\n            @param isExactMatch <bool> Default True - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return -  If result is found, the following dict is returned. If no match found on the given pid, or the pid is not found running, None is returned.\n                {\n                    'searchPortion' : The search portion provided\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or \"unknown\" if one could not be determined\n                    'cmdline'       : Commandline string\n                    'fds'           : List of file descriptors assigned to this file (could be mapped several times)\n                    'filenames'     : List of the filenames matched\n                }"
        ],
        [
            "scanAllProcessessForOpenFile - Scans all processes on the system for a given filename\n\n            @param searchPortion <str> - Filename to check\n            @param isExactMatch <bool> Default True - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return - <dict> - A dictionary of pid -> mappingResults for each pid that matched the search pattern. For format of \"mappingResults\", @see scanProcessForOpenFile"
        ],
        [
            "Create and connect to socket for TCP communication with hub."
        ],
        [
            "Send TCP command to hub and return response."
        ],
        [
            "Receive TCP response, looping to get whole thing or timeout."
        ],
        [
            "Get current light data as dictionary with light zids as keys."
        ],
        [
            "Get current light data, set and return as list of Bulb objects."
        ],
        [
            "Set brightness of bulb."
        ],
        [
            "Set color and brightness of bulb."
        ],
        [
            "Update light objects to their current values."
        ],
        [
            "This function takes a file path beginning with edgar and stores the form in a directory.\n        The default directory is sec_filings but can be changed through a keyword argument."
        ],
        [
            "read file as is"
        ],
        [
            "Clean up after ourselves, removing created files.\n    @param {[String]} A list of file paths specifying the files we've created\n        during run. Will all be deleted.\n    @return {None}"
        ],
        [
            "Create an index file in the given location, supplying known lists of\n    present image files and subdirectories.\n    @param {String} root_dir - The root directory of the entire crawl. Used to\n        ascertain whether the given location is the top level.\n    @param {String} location - The current directory of the crawl. The index\n        file will be created here.\n    @param {[String]} image_files - A list of image file names in the location.\n        These will be displayed in the index file's gallery.\n    @param {[String]} dirs - The subdirectories of the location directory.\n        These will be displayed as links further down the file structure.\n    @param {Boolean=False} force_no_processing - If True, do not attempt to\n        actually process thumbnails, PIL images or anything. Simply index\n        <img> tags with original file src attributes.\n    @return {String} The full path (location plus filename) of the newly\n        created index file. Intended for usage cleaning up created files."
        ],
        [
            "Crawl the root directory downwards, generating an index HTML file in each\n    directory on the way down.\n    @param {String} root_dir - The top level directory to crawl down from. In\n        normal usage, this will be '.'.\n    @param {Boolean=False} force_no_processing - If True, do not attempt to\n        actually process thumbnails, PIL images or anything. Simply index\n        <img> tags with original file src attributes.\n    @return {[String]} Full file paths of all created files."
        ],
        [
            "Get an instance of PIL.Image from the given file.\n    @param {String} dir_path - The directory containing the image file\n    @param {String} image_file - The filename of the image file within dir_path\n    @return {PIL.Image} An instance of the image file as a PIL Image, or None\n        if the functionality is not available. This could be because PIL is not\n        present, or because it can't process the given file type."
        ],
        [
            "Get base-64 encoded data as a string for the given image. Fallback to return\n    fallback_image_file if cannot get the image data or img is None.\n    @param {Image} img - The PIL Image to get src data for\n    @param {String} fallback_image_file - The filename of the image file,\n        to be used when image data capture fails\n    @return {String} The base-64 encoded image data string, or path to the file\n        itself if not supported."
        ],
        [
            "Get a PIL.Image from the given image file which has been scaled down to\n    THUMBNAIL_WIDTH wide.\n    @param {String} dir_path - The directory containing the image file\n    @param {String} image_file - The filename of the image file within dir_path\n    @return {PIL.Image} An instance of the thumbnail as a PIL Image, or None\n        if the functionality is not available. See _get_image_from_file for\n        details."
        ],
        [
            "Run the image server. This is blocking. Will handle user KeyboardInterrupt\n    and other exceptions appropriately and return control once the server is\n    stopped.\n    @return {None}"
        ],
        [
            "Generate indexes and run server from the given directory downwards.\n    @param {String} dir_path - The directory path (absolute, or relative to CWD)\n    @return {None}"
        ],
        [
            "USE carefully ^^"
        ],
        [
            "random blending masks"
        ],
        [
            "z value as like a seed"
        ],
        [
            "Converts a permutation into a permutation matrix.\n\n    `matches` is a dictionary whose keys are vertices and whose values are\n    partners. For each vertex ``u`` and ``v``, entry (``u``, ``v``) in the\n    returned matrix will be a ``1`` if and only if ``matches[u] == v``.\n\n    Pre-condition: `matches` must be a permutation on an initial subset of the\n    natural numbers.\n\n    Returns a permutation matrix as a square NumPy array."
        ],
        [
            "Convenience function that creates a block matrix with the specified\n    blocks.\n\n    Each argument must be a NumPy matrix. The two top matrices must have the\n    same number of rows, as must the two bottom matrices. The two left matrices\n    must have the same number of columns, as must the two right matrices."
        ],
        [
            "Returns the adjacency matrix of a bipartite graph whose biadjacency\n    matrix is `A`.\n\n    `A` must be a NumPy array.\n\n    If `A` has **m** rows and **n** columns, then the returned matrix has **m +\n    n** rows and columns."
        ],
        [
            "Returns the Boolean matrix in the same shape as `D` with ones exactly\n    where there are nonzero entries in `D`.\n\n    `D` must be a NumPy array."
        ],
        [
            "Returns the result of incrementing `version`.\n\n    If `which` is not specified, the \"patch\" part of the version number will be\n    incremented.  If `which` is specified, it must be ``'major'``, ``'minor'``,\n    or ``'patch'``. If it is one of these three strings, the corresponding part\n    of the version number will be incremented instead of the patch number.\n\n    Returns a string representing the next version number.\n\n    Example::\n\n        >>> bump_version('2.7.1')\n        '2.7.2'\n        >>> bump_version('2.7.1', 'minor')\n        '2.8.0'\n        >>> bump_version('2.7.1', 'major')\n        '3.0.0'"
        ],
        [
            "Gets the current version from the specified file.\n\n    This function assumes the file includes a string of the form::\n\n        <pattern> = <version>"
        ],
        [
            "Prints the specified message and exits the program with the specified\n    exit status."
        ],
        [
            "Tags the current version."
        ],
        [
            "initialize with templates' path\n        parameters\n          templates_path    str    the position of templates directory\n          global_data       dict   globa data can be got in any templates"
        ],
        [
            "Render data with template, return html unicodes.\n        parameters\n          template   str  the template's filename\n          data       dict the data to render"
        ],
        [
            "Render data with template and then write to path"
        ],
        [
            "shortcut to render data with `template`. Just add exception\n    catch to `renderer.render`"
        ],
        [
            "Get the DataFrame for this view.\n        Defaults to using `self.dataframe`.\n\n        This method should always be used rather than accessing `self.dataframe`\n        directly, as `self.dataframe` gets evaluated only once, and those results\n        are cached for all subsequent requests.\n\n        You may want to override this if you need to provide different\n        dataframes depending on the incoming request."
        ],
        [
            "Indexes the row based on the request parameters."
        ],
        [
            "Returns the row the view is displaying.\n\n        You may want to override this if you need to provide non-standard\n        queryset lookups.  Eg if objects are referenced using multiple\n        keyword arguments in the url conf."
        ],
        [
            "The paginator instance associated with the view, or `None`."
        ],
        [
            "Return a single page of results, or `None` if pagination is disabled."
        ],
        [
            "parse config, return a dict"
        ],
        [
            "shortcut to render data with `template` and then write to `path`.\n    Just add exception catch to `renderer.render_to`"
        ],
        [
            "Parse ascii post source, return dict"
        ],
        [
            "parse post source files name to datetime object"
        ],
        [
            "run a server binding to port"
        ],
        [
            "get source files' update time"
        ],
        [
            "watch files for changes, if changed, rebuild blog. this thread\n        will quit if the main process ends"
        ],
        [
            "Deploy new blog to current directory"
        ],
        [
            "Temporarily update the context to use the BlockContext for the given alias."
        ],
        [
            "Find the first matching block in the current block_context"
        ],
        [
            "Load a series of widget libraries."
        ],
        [
            "Return a list of widget names for the provided field."
        ],
        [
            "Allow reuse of a block within a template.\n\n    {% reuse '_myblock' foo=bar %}\n\n    If passed a list of block names, will use the first that matches:\n\n    {% reuse list_of_block_names .... %}"
        ],
        [
            "When dealing with optgroups, ensure that the value is properly force_text'd."
        ],
        [
            "Message instances are namedtuples of type `Message`.\n        The date field is already serialized in datetime.isoformat ECMA-262 format"
        ],
        [
            "Send a message to a list of users without passing through `django.contrib.messages`\n\n    :param users: an iterable containing the recipients of the messages\n    :param level: message level\n    :param message_text: the string containing the message\n    :param extra_tags: like the Django api, a string containing extra tags for the message\n    :param date: a date, different than the default timezone.now\n    :param url: an optional url\n    :param fail_silently: not used at the moment"
        ],
        [
            "Send a message to all users aka broadcast.\n\n    :param level: message level\n    :param message_text: the string containing the message\n    :param extra_tags: like the Django api, a string containing extra tags for the message\n    :param date: a date, different than the default timezone.now\n    :param url: an optional url\n    :param fail_silently: not used at the moment"
        ],
        [
            "Mark message instance as read for user.\n    Returns True if the message was `unread` and thus actually marked as `read` or False in case\n    it is already `read` or it does not exist at all.\n\n    :param user: user instance for the recipient\n    :param message: a Message instance to mark as read"
        ],
        [
            "Mark all message instances for a user as read.\n\n    :param user: user instance for the recipient"
        ],
        [
            "Renders a list of archived messages for the current user"
        ],
        [
            "Retrieve unread messages for current user, both from the inbox and\n        from other storages"
        ],
        [
            "If the message level was configured for being stored and request.user\n        is not anonymous, save it to the database. Otherwise, let some other\n        class handle the message.\n\n        Notice: controls like checking the message is not empty and the level\n        is above the filter need to be performed here, but it could happen\n        they'll be performed again later if the message does not need to be\n        stored."
        ],
        [
            "persistent messages are already in the database inside the 'archive',\n        so we can say they're already \"stored\".\n        Here we put them in the inbox, or remove from the inbox in case the\n        messages were iterated.\n\n        messages contains only new msgs if self.used==True\n        else contains both new and unread messages"
        ],
        [
            "Like the base class method, prepares a list of messages for storage\n        but avoid to do this for `models.Message` instances."
        ],
        [
            "Main entry point for script."
        ],
        [
            "initializes a base logger\n\n    you can use this to init a logger in any of your files.\n    this will use config.py's LOGGER param and logging.dictConfig to configure\n    the logger for you.\n\n    :param int|logging.LEVEL base_level: desired base logging level\n    :param int|logging.LEVEL verbose_level: desired verbose logging level\n    :param dict logging_dict: dictConfig based configuration.\n     used to override the default configuration from config.py\n    :rtype: `python logger`"
        ],
        [
            "Configure an object with a user-supplied factory."
        ],
        [
            "sets the global verbosity level for console and the jocker_lgr logger.\n\n    :param bool is_verbose_output: should be output be verbose"
        ],
        [
            "returns a configuration object\n\n    :param string config_file: path to config file"
        ],
        [
            "generates a Dockerfile, builds an image and pushes it to DockerHub\n\n    A `Dockerfile` will be generated by Jinja2 according to the `varsfile`\n    imported. If build is true, an image will be generated from the\n    `outputfile` which is the generated Dockerfile and committed to the\n    image:tag string supplied to `build`.\n    If push is true, a build will be triggered and the produced image\n    will be pushed to DockerHub upon completion.\n\n    :param string varsfile: path to file with variables.\n    :param string templatefile: path to template file to use.\n    :param string outputfile: path to output Dockerfile.\n    :param string configfile: path to yaml file with docker-py config.\n    :param bool dryrun: mock run.\n    :param build: False or the image:tag to build to.\n    :param push: False or the image:tag to build to. (triggers build)\n    :param bool verbose: verbose output."
        ],
        [
            "since the push process outputs a single unicode string consisting of\n        multiple JSON formatted \"status\" lines, we need to parse it so that it\n        can be read as multiple strings.\n\n        This will receive the string as an input, count curly braces and ignore\n        any newlines. When the curly braces stack is 0, it will append the\n        entire string it has read up until then to a list and so forth.\n\n        :param string: the string to parse\n        :rtype: list of JSON's"
        ],
        [
            "Uploads an image file to Imgur"
        ],
        [
            "Return true if the IP address is in dotted decimal notation."
        ],
        [
            "Return true if the IP address is in binary notation."
        ],
        [
            "Return true if the IP address is in octal notation."
        ],
        [
            "Return true if the IP address is in decimal notation."
        ],
        [
            "Function internally used to check if the given netmask\n    is of the specified notation."
        ],
        [
            "Return true if the netmask is in bits notatation."
        ],
        [
            "Return true if the netmask is in wildcard bits notatation."
        ],
        [
            "Dotted decimal notation to decimal conversion."
        ],
        [
            "Decimal to dotted decimal notation conversion."
        ],
        [
            "Hexadecimal to decimal conversion."
        ],
        [
            "Octal to decimal conversion."
        ],
        [
            "Binary to decimal conversion."
        ],
        [
            "Generate a table to convert a whole byte to binary.\n    This code was taken from the Python Cookbook, 2nd edition - O'Reilly."
        ],
        [
            "Decimal to binary conversion."
        ],
        [
            "Bits to decimal conversion."
        ],
        [
            "Wildcard bits to decimal conversion."
        ],
        [
            "Function internally used to detect the notation of the\n    given IP or netmask."
        ],
        [
            "Internally used to convert IPs and netmasks to other notations."
        ],
        [
            "Convert among IP address notations.\n\n    Given an IP address, this function returns the address\n    in another notation.\n\n    @param ip: the IP address.\n    @type ip: integers, strings or object with an appropriate __str()__ method.\n\n    @param notation: the notation of the output (default: IP_DOT).\n    @type notation: one of the IP_* constants, or the equivalent strings.\n\n    @param inotation: force the input to be considered in the given notation\n                    (default the notation of the input is autodetected).\n    @type inotation: one of the IP_* constants, or the equivalent strings.\n\n    @param check: force the notation check on the input.\n    @type check: True force the check, False force not to check and None\n                do the check only if the inotation is unknown.\n\n    @return: a string representing the IP in the selected notation.\n\n    @raise ValueError: raised when the input is in unknown notation."
        ],
        [
            "Convert a netmask to another notation."
        ],
        [
            "Sum two IP addresses."
        ],
        [
            "Subtract two IP addresses."
        ],
        [
            "Return the bits notation of the netmask."
        ],
        [
            "Return the wildcard bits notation of the netmask."
        ],
        [
            "Set the IP address and the netmask."
        ],
        [
            "Change the current IP."
        ],
        [
            "Change the current netmask."
        ],
        [
            "Return true if the given address in amongst the usable addresses,\n        or if the given CIDR is contained in this one."
        ],
        [
            "Upload a file to S3 possibly using the multi-part uploader\n        Return the key uploaded"
        ],
        [
            "Copy a file from one bucket into another"
        ],
        [
            "Recursively upload a ``folder`` into a backet.\n\n        :param bucket: bucket where to upload the folder to\n        :param folder: the folder location in the local file system\n        :param key: Optional key where the folder is uploaded\n        :param skip: Optional list of files to skip\n        :param content_types: Optional dictionary mapping suffixes to\n            content types\n        :return: a coroutine"
        ],
        [
            "Coroutine for uploading a single file"
        ],
        [
            "Trigger an ``event`` on this channel"
        ],
        [
            "Connect to a Pusher websocket"
        ],
        [
            "Handle websocket incoming messages"
        ],
        [
            "Constant time string comparison"
        ],
        [
            "Decodes a limited set of HTML entities."
        ],
        [
            "Set signature passphrases"
        ],
        [
            "Set encryption passphrases"
        ],
        [
            "Set algorithms used for sealing. Defaults can not be overridden."
        ],
        [
            "Get algorithms used for sealing"
        ],
        [
            "Private function for setting options used for sealing"
        ],
        [
            "Verify sealed data signature"
        ],
        [
            "Encode data with specific algorithm"
        ],
        [
            "Decode data with specific algorithm"
        ],
        [
            "Add signature to data"
        ],
        [
            "Verify and remove signature"
        ],
        [
            "Verify and remove magic"
        ],
        [
            "Add header to data"
        ],
        [
            "Read header from data"
        ],
        [
            "Remove header from data"
        ],
        [
            "Read header version from data"
        ],
        [
            "Get algorithm info"
        ],
        [
            "Generate and return PBKDF2 key"
        ],
        [
            "Update algorithm definition type dictionaries"
        ],
        [
            "This function populates the internal tableOfContents list with the contents\n        of the zip file TOC. If the server does not support ranged requests, this will raise\n        and exception. It will also throw an exception if the TOC cannot be found."
        ],
        [
            "This function will extract a single file from the remote zip without downloading\n        the entire zip file. The filename argument should match whatever is in the 'filename'\n        key of the tableOfContents."
        ],
        [
            "Does photometry and estimates uncertainties by calculating the scatter around a linear fit to the data\n        in each orientation. This function is called by other functions and generally the user will not need\n        to interact with it directly."
        ],
        [
            "Creates the figure shown in ``adjust_aperture`` for visualization purposes. Called by other functions\n        and generally not called by the user directly.\n\n        Args: \n            img: The data frame to be passed through to be plotted. A cutout of the ``integrated_postcard``"
        ],
        [
            "Identify the centroid positions for the target star at all epochs. Useful for verifying that there is\n        no correlation between flux and position, as might be expected for high proper motion stars."
        ],
        [
            "Identify the \"expected\" flux value at the time of each observation based on the \n        Kepler long-cadence data, to ensure variations observed are not the effects of a single\n        large starspot. Only works if the target star was targeted for long or short cadence\n        observations during the primary mission."
        ],
        [
            "Estimate the photometric uncertainties on each data point following Equation A.2 of The Paper.\n        Based on the kepcal package of Dan Foreman-Mackey."
        ],
        [
            "Dump single field."
        ],
        [
            "Disassemble serialized protocol buffers file."
        ],
        [
            "Find all missing imports in list of Pbd instances."
        ],
        [
            "Write fasta_dict to fasta_file\n\n    :param fasta_dict: returned by fasta_file_to_dict\n    :param fasta_file: output file can be a string path or a file object\n    :param line_char_limit: None = no limit (default)\n    :return: None"
        ],
        [
            "Helper function to record and log an error message\n\n        :param line_data: dict\n        :param error_info: dict\n        :param logger:\n        :param log_level: int\n        :return:"
        ],
        [
            "checks whether child features are within the coordinate boundaries of parent features\n\n        :return:"
        ],
        [
            "1. get a list of CDS with the same parent\n        2. sort according to strand\n        3. calculate and validate phase"
        ],
        [
            "Transfer children from old_parent to new_parent\n\n        :param old_parent: feature_id(str) or line_index(int) or line_data(dict) or feature\n        :param new_parent: feature_id(str) or line_index(int) or line_data(dict)\n        :return: List of children transferred"
        ],
        [
            "Marks line_data and all of its associated feature's 'line_status' as 'removed', does not actually remove the line_data from the data structure.\n        The write function checks the 'line_status' when writing the gff file.\n        Find the root parent of line_data of type root_type, remove all of its descendants.\n        If the root parent has a parent with no children after the remove, remove the root parent's parent recursively.\n\n        :param line_data:\n        :param root_type:\n        :return:"
        ],
        [
            "given a filename, return the ABFs ID string."
        ],
        [
            "Determine the protocol used to record an ABF file"
        ],
        [
            "given the bytestring ABF header, make and launch HTML."
        ],
        [
            "iterate over every sweep"
        ],
        [
            "read the header and populate self with information about comments"
        ],
        [
            "given a sweep, return the protocol as condensed sequence.\n        This is better for comparing similarities and determining steps.\n        There should be no duplicate numbers."
        ],
        [
            "return the average of part of the current sweep."
        ],
        [
            "Return a sweep which is the average of multiple sweeps.\n        For now, standard deviation is lost."
        ],
        [
            "create kernel based on this ABF info."
        ],
        [
            "Get the filtered sweepY of the current sweep.\n        Only works if self.kernel has been generated."
        ],
        [
            "Given a list of list of dicts, return just the dicts."
        ],
        [
            "given a key, return a list of values from the matrix with that key."
        ],
        [
            "given a recarray, return it as a list of dicts."
        ],
        [
            "given text, make it a temporary HTML file and launch it."
        ],
        [
            "show everything we can about an object's projects and methods."
        ],
        [
            "Put 2d numpy data into a temporary HTML file."
        ],
        [
            "given a string or a path to an XML file, return an XML object."
        ],
        [
            "mono-exponential curve."
        ],
        [
            "return a list of Is where the data first crosses above threshold."
        ],
        [
            "Try to format anything as a 2D matrix with column names."
        ],
        [
            "save something to a pickle file"
        ],
        [
            "convert a dictionary to a pretty formatted string."
        ],
        [
            "determine the comment cooked in the protocol."
        ],
        [
            "scan an ABF directory and subdirectory. Try to do this just once.\n    Returns ABF files, SWHLab files, and groups."
        ],
        [
            "given an ABF file name, return the ABF of its parent."
        ],
        [
            "given an ABF and the groups dict, return the ID of its parent."
        ],
        [
            "given an ABF, find the parent, return that line of experiments.txt"
        ],
        [
            "given a path or list of files, return ABF IDs."
        ],
        [
            "May be given an ABF object or filename."
        ],
        [
            "return an \"FTP\" object after logging in."
        ],
        [
            "upload everything from localFolder into the current FTP folder."
        ],
        [
            "Only scott should do this. Upload new version to site."
        ],
        [
            "use the GUI to ask for a string."
        ],
        [
            "use the GUI to pop up a message."
        ],
        [
            "use the GUI to ask YES or NO."
        ],
        [
            "check out the arguments and figure out what to do."
        ],
        [
            "provide all stats on the first AP."
        ],
        [
            "return average of a feature divided by sweep."
        ],
        [
            "continuously monitor a folder for new abfs and try to analyze them.\n    This is intended to watch only one folder, but can run multiple copies."
        ],
        [
            "easy way to plot a gain function."
        ],
        [
            "draw vertical lines at comment points. Defaults to seconds."
        ],
        [
            "stamp the bottom with file info."
        ],
        [
            "makes a new matplotlib figure with default dims and DPI.\n    Also labels it with pA or mV depending on ABF."
        ],
        [
            "Save the pylab figure somewhere.\n    If fname==False, show it instead.\n    Height force > dpi force\n    if a tag is given instead of a filename, save it alongside the ABF"
        ],
        [
            "if the module is in this path, load it from the local folder."
        ],
        [
            "Called to update the state of the iterator.  This methods\n        receives the set of task ids from the previous set of tasks\n        together with the launch information to allow the output\n        values to be parsed using the output_extractor. This data is then\n        used to determine the next desired point in the parameter\n        space by calling the _update_state method."
        ],
        [
            "When dynamic, not all argument values may be available."
        ],
        [
            "Summarizes the trace of values used to update the DynamicArgs\n        and the arguments subsequently returned. May be used to\n        implement the summary method."
        ],
        [
            "Takes as input a list or tuple of two elements. First the\n        value returned by incrementing by 'stepsize' followed by the\n        value returned after a 'stepsize' decrement."
        ],
        [
            "given a filename or ABF object, try to analyze it."
        ],
        [
            "frame the current matplotlib plot with ABF info, and optionally save it.\n    Note that this is entirely independent of the ABFplot class object.\n    if saveImage is False, show it instead.\n\n    Datatype should be:\n        * plot\n        * experiment"
        ],
        [
            "make sure a figure is ready."
        ],
        [
            "save the existing figure. does not close it."
        ],
        [
            "plot every sweep of an ABF file."
        ],
        [
            "plot the current sweep protocol."
        ],
        [
            "plot the protocol of all sweeps."
        ],
        [
            "Given ABFs and TIFs formatted long style, rename each of them to prefix their number with a different number.\n\n    Example: 2017_10_11_0011.abf\n    Becomes: 2017_10_11_?011.abf\n    where ? can be any character."
        ],
        [
            "given a list of files, return a dict organized by extension."
        ],
        [
            "given files and cells, return a dict of files grouped by cell."
        ],
        [
            "populate class properties relating to files in the folder."
        ],
        [
            "generate list of cells with links. keep this simple.\n        automatically generates splash page and regnerates frames."
        ],
        [
            "generate a data view for every ABF in the project folder."
        ],
        [
            "hyperpolarization step. Use to calculate tau and stuff."
        ],
        [
            "IC steps. Use to determine gain function."
        ],
        [
            "IC steps. See how hyperpol. step affects things."
        ],
        [
            "repeated membrane tests."
        ],
        [
            "fast sweeps, 1 step per sweep, for clean IV without fast currents."
        ],
        [
            "repeated membrane tests, likely with drug added. Maybe IPSCs."
        ],
        [
            "combination of membrane test and IV steps."
        ],
        [
            "OBSOLETE WAY TO INDEX A FOLDER."
        ],
        [
            "A custom save method that handles figuring out when something is activated or deactivated."
        ],
        [
            "It is impossible to delete an activatable model unless force is True. This function instead sets it to inactive."
        ],
        [
            "Write to file_handle if supplied, othewise print output"
        ],
        [
            "A helper method that supplies the root directory name given a\n        timestamp."
        ],
        [
            "The log contains the tids and corresponding specifications\n        used during launch with the specifications in JSON format."
        ],
        [
            "All launchers should call this method to write the info file\n        at the end of the launch. The .info file is saved given\n        setup_info supplied by _setup_launch into the\n        root_directory. When called without setup_info, the existing\n        info file is updated with the end-time."
        ],
        [
            "Launches processes defined by process_commands, but only\n        executes max_concurrency processes at a time; if a process\n        completes and there are still outstanding processes to be\n        executed, the next processes are run until max_concurrency is\n        reached again."
        ],
        [
            "A succinct summary of the Launcher configuration.  Unlike the\n        repr, a summary does not have to be complete but must supply\n        key information relevant to the user."
        ],
        [
            "The method that actually runs qsub to invoke the python\n        process with the necessary commands to trigger the next\n        collation step and next block of jobs."
        ],
        [
            "This method handles static argument specifiers and cases where\n        the dynamic specifiers cannot be queued before the arguments\n        are known."
        ],
        [
            "Aggregates all process_commands and the designated output files into a\n        list, and outputs it as JSON, after which the wrapper script is called."
        ],
        [
            "Performs consistency checks across all the launchers."
        ],
        [
            "Launches all available launchers."
        ],
        [
            "Runs the review process for all the launchers."
        ],
        [
            "Helper to prompt the user for input on the commandline."
        ],
        [
            "The implementation in the base class simply checks there is no\n        clash between the metadata and data keys."
        ],
        [
            "Returns the full path for saving the file, adding an extension\n        and making the filename unique as necessary."
        ],
        [
            "Returns a boolean indicating whether the filename has an\n        appropriate extension for this class."
        ],
        [
            "Data may be either a PIL Image object or a Numpy array."
        ],
        [
            "return \"YYYY-MM-DD\" when the file was modified."
        ],
        [
            "returns a dict of active folders with days as keys."
        ],
        [
            "given some data and a list of X posistions, return the normal\n    distribution curve as a Y point at each of those Xs."
        ],
        [
            "show basic info about ABF class variables."
        ],
        [
            "read the ABF header and save it HTML formatted."
        ],
        [
            "use 1 colormap for the whole abf. You can change it!."
        ],
        [
            "return self.dataY around a time point. All units are seconds.\n        if thisSweep==False, the time point is considered to be experiment time\n            and an appropriate sweep may be selected. i.e., with 10 second\n            sweeps and timePint=35, will select the 5s mark of the third sweep"
        ],
        [
            "RETURNS filtered trace. Desn't filter it in place."
        ],
        [
            "Raises a ValidationError for any ActivatableModel that has ForeignKeys or OneToOneFields that will\n    cause cascading deletions to occur. This function also raises a ValidationError if the activatable\n    model has not defined a Boolean field with the field name defined by the ACTIVATABLE_FIELD_NAME variable\n    on the model."
        ],
        [
            "Helper function to convet an Args object to a HoloViews Table"
        ],
        [
            "Method to define the positional arguments and keyword order\n        for pretty printing."
        ],
        [
            "Formats the elements of an argument set appropriately"
        ],
        [
            "Returns a dictionary like object with the lists of values\n        collapsed by their respective key. Useful to find varying vs\n        constant keys and to find how fast keys vary."
        ],
        [
            "A succinct summary of the argument specifier. Unlike the repr,\n        a summary does not have to be complete but must supply the\n        most relevant information about the object to the user."
        ],
        [
            "Returns the specs, the remaining kwargs and whether or not the\n        constructor was called with kwarg or explicit specs."
        ],
        [
            "Convenience method to inspect the available argument values in\n        human-readable format. The ordering of keys is determined by\n        how quickly they vary.\n\n        The exclude list allows specific keys to be excluded for\n        readability (e.g. to hide long, absolute filenames)."
        ],
        [
            "The lexical sort order is specified by a list of string\n        arguments. Each string is a key name prefixed by '+' or '-'\n        for ascending and descending sort respectively. If the key is\n        not found in the operand's set of varying keys, it is ignored."
        ],
        [
            "Simple replacement for numpy linspace"
        ],
        [
            "Parses the log file generated by a launcher and returns\n        dictionary with tid keys and specification values.\n\n        Ordering can be maintained by setting dict_type to the\n        appropriate constructor (i.e. OrderedDict). Keys are converted\n        from unicode to strings for kwarg use."
        ],
        [
            "Writes the supplied specifications to the log path. The data\n        may be supplied as either as a an Args or as a list of\n        dictionaries.\n\n        By default, specifications will be appropriately appended to\n        an existing log file. This can be disabled by setting\n        allow_append to False."
        ],
        [
            "Load all the files in a given directory selecting only files\n        with the given extension if specified. The given kwargs are\n        passed through to the normal constructor."
        ],
        [
            "Return the fields specified in the pattern using Python's\n        formatting mini-language."
        ],
        [
            "Loads the files that match the given pattern."
        ],
        [
            "From the pattern decomposition, finds the absolute paths\n        matching the pattern."
        ],
        [
            "Convenience method to directly chain a pattern processed by\n        FilePattern into a FileInfo instance.\n\n        Note that if a default filetype has been set on FileInfo, the\n        filetype argument may be omitted."
        ],
        [
            "Load the file contents into the supplied Table using the\n        specified key and filetype. The input table should have the\n        filenames as values which will be replaced by the loaded\n        data. If data_key is specified, this key will be used to index\n        the loaded data to retrive the specified item."
        ],
        [
            "Load the file contents into the supplied dataframe using the\n        specified key and filetype."
        ],
        [
            "Generates the union of the source.specs and the metadata\n        dictionary loaded by the filetype object."
        ],
        [
            "Push new data into the buffer. Resume looping if paused."
        ],
        [
            "Create a plot of one area of interest of a single sweep."
        ],
        [
            "Inelegant for now, but lets you manually analyze every ABF in a folder."
        ],
        [
            "Reanalyze data for a single ABF. Also remakes child and parent html."
        ],
        [
            "scan folder1 and folder2 into files1 and files2.\n        since we are on windows, simplify things by making them all lowercase.\n        this WILL cause problems on 'nix operating systems.If this is the case,\n        just run a script to rename every file to all lowercase."
        ],
        [
            "run this to turn all folder1 TIFs and JPGs into folder2 data.\n        TIFs will be treated as micrographs and converted to JPG with enhanced\n        contrast. JPGs will simply be copied over."
        ],
        [
            "analyze every unanalyzed ABF in the folder."
        ],
        [
            "return appropriate HTML determined by file extension."
        ],
        [
            "generate a generic flat file html for an ABF parent. You could give\n        this a single ABF ID, its parent ID, or a list of ABF IDs.\n        If a child ABF is given, the parent will automatically be used."
        ],
        [
            "create ID_plot.html of just intrinsic properties."
        ],
        [
            "This applies a kernel to a signal through convolution and returns the result.\n\n    Some magic is done at the edges so the result doesn't apprach zero:\n        1. extend the signal's edges with len(kernel)/2 duplicated values\n        2. perform the convolution ('same' mode)\n        3. slice-off the ends we added\n        4. return the same number of points as the original"
        ],
        [
            "simple timer. returns a time object, or a string."
        ],
        [
            "if the value is in the list, move it to the front and return it."
        ],
        [
            "if the value is in the list, move it to the back and return it."
        ],
        [
            "given a list and a list of items to be first, return the list in the\n    same order except that it begins with each of the first items."
        ],
        [
            "given a list of goofy ABF names, return it sorted intelligently.\n    This places things like 16o01001 after 16901001."
        ],
        [
            "when given a dictionary where every key contains a list of IDs, replace\n    the keys with the list of files matching those IDs. This is how you get a\n    list of files belonging to each child for each parent."
        ],
        [
            "given a groups dictionary and an ID, return its actual parent ID."
        ],
        [
            "return the semi-temporary user folder"
        ],
        [
            "Coroutine wrapper to catch errors after async scheduling.\n\n    Args:\n        emitter (EventEmitter): The event emitter that is attempting to\n            call a listener.\n        event (str): The event that triggered the emitter.\n        listener (async def): The async def that was used to generate the coro.\n        coro (coroutine): The coroutine that should be tried.\n\n    If an exception is caught the function will use the emitter to emit the\n    failure event. If, however, the current event _is_ the failure event then\n    the method reraises. The reraised exception may show in debug mode for the\n    event loop but is otherwise silently dropped."
        ],
        [
            "Check if the listener limit is hit and warn if needed."
        ],
        [
            "Bind a listener to a particular event.\n\n        Args:\n            event (str): The name of the event to listen for. This may be any\n                string value.\n            listener (def or async def): The callback to execute when the event\n                fires. This may be a sync or async function."
        ],
        [
            "Add a listener that is only called once."
        ],
        [
            "Remove a listener from the emitter.\n\n        Args:\n            event (str): The event name on which the listener is bound.\n            listener: A reference to the same object given to add_listener.\n\n        Returns:\n            bool: True if a listener was removed else False.\n\n        This method only removes one listener at a time. If a listener is\n        attached multiple times then this method must be called repeatedly.\n        Additionally, this method removes listeners first from the those\n        registered with 'on' or 'add_listener'. If none are found it continue\n        to remove afterwards from those added with 'once'."
        ],
        [
            "Schedule a coroutine for execution.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (async def): The async def that needs to be executed.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        The values of *args and **kwargs are passed, unaltered, to the async\n        def when generating the coro. If there is an exception generating the\n        coro, such as the wrong number of arguments, the emitter's error event\n        is triggered. If the triggering event _is_ the emitter's error event\n        then the exception is reraised. The reraised exception may show in\n        debug mode for the event loop but is otherwise silently dropped."
        ],
        [
            "Execute a sync function.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def): The def that needs to be executed.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        The values of *args and **kwargs are passed, unaltered, to the def\n        when exceuting. If there is an exception executing the def, such as the\n        wrong number of arguments, the emitter's error event is triggered. If\n        the triggering event _is_ the emitter's error event then the exception\n        is reraised. The reraised exception may show in debug mode for the\n        event loop but is otherwise silently dropped."
        ],
        [
            "Dispatch an event to a listener.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def or async def): The listener to trigger.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method inspects the listener. If it is a def it dispatches the\n        listener to a method that will execute that def. If it is an async def\n        it dispatches it to a method that will schedule the resulting coro with\n        the event loop."
        ],
        [
            "Call each listener for the event with the given arguments.\n\n        Args:\n            event (str): The event to trigger listeners on.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method passes all arguments other than the event name directly\n        to the listeners. If a listener raises an exception for any reason the\n        'listener-error', or current value of LISTENER_ERROR_EVENT, is emitted.\n        Listeners to this event are given the event name, listener object, and\n        the exception raised. If an error listener fails it does so silently.\n\n        All event listeners are fired in a deferred way so this method returns\n        immediately. The calling coro must yield at some point for the event\n        to propagate to the listeners."
        ],
        [
            "Get the number of listeners for the event.\n\n        Args:\n            event (str): The event for which to count all listeners.\n\n        The resulting count is a combination of listeners added using\n        'on'/'add_listener' and 'once'."
        ],
        [
            "Convert each TIF to PNG. Return filenames of new PNGs."
        ],
        [
            "given an ID and the dict of files, generate a static html for that abf."
        ],
        [
            "expects a folder of ABFs."
        ],
        [
            "simple example how to load an ABF file and plot every sweep."
        ],
        [
            "plot X and Y data, then shade its background by variance."
        ],
        [
            "create some fancy graphs to show color-coded variances."
        ],
        [
            "run this before analysis. Checks if event detection occured.\n        If not, runs AP detection on all sweeps."
        ],
        [
            "runs AP detection on every sweep."
        ],
        [
            "Return package author and version as listed in `init.py`."
        ],
        [
            "Create an API subclass with fewer methods than its base class.\n\n    Arguments:\n      name (:py:class:`str`): The name of the new class.\n      docstring (:py:class:`str`): The docstring for the new class.\n      remove_methods (:py:class:`dict`): The methods to remove from\n        the base class's :py:attr:`API_METHODS` for the subclass. The\n        key is the name of the root method (e.g. ``'auth'`` for\n        ``'auth.test'``, the value is either a tuple of child method\n        names (e.g. ``('test',)``) or, if all children should be\n        removed, the special value :py:const:`ALL`.\n      base (:py:class:`type`, optional): The base class (defaults to\n        :py:class:`SlackApi`).\n\n    Returns:\n      :py:class:`type`: The new subclass.\n\n    Raises:\n      :py:class:`KeyError`: If the method wasn't in the superclass."
        ],
        [
            "Execute a specified Slack Web API method.\n\n        Arguments:\n          method (:py:class:`str`): The name of the method.\n          **params (:py:class:`dict`): Any additional parameters\n            required.\n\n        Returns:\n          :py:class:`dict`: The JSON data from the response.\n\n        Raises:\n          :py:class:`aiohttp.web_exceptions.HTTPException`: If the HTTP\n            request returns a code other than 200 (OK).\n          SlackApiError: If the Slack API is reached but the response\n           contains an error message."
        ],
        [
            "Whether a given method exists in the known API.\n\n        Arguments:\n          method (:py:class:`str`): The name of the method.\n\n        Returns:\n          :py:class:`bool`: Whether the method is in the known API."
        ],
        [
            "Extend XPath evaluation with Parsley extensions' namespace"
        ],
        [
            "Try and convert matching Elements to unicode strings.\n\n        If this fails, the selector evaluation probably already\n        returned some string(s) of some sort, or boolean value,\n        or int/float, so return that instead."
        ],
        [
            "Join the real-time messaging service.\n\n        Arguments:\n          filters (:py:class:`dict`, optional): Dictionary mapping\n            message filters to the functions they should dispatch to.\n            Use a :py:class:`collections.OrderedDict` if precedence is\n            important; only one filter, the first match, will be\n            applied to each message."
        ],
        [
            "Handle an incoming message appropriately.\n\n        Arguments:\n          message (:py:class:`aiohttp.websocket.Message`): The incoming\n            message to handle.\n          filters (:py:class:`list`): The filters to apply to incoming\n            messages."
        ],
        [
            "If you send a message directly to me"
        ],
        [
            "Create a new instance from the API token.\n\n        Arguments:\n          token (:py:class:`str`, optional): The bot's API token\n            (defaults to ``None``, which means looking in the\n            environment).\n          api_cls (:py:class:`type`, optional): The class to create\n            as the ``api`` argument for API access (defaults to\n            :py:class:`aslack.slack_api.SlackBotApi`).\n\n        Returns:\n          :py:class:`SlackBot`: The new instance."
        ],
        [
            "Format an outgoing message for transmission.\n\n        Note:\n          Adds the message type (``'message'``) and incremental ID.\n\n        Arguments:\n          channel (:py:class:`str`): The channel to send to.\n          text (:py:class:`str`): The message text to send.\n\n        Returns:\n          :py:class:`str`: The JSON string of the message."
        ],
        [
            "Get the WebSocket URL for the RTM session.\n\n        Warning:\n          The URL expires if the session is not joined within 30\n          seconds of the API call to the start endpoint.\n\n        Returns:\n          :py:class:`str`: The socket URL."
        ],
        [
            "Generates the instructions for a bot and its filters.\n\n        Note:\n          The guidance for each filter is generated by combining the\n          docstrings of the predicate filter and resulting dispatch\n          function with a single space between. The class's\n          :py:attr:`INSTRUCTIONS` and the default help command are\n          added.\n\n        Arguments:\n          filters (:py:class:`list`): The filters to apply to incoming\n            messages.\n\n        Returns:\n          :py:class:`str`: The bot's instructions."
        ],
        [
            "Respond to a message on the current socket.\n\n        Args:\n          channel (:py:class:`str`): The channel to send to.\n          text (:py:class:`str`): The message text to send."
        ],
        [
            "Check the first message matches the expected handshake.\n\n        Note:\n          The handshake is provided as :py:attr:`RTM_HANDSHAKE`.\n\n        Arguments:\n          msg (:py:class:`aiohttp.Message`): The message to validate.\n\n        Raises:\n          :py:class:`SlackApiError`: If the data doesn't match the\n            expected handshake."
        ],
        [
            "Returns list of paths to tested apps"
        ],
        [
            "Get the imported task classes for each task that will be run"
        ],
        [
            "Get the options for each task that will be run"
        ],
        [
            "Write the data from the db to a CLDF dataset according to the metadata in `self.dataset`.\n\n        :param dest:\n        :param mdname:\n        :return: path of the metadata file"
        ],
        [
            "A user-friendly description of the handler.\n\n        Returns:\n          :py:class:`str`: The handler's description."
        ],
        [
            "Create a Parselet instance from a file containing\n        the Parsley script as a JSON object\n\n        >>> import parslepy\n        >>> with open('parselet.json') as fp:\n        ...     parslepy.Parselet.from_jsonfile(fp)\n        ...\n        <parslepy.base.Parselet object at 0x2014e50>\n\n        :param file fp: an open file-like pointer containing the Parsley script\n        :rtype: :class:`.Parselet`\n\n        Other arguments: same as for :class:`.Parselet` contructor"
        ],
        [
            "Create a Parselet instance from a file containing\n        the Parsley script as a YAML object\n\n        >>> import parslepy\n        >>> with open('parselet.yml') as fp:\n        ...     parslepy.Parselet.from_yamlfile(fp)\n        ...\n        <parslepy.base.Parselet object at 0x2014e50>\n\n        :param file fp: an open file-like pointer containing the Parsley script\n        :rtype: :class:`.Parselet`\n\n        Other arguments: same as for :class:`.Parselet` contructor"
        ],
        [
            "Interpret input lines as a JSON Parsley script.\n        Python-style comment lines are skipped."
        ],
        [
            "Build part of the abstract Parsley extraction tree\n\n        Arguments:\n        parselet_node (dict) -- part of the Parsley tree to compile\n                                (can be the root dict/node)\n        level (int)          -- current recursion depth (used for debug)"
        ],
        [
            "Use CLDF reference properties to implicitely create foreign key constraints.\n\n        :param component: A Table object or `None`."
        ],
        [
            "Create a URL for the specified endpoint.\n\n        Arguments:\n          endpoint (:py:class:`str`): The API endpoint to access.\n          root: (:py:class:`str`, optional): The root URL for the\n            service API.\n          params: (:py:class:`dict`, optional): The values for format\n            into the created URL (defaults to ``None``).\n          url_params: (:py:class:`dict`, optional): Parameters to add\n            to the end of the URL (defaults to ``None``).\n\n        Returns:\n          :py:class:`str`: The resulting URL."
        ],
        [
            "Raise an appropriate error for a given response.\n\n    Arguments:\n      response (:py:class:`aiohttp.ClientResponse`): The API response.\n\n    Raises:\n      :py:class:`aiohttp.web_exceptions.HTTPException`: The appropriate\n        error for the response's status."
        ],
        [
            "Truncate the supplied text for display.\n\n    Arguments:\n      text (:py:class:`str`): The text to truncate.\n      max_len (:py:class:`int`, optional): The maximum length of the\n        text before truncation (defaults to 350 characters).\n      end (:py:class:`str`, optional): The ending to use to show that\n        the text was truncated (defaults to ``'...'``).\n\n    Returns:\n      :py:class:`str`: The truncated text."
        ],
        [
            "Add a source, either specified by glottolog reference id, or as bibtex record."
        ],
        [
            "Returns a cache key consisten of a username and image size."
        ],
        [
            "Decorator to cache the result of functions that take a ``user`` and a\n    ``size`` value."
        ],
        [
            "Function to be called when saving or changing an user's avatars."
        ],
        [
            "Returns a field object instance for a given PrefProxy object.\n\n    :param PrefProxy pref_proxy:\n\n    :rtype: models.Field"
        ],
        [
            "Updates field object with data from a PrefProxy object.\n\n    :param models.Field field_obj:\n\n    :param PrefProxy pref_proxy:"
        ],
        [
            "Returns preferences model class dynamically crated for a given app or None on conflict."
        ],
        [
            "Returns locals dictionary from a given frame.\n\n    :param int stepback:\n\n    :rtype: dict"
        ],
        [
            "Generator to walk through variables considered as preferences\n    in locals dict of a given frame.\n\n    :param int stepback:\n\n    :rtype: tuple"
        ],
        [
            "Prints file details in the current directory"
        ],
        [
            "Attempt to bind the args to the type signature. First try to just bind\n        to the signature, then ensure that all arguments match the parameter\n        types."
        ],
        [
            "For every parameter, create a matcher if the parameter has an\n        annotation."
        ],
        [
            "Makes a wrapper function that executes a dispatch call for func. The\n        wrapper has the dispatch and dispatch_first attributes, so that\n        additional overloads can be added to the group."
        ],
        [
            "Adds the decorated function to this dispatch."
        ],
        [
            "Adds the decorated function to this dispatch, at the FRONT of the order.\n        Useful for allowing third parties to add overloaded functionality\n        to be executed before default functionality."
        ],
        [
            "Dispatch a call. Call the first function whose type signature matches\n        the arguemts."
        ],
        [
            "reprojette en WGS84 et recupere l'extend"
        ],
        [
            "Convert GRIB to Tif"
        ],
        [
            "Triggered on dynamic preferences model save.\n     Issues DB save and reread."
        ],
        [
            "Binds PrefProxy objects to module variables used by apps as preferences.\n\n    :param list|tuple values: Preference values.\n\n    :param str|unicode category: Category name the preference belongs to.\n\n    :param Field field: Django model field to represent this preference.\n\n    :param str|unicode verbose_name: Field verbose name.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    :rtype: list"
        ],
        [
            "Registers dynamically created preferences models for Admin interface.\n\n    :param admin.AdminSite admin_site: AdminSite object."
        ],
        [
            "Automatically discovers and registers all preferences available in all apps.\n\n    :param admin.AdminSite admin_site: Custom AdminSite object."
        ],
        [
            "Restores the original values of module variables\n    considered preferences if they are still PatchedLocal\n    and not PrefProxy."
        ],
        [
            "Replaces a settings module with a Module proxy to intercept\n    an access to settings.\n\n    :param int depth: Frame count to go backward."
        ],
        [
            "Registers preferences that should be handled by siteprefs.\n\n    Expects preferences as *args.\n\n    Use keyword arguments to batch apply params supported by\n    ``PrefProxy`` to all preferences not constructed by ``pref`` and ``pref_group``.\n\n    Batch kwargs:\n\n        :param str|unicode help_text: Field help text.\n\n        :param bool static: Leave this preference static (do not store in DB).\n\n        :param bool readonly: Make this field read only.\n\n    :param bool swap_settings_module: Whether to automatically replace settings module\n        with a special ``ProxyModule`` object to access dynamic values of settings\n        transparently (so not to bother with calling ``.value`` of ``PrefProxy`` object)."
        ],
        [
            "Marks preferences group.\n\n    :param str|unicode title: Group title\n\n    :param list|tuple prefs: Preferences to group.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only."
        ],
        [
            "Marks a preference.\n\n    :param preference: Preference variable.\n\n    :param Field field: Django model field to represent this preference.\n\n    :param str|unicode verbose_name: Field verbose name.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    :rtype: PrefProxy|None"
        ],
        [
            "Generate the ``versionwarning-data.json`` file.\n\n    This file is included in the output and read by the AJAX request when\n    accessing to the documentation and used to compare the live versions with\n    the curent one.\n\n    Besides, this file contains meta data about the project, the API to use and\n    the banner itself."
        ],
        [
            "Gives objective functions a number of dimensions and parameter range\n\n    Parameters\n    ----------\n    param_scales : (int, int)\n        Scale (std. dev.) for choosing each parameter\n\n    xstar : array_like\n        Optimal parameters"
        ],
        [
            "Pointwise minimum of two quadratic bowls"
        ],
        [
            "Objective and gradient for the rosenbrock function"
        ],
        [
            "Beale's function"
        ],
        [
            "Booth's function"
        ],
        [
            "Three-hump camel function"
        ],
        [
            "One of the Bohachevsky functions"
        ],
        [
            "Dixon-Price function"
        ],
        [
            "Styblinski-Tang function"
        ],
        [
            "Return a list of buckets in MimicDB.\n\n        :param boolean force: If true, API call is forced to S3"
        ],
        [
            "Return a bucket from MimicDB if it exists. Return a\n        S3ResponseError if the bucket does not exist and validate is passed.\n\n        :param boolean force: If true, API call is forced to S3"
        ],
        [
            "Add the bucket to MimicDB after successful creation."
        ],
        [
            "Sync either a list of buckets or the entire connection.\n\n        Force all API calls to S3 and populate the database with the current\n        state of S3.\n\n        :param \\*string \\*buckets: Buckets to sync"
        ],
        [
            "Return the key from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3"
        ],
        [
            "Return None if key is not in the bucket set.\n\n        Pass 'force' in the headers to check S3 for the key, and after fetching\n        the key from S3, save the metadata and key to the bucket set."
        ],
        [
            "Return a list of keys from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3"
        ],
        [
            "Remove each key or key name in an iterable from the bucket set."
        ],
        [
            "Remove key name from bucket set."
        ],
        [
            "Return an iterable of keys from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3"
        ],
        [
            "Sync a bucket.\n\n        Force all API calls to S3 and populate the database with the current state of S3."
        ],
        [
            "Minimize the proximal operator of a given objective using L-BFGS\n\n    Parameters\n    ----------\n    f_df : function\n        Returns the objective and gradient of the function to minimize\n\n    maxiter : int\n        Maximum number of L-BFGS iterations"
        ],
        [
            "Applies a smoothing operator along one dimension\n\n    currently only accepts a matrix as input\n\n    Parameters\n    ----------\n    penalty : float\n\n    axis : int, optional\n        Axis along which to apply the smoothing (Default: 0)\n\n    newshape : tuple, optional\n        Desired shape of the parameters to apply the nuclear norm to. The given\n        parameters are reshaped to an array with this shape, or not reshaped if\n        the value of newshape is None. (Default: None)"
        ],
        [
            "Projection onto the semidefinite cone"
        ],
        [
            "Projection onto the probability simplex\n\n    http://arxiv.org/pdf/1309.1541v1.pdf"
        ],
        [
            "Applies a proximal operator to the columns of a matrix"
        ],
        [
            "Turns a coroutine into a gradient based optimizer."
        ],
        [
            "Adds a proximal operator to the list of operators"
        ],
        [
            "Set key attributes to retrived metadata. Might be extended in the\n        future to support more attributes."
        ],
        [
            "Called internally for any type of upload. After upload finishes,\n        make sure the key is in the bucket set and save the metadata."
        ],
        [
            "Memoizes an objective + gradient function, and splits it into\n    two functions that return just the objective and gradient, respectively.\n\n    Parameters\n    ----------\n    f_df : function\n        Must be unary (takes a single argument)\n\n    xref : list, dict, or array_like\n        The form of the parameters\n\n    size : int, optional\n        Size of the cache (Default=1)"
        ],
        [
            "Decorates a function with the given docstring\n\n    Parameters\n    ----------\n    docstr : string"
        ],
        [
            "Compares the numerical gradient to the analytic gradient\n\n    Parameters\n    ----------\n    f_df : function\n        The analytic objective and gradient function to check\n\n    x0 : array_like\n        Parameter values to check the gradient at\n\n    stepsize : float, optional\n        Stepsize for the numerical gradient. Too big and this will poorly estimate the gradient.\n        Too small and you will run into precision issues (default: 1e-6)\n\n    tol : float, optional\n        Tolerance to use when coloring correct/incorrect gradients (default: 1e-5)\n\n    width : int, optional\n        Width of the table columns (default: 15)\n\n    style : string, optional\n        Style of the printed table, see tableprint for a list of styles (default: 'round')"
        ],
        [
            "Evaluate the files identified for checksum."
        ],
        [
            "Check the integrity of the datapackage.json"
        ],
        [
            "Guess the filetype and read the file into row sets"
        ],
        [
            "Guess schema using messytables"
        ],
        [
            "Calculates a checksum for a Finnish national reference number"
        ],
        [
            "Helper to make sure the given character is valid for a reference number"
        ],
        [
            "Creates the huge number from ISO alphanumeric ISO reference"
        ],
        [
            "Validates ISO reference number"
        ],
        [
            "Calculates virtual barcode for IBAN account number and ISO reference\n\n    Arguments:\n        iban {string} -- IBAN formed account number\n        reference {string} -- ISO 11649 creditor reference\n        amount {decimal.Decimal} -- Amount in euros, 0.01 - 999999.99\n        due {datetime.date} -- due date"
        ],
        [
            "Add a normal file including its source"
        ],
        [
            "Run the executable and capture the input and output..."
        ],
        [
            "Add files to the repository by explicitly specifying them or by\n    specifying a pattern over files accessed during execution of an\n    executable.\n\n    Parameters\n    ----------\n\n    repo: Repository\n\n    args: files or command line\n         (a) If simply adding files, then the list of files that must\n         be added (including any additional arguments to be passed to\n         git\n         (b) If files to be added are an output of a command line, then\n         args is the command lined\n    targetdir: Target directory to store the files\n    execute: Args are not files to be added but scripts that must be run.\n    includes: patterns used to select files to\n    script: Is this a script?\n    generator: Is this a generator\n    source: Link to the original source of the data"
        ],
        [
            "For various actions we need files that match patterns"
        ],
        [
            "Run a specific command using the manager"
        ],
        [
            "Get metadata for a given file"
        ],
        [
            "Lookup all available repos"
        ],
        [
            "Working directory for the repo"
        ],
        [
            "Add repo to the internal lookup table..."
        ],
        [
            "Lookup a repo based on username reponame"
        ],
        [
            "Run a shell command within the repo's context\n\n    Parameters\n    ----------\n\n    repo: Repository object\n    args: Shell command"
        ],
        [
            "Check if the datapackage exists..."
        ],
        [
            "Create the datapackage file.."
        ],
        [
            "Initialize an empty repository with datapackage.json\n\n    Parameters\n    ----------\n\n    username: Name of the user\n    reponame: Name of the repo\n    setup: Specify the 'configuration' (git only, git+s3 backend etc)\n    force: Force creation of the files\n    options: Dictionary with content of dgit.json, if available.\n    noinput: Automatic operation with no human interaction"
        ],
        [
            "Update metadata with the content of the files"
        ],
        [
            "Update metadata with the commit information"
        ],
        [
            "Update metadata with the action history"
        ],
        [
            "Update metadata host information"
        ],
        [
            "Collect information from the dependent repo's"
        ],
        [
            "Post to metadata server\n\n    Parameters\n    ----------\n\n    repo: Repository object (result of lookup)"
        ],
        [
            "Show details of available plugins\n\n    Parameters\n    ----------\n    what: Class of plugins e.g., backend\n    name: Name of the plugin e.g., s3\n    version: Version of the plugin\n    details: Show details be shown?"
        ],
        [
            "Load all plugins from dgit extension"
        ],
        [
            "Registering a plugin\n\n        Params\n        ------\n        what: Nature of the plugin (backend, instrumentation, repo)\n        obj: Instance of the plugin"
        ],
        [
            "Search for a plugin"
        ],
        [
            "Instantiate the validation specification"
        ],
        [
            "Validate the content of the files for consistency. Validators can\n    look as deeply as needed into the files. dgit treats them all as\n    black boxes.\n\n    Parameters\n    ----------\n\n    repo: Repository object\n    validator_name: Name of validator, if any. If none, then all validators specified in dgit.json will be included.\n    filename: Pattern that specifies files that must be processed by the validators selected. If none, then the default specification in dgit.json is used.\n    rules: Pattern specifying the files that have rules that validators will use\n    show: Print the validation results on the terminal\n\n    Returns\n    -------\n\n    status: A list of dictionaries, each with target file processed, rules file applied, status of the validation and any error  message."
        ],
        [
            "Check if a URL exists"
        ],
        [
            "Post to the metadata server\n\n        Parameters\n        ----------\n\n        repo"
        ],
        [
            "imports and returns module class from ``path.to.module.Class``\n    argument"
        ],
        [
            "Find max 5 executables that are responsible for this repo."
        ],
        [
            "Automatically get repo\n\n    Parameters\n    ----------\n\n    autooptions: dgit.json content"
        ],
        [
            "Look through the local directory to pick up files to check"
        ],
        [
            "Cleanup the paths and add"
        ],
        [
            "This will try to pull in a stream from an external source. Once a\n        stream has been successfully pulled it is assigned a 'local stream\n        name' which can be used to access the stream from the EMS.\n\n        :param uri: The URI of the external stream. Can be RTMP, RTSP or\n            unicast/multicast (d) mpegts\n        :type uri: str\n\n        :param keepAlive: If keepAlive is set to 1, the server will attempt to\n            reestablish connection with a stream source after a connection has\n            been lost. The reconnect will be attempted once every second\n            (default: 1 true)\n        :type keepAlive: int\n\n        :param localStreamName: If provided, the stream will be given this\n            name. Otherwise, a fallback techniques used to determine the stream\n            name (based on the URI)\n        :type localStreamName: str\n\n        :param forceTcp: If 1 and if the stream is RTSP, a TCP connection will\n            be forced. Otherwise the transport mechanism will be negotiated\n            (UDP or TCP) (default: 1 true)\n        :type forceTcp: int\n\n        :param tcUrl: When specified, this value will be used to set the TC URL\n            in the initial RTMP connect invoke\n        :type tcUrl: str\n\n        :param pageUrl: When specified, this value will be used to set the\n            originating web page address in the initial RTMP connect invoke\n        :type pageUrl: str\n\n        :param swfUrl: When specified, this value will be used to set the\n            originating swf URL in the initial RTMP connect invoke\n        :type swfUrl: str\n\n        :param rangeStart: For RTSP and RTMP connections. A value from which\n            the playback should start expressed in seconds. There are 2 special\n            values: -2 and -1. For more information, please read about\n            start/len parameters here:\n            http://livedocs.adobe.com/flashmediaserver/3.0/hpdocs/help.html?content=00000185.html\n        :type rangeStart: int\n\n        :param rangeEnd: The length in seconds for the playback. -1 is a\n            special value. For more information, please read about start/len\n            parameters here:\n            http://livedocs.adobe.com/flashmediaserver/3.0/hpdocs/help.html?content=00000185.html\n        :type rangeEnd: int\n\n        :param ttl: Sets the IP_TTL (time to live) option on the socket\n        :type ttl: int\n\n        :param tos: Sets the IP_TOS (Type of Service) option on the socket\n        :type tos: int\n\n        :param rtcpDetectionInterval: How much time (in seconds) should the\n            server wait for RTCP packets before declaring the RTSP stream as a\n            RTCP-less stream\n        :type rtcpDetectionInterval: int\n\n        :param emulateUserAgent: When specified, this value will be used as the\n            user agent string. It is meaningful only for RTMP\n        :type emulateUserAgent: str\n\n        :param isAudio: If 1 and if the stream is RTP, it indicates that the\n            currently pulled stream is an audio source. Otherwise the pulled\n            source is assumed as a video source\n        :type isAudio: int\n\n        :param audioCodecBytes: The audio codec setup of this RTP stream if it\n            is audio. Represented as hex format without '0x' or 'h'. For\n            example: audioCodecBytes=1190\n        :type audioCodecBytes: str\n\n        :param spsBytes: The video SPS bytes of this RTP stream if it is video.\n            It should be base 64 encoded.\n        :type spsBytes: str\n\n        :param ppsBytes: The video PPS bytes of this RTP stream if it is video.\n            It should be base 64 encoded\n        :type ppsBytes: str\n\n        :param ssmIp: The source IP from source-specific-multicast. Only usable\n            when doing UDP based pull\n        :type ssmIp: str\n\n        :param httpProxy: This parameter has two valid values: IP:Port - This\n            value combination specifies an RTSP HTTP Proxy from which the RTSP\n            stream should be pulled from Self - Specifying \"self\" as the value\n            implies pulling RTSP over HTTP\n        :type httpProxy: str\n\n        :link: http://docs.evostream.com/ems_api_definition/pullstream"
        ],
        [
            "Records any inbound stream. The record command allows users to record\n        a stream that may not yet exist. When a new stream is brought into\n        the server, it is checked against a list of streams to be recorded.\n\n        Streams can be recorded as FLV files, MPEG-TS files or as MP4 files.\n\n        :param localStreamName: The name of the stream to be used as input\n            for recording.\n        :type localStreamName: str\n\n        :param pathToFile: Specify path and file name to write to.\n        :type pathToFile: str\n\n        :param type: `ts`, `mp4` or `flv`\n        :type type: str\n\n        :param overwrite: If false, when a file already exists for the stream\n            name, a new file will be created with the next appropriate number\n            appended. If 1 (true), files with the same name will be\n            overwritten.\n        :type overwrite: int\n\n        :param keepAlive: If 1 (true), the server will restart recording every\n            time the stream becomes available again.\n        :type keepAlive: int\n\n        :param chunkLength: If non-zero the record command will start a new\n            recording file after ChunkLength seconds have elapsed.\n        :type chunkLength: int\n\n        :param waitForIDR: This is used if the recording is being chunked.\n            When true, new files will only be created on IDR boundaries.\n        :type waitForIDR: int\n\n        :param winQtCompat: Mandates 32bit header fields to ensure\n            compatibility with Windows QuickTime.\n        :type winQtCompat: int\n\n        :param dateFolderStructure: If set to 1 (true), folders will be\n            created with names in `YYYYMMDD` format. Recorded files will be\n            placed inside these folders based on the date they were created.\n        :type dateFolderStructure: int\n\n        :link: http://docs.evostream.com/ems_api_definition/record"
        ],
        [
            "Creates an RTMP ingest point, which mandates that streams pushed into\n        the EMS have a target stream name which matches one Ingest Point\n        privateStreamName.\n\n        :param privateStreamName: The name that RTMP Target Stream Names must\n            match.\n        :type privateStreamName: str\n\n        :param publicStreamName: The name that is used to access the stream\n            pushed to the privateStreamName. The publicStreamName becomes the\n            streams localStreamName.\n        :type publicStreamName: str\n\n        :link: http://docs.evostream.com/ems_api_definition/createingestpoint"
        ],
        [
            "Instantiate the generator and filename specification"
        ],
        [
            "Helper function to run commands\n\n        Parameters\n        ----------\n        cmd : list\n              Arguments to git command"
        ],
        [
            "Run a generic command within the repo. Assumes that you are\n        in the repo's root directory"
        ],
        [
            "Initialize a Git repo\n\n        Parameters\n        ----------\n\n        username, reponame : Repo name is tuple (name, reponame)\n        force: force initialization of the repo even if exists\n        backend: backend that must be used for this (e.g. s3)"
        ],
        [
            "Delete files from the repo"
        ],
        [
            "Cleanup the repo"
        ],
        [
            "Get the permalink to command that generated the dataset"
        ],
        [
            "Add files to the repo"
        ],
        [
            "Marks the invoice as sent in Holvi\n\n        If send_email is False then the invoice is *not* automatically emailed to the recipient\n        and your must take care of sending the invoice yourself."
        ],
        [
            "Convert our Python object to JSON acceptable to Holvi API"
        ],
        [
            "API wrapper documentation"
        ],
        [
            "Saves this order to Holvi, returns a tuple with the order itself and checkout_uri"
        ],
        [
            "Return source code based on tokens.\n\n    This is like tokenize.untokenize(), but it preserves spacing between\n    tokens. So if the original soure code had multiple spaces between\n    some tokens or if escaped newlines were used, those things will be\n    reflected by untokenize()."
        ],
        [
            "Load profile INI"
        ],
        [
            "Update the profile"
        ],
        [
            "Insert hook into the repo"
        ],
        [
            "Try the library. If it doesnt work, use the command line.."
        ],
        [
            "Run a shell command"
        ],
        [
            "Get the commit history for a given dataset"
        ],
        [
            "Look at files and compute the diffs intelligently"
        ],
        [
            "Execute command and wait for it to finish. Proceed with caution because\n        if you run a command that causes a prompt this will hang"
        ],
        [
            "Enter sudo mode"
        ],
        [
            "Install specified packages using apt-get. -y options are\n        automatically used. Waits for command to finish.\n\n        Parameters\n        ----------\n        package_names: list-like of str\n        raise_on_error: bool, default False\n            If True then raise ValueError if stderr is not empty\n            debconf often gives tty error"
        ],
        [
            "Install specified python packages using pip. -U option added\n        Waits for command to finish.\n\n        Parameters\n        ----------\n        package_names: list-like of str\n        raise_on_error: bool, default True\n            If True then raise ValueError if stderr is not empty"
        ],
        [
            "Install all requirements contained in the given file path\n        Waits for command to finish.\n\n        Parameters\n        ----------\n        requirements: str\n            Path to requirements.txt\n        raise_on_error: bool, default True\n            If True then raise ValueError if stderr is not empty"
        ],
        [
            "Create fiji-macros for stitching all channels and z-stacks for a well.\n\n    Parameters\n    ----------\n    path : string\n        Well path.\n    output_folder : string\n        Folder to store images. If not given well path is used.\n\n    Returns\n    -------\n    output_files, macros : tuple\n        Tuple with filenames and macros for stitched well."
        ],
        [
            "Lossless compression. Save images as PNG and TIFF tags to json. Can be\n    reversed with `decompress`. Will run in multiprocessing, where\n    number of workers is decided by ``leicaexperiment.experiment._pools``.\n\n    Parameters\n    ----------\n    images : list of filenames\n        Images to lossless compress.\n    delete_tif : bool\n        Wheter to delete original images.\n    folder : string\n        Where to store images. Basename will be kept.\n\n    Returns\n    -------\n    list of filenames\n        List of compressed files."
        ],
        [
            "Lossless compression. Save image as PNG and TIFF tags to json. Process\n    can be reversed with `decompress`.\n\n    Parameters\n    ----------\n    image : string\n        TIF-image which should be compressed lossless.\n    delete_tif : bool\n        Wheter to delete original images.\n    force : bool\n        Wheter to compress even if .png already exists.\n\n    Returns\n    -------\n    string\n        Filename of compressed image, or empty string if compress failed."
        ],
        [
            "Set self.path, self.dirname and self.basename."
        ],
        [
            "List of paths to images."
        ],
        [
            "Get path of specified image.\n\n        Parameters\n        ----------\n        well_row : int\n            Starts at 0. Same as --U in files.\n        well_column : int\n            Starts at 0. Same as --V in files.\n        field_row : int\n            Starts at 0. Same as --Y in files.\n        field_column : int\n            Starts at 0. Same as --X in files.\n\n        Returns\n        -------\n        string\n            Path to image or empty string if image is not found."
        ],
        [
            "Get list of paths to images in specified well.\n\n\n        Parameters\n        ----------\n        well_row : int\n            Starts at 0. Same as --V in files.\n        well_column : int\n            Starts at 0. Save as --U in files.\n\n        Returns\n        -------\n        list of strings\n            Paths to images or empty list if no images are found."
        ],
        [
            "Stitches all wells in experiment with ImageJ. Stitched images are\n        saved in experiment root.\n\n        Images which already exists are omitted stitching.\n\n        Parameters\n        ----------\n        folder : string\n            Where to store stitched images. Defaults to experiment path.\n\n        Returns\n        -------\n        list\n            Filenames of stitched images. Files which already exists before\n            stitching are also returned."
        ],
        [
            "Lossless compress all images in experiment to PNG. If folder is\n        omitted, images will not be moved.\n\n        Images which already exists in PNG are omitted.\n\n        Parameters\n        ----------\n        folder : string\n            Where to store PNGs. Defaults to the folder they are in.\n        delete_tif : bool\n            If set to truthy value, ome.tifs will be deleted after compression.\n\n        Returns\n        -------\n        list\n            Filenames of PNG images. Files which already exists before\n            compression are also returned."
        ],
        [
            "Get OME-XML metadata of given field.\n\n        Parameters\n        ----------\n        well_row : int\n            Y well coordinate. Same as --V in files.\n        well_column : int\n            X well coordinate. Same as --U in files.\n        field_row : int\n            Y field coordinate. Same as --Y in files.\n        field_column : int\n            X field coordinate. Same as --X in files.\n\n        Returns\n        -------\n        lxml.objectify.ObjectifiedElement\n            lxml object of OME-XML found in slide/chamber/field/metadata."
        ],
        [
            "Get a list of stitch coordinates for the given well.\n\n        Parameters\n        ----------\n        well_row : int\n            Y well coordinate. Same as --V in files.\n        well_column : int\n            X well coordinate. Same as --U in files.\n\n        Returns\n        -------\n        (xs, ys, attr) : tuples with float and collections.OrderedDict\n            Tuple of x's, y's and attributes."
        ],
        [
            "Create a new droplet\n\n        Parameters\n        ----------\n        name: str\n            Name of new droplet\n        region: str\n            slug for region (e.g., sfo1, nyc1)\n        size: str\n            slug for droplet size (e.g., 512mb, 1024mb)\n        image: int or str\n            image id (e.g., 12352) or slug (e.g., 'ubuntu-14-04-x64')\n        ssh_keys: list, optional\n            default SSH keys to be added on creation\n            this is highly recommended for ssh access\n        backups: bool, optional\n            whether automated backups should be enabled for the Droplet.\n            Automated backups can only be enabled when the Droplet is created.\n        ipv6: bool, optional\n            whether IPv6 is enabled on the Droplet\n        private_networking: bool, optional\n            whether private networking is enabled for the Droplet. Private\n            networking is currently only available in certain regions\n        wait: bool, default True\n            if True then block until creation is complete"
        ],
        [
            "Retrieve a droplet by id\n\n        Parameters\n        ----------\n        id: int\n            droplet id\n\n        Returns\n        -------\n        droplet: DropletActions"
        ],
        [
            "Restore this droplet with given image id\n\n        A Droplet restoration will rebuild an image using a backup image.\n        The image ID that is passed in must be a backup of the current Droplet\n        instance. The operation will leave any embedded SSH keys intact.\n\n        Parameters\n        ----------\n        image: int or str\n            int for image id and str for image slug\n        wait: bool, default True\n            Whether to block until the pending action is completed"
        ],
        [
            "Rebuild this droplet with given image id\n\n        Parameters\n        ----------\n        image: int or str\n            int for image id and str for image slug\n        wait: bool, default True\n            Whether to block until the pending action is completed"
        ],
        [
            "Change the name of this droplet\n\n        Parameters\n        ----------\n        name: str\n            New name for the droplet\n        wait: bool, default True\n            Whether to block until the pending action is completed\n\n        Raises\n        ------\n        APIError if region does not support private networking"
        ],
        [
            "Change the kernel of this droplet\n\n        Parameters\n        ----------\n        kernel_id: int\n            Can be retrieved from output of self.kernels()\n        wait: bool, default True\n            Whether to block until the pending action is completed\n\n        Raises\n        ------\n        APIError if region does not support private networking"
        ],
        [
            "Delete this droplet\n\n        Parameters\n        ----------\n        wait: bool, default True\n            Whether to block until the pending action is completed"
        ],
        [
            "wait for all actions to complete on a droplet"
        ],
        [
            "Open SSH connection to droplet\n\n        Parameters\n        ----------\n        interactive: bool, default False\n            If True then SSH client will prompt for password when necessary\n            and also print output to console"
        ],
        [
            "Send a request to the REST API\n\n        Parameters\n        ----------\n        kind: str, {get, delete, put, post, head}\n        resource: str\n        url_components: list or tuple to be appended to the request URL\n\n        Notes\n        -----\n        kwargs contain request parameters to be sent as request data"
        ],
        [
            "Properly formats array types"
        ],
        [
            "create request url for resource"
        ],
        [
            "Send a request for this resource to the API\n\n        Parameters\n        ----------\n        kind: str, {'get', 'delete', 'put', 'post', 'head'}"
        ],
        [
            "Send list request for all members of a collection"
        ],
        [
            "Get single unit of collection"
        ],
        [
            "Transfer this image to given region\n\n        Parameters\n        ----------\n        region: str\n            region slug to transfer to (e.g., sfo1, nyc1)"
        ],
        [
            "id or slug"
        ],
        [
            "id or fingerprint"
        ],
        [
            "Creates a new domain\n\n        Parameters\n        ----------\n        name: str\n            new domain name\n        ip_address: str\n            IP address for the new domain"
        ],
        [
            "Get a list of all domain records for the given domain name\n\n        Parameters\n        ----------\n        name: str\n            domain name"
        ],
        [
            "Change the name of this domain record\n\n        Parameters\n        ----------\n        id: int\n            domain record id\n        name: str\n            new name of record"
        ],
        [
            "Retrieve a single domain record given the id"
        ],
        [
            "Logs the user on to FogBugz.\n\n        Returns None for a successful login."
        ],
        [
            "Chop list_ into n chunks. Returns a list."
        ],
        [
            "return first droplet"
        ],
        [
            "Take a snapshot of a droplet\n\n    Parameters\n    ----------\n    name: str\n        name for snapshot"
        ],
        [
            "Retrieves the allowed operations for this request."
        ],
        [
            "Assets if the requested operations are allowed in this context."
        ],
        [
            "Fills the response object from the passed data."
        ],
        [
            "Processes a `GET` request."
        ],
        [
            "Processes a `POST` request."
        ],
        [
            "Processes a `PUT` request."
        ],
        [
            "Processes a `DELETE` request."
        ],
        [
            "Processes a `LINK` request.\n\n        A `LINK` request is asking to create a relation from the currently\n        represented URI to all of the `Link` request headers."
        ],
        [
            "Creates a base Django project"
        ],
        [
            "Helper function that performs an `ilike` query if a string value\n    is passed, otherwise the normal default operation."
        ],
        [
            "Parse the querystring into a normalized form."
        ],
        [
            "Return objects representing segments."
        ],
        [
            "we expect foo=bar"
        ],
        [
            "Set the value of this attribute for the passed object."
        ],
        [
            "Consumes set specifiers as text and forms a generator to retrieve\n    the requested ranges.\n\n    @param[in] specifiers\n        Expected syntax is from the byte-range-specifier ABNF found in the\n        [RFC 2616]; eg. 15-17,151,-16,26-278,15\n\n    @returns\n        Consecutive tuples that describe the requested range; eg. (1, 72) or\n        (1, 1) [read as 1 to 72 or 1 to 1]."
        ],
        [
            "Paginate an iterable during a request.\n\n    Magically splicling an iterable in our supported ORMs allows LIMIT and\n    OFFSET queries. We should probably delegate this to the ORM or something\n    in the future."
        ],
        [
            "Decorate test methods with this if you don't require strict index checking"
        ],
        [
            "Read and return the request data.\n\n        @param[in] deserialize\n            True to deserialize the resultant text using a determiend format\n            or the passed format.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer."
        ],
        [
            "Updates the active resource configuration to the passed\n    keyword arguments.\n\n    Invoking this method without passing arguments will just return the\n    active resource configuration.\n\n    @returns\n        The previous configuration."
        ],
        [
            "This decorator wraps descriptor methods with a new method that tries\n    to delegate to a function of the same name defined on the owner instance\n    for convenience for dispatcher clients."
        ],
        [
            "Given a single decorated handler function,\n        prepare, append desired data to self.registry."
        ],
        [
            "Find the first method this input dispatches to."
        ],
        [
            "Given a node, return the string to use in computing the\n        matching visitor methodname. Can also be a generator of strings."
        ],
        [
            "Find all method names this input dispatches to."
        ],
        [
            "Parse string to create an instance\n\n          :param str s: String with requirement to parse\n          :param bool required: Is this requirement required to be fulfilled? If not, then it is a filter."
        ],
        [
            "Add requirements to be managed\n\n        :param list/Requirement requirements: List of :class:`BumpRequirement` or :class:`pkg_resources.Requirement`\n        :param bool required: Set required flag for each requirement if provided."
        ],
        [
            "Check if requirement is already satisfied by what was previously checked\n\n        :param Requirement req: Requirement to check"
        ],
        [
            "Add new requirements that must be fulfilled for this bump to occur"
        ],
        [
            "Parse changes for requirements\n\n        :param list changes:"
        ],
        [
            "Bump dependencies using given requirements.\n\n          :param RequirementsManager bump_reqs: Bump requirements manager\n          :param dict kwargs: Additional args from argparse. Some bumpers accept user options, and some not.\n          :return: List of :class:`Bump` changes made."
        ],
        [
            "Restore content in target file to be before any changes"
        ],
        [
            "Transforms the object into an acceptable format for transmission.\n\n        @throws ValueError\n            To indicate this serializer does not support the encoding of the\n            specified object."
        ],
        [
            "Extends a collection with a value."
        ],
        [
            "Merges a named option collection."
        ],
        [
            "All package info for given package"
        ],
        [
            "All versions for package"
        ],
        [
            "Flush and close the stream.\n\n        This is called automatically by the base resource on resources\n        unless the resource is operating asynchronously; in that case,\n        this method MUST be called in order to signal the end of the request.\n        If not the request will simply hang as it is waiting for some\n        thread to tell it to return to the client."
        ],
        [
            "Writes the given chunk to the output buffer.\n\n        @param[in] chunk\n            Either a byte array, a unicode string, or a generator. If `chunk`\n            is a generator then calling `self.write(<generator>)` is\n            equivalent to:\n\n            @code\n                for x in <generator>:\n                    self.write(x)\n                    self.flush()\n            @endcode\n\n        @param[in] serialize\n            True to serialize the lines in a determined serializer.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer."
        ],
        [
            "Serializes the data into this response using a serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n\n        @returns\n            A tuple of the serialized text and an instance of the\n            serializer used."
        ],
        [
            "Flush the write buffers of the stream.\n\n        This results in writing the current contents of the write buffer to\n        the transport layer, initiating the HTTP/1.1 response. This initiates\n        a streaming response. If the `Content-Length` header is not given\n        then the chunked `Transfer-Encoding` is applied."
        ],
        [
            "Writes the passed chunk and flushes it to the client."
        ],
        [
            "Writes the passed chunk, flushes it to the client,\n        and terminates the connection."
        ],
        [
            "This ``Context Manager`` is used to move the contents of a directory\n    elsewhere temporarily and put them back upon exit.  This allows testing\n    code to use the same file directories as normal code without fear of\n    damage.\n\n    The name of the temporary directory which contains your files is yielded.\n\n    :param dirname:\n        Path name of the directory to be replaced.\n\n\n    Example:\n\n    .. code-block:: python\n\n        with replaced_directory('/foo/bar/') as rd:\n            # \"/foo/bar/\" has been moved & renamed\n            with open('/foo/bar/thing.txt', 'w') as f:\n                f.write('stuff')\n                f.close()\n\n\n        # got here? => \"/foo/bar/ is now restored and temp has been wiped, \n        # \"thing.txt\" is gone"
        ],
        [
            "This ``Context Manager`` redirects STDOUT to a ``StringIO`` objects\n    which is returned from the ``Context``.  On exit STDOUT is restored.\n\n    Example:\n\n    .. code-block:: python\n\n        with capture_stdout() as capture:\n            print('foo')\n\n        # got here? => capture.getvalue() will now have \"foo\\\\n\""
        ],
        [
            "This ``Context Manager`` redirects STDERR to a ``StringIO`` objects\n    which is returned from the ``Context``.  On exit STDERR is restored.\n\n    Example:\n\n    .. code-block:: python\n\n        with capture_stderr() as capture:\n            print('foo')\n\n        # got here? => capture.getvalue() will now have \"foo\\\\n\""
        ],
        [
            "Builds the URL configuration for this resource."
        ],
        [
            "Dump an object in req format to the fp given.\n\n    :param Mapping obj: The object to serialize.  Must have a keys method.\n    :param fp: A writable that can accept all the types given.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types."
        ],
        [
            "Dump an object in req format to a string.\n\n    :param Mapping obj: The object to serialize.  Must have a keys method.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types."
        ],
        [
            "Load an object from the file pointer.\n\n    :param fp: A readable filehandle.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    :param cls: A callable that returns a Mapping that is filled with pairs.  The most common alternate option would be OrderedDict.\n    :param list_cls: A callable that takes an iterable and returns a sequence."
        ],
        [
            "Loads an object from a string.\n\n    :param s: An object to parse\n    :type s: bytes or str\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    :param cls: A callable that returns a Mapping that is filled with pairs.  The most common alternate option would be OrderedDict.\n    :param list_cls: A callable that takes an iterable and returns a sequence."
        ],
        [
            "Reverse all bumpers"
        ],
        [
            "Expand targets by looking for '-r' in targets."
        ],
        [
            "Gets the Nginx config for the project"
        ],
        [
            "Creates base directories for app, virtualenv, and nginx"
        ],
        [
            "Creates the virtualenv for the project"
        ],
        [
            "Creates the Nginx configuration for the project"
        ],
        [
            "Creates scripts to start and stop the application"
        ],
        [
            "Creates the full project"
        ],
        [
            "Dasherizes the passed value."
        ],
        [
            "Redirect to the canonical URI for this resource."
        ],
        [
            "Parses out parameters and separates them out of the path.\n\n        This uses one of the many defined patterns on the options class. But,\n        it defaults to a no-op if there are no defined patterns."
        ],
        [
            "Traverses down the path and determines the accessed resource.\n\n        This makes use of the patterns array to implement simple traversal.\n        This defaults to a no-op if there are no defined patterns."
        ],
        [
            "Helper method used in conjunction with the view handler to\n        stream responses to the client."
        ],
        [
            "Deserializes the text using a determined deserializer.\n\n        @param[in] request\n            The request object to pull information from; normally used to\n            determine the deserialization format (when `format` is\n            not provided).\n\n        @param[in] text\n            The text to be deserialized. Can be left blank and the\n            request will be read.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.\n\n        @returns\n            A tuple of the deserialized data and an instance of the\n            deserializer used."
        ],
        [
            "Serializes the data using a determined serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] response\n            The response object to serialize the data to.\n            If this method is invoked as an instance method, the response\n            object can be omitted and it will be taken from the instance.\n\n        @param[in] request\n            The request object to pull information from; normally used to\n            determine the serialization format (when `format` is not provided).\n            May be used by some serializers as well to pull additional headers.\n            If this method is invoked as an instance method, the request\n            object can be omitted and it will be taken from the instance.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n\n        @returns\n            A tuple of the serialized text and an instance of the\n            serializer used."
        ],
        [
            "Entry-point of the dispatch cycle for this resource.\n\n        Performs common work such as authentication, decoding, etc. before\n        handing complete control of the result to a function with the\n        same name as the request method."
        ],
        [
            "Ensure we are authenticated."
        ],
        [
            "Ensure we are allowed to access this resource."
        ],
        [
            "Ensure that we're allowed to use this HTTP method."
        ],
        [
            "Processes every request.\n\n        Directs control flow to the appropriate HTTP/1.1 method."
        ],
        [
            "Process an `OPTIONS` request.\n\n        Used to initiate a cross-origin request. All handling specific to\n        CORS requests is done on every request however this method also\n        returns a list of available methods."
        ],
        [
            "Wraps the decorated function in a lightweight resource."
        ],
        [
            "Render to cookie strings."
        ],
        [
            "update self with cookie_string."
        ],
        [
            "Adds a method to the internal lists of allowed or denied methods.\n        Each object in the internal list contains a resource ARN and a\n        condition statement. The condition statement can be null."
        ],
        [
            "This function loops over an array of objects containing\n        a resourceArn and conditions statement and generates\n        the array of statements for the policy."
        ],
        [
            "AWS doesn't quite have Swagger 2.0 validation right and will fail\n        on some refs. So, we need to convert to deref before\n        upload."
        ],
        [
            "Check all necessary system requirements to exist.\n\n    :param pre_requirements:\n        Sequence of pre-requirements to check by running\n        ``where <pre_requirement>`` on Windows and ``which ...`` elsewhere."
        ],
        [
            "Convert config dict to arguments list.\n\n    :param config: Configuration dict."
        ],
        [
            "Create virtual environment.\n\n    :param env: Virtual environment name.\n    :param args: Pass given arguments to ``virtualenv`` script.\n    :param recerate: Recreate virtual environment? By default: False\n    :param ignore_activated:\n        Ignore already activated virtual environment and create new one. By\n        default: False\n    :param quiet: Do not output messages into terminal. By default: False"
        ],
        [
            "Decorator to error handling."
        ],
        [
            "Install library or project into virtual environment.\n\n    :param env: Use given virtual environment name.\n    :param requirements: Use given requirements file for pip.\n    :param args: Pass given arguments to pip script.\n    :param ignore_activated:\n        Do not run pip inside already activated virtual environment. By\n        default: False\n    :param install_dev_requirements:\n        When enabled install prefixed or suffixed dev requirements after\n        original installation process completed. By default: False\n    :param quiet: Do not output message to terminal. By default: False"
        ],
        [
            "Iterate over dict items."
        ],
        [
            "Iterate over dict keys."
        ],
        [
            "r\"\"\"Bootstrap Python projects and libraries with virtualenv and pip.\n\n    Also check system requirements before bootstrap and run post bootstrap\n    hook if any.\n\n    :param \\*args: Command line arguments list."
        ],
        [
            "Parse args from command line by creating argument parser instance and\n    process it.\n\n    :param args: Command line arguments list."
        ],
        [
            "r\"\"\"Run pip command in given or activated virtual environment.\n\n    :param env: Virtual environment name.\n    :param cmd: Pip subcommand to run.\n    :param ignore_activated:\n        Ignore activated virtual environment and use given venv instead. By\n        default: False\n    :param \\*\\*kwargs:\n        Additional keyword arguments to be passed to :func:`~run_cmd`"
        ],
        [
            "Convert config dict to command line args line.\n\n    :param config: Configuration dict.\n    :param bootstrap: Bootstrapper configuration dict."
        ],
        [
            "Print error message to stderr, using ANSI-colors.\n\n    :param message: Message to print\n    :param wrap:\n        Wrap message into ``ERROR: <message>. Exit...`` template. By default:\n        True"
        ],
        [
            "Print message via ``subprocess.call`` function.\n\n    This helps to ensure consistent output and avoid situations where print\n    messages actually shown after messages from all inner threads.\n\n    :param message: Text message to print."
        ],
        [
            "Read and parse configuration file. By default, ``filename`` is relative\n    path to current work directory.\n\n    If no config file found, default ``CONFIG`` would be used.\n\n    :param filename: Read config from given filename.\n    :param args: Parsed command line arguments."
        ],
        [
            "r\"\"\"Call given command with ``subprocess.call`` function.\n\n    :param cmd: Command to run.\n    :type cmd: tuple or str\n    :param echo:\n        If enabled show command to call and its output in STDOUT, otherwise\n        hide all output. By default: False\n    :param fail_silently: Do not raise exception on error. By default: False\n    :param \\*\\*kwargs:\n        Additional keyword arguments to be passed to ``subprocess.call``\n        function. STDOUT and STDERR streams would be setup inside of function\n        to ensure hiding command output in case of disabling ``echo``."
        ],
        [
            "Run post-bootstrap hook if any.\n\n    :param hook: Hook to run.\n    :param config: Configuration dict.\n    :param quiet: Do not output messages to STDOUT/STDERR. By default: False"
        ],
        [
            "Save error traceback to bootstrapper log file.\n\n    :param err: Catched exception."
        ],
        [
            "Convert Python object to string.\n\n    :param value: Python object to convert.\n    :param encoding: Encoding to use if in Python 2 given object is unicode.\n    :param errors: Errors mode to use if in Python 2 given object is unicode."
        ],
        [
            "Copy file from `src` path to `dst` path. If `dst` already exists, will add '+' characters\n    to the end of the basename without extension.\n\n    Parameters\n    ----------\n    src: str\n\n    dst: str\n\n    Returns\n    -------\n    dstpath: str"
        ],
        [
            "Returns the absolute path of folderpath.\n    If the path does not exist, will raise IOError."
        ],
        [
            "Return the extension of fpath.\n\n    Parameters\n    ----------\n    fpath: string\n    File name or path\n\n    check_if_exists: bool\n\n    allowed_exts: dict\n    Dictionary of strings, where the key if the last part of a complex ('.' separated) extension\n    and the value is the previous part.\n    For example: for the '.nii.gz' extension I would have a dict as {'.gz': ['.nii',]}\n\n    Returns\n    -------\n    str\n    The extension of the file name or path"
        ],
        [
            "Add the extension ext to fpath if it doesn't have it.\n\n    Parameters\n    ----------\n    filepath: str\n    File name or path\n\n    ext: str\n    File extension\n\n    check_if_exists: bool\n\n    Returns\n    -------\n    File name or path with extension added, if needed."
        ],
        [
            "Joins path to each line in filelist\n\n    Parameters\n    ----------\n    path: str\n\n    filelist: list of str\n\n    Returns\n    -------\n    list of filepaths"
        ],
        [
            "Deletes all files in filelist\n\n    Parameters\n    ----------\n    filelist: list of str\n        List of the file paths to be removed\n\n    folder: str\n        Path to be used as common directory for all file paths in filelist"
        ],
        [
            "Returns the length of the file using the 'wc' GNU command\n\n    Parameters\n    ----------\n    filepath: str\n\n    Returns\n    -------\n    float"
        ],
        [
            "Merge two dictionaries.\n\n    Values that evaluate to true take priority over falsy values.\n    `dict_1` takes priority over `dict_2`."
        ],
        [
            "Return a folder path if it exists.\n\n    First will check if it is an existing system path, if it is, will return it\n    expanded and absoluted.\n\n    If this fails will look for the rcpath variable in the app_name rcfiles or\n    exclusively within the given section_name, if given.\n\n    Parameters\n    ----------\n    rcpath: str\n        Existing folder path or variable name in app_name rcfile with an\n        existing one.\n\n    section_name: str\n        Name of a section in the app_name rcfile to look exclusively there for\n        variable names.\n\n    app_name: str\n        Name of the application to look for rcfile configuration files.\n\n    Returns\n    -------\n    sys_path: str\n        A expanded absolute file or folder path if the path exists.\n\n    Raises\n    ------\n    IOError if the proposed sys_path does not exist."
        ],
        [
            "Read environment variables and config files and return them merged with\n    predefined list of arguments.\n\n    Parameters\n    ----------\n    appname: str\n        Application name, used for config files and environment variable\n        names.\n\n    section: str\n        Name of the section to be read. If this is not set: appname.\n\n    args:\n        arguments from command line (optparse, docopt, etc).\n\n    strip_dashes: bool\n        Strip dashes prefixing key names from args dict.\n\n    Returns\n    --------\n    dict\n        containing the merged variables of environment variables, config\n        files and args.\n\n    Raises\n    ------\n    IOError\n        In case the return value is empty.\n\n    Notes\n    -----\n    Environment variables are read if they start with appname in uppercase\n    with underscore, for example:\n\n        TEST_VAR=1\n\n    Config files compatible with ConfigParser are read and the section name\n    appname is read, example:\n\n        [appname]\n        var=1\n\n    We can also have host-dependent configuration values, which have\n    priority over the default appname values.\n\n        [appname]\n        var=1\n\n        [appname:mylinux]\n        var=3\n\n\n    For boolean flags do not try to use: 'True' or 'False',\n                                         'on' or 'off',\n                                         '1' or '0'.\n    Unless you are willing to parse this values by yourself.\n    We recommend commenting the variables out with '#' if you want to set a\n    flag to False and check if it is in the rcfile cfg dict, i.e.:\n\n        flag_value = 'flag_variable' in cfg\n\n\n    Files are read from: /etc/appname/config,\n                         /etc/appfilerc,\n                         ~/.config/appname/config,\n                         ~/.config/appname,\n                         ~/.appname/config,\n                         ~/.appnamerc,\n                         appnamerc,\n                         .appnamerc,\n                         appnamerc file found in 'path' folder variable in args,\n                         .appnamerc file found in 'path' folder variable in args,\n                         file provided by 'config' variable in args.\n\n    Example\n    -------\n        args = rcfile(__name__, docopt(__doc__, version=__version__))"
        ],
        [
            "Return the dictionary containing the rcfile section configuration\n    variables.\n\n    Parameters\n    ----------\n    section_name: str\n        Name of the section in the rcfiles.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    settings: dict\n        Dict with variable values"
        ],
        [
            "Return the value of the variable in the section_name section of the\n    app_name rc file.\n\n    Parameters\n    ----------\n    var_name: str\n        Name of the variable to be searched for.\n\n    section_name: str\n        Name of the section in the rcfiles.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    var_value: str\n        The value of the variable with given var_name."
        ],
        [
            "Return the section and the value of the variable where the first\n    var_name is found in the app_name rcfiles.\n\n    Parameters\n    ----------\n    var_name: str\n        Name of the variable to be searched for.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    section_name: str\n        Name of the section in the rcfiles where var_name was first found.\n\n    var_value: str\n        The value of the first variable with given var_name."
        ],
        [
            "Filters the lst using pattern.\n    If pattern starts with '(' it will be considered a re regular expression,\n    otherwise it will use fnmatch filter.\n\n    :param lst: list of strings\n\n    :param pattern: string\n\n    :return: list of strings\n    Filtered list of strings"
        ],
        [
            "Given a nested dictionary adict.\n    This returns its childen just below the path.\n    The path is a string composed of adict keys separated by sep.\n\n    :param adict: nested dict\n\n    :param path: str\n\n    :param sep: str\n\n    :return: dict or list or leaf of treemap"
        ],
        [
            "Given a nested dictionary, this returns all its leave elements in a list.\n\n    :param adict:\n\n    :return: list"
        ],
        [
            "Looks for path_regex within base_path. Each match is append\n    in the returned list.\n    path_regex may contain subfolder structure.\n    If any part of the folder structure is a\n\n    :param base_path: str\n\n    :param path_regex: str\n\n    :return list of strings"
        ],
        [
            "Will create dirpath folder. If dirpath already exists and overwrite is False,\n        will append a '+' suffix to dirpath until dirpath does not exist."
        ],
        [
            "Imports filetree and root_path variable values from the filepath.\n\n        :param filepath:\n        :return: root_path and filetree"
        ],
        [
            "Remove the nodes that match the pattern."
        ],
        [
            "Return the number of nodes that match the pattern.\n\n        :param pattern:\n\n        :param adict:\n        :return: int"
        ],
        [
            "Converts an array-like to an array of floats\n\n    The new dtype will be np.float32 or np.float64, depending on the original\n    type. The function can create a copy or modify the argument depending\n    on the argument copy.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n\n    copy : bool, optional\n        If True, a copy of X will be created. If False, a copy may still be\n        returned if X's dtype is not a floating point type.\n\n    Returns\n    -------\n    XT : {array, sparse matrix}\n        An array of type np.float"
        ],
        [
            "Make arrays indexable for cross-validation.\n\n    Checks consistent length, passes through None, and ensures that everything\n    can be indexed by converting sparse matrices to csr and converting\n    non-interable objects to arrays.\n\n    Parameters\n    ----------\n    iterables : lists, dataframes, arrays, sparse matrices\n        List of objects to ensure sliceability."
        ],
        [
            "Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is converted to an at least 2nd numpy array.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc.  None means that sparse matrix input will raise an error.\n        If the input is sparse but not in the allowed format, it will be\n        converted to the first listed format.\n\n    order : 'F', 'C' or None (default=None)\n        Whether an array will be forced to be fortran or c-style.\n\n    copy : boolean (default=False)\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : boolean (default=True)\n        Whether to raise an error on np.inf and np.nan in X.\n\n    ensure_2d : boolean (default=True)\n        Whether to make X at least 2d.\n\n    allow_nd : boolean (default=False)\n        Whether to allow X.ndim > 2.\n\n    Returns\n    -------\n    X_converted : object\n        The converted and validated X."
        ],
        [
            "Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X 2d and y 1d.\n    Standard input checks are only applied to y. For multi-label y,\n    set multi_ouput=True to allow 2d and sparse y.\n\n    Parameters\n    ----------\n    X : nd-array, list or sparse matrix\n        Input data.\n\n    y : nd-array, list or sparse matrix\n        Labels.\n\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc.  None means that sparse matrix input will raise an error.\n        If the input is sparse but not in the allowed format, it will be\n        converted to the first listed format.\n\n    order : 'F', 'C' or None (default=None)\n        Whether an array will be forced to be fortran or c-style.\n\n    copy : boolean (default=False)\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : boolean (default=True)\n        Whether to raise an error on np.inf and np.nan in X.\n\n    ensure_2d : boolean (default=True)\n        Whether to make X at least 2d.\n\n    allow_nd : boolean (default=False)\n        Whether to allow X.ndim > 2.\n\n    Returns\n    -------\n    X_converted : object\n        The converted and validated X."
        ],
        [
            "Ravel column or 1d numpy array, else raises an error\n\n    Parameters\n    ----------\n    y : array-like\n\n    Returns\n    -------\n    y : array"
        ],
        [
            "Warning utility function to check that data type is floating point.\n\n    Returns True if a warning was raised (i.e. the input is not float) and\n    False otherwise, for easier input validation."
        ],
        [
            "Convert an arbitrary array to numpy.ndarray.\n\n    In the case of a memmap array, a copy is automatically made to break the\n    link with the underlying file (whatever the value of the \"copy\" keyword).\n\n    The purpose of this function is mainly to get rid of memmap objects, but\n    it can be used for other purposes. In particular, combining copying and\n    casting can lead to performance improvements in some cases, by avoiding\n    unnecessary copies.\n\n    If not specified, input array order is preserved, in all cases, even when\n    a copy is requested.\n\n    Caveat: this function does not copy during bool to/from 1-byte dtype\n    conversions. This can lead to some surprising results in some rare cases.\n    Example:\n\n        a = numpy.asarray([0, 1, 2], dtype=numpy.int8)\n        b = as_ndarray(a, dtype=bool)  # array([False, True, True], dtype=bool)\n        c = as_ndarray(b, dtype=numpy.int8)  # array([0, 1, 2], dtype=numpy.int8)\n\n    The usually expected result for the last line would be array([0, 1, 1])\n    because True evaluates to 1. Since there is no copy made here, the original\n    array is recovered.\n\n    Parameters\n    ----------\n    arr: array-like\n        input array. Any value accepted by numpy.asarray is valid.\n\n    copy: bool\n        if True, force a copy of the array. Always True when arr is a memmap.\n\n    dtype: any numpy dtype\n        dtype of the returned array. Performing copy and type conversion at the\n        same time can in some cases avoid an additional copy.\n\n    order: string\n        gives the order of the returned array.\n        Valid values are: \"C\", \"F\", \"A\", \"K\", None.\n        default is \"K\". See ndarray.copy() for more information.\n\n    Returns\n    -------\n    ret: np.ndarray\n        Numpy array containing the same data as arr, always of class\n        numpy.ndarray, and with no link to any underlying file."
        ],
        [
            "Call FSL tools to apply transformations to a given atlas to a functional image.\n    Given the transformation matrices.\n\n    Parameters\n    ----------\n    atlas_filepath: str\n        Path to the 3D atlas volume file.\n\n    anatbrain_filepath: str\n        Path to the anatomical brain volume file (skull-stripped and registered to the same space as the atlas,\n        e.g., MNI).\n\n    meanfunc_filepath: str\n        Path to the average functional image to be used as reference in the last applywarp step.\n\n    atlas2anat_nonlin_xfm_filepath: str\n        Path to the atlas to anatomical brain linear transformation .mat file.\n        If you have the inverse transformation, i.e., anatomical brain to atlas, set is_atlas2anat_inverted to True.\n\n    is_atlas2anat_inverted: bool\n        If False will have to calculate the inverse atlas2anat transformation to apply the transformations.\n        This step will be performed with FSL invwarp.\n\n    anat2func_lin_xfm_filepath: str\n        Path to the anatomical to functional .mat linear transformation file.\n\n    atlasinanat_out_filepath: str\n        Path to output file which will contain the 3D atlas in the subject anatomical space.\n\n    atlasinfunc_out_filepath: str\n        Path to output file which will contain the 3D atlas in the subject functional space.\n\n    verbose: bool\n        If verbose will show DEBUG log info.\n\n    rewrite: bool\n        If True will re-run all the commands overwriting any existing file. Otherwise will check if\n        each file exists and if it does won't run the command.\n\n    parallel: bool\n        If True will launch the commands using ${FSLDIR}/fsl_sub to use the cluster infrastructure you have setup\n        with FSL (SGE or HTCondor)."
        ],
        [
            "Convert a FWHM value to sigma in a Gaussian kernel.\n\n    Parameters\n    ----------\n    fwhm: float or numpy.array\n       fwhm value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       sigma values"
        ],
        [
            "Convert a sigma in a Gaussian kernel to a FWHM value.\n\n    Parameters\n    ----------\n    sigma: float or numpy.array\n       sigma value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       fwhm values corresponding to `sigma` values"
        ],
        [
            "Smooth images with a a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        3D or 4D array, with image number as last dimension.\n\n    affine: numpy.ndarray\n        Image affine transformation matrix for image.\n\n    fwhm: scalar, numpy.ndarray\n        Smoothing kernel size, as Full-Width at Half Maximum (FWHM) in millimeters.\n        If a scalar is given, kernel width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n\n    copy: bool\n        if True, will make a copy of the input array. Otherwise will directly smooth the input array.\n\n    Returns\n    -------\n    smooth_arr: numpy.ndarray"
        ],
        [
            "Smooth images using a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of each image in images.\n    In all cases, non-finite values in input are zeroed.\n\n    Parameters\n    ----------\n    imgs: str or img-like object or iterable of img-like objects\n        See boyle.nifti.read.read_img\n        Image(s) to smooth.\n\n    fwhm: scalar or numpy.ndarray\n        Smoothing kernel size, as Full-Width at Half Maximum (FWHM) in millimeters.\n        If a scalar is given, kernel width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n\n    Returns\n    -------\n    smooth_imgs: nibabel.Nifti1Image or list of.\n        Smooth input image/s."
        ],
        [
            "Smooth images by applying a Gaussian filter.\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    This is copied and slightly modified from nilearn:\n    https://github.com/nilearn/nilearn/blob/master/nilearn/image/image.py\n    Added the **kwargs argument.\n\n    Parameters\n    ==========\n    arr: numpy.ndarray\n        4D array, with image number as last dimension. 3D arrays are also\n        accepted.\n    affine: numpy.ndarray\n        (4, 4) matrix, giving affine transformation for image. (3, 3) matrices\n        are also accepted (only these coefficients are used).\n        If fwhm='fast', the affine is not used and can be None\n    fwhm: scalar, numpy.ndarray, 'fast' or None\n        Smoothing strength, as a full-width at half maximum, in millimeters.\n        If a scalar is given, width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n        If fwhm == 'fast', a fast smoothing will be performed with\n        a filter [0.2, 1, 0.2] in each direction and a normalisation\n        to preserve the local average value.\n        If fwhm is None, no filtering is performed (useful when just removal\n        of non-finite values is needed).\n    ensure_finite: bool\n        if True, replace every non-finite values (like NaNs) by zero before\n        filtering.\n    copy: bool\n        if True, input array is not modified. False by default: the filtering\n        is performed in-place.\n    kwargs: keyword-arguments\n        Arguments for the ndimage.gaussian_filter1d function.\n\n    Returns\n    =======\n    filtered_arr: numpy.ndarray\n        arr, filtered.\n    Notes\n    =====\n    This function is most efficient with arr in C order."
        ],
        [
            "Smooth images by applying a Gaussian filter.\n    Apply a Gaussian filter along the three first dimensions of arr.\n    In all cases, non-finite values in input image are replaced by zeros.\n\n    This is copied and slightly modified from nilearn:\n    https://github.com/nilearn/nilearn/blob/master/nilearn/image/image.py\n    Added the **kwargs argument.\n\n    Parameters\n    ==========\n    imgs: Niimg-like object or iterable of Niimg-like objects\n        See http://nilearn.github.io/manipulating_images/manipulating_images.html#niimg.\n        Image(s) to smooth.\n    fwhm: scalar, numpy.ndarray, 'fast' or None\n        Smoothing strength, as a Full-Width at Half Maximum, in millimeters.\n        If a scalar is given, width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n        If fwhm == 'fast', a fast smoothing will be performed with\n        a filter [0.2, 1, 0.2] in each direction and a normalisation\n        to preserve the scale.\n        If fwhm is None, no filtering is performed (useful when just removal\n        of non-finite values is needed)\n    Returns\n    =======\n    filtered_img: nibabel.Nifti1Image or list of.\n        Input image, filtered. If imgs is an iterable, then filtered_img is a\n        list."
        ],
        [
            "Create requests session with any required auth headers\n        applied.\n\n        :rtype: requests.Session."
        ],
        [
            "Create requests session with AAD auth headers\n\n        :rtype: requests.Session."
        ],
        [
            "Return a grid with coordinates in 3D physical space for `img`."
        ],
        [
            "Gets a 3D CoordinateMap from img.\n\n    Parameters\n    ----------\n    img: nib.Nifti1Image or nipy Image\n\n    Returns\n    -------\n    nipy.core.reference.coordinate_map.CoordinateMap"
        ],
        [
            "Return the header and affine matrix from a Nifti file.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    hdr, aff"
        ],
        [
            "Return the voxel matrix of the Nifti file.\n    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    copy: bool\n    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.\n\n    Returns\n    -------\n    array_like"
        ],
        [
            "Read a Nifti file and return as nipy.Image\n\n    Parameters\n    ----------\n    param nii_file: str\n        Nifti file path\n\n    Returns\n    -------\n    nipy.Image"
        ],
        [
            "From the list of absolute paths to nifti files, creates a Numpy array\n    with the data.\n\n    Parameters\n    ----------\n    img_filelist:  list of str\n        List of absolute file paths to nifti files. All nifti files must have\n        the same shape.\n\n    outdtype: dtype\n        Type of the elements of the array, if not set will obtain the dtype from\n        the first nifti file.\n\n    Returns\n    -------\n    outmat: Numpy array with shape N x prod(vol.shape)\n            containing the N files as flat vectors.\n\n    vol_shape: Tuple with shape of the volumes, for reshaping."
        ],
        [
            "Crops image to a smaller size\n\n    Crop img to size indicated by slices and modify the affine accordingly.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n        Image to be cropped.\n\n    slices: list of slices\n        Defines the range of the crop.\n        E.g. [slice(20, 200), slice(40, 150), slice(0, 100)]\n        defines a 3D cube\n\n        If slices has less entries than image has dimensions,\n        the slices will be applied to the first len(slices) dimensions.\n\n    copy: boolean\n        Specifies whether cropped data is to be copied or not.\n        Default: True\n\n    Returns\n    -------\n    cropped_img: img-like object\n        Cropped version of the input image"
        ],
        [
            "Crops img as much as possible\n\n    Will crop img, removing as many zero entries as possible\n    without touching non-zero entries. Will leave one voxel of\n    zero padding around the obtained non-zero area in order to\n    avoid sampling issues later on.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n        Image to be cropped.\n\n    rtol: float\n        relative tolerance (with respect to maximal absolute\n        value of the image), under which values are considered\n        negligeable and thus croppable.\n\n    copy: boolean\n        Specifies whether cropped data is copied or not.\n\n    Returns\n    -------\n    cropped_img: image\n        Cropped version of the input image"
        ],
        [
            "Create a new image of the same class as the reference image\n\n    Parameters\n    ----------\n    ref_niimg: image\n        Reference image. The new image will be of the same type.\n\n    data: numpy array\n        Data to be stored in the image\n\n    affine: 4x4 numpy array, optional\n        Transformation matrix\n\n    copy_header: boolean, optional\n        Indicated if the header of the reference image should be used to\n        create the new image\n\n    Returns\n    -------\n    new_img: image\n        A loaded image with the same type (and header) as the reference image."
        ],
        [
            "Return the h5py.File given its file path.\n\n    Parameters\n    ----------\n    file_path: string\n        HDF5 file path\n\n    mode: string\n        r   Readonly, file must exist\n        r+  Read/write, file must exist\n        w   Create file, truncate if exists\n        w-  Create file, fail if exists\n        a   Read/write if exists, create otherwise (default)\n\n    Returns\n    -------\n    h5file: h5py.File"
        ],
        [
            "Return all dataset contents from h5path group in h5file in an OrderedDict.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to read datasets from\n\n    Returns\n    -------\n    datasets: OrderedDict\n        Dict with variables contained in file_path/h5path"
        ],
        [
            "Return the node of type node_type names within h5path of h5file.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to get the group names from\n\n    node_type: h5py object type\n        HDF5 object type\n\n    Returns\n    -------\n    names: list of str\n        List of names"
        ],
        [
            "self.mask setter\n\n        Parameters\n        ----------\n        image: str or img-like object.\n            See NeuroImage constructor docstring."
        ],
        [
            "Read the images, load them into self.items and set the labels."
        ],
        [
            "Save the Numpy array created from to_matrix function to the output_file.\n\n        Will save into the file: outmat, mask_indices, vol_shape and self.others (put here whatever you want)\n\n            data: Numpy array with shape N x prod(vol.shape)\n                  containing the N files as flat vectors.\n\n            mask_indices: matrix with indices of the voxels in the mask\n\n            vol_shape: Tuple with shape of the volumes, for reshaping.\n\n        Parameters\n        ----------\n        output_file: str\n            Path to the output file. The extension of the file will be taken into account for the file format.\n            Choices of extensions: '.pyshelf' or '.shelf' (Python shelve)\n                                   '.mat' (Matlab archive),\n                                   '.hdf5' or '.h5' (HDF5 file)\n\n        smooth_fwhm: int\n            Integer indicating the size of the FWHM Gaussian smoothing kernel\n            to smooth the subject volumes before creating the data matrix\n\n        outdtype: dtype\n            Type of the elements of the array, if None will obtain the dtype from\n            the first nifti file."
        ],
        [
            "Writes msg to stderr and exits with return code"
        ],
        [
            "Calls the command\n\n    Parameters\n    ----------\n    cmd_args: list of str\n        Command name to call and its arguments in a list.\n\n    Returns\n    -------\n    Command output"
        ],
        [
            "Call CLI command with arguments and returns its return value.\n\n    Parameters\n    ----------\n    cmd_name: str\n        Command name or full path to the binary file.\n\n    arg_strings: list of str\n        Argument strings list.\n\n    Returns\n    -------\n    return_value\n        Command return value."
        ],
        [
            "Tries to submit cmd to HTCondor, if it does not succeed, it will\n    be called with subprocess.call.\n\n    Parameters\n    ----------\n    cmd: string\n        Command to be submitted\n\n    Returns\n    -------"
        ],
        [
            "Submits cmd to HTCondor queue\n\n    Parameters\n    ----------\n    cmd: string\n        Command to be submitted\n\n    Returns\n    -------\n    int\n        returncode value from calling the submission command."
        ],
        [
            "Clean previously built package artifacts."
        ],
        [
            "Upload the package to an index server.\n\n    This implies cleaning and re-building the package.\n\n    :param repo: Required. Name of the index server to upload to, as specifies\n        in your .pypirc configuration file."
        ],
        [
            "Load all Service Fabric commands"
        ],
        [
            "Open a volumetric file using the tools following the file extension.\n\n    Parameters\n    ----------\n    filepath: str\n        Path to a volume file\n\n    Returns\n    -------\n    volume_data: np.ndarray\n        Volume data\n\n    pixdim: 1xN np.ndarray\n        Vector with the description of the voxels physical size (usually in mm) for each volume dimension.\n\n    Raises\n    ------\n    IOError\n        In case the file is not found."
        ],
        [
            "Will rename all files in file_lst to a padded serial\n    number plus its extension\n\n    :param file_lst: list of path.py paths"
        ],
        [
            "Search for dicoms in folders and save file paths into\n        self.dicom_paths set.\n\n        :param folders: str or list of str"
        ],
        [
            "Overwrites self.items with the given set of files.\n        Will filter the fileset and keep only Dicom files.\n\n        Parameters\n        ----------\n        fileset: iterable of str\n        Paths to files\n\n        check_if_dicoms: bool\n        Whether to check if the items in fileset are dicom file paths"
        ],
        [
            "Update this set with the union of itself and dicomset.\n\n        Parameters\n        ----------\n        dicomset: DicomFileSet"
        ],
        [
            "Copies all files within this set to the output_folder\n\n        Parameters\n        ----------\n        output_folder: str\n        Path of the destination folder of the files\n\n        rename_files: bool\n        Whether or not rename the files to a sequential format\n\n        mkdir: bool\n        Whether to make the folder if it does not exist\n\n        verbose: bool\n        Whether to print to stdout the files that are beind copied"
        ],
        [
            "Creates a lambda function to read DICOM files.\n        If store_store_metadata is False, will only return the file path.\n        Else if you give header_fields, will return only the set of of\n        header_fields within a DicomFile object or the whole DICOM file if\n        None.\n\n        :return: function\n        This function has only one parameter: file_path"
        ],
        [
            "Generator that yields one by one the return value for self.read_dcm\n        for each file within this set"
        ],
        [
            "Return a set of unique field values from a list of DICOM files\n\n    Parameters\n    ----------\n    dcm_file_list: iterable of DICOM file paths\n\n    field_name: str\n     Name of the field from where to get each value\n\n    Returns\n    -------\n    Set of field values"
        ],
        [
            "Returns a list of the dicom files within root_path\n\n    Parameters\n    ----------\n    root_path: str\n    Path to the directory to be recursively searched for DICOM files.\n\n    Returns\n    -------\n    dicoms: set\n    Set of DICOM absolute file paths"
        ],
        [
            "Tries to read the file using dicom.read_file,\n    if the file exists and dicom.read_file does not raise\n    and Exception returns True. False otherwise.\n\n    :param filepath: str\n     Path to DICOM file\n\n    :return: bool"
        ],
        [
            "Group in a dictionary all the DICOM files in dicom_paths\n    separated by the given `hdr_field` tag value.\n\n    Parameters\n    ----------\n    dicom_paths: str\n        Iterable of DICOM file paths.\n\n    hdr_field: str\n        Name of the DICOM tag whose values will be used as key for the group.\n\n    Returns\n    -------\n    dicom_groups: dict of dicom_paths"
        ],
        [
            "Return the attributes values from this DicomFile\n\n        Parameters\n        ----------\n        attributes: str or list of str\n         DICOM field names\n\n        default: str\n         Default value if the attribute does not exist.\n\n        Returns\n        -------\n        Value of the field or list of values."
        ],
        [
            "Concatenate `images` in the direction determined in `axis`.\n\n    Parameters\n    ----------\n    images: list of str or img-like object.\n        See NeuroImage constructor docstring.\n\n    axis: str\n      't' : concatenate images in time\n      'x' : concatenate images in the x direction\n      'y' : concatenate images in the y direction\n      'z' : concatenate images in the z direction\n\n    Returns\n    -------\n    merged: img-like object"
        ],
        [
            "Picks a function whose first argument is an `img`, processes its\n    data and returns a numpy array. This decorator wraps this numpy array\n    into a nibabel.Nifti1Image."
        ],
        [
            "Pixelwise division or divide by a number"
        ],
        [
            "Return the image with the given `mask` applied."
        ],
        [
            "Return an image with the binarised version of the data of `img`."
        ],
        [
            "Return a z-scored version of `icc`.\n    This function is based on GIFT `icatb_convertImageToZScores` function."
        ],
        [
            "Return the thresholded z-scored `icc`."
        ],
        [
            "Write the content of the `meta_dict` into `filename`.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file\n\n    meta_dict: dict\n        Dictionary with the fields of the metadata .mhd file"
        ],
        [
            "Write the data into a raw format file. Big endian is always used.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file\n\n    data: numpy.ndarray\n        n-dimensional image data array."
        ],
        [
            "Write the `data` and `meta_dict` in two files with names\n    that use `filename` as a prefix.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file.\n        This is going to be used as a preffix.\n        Two files will be created, one with a '.mhd' extension\n        and another with '.raw'. If `filename` has any of these already\n        they will be taken into account to build the filenames.\n\n    data: numpy.ndarray\n        n-dimensional image data array.\n\n    shape: tuple\n        Tuple describing the shape of `data`\n        Default: data.shape\n\n    meta_dict: dict\n        Dictionary with the fields of the metadata .mhd file\n        Default: {}\n\n    Returns\n    -------\n    mhd_filename: str\n        Path to the .mhd file\n\n    raw_filename: str\n        Path to the .raw file"
        ],
        [
            "Copy .mhd and .raw files to dst.\n\n    If dst is a folder, won't change the file, but if dst is another filepath,\n    will modify the ElementDataFile field in the .mhd to point to the\n    new renamed .raw file.\n\n    Parameters\n    ----------\n    src: str\n        Path to the .mhd file to be copied\n\n    dst: str\n        Path to the destination of the .mhd and .raw files.\n        If a new file name is given, the extension will be ignored.\n\n    Returns\n    -------\n    dst: str"
        ],
        [
            "SPSS .sav files to Pandas DataFrame through Rpy2\n\n    :param input_file: string\n\n    :return:"
        ],
        [
            "SPSS .sav files to Pandas DataFrame through savreader module\n\n    :param input_file: string\n\n    :return:"
        ],
        [
            "Valid extensions '.pyshelf', '.mat', '.hdf5' or '.h5'\n\n        @param filename: string\n\n        @param varnames: list of strings\n        Names of the variables\n\n        @param varlist: list of objects\n        The objects to be saved"
        ],
        [
            "Create CLI environment"
        ],
        [
            "Find all the ROIs in img and returns a similar volume with the ROIs\n    emptied, keeping only their border voxels.\n\n    This is useful for DTI tractography.\n\n    Parameters\n    ----------\n    img: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    np.ndarray\n        an array of same shape as img_data"
        ],
        [
            "Return the largest connected component of a 3D array.\n\n    Parameters\n    -----------\n    volume: numpy.array\n        3D boolean array.\n\n    Returns\n    --------\n    volume: numpy.array\n        3D boolean array with only one connected component."
        ],
        [
            "Return as mask for `volume` that includes only areas where\n    the connected components have a size bigger than `min_cluster_size`\n    in number of voxels.\n\n    Parameters\n    -----------\n    volume: numpy.array\n        3D boolean array.\n\n    min_cluster_size: int\n        Minimum size in voxels that the connected component must have.\n\n    Returns\n    --------\n    volume: numpy.array\n        3D int array with a mask excluding small connected components."
        ],
        [
            "Look for the files in filelist containing the names in roislist, these files will be opened, binarised\n    and merged in one mask.\n\n    Parameters\n    ----------\n    roislist: list of strings\n        Names of the ROIs, which will have to be in the names of the files in filelist.\n\n    filelist: list of strings\n        List of paths to the volume files containing the ROIs.\n\n    Returns\n    -------\n    numpy.ndarray\n        Mask volume"
        ],
        [
            "Return a sorted list of the non-zero unique values of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        The data array\n\n    Returns\n    -------\n    list of items of arr."
        ],
        [
            "Get the center of mass for each ROI in the given volume.\n\n    Parameters\n    ----------\n    vol: numpy ndarray\n        Volume with different values for each ROI.\n\n    Returns\n    -------\n    OrderedDict\n        Each entry in the dict has the ROI value as key and the center_of_mass coordinate as value."
        ],
        [
            "Extracts the values in `datavol` that are in the ROI with value `roivalue` in `roivol`.\n    The ROI can be masked by `maskvol`.\n\n    Parameters\n    ----------\n    datavol: numpy.ndarray\n        4D timeseries volume or a 3D volume to be partitioned\n\n    roivol: numpy.ndarray\n        3D ROIs volume\n\n    roivalue: int or float\n        A value from roivol that represents the ROI to be used for extraction.\n\n    maskvol: numpy.ndarray\n        3D mask volume\n\n    zeroe: bool\n        If true will remove the null timeseries voxels.  Only applied to timeseries (4D) data.\n\n    Returns\n    -------\n    values: np.array\n        An array of the values in the indicated ROI.\n        A 2D matrix if `datavol` is 4D or a 1D vector if `datavol` is 3D."
        ],
        [
            "Pick one 3D volume from a 4D nifti image file\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Volume defining different ROIs.\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    vol_idx: int\n        Index of the 3D volume to be extracted from the 4D volume.\n\n    Returns\n    -------\n    vol, hdr, aff\n        The data array, the image header and the affine transform matrix."
        ],
        [
            "Returns a h5py dataset given its registered name.\n\n        :param ds_name: string\n        Name of the dataset to be returned.\n\n        :return:"
        ],
        [
            "Creates a Dataset with unknown size.\n        Resize it before using.\n\n        :param ds_name: string\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py DataSet"
        ],
        [
            "Saves a Numpy array in a dataset in the HDF file, registers it as\n        ds_name and returns the h5py dataset.\n\n        :param ds_name: string\n        Registration name of the dataset to be registered.\n\n        :param data: Numpy ndarray\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py dataset"
        ],
        [
            "See create_dataset."
        ],
        [
            "Will get the names of the index colums of df, obtain their ranges from\n        range_values dict and return a reindexed version of df with the given\n        range values.\n\n        :param df: pandas DataFrame\n\n        :param range_values: dict or array-like\n        Must contain for each index column of df an entry with all the values\n        within the range of the column.\n\n        :param fill_value: scalar or 'nearest', default 0\n        Value to use for missing values. Defaults to 0, but can be any\n        \"compatible\" value, e.g., NaN.\n        The 'nearest' mode will fill the missing value with the nearest value in\n         the column.\n\n        :param fill_method:  {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n        Method to use for filling holes in reindexed DataFrame\n        'pad' / 'ffill': propagate last valid observation forward to next valid\n        'backfill' / 'bfill': use NEXT valid observation to fill gap\n\n        :return: pandas Dataframe and used column ranges\n        reindexed DataFrame and dict with index column ranges"
        ],
        [
            "Retrieve pandas object or group of Numpy ndarrays\n        stored in file\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        obj : type of object stored in file"
        ],
        [
            "Store object in HDFStore\n\n        Parameters\n        ----------\n        key : str\n\n        value : {Series, DataFrame, Panel, Numpy ndarray}\n\n        format : 'fixed(f)|table(t)', default is 'fixed'\n            fixed(f) : Fixed format\n                Fast writing/reading. Not-appendable, nor searchable\n\n            table(t) : Table format\n                Write as a PyTables Table structure which may perform worse but allow more flexible operations\n                like searching/selecting subsets of the data\n\n        append : boolean, default False\n            This will force Table format, append the input data to the\n            existing.\n\n        encoding : default None, provide an encoding for strings"
        ],
        [
            "Returns a PyTables HDF Array from df in the shape given by its index columns range values.\n\n        :param key: string object\n\n        :param df: pandas DataFrame\n\n        :param range_values: dict or array-like\n        Must contain for each index column of df an entry with all the values\n        within the range of the column.\n\n        :param loop_multiindex: bool\n        Will loop through the first index in a multiindex dataframe, extract a\n        dataframe only for one value, complete and fill the missing values and\n        store in the HDF.\n        If this is True, it will not use unstack.\n        This is as fast as unstacking.\n\n        :param unstack: bool\n        Unstack means that this will use the first index name to\n        unfold the DataFrame, and will create a group with as many datasets\n        as valus has this first index.\n        Use this if you think the filled dataframe won't fit in your RAM memory.\n        If set to False, this will transform the dataframe in memory first\n        and only then save it.\n\n        :param fill_value: scalar or 'nearest', default 0\n        Value to use for missing values. Defaults to 0, but can be any\n        \"compatible\" value, e.g., NaN.\n        The 'nearest' mode will fill the missing value with the nearest value in\n         the column.\n\n        :param fill_method:  {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n        Method to use for filling holes in reindexed DataFrame\n        'pad' / 'ffill': propagate last valid observation forward to next valid\n        'backfill' / 'bfill': use NEXT valid observation to fill gap\n\n        :return: PyTables data node"
        ],
        [
            "Set a smoothing Gaussian kernel given its FWHM in mm."
        ],
        [
            "First set_mask and the get_masked_data.\n\n        Parameters\n        ----------\n        mask_img:  nifti-like image, NeuroImage or str\n            3D mask array: True where a voxel should be used.\n            Can either be:\n            - a file path to a Nifti image\n            - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n            If niimg is a string, consider it as a path to Nifti image and\n            call nibabel.load on it. If it is an object, check if get_data()\n            and get_affine() methods are present, raise TypeError otherwise.\n\n        Returns\n        -------\n        The masked data deepcopied"
        ],
        [
            "Sets a mask img to this. So every operation to self, this mask will be taken into account.\n\n        Parameters\n        ----------\n        mask_img: nifti-like image, NeuroImage or str\n            3D mask array: True where a voxel should be used.\n            Can either be:\n            - a file path to a Nifti image\n            - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n            If niimg is a string, consider it as a path to Nifti image and\n            call nibabel.load on it. If it is an object, check if get_data()\n            and get_affine() methods are present, raise TypeError otherwise.\n\n        Note\n        ----\n        self.img and mask_file must have the same shape.\n\n        Raises\n        ------\n        FileNotFound, NiftiFilesNotCompatible"
        ],
        [
            "Return the data masked with self.mask\n\n        Parameters\n        ----------\n        data: np.ndarray\n\n        Returns\n        -------\n        masked np.ndarray\n\n        Raises\n        ------\n        ValueError if the data and mask dimensions are not compatible.\n        Other exceptions related to numpy computations."
        ],
        [
            "Set self._smooth_fwhm and then smooths the data.\n        See boyle.nifti.smooth.smooth_imgs.\n\n        Returns\n        -------\n        the smoothed data deepcopied."
        ],
        [
            "Return a vector of the masked data.\n\n        Returns\n        -------\n        np.ndarray, tuple of indices (np.ndarray), tuple of the mask shape"
        ],
        [
            "Save this object instance in outpath.\n\n        Parameters\n        ----------\n        outpath: str\n            Output file path"
        ],
        [
            "Setup logging configuration."
        ],
        [
            "Return a 3D volume from a 4D nifti image file\n\n    Parameters\n    ----------\n    filename: str\n        Path to the 4D .mhd file\n\n    vol_idx: int\n        Index of the 3D volume to be extracted from the 4D volume.\n\n    Returns\n    -------\n    vol, hdr\n        The data array and the new 3D image header."
        ],
        [
            "A wrapper for mem.cache that flushes the cache if the version\n        number of nibabel has changed."
        ],
        [
            "Saves a Nifti1Image into an HDF5 group.\n\n    Parameters\n    ----------\n    h5group: h5py Group\n        Output HDF5 file path\n\n    spatial_img: nibabel SpatialImage\n        Image to be saved\n\n    h5path: str\n        HDF5 group path where the image data will be saved.\n        Datasets will be created inside the given group path:\n        'data', 'extra', 'affine', the header information will\n        be set as attributes of the 'data' dataset."
        ],
        [
            "Saves a Nifti1Image into an HDF5 file.\n\n    Parameters\n    ----------\n    file_path: string\n        Output HDF5 file path\n\n    spatial_img: nibabel SpatialImage\n        Image to be saved\n\n    h5path: string\n        HDF5 group path where the image data will be saved.\n        Datasets will be created inside the given group path:\n        'data', 'extra', 'affine', the header information will\n        be set as attributes of the 'data' dataset.\n        Default: '/img'\n\n    append: bool\n        True if you don't want to erase the content of the file\n        if it already exists, False otherwise.\n\n    Note\n    ----\n    HDF5 open modes\n    >>> 'r' Readonly, file must exist\n    >>> 'r+' Read/write, file must exist\n    >>> 'w' Create file, truncate if exists\n    >>> 'w-' Create file, fail if exists\n    >>> 'a' Read/write if exists, create otherwise (default)"
        ],
        [
            "Transforms an H5py Attributes set to a dict.\n    Converts unicode string keys into standard strings\n    and each value into a numpy array.\n\n    Parameters\n    ----------\n    h5attrs: H5py Attributes\n\n    Returns\n    --------\n    dict"
        ],
        [
            "Returns in a list all images found under h5group.\n\n    Parameters\n    ----------\n    h5group: h5py.Group\n        HDF group\n\n    Returns\n    -------\n    list of nifti1Image"
        ],
        [
            "Inserts all given nifti files from file_list into one dataset in fname.\n    This will not check if the dimensionality of all files match.\n\n    Parameters\n    ----------\n    file_path: string\n        HDF5 file path\n\n    h5path: string\n\n    file_list: list of strings\n\n    newshape: tuple or lambda function\n        If None, it will not reshape the images.\n        If a lambda function, this lambda will receive only the shape array.\n        e.g., newshape = lambda x: (np.prod(x[0:3]), x[3])\n        If a tuple, it will try to reshape all the images with the same shape.\n        It must work for all the images in file_list.\n\n    concat_axis: int\n        Axis of concatenation after reshaping\n\n    dtype: data type\n    Dataset data type\n    If not set, will use the type of the first file.\n\n    append: bool\n\n    Raises\n    ------\n    ValueError if concat_axis is bigger than data dimensionality.\n\n    Note\n    ----\n    For now, this only works if the dataset ends up being a 2D matrix.\n    I haven't tested for multi-dimensionality concatenations."
        ],
        [
            "Generate all combinations of the elements of iterable and its subsets.\n\n    Parameters\n    ----------\n    iterable: list, set or dict or any iterable object\n\n    Returns\n    -------\n    A generator of all possible combinations of the iterable.\n\n    Example:\n    -------\n    >>> for i in treefall([1, 2, 3, 4, 5]): print(i)\n    >>> (1, 2, 3)\n    >>> (1, 2)\n    >>> (1, 3)\n    >>> (2, 3)\n    >>> (1,)\n    >>> (2,)\n    >>> (3,)\n    >>> ()"
        ],
        [
            "List existing reliable dictionaries.\n\n    List existing reliable dictionaries and respective schema for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str"
        ],
        [
            "Query Schema information for existing reliable dictionaries.\n\n    Query Schema information existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param dictionary: Name of the reliable dictionary.\n    :type dictionary: str\n    :param output_file: Optional file to save the schema."
        ],
        [
            "Query existing reliable dictionary.\n\n    Query existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param dictionary_name: Name of the reliable dictionary.\n    :type dictionary_name: str\n    :param query_string: An OData query string. For example $top=10. Check https://www.odata.org/documentation/ for more information.\n    :type query_string: str\n    :param partition_key: Optional partition key of the desired partition, either a string if named schema or int if Int64 schema\n    :type partition_id: str\n    :param partition_id: Optional partition GUID of the owning reliable dictionary.\n    :type partition_id: str\n    :param output_file: Optional file to save the schema."
        ],
        [
            "Execute create, update, delete operations on existing reliable dictionaries.\n\n    carry out create, update and delete operations on existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param output_file: input file with list of json to provide the operation information for reliable dictionaries."
        ],
        [
            "Verify arguments for select command"
        ],
        [
            "Get AAD token"
        ],
        [
            "Use openpyxl to read an Excel file."
        ],
        [
            "Return the expanded absolute path of `xl_path` if\n    if exists and 'xlrd' or 'openpyxl' depending on\n    which module should be used for the Excel file in `xl_path`.\n\n    Parameters\n    ----------\n    xl_path: str\n        Path to an Excel file\n\n    Returns\n    -------\n    xl_path: str\n        User expanded and absolute path to `xl_path`\n\n    module: str\n        The name of the module you should use to process the\n        Excel file.\n        Choices: 'xlrd', 'pyopenxl'\n\n    Raises\n    ------\n    IOError\n        If the file does not exist\n\n    RuntimError\n        If a suitable reader for xl_path is not found"
        ],
        [
            "Return the workbook from the Excel file in `xl_path`."
        ],
        [
            "Return a list with the name of the sheets in\n    the Excel file in `xl_path`."
        ],
        [
            "Return a pandas DataFrame with the concat'ed\n    content of the `sheetnames` from the Excel file in\n    `xl_path`.\n\n    Parameters\n    ----------\n    xl_path: str\n        Path to the Excel file\n\n    sheetnames: list of str\n        List of existing sheet names of `xl_path`.\n        If None, will use all sheets from `xl_path`.\n\n    add_tab_names: bool\n        If True will add a 'Tab' column which says from which\n        tab the row comes from.\n\n    Returns\n    -------\n    df: pandas.DataFrame"
        ],
        [
            "Raise an AttributeError if `df` does not have a column named as an item of\n    the list of strings `col_names`."
        ],
        [
            "Return a list of not null values from the `col_name` column of `df`."
        ],
        [
            "Return a DataFrame with the duplicated values of the column `col_name`\n    in `df`."
        ],
        [
            "Return the duplicated items in `values`"
        ],
        [
            "Convert to string all values in `data`.\n\n    Parameters\n    ----------\n    data: dict[str]->object\n\n    Returns\n    -------\n    string_data: dict[str]->str"
        ],
        [
            "Search for items in `table` that have the same field sub-set values as in `sample`.\n    Expecting it to be unique, otherwise will raise an exception.\n\n    Parameters\n    ----------\n    table: tinydb.table\n    sample: dict\n        Sample data\n\n    Returns\n    -------\n    search_result: tinydb.database.Element\n        Unique item result of the search.\n\n    Raises\n    ------\n    KeyError:\n        If the search returns for more than one entry."
        ],
        [
            "Search in `table` an item with the value of the `unique_fields` in the `sample` sample.\n    Check if the the obtained result is unique. If nothing is found will return an empty list,\n    if there is more than one item found, will raise an IndexError.\n\n    Parameters\n    ----------\n    table: tinydb.table\n\n    sample: dict\n        Sample data\n\n    unique_fields: list of str\n        Name of fields (keys) from `data` which are going to be used to build\n        a sample to look for exactly the same values in the database.\n        If None, will use every key in `data`.\n\n    Returns\n    -------\n    eid: int\n        Id of the object found with same `unique_fields`.\n        None if none is found.\n\n    Raises\n    ------\n    MoreThanOneItemError\n        If more than one example is found."
        ],
        [
            "Create a TinyDB query that looks for items that have each field in `sample` with a value\n    compared with the correspondent operation in `operators`.\n\n    Parameters\n    ----------\n    sample: dict\n        The sample data\n\n    operators: str or list of str\n        A list of comparison operations for each field value in `sample`.\n        If this is a str, will use the same operator for all `sample` fields.\n        If you want different operators for each field, remember to use an OrderedDict for `sample`.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query"
        ],
        [
            "Create a tinyDB Query object that looks for items that confirms the correspondent operator\n    from `operators` for each `field_names` field values from `data`.\n\n    Parameters\n    ----------\n    data: dict\n        The data sample\n\n    field_names: str or list of str\n        The name of the fields in `data` that will be used for the query.\n\n    operators: str or list of str\n        A list of comparison operations for each field value in `field_names`.\n        If this is a str, will use the same operator for all `field_names`.\n        If you want different operators for each field, remember to use an OrderedDict for `data`.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query"
        ],
        [
            "Create a tinyDB Query object that is the concatenation of each query in `queries`.\n    The concatenation operator is taken from `operators`.\n\n    Parameters\n    ----------\n    queries: list of tinydb.Query\n        The list of tinydb.Query to be joined.\n\n    operators: str or list of str\n        List of binary operators to join `queries` into one query.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query"
        ],
        [
            "Return the element in `table_name` with Object ID `eid`.\n        If None is found will raise a KeyError exception.\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table to look in.\n\n        eid: int\n            The Object ID of the element to look for.\n\n        Returns\n        -------\n        elem: tinydb.database.Element\n\n        Raises\n        ------\n        KeyError\n            If the element with ID `eid` is not found."
        ],
        [
            "Search in `table` an item with the value of the `unique_fields` in the `data` sample.\n        Check if the the obtained result is unique. If nothing is found will return an empty list,\n        if there is more than one item found, will raise an IndexError.\n\n        Parameters\n        ----------\n        table_name: str\n\n        sample: dict\n            Sample data\n\n        unique_fields: list of str\n            Name of fields (keys) from `data` which are going to be used to build\n            a sample to look for exactly the same values in the database.\n            If None, will use every key in `data`.\n\n        Returns\n        -------\n        eid: int\n            Id of the object found with same `unique_fields`.\n            None if none is found.\n\n        Raises\n        ------\n        MoreThanOneItemError\n            If more than one example is found."
        ],
        [
            "Return True if an item with the value of `unique_fields`\n        from `data` is unique in the table with `table_name`.\n        False if no sample is found or more than one is found.\n\n        See function `find_unique` for more details.\n\n        Parameters\n        ----------\n        table_name: str\n\n        sample: dict\n            Sample data for query\n\n        unique_fields: str or list of str\n\n        Returns\n        -------\n        is_unique: bool"
        ],
        [
            "Update the unique matching element to have a given set of fields.\n\n        Parameters\n        ----------\n        table_name: str\n\n        fields: dict or function[dict -> None]\n            new data/values to insert into the unique element\n            or a method that will update the elements.\n\n        data: dict\n            Sample data for query\n\n        cond: tinydb.Query\n            which elements to update\n\n        unique_fields: list of str\n\n        raise_if_not_found: bool\n            Will raise an exception if the element is not found for update.\n\n        Returns\n        -------\n        eid: int\n            The eid of the updated element if found, None otherwise."
        ],
        [
            "Return the number of items that match the `sample` field values\n        in table `table_name`.\n        Check function search_sample for more details."
        ],
        [
            "Check for get_data and get_affine method in an object\n\n    Parameters\n    ----------\n    obj: any object\n        Tested object\n\n    Returns\n    -------\n    is_img: boolean\n        True if get_data and get_affine methods are present and callable,\n        False otherwise."
        ],
        [
            "Get the data in the image without having a side effect on the Nifti1Image object\n\n    Parameters\n    ----------\n    img: Nifti1Image\n\n    Returns\n    -------\n    np.ndarray"
        ],
        [
            "Return the shape of img.\n\n    Paramerers\n    -----------\n    img:\n\n    Returns\n    -------\n    shape: tuple"
        ],
        [
            "Return true if one_img and another_img have the same shape.\n    False otherwise.\n    If both are nibabel.Nifti1Image will also check for affine matrices.\n\n    Parameters\n    ----------\n    one_img: nibabel.Nifti1Image or np.ndarray\n\n    another_img: nibabel.Nifti1Image  or np.ndarray\n\n    only_check_3d: bool\n        If True will check only the 3D part of the affine matrices when they have more dimensions.\n\n    Raises\n    ------\n    NiftiFilesNotCompatible"
        ],
        [
            "Return True if the affine matrix of one_img is close to the affine matrix of another_img.\n    False otherwise.\n\n    Parameters\n    ----------\n    one_img: nibabel.Nifti1Image\n\n    another_img: nibabel.Nifti1Image\n\n    only_check_3d: bool\n        If True will extract only the 3D part of the affine matrices when they have more dimensions.\n\n    Returns\n    -------\n    bool\n\n    Raises\n    ------\n    ValueError"
        ],
        [
            "Printing of img or imgs"
        ],
        [
            "Returns true if array1 and array2 have the same shapes, false\n    otherwise.\n\n    Parameters\n    ----------\n    array1: numpy.ndarray\n\n    array2: numpy.ndarray\n\n    nd_to_check: int\n        Number of the dimensions to check, i.e., if == 3 then will check only the 3 first numbers of array.shape.\n    Returns\n    -------\n    bool"
        ],
        [
            "Create a list of regex matches that result from the match_regex\n    of all file names within wd.\n    The list of files will have wd as path prefix.\n\n    @param regex: string\n    @param wd: string\n    working directory\n    @return:"
        ],
        [
            "Returns absolute paths of folders that match the regex within folder_path and\n    all its children folders.\n\n    Note: The regex matching is done using the match function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: string\n\n    Returns\n    -------\n    A list of strings."
        ],
        [
            "Creates a list of files that match the search_regex within file_dir.\n    The list of files will have file_dir as path prefix.\n\n    Parameters\n    ----------\n    @param file_dir:\n\n    @param search_regex:\n\n    Returns:\n    --------\n    List of paths to files that match the search_regex"
        ],
        [
            "Returns absolute paths of files that match the regex within file_dir and\n    all its children folders.\n\n    Note: The regex matching is done using the search function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: string\n\n    Returns\n    -------\n    A list of strings."
        ],
        [
            "Returns absolute paths of files that match the regexs within folder_path and\n    all its children folders.\n\n    This is an iterator function that will use yield to return each set of\n    file_paths in one iteration.\n\n    Will only return value if all the strings in regex match a file name.\n\n    Note: The regex matching is done using the search function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: strings\n\n    Returns\n    -------\n    A list of strings."
        ],
        [
            "Generator that loops through all absolute paths of the files within folder\n\n    Parameters\n    ----------\n    folder: str\n    Root folder start point for recursive search.\n\n    Yields\n    ------\n    fpath: str\n    Absolute path of one file in the folders"
        ],
        [
            "Uses glob to find all files or folders that match the regex\n    starting from the base_directory.\n\n    Parameters\n    ----------\n    base_directory: str\n\n    regex: str\n\n    Returns\n    -------\n    files: list"
        ],
        [
            "Append key-value pairs to msg, for display.\n\n    Parameters\n    ----------\n    msg: string\n        arbitrary message\n    kwargs: dict\n        arbitrary dictionary\n\n    Returns\n    -------\n    updated_msg: string\n        msg, with \"key: value\" appended. Only string values are appended.\n\n    Example\n    -------\n    >>> compose_err_msg('Error message with arguments...', arg_num=123, \\\n        arg_str='filename.nii', arg_bool=True)\n    'Error message with arguments...\\\\narg_str: filename.nii'\n    >>>"
        ],
        [
            "Gets a list of DICOM file absolute paths and returns a list of lists of\n    DICOM file paths. Each group contains a set of DICOM files that have\n    exactly the same headers.\n\n    Parameters\n    ----------\n    dicom_file_paths: list of str\n        List or set of DICOM file paths\n\n    header_fields: list of str\n        List of header field names to check on the comparisons of the DICOM files.\n\n    Returns\n    -------\n    dict of DicomFileSets\n        The key is one filepath representing the group (the first found)."
        ],
        [
            "Copy the DICOM file groups to folder_path. Each group will be copied into\n    a subfolder with named given by groupby_field.\n\n    Parameters\n    ----------\n    dicom_groups: boyle.dicom.sets.DicomFileSet\n\n    folder_path: str\n     Path to where copy the DICOM files.\n\n    groupby_field_name: str\n     DICOM field name. Will get the value of this field to name the group\n     folder."
        ],
        [
            "Calculates the DicomFileDistance between all files in dicom_files, using an\n    weighted Levenshtein measure between all field names in field_weights and\n    their corresponding weights.\n\n    Parameters\n    ----------\n    dicom_files: iterable of str\n        Dicom file paths\n\n    field_weights: dict of str to float\n        A dict with header field names to float scalar values, that\n        indicate a distance measure ratio for the levenshtein distance\n        averaging of all the header field names in it. e.g., {'PatientID': 1}\n\n    dist_method_cls: DicomFileDistance class\n        Distance method object to compare the files.\n        If None, the default DicomFileDistance method using Levenshtein\n        distance between the field_wieghts will be used.\n\n    kwargs: DicomFileDistance instantiation named arguments\n        Apart from the field_weitghts argument.\n\n    Returns\n    -------\n    file_dists: np.ndarray or scipy.sparse.lil_matrix of shape NxN\n        Levenshtein distances between each of the N items in dicom_files."
        ],
        [
            "Check the field values in self.dcmf1 and self.dcmf2 and returns True\n        if all the field values are the same, False otherwise.\n\n        Returns\n        -------\n        bool"
        ],
        [
            "Updates the status of the file clusters comparing the cluster\n        key files with a levenshtein weighted measure using either the\n        header_fields or self.header_fields.\n\n        Parameters\n        ----------\n        field_weights: dict of strings with floats\n            A dict with header field names to float scalar values, that indicate a distance measure\n            ratio for the levenshtein distance averaging of all the header field names in it.\n            e.g., {'PatientID': 1}"
        ],
        [
            "Thresholds a distance matrix and returns the result.\n\n        Parameters\n        ----------\n\n        dist_matrix: array_like\n        Input array or object that can be converted to an array.\n\n        perc_thr: float in range of [0,100]\n        Percentile to compute which must be between 0 and 100 inclusive.\n\n        k: int, optional\n        Diagonal above which to zero elements.\n        k = 0 (the default) is the main diagonal,\n        k < 0 is below it and k > 0 is above.\n\n        Returns\n        -------\n        array_like"
        ],
        [
            "Returns a list of 2-tuples with pairs of dicom groups that\n        are in the same folder within given depth.\n\n        Parameters\n        ----------\n        folder_depth: int\n        Path depth to check for folder equality.\n\n        Returns\n        -------\n        list of tuples of str"
        ],
        [
            "Extend the lists within the DICOM groups dictionary.\n        The indices will indicate which list have to be extended by which\n        other list.\n\n        Parameters\n        ----------\n        indices: list or tuple of 2 iterables of int, bot having the same len\n             The indices of the lists that have to be merged, both iterables\n             items will be read pair by pair, the first is the index to the\n             list that will be extended with the list of the second index.\n             The indices can be constructed with Numpy e.g.,\n             indices = np.where(square_matrix)"
        ],
        [
            "Copy the file groups to folder_path. Each group will be copied into\n        a subfolder with named given by groupby_field.\n\n        Parameters\n        ----------\n        folder_path: str\n         Path to where copy the DICOM files.\n\n        groupby_field_name: str\n         DICOM field name. Will get the value of this field to name the group\n         folder. If empty or None will use the basename of the group key file."
        ],
        [
            "Return a dictionary where the key is the group key file path and\n        the values are sets of unique values of the field name of all DICOM\n        files in the group.\n\n        Parameters\n        ----------\n        field_name: str\n         Name of the field to read from all files\n\n        field_to_use_as_key: str\n         Name of the field to get the value and use as key.\n         If None, will use the same key as the dicom_groups.\n\n        Returns\n        -------\n        Dict of sets"
        ],
        [
            "Gets a config by name.\n\n    In the case where the config name is not found, will use fallback value."
        ],
        [
            "Checks if a config value is set to a valid bool value."
        ],
        [
            "Set a config by name to a value."
        ],
        [
            "Path to certificate related files, either a single file path or a\n    tuple. In the case of no security, returns None."
        ],
        [
            "Set AAD token cache."
        ],
        [
            "Set AAD metadata."
        ],
        [
            "Set certificate usage paths"
        ],
        [
            "Returns a list with of the objects in olist that have a fieldname valued as fieldval\n\n    Parameters\n    ----------\n    olist: list of objects\n\n    fieldname: string\n\n    fieldval: anything\n\n    Returns\n    -------\n    list of objets"
        ],
        [
            "Checks whether the re module can compile the given regular expression.\n\n    Parameters\n    ----------\n    string: str\n\n    Returns\n    -------\n    boolean"
        ],
        [
            "Returns True if the given string is considered a fnmatch\n    regular expression, False otherwise.\n    It will look for\n\n    :param string: str"
        ],
        [
            "Return index of the nth match found of pattern in strings\n\n    Parameters\n    ----------\n    strings: list of str\n        List of strings\n\n    pattern: str\n        Pattern to be matched\n\n    nth: int\n        Number of times the match must happen to return the item index.\n\n    lookup_func: callable\n        Function to match each item in strings to the pattern, e.g., re.match or re.search.\n\n    Returns\n    -------\n    index: int\n        Index of the nth item that matches the pattern.\n        If there are no n matches will return -1"
        ],
        [
            "Generate a dcm2nii configuration file that disable the interactive\n    mode."
        ],
        [
            "Converts all DICOM files within `work_dir` into one or more\n    NifTi files by calling dcm2nii on this folder.\n\n    Parameters\n    ----------\n    work_dir: str\n        Path to the folder that contain the DICOM files\n\n    arguments: str\n        String containing all the flag arguments for `dcm2nii` CLI.\n\n    Returns\n    -------\n    sys_code: int\n        dcm2nii execution return code"
        ],
        [
            "Call MRICron's `dcm2nii` to convert the DICOM files inside `input_dir`\n    to Nifti and save the Nifti file in `output_dir` with a `filename` prefix.\n\n    Parameters\n    ----------\n    input_dir: str\n        Path to the folder that contains the DICOM files\n\n    output_dir: str\n        Path to the folder where to save the NifTI file\n\n    filename: str\n        Output file basename\n\n    Returns\n    -------\n    filepaths: list of str\n        List of file paths created in `output_dir`."
        ],
        [
            "Return a subset of `filepaths`. Keep only the files that have a basename longer than the\n    others with same suffix.\n    This works based on that dcm2nii appends a preffix character for each processing\n    step it does automatically in the DICOM to NifTI conversion.\n\n    Parameters\n    ----------\n    filepaths: iterable of str\n\n    Returns\n    -------\n    cleaned_paths: iterable of str"
        ],
        [
            "Transform a named tuple into a dictionary"
        ],
        [
            "Extend the within a dict of lists. The indices will indicate which\n    list have to be extended by which other list.\n\n    Parameters\n    ----------\n    adict: OrderedDict\n        An ordered dictionary of lists\n\n    indices: list or tuple of 2 iterables of int, bot having the same length\n        The indices of the lists that have to be merged, both iterables items\n         will be read pair by pair, the first is the index to the list that\n         will be extended with the list of the second index.\n         The indices can be constructed with Numpy e.g.,\n         indices = np.where(square_matrix)\n\n    pop_later: bool\n        If True will oop out the lists that are indicated in the second\n         list of indices.\n\n    copy: bool\n        If True will perform a deep copy of the input adict before\n         modifying it, hence not changing the original input.\n\n    Returns\n    -------\n    Dictionary of lists\n\n    Raises\n    ------\n    IndexError\n        If the indices are out of range"
        ],
        [
            "Return a dict of lists from a list of dicts with the same keys.\n    For each dict in list_of_dicts with look for the values of the\n    given keys and append it to the output dict.\n\n    Parameters\n    ----------\n    list_of_dicts: list of dicts\n\n    keys: list of str\n        List of keys to create in the output dict\n        If None will use all keys in the first element of list_of_dicts\n    Returns\n    -------\n    DefaultOrderedDict of lists"
        ],
        [
            "Imports the contents of filepath as a Python module.\n\n    :param filepath: string\n\n    :param mod_name: string\n    Name of the module when imported\n\n    :return: module\n    Imported module"
        ],
        [
            "Copies the files in the built file tree map\n    to despath.\n\n    :param configfile: string\n     Path to the FileTreeMap config file\n\n    :param destpath: string\n     Path to the files destination\n\n    :param overwrite: bool\n     Overwrite files if they already exist.\n\n    :param sub_node: string\n     Tree map configuration sub path.\n     Will copy only the contents within this sub-node"
        ],
        [
            "Transforms the input .sav SPSS file into other format.\n    If you don't specify an outputfile, it will use the\n    inputfile and change its extension to .csv"
        ],
        [
            "Load a Nifti mask volume.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n    -------\n    nibabel.Nifti1Image with boolean data."
        ],
        [
            "Load a Nifti mask volume and return its data matrix as boolean and affine.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n    -------\n    numpy.ndarray with dtype==bool, numpy.ndarray of affine transformation"
        ],
        [
            "Creates a binarised mask with the union of the files in filelist.\n\n    Parameters\n    ----------\n    filelist: list of img-like object or boyle.nifti.NeuroImage or str\n        List of paths to the volume files containing the ROIs.\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    ndarray of bools\n        Mask volume\n\n    Raises\n    ------\n    ValueError"
        ],
        [
            "Read a Nifti file nii_file and a mask Nifti file.\n    Returns the voxels in nii_file that are within the mask, the mask indices\n    and the mask shape.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    mask_img: img-like object or boyle.nifti.NeuroImage or str\n        3D mask array: True where a voxel should be used.\n        See img description.\n\n    Returns\n    -------\n    vol[mask_indices], mask_indices\n\n    Note\n    ----\n    nii_file and mask_file must have the same shape.\n\n    Raises\n    ------\n    NiftiFilesNotCompatible, ValueError"
        ],
        [
            "Read a Nifti file nii_file and a mask Nifti file.\n    Extract the signals in nii_file that are within the mask, the mask indices\n    and the mask shape.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    mask_img: img-like object or boyle.nifti.NeuroImage or str\n        3D mask array: True where a voxel should be used.\n        See img description.\n\n    smooth_mm: float #TBD\n        (optional) The size in mm of the FWHM Gaussian kernel to smooth the signal.\n        If True, remove_nans is True.\n\n    remove_nans: bool #TBD\n        If remove_nans is True (default), the non-finite values (NaNs and\n        infs) found in the images will be replaced by zeros.\n\n    Returns\n    -------\n    session_series, mask_data\n\n    session_series: numpy.ndarray\n        2D array of series with shape (voxel number, image number)\n\n    Note\n    ----\n    nii_file and mask_file must have the same shape.\n\n    Raises\n    ------\n    FileNotFound, NiftiFilesNotCompatible"
        ],
        [
            "Transform a given vector to a volume. This is a reshape function for\n    3D flattened and maybe masked vectors.\n\n    Parameters\n    ----------\n    arr: np.array\n        1-Dimensional array\n\n    mask: numpy.ndarray\n        Mask image. Must have 3 dimensions, bool dtype.\n\n    Returns\n    -------\n    np.ndarray"
        ],
        [
            "Transform a given vector to a volume. This is a reshape function for\n    4D flattened masked matrices where the second dimension of the matrix\n    corresponds to the original 4th dimension.\n\n    Parameters\n    ----------\n    arr: numpy.array\n        2D numpy.array\n\n    mask: numpy.ndarray\n        Mask image. Must have 3 dimensions, bool dtype.\n\n    dtype: return type\n        If None, will get the type from vector\n\n    Returns\n    -------\n    data: numpy.ndarray\n        Unmasked data.\n        Shape: (mask.shape[0], mask.shape[1], mask.shape[2], X.shape[1])"
        ],
        [
            "From the list of absolute paths to nifti files, creates a Numpy array\n    with the masked data.\n\n    Parameters\n    ----------\n    img_filelist: list of str\n        List of absolute file paths to nifti files. All nifti files must have\n        the same shape.\n\n    mask_file: str\n        Path to a Nifti mask file.\n        Should be the same shape as the files in nii_filelist.\n\n    outdtype: dtype\n        Type of the elements of the array, if not set will obtain the dtype from\n        the first nifti file.\n\n    Returns\n    -------\n    outmat:\n        Numpy array with shape N x prod(vol.shape) containing the N files as flat vectors.\n\n    mask_indices:\n        Tuple with the 3D spatial indices of the masking voxels, for reshaping\n        with vol_shape and remapping.\n\n    vol_shape:\n        Tuple with shape of the volumes, for reshaping."
        ],
        [
            "Create a client for Service Fabric APIs."
        ],
        [
            "Aggregate the rows of the DataFrame into a single value.\n\n        :param clazz: name of a class that extends class Callable\n        :type clazz: class\n        :param new_col: name of the new column\n        :type new_col: str\n        :param args: list of column names of the object that function \n        should be applied to\n        :type args: tuple\n        :return: returns a new dataframe object with the aggregated value\n        :rtype: DataFrame"
        ],
        [
            "Pipeable grouping method.\n\n    Takes either\n      - a dataframe and a tuple of strings for grouping,\n      - a tuple of strings if a dataframe has already been piped into.\n    \n    :Example:\n        \n    group(dataframe, \"column\")\n    \n    :Example:\n    \n    dataframe >> group(\"column\")\n    \n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a grouped dataframe object\n    :rtype: GroupedDataFrame"
        ],
        [
            "Pipeable aggregation method.\n    \n    Takes either \n     - a dataframe and a tuple of arguments required for aggregation,\n     - a tuple of arguments if a dataframe has already been piped into.\n    In any case one argument has to be a class that extends callable.\n\n    :Example:\n\n    aggregate(dataframe, Function, \"new_col_name\", \"old_col_name\")\n\n    :Example:\n\n    dataframe >> aggregate(Function, \"new_col_name\", \"old_col_name\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame"
        ],
        [
            "Pipeable subsetting method.\n\n    Takes either\n     - a dataframe and a tuple of arguments required for subsetting,\n     - a tuple of arguments if a dataframe has already been piped into.\n\n    :Example:\n        \n    subset(dataframe, \"column\")\n    \n    :Example:\n    \n    dataframe >> subset(\"column\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame"
        ],
        [
            "Pipeable modification method \n    \n    Takes either \n     - a dataframe and a tuple of arguments required for modification,\n     - a tuple of arguments if a dataframe has already been piped into.\n    In any case one argument has to be a class that extends callable.\n\n    :Example:\n\n    modify(dataframe, Function, \"new_col_name\", \"old_col_name\")\n    \n    :Example:\n\n    dataframe >> modify(Function, \"new_col_name\", \"old_col_name\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame"
        ],
        [
            "Escape a single character"
        ],
        [
            "Escape a string so that it only contains characters in a safe set.\n\n    Characters outside the safe list will be escaped with _%x_,\n    where %x is the hex value of the character.\n\n    If `allow_collisions` is True, occurrences of `escape_char`\n    in the input will not be escaped.\n\n    In this case, `unescape` cannot be used to reverse the transform\n    because occurrences of the escape char in the resulting string are ambiguous.\n    Only use this mode when:\n\n    1. collisions cannot occur or do not matter, and\n    2. unescape will never be called.\n\n    .. versionadded: 1.0\n        allow_collisions argument.\n        Prior to 1.0, behavior was the same as allow_collisions=False (default)."
        ],
        [
            "Unescape a string escaped with `escape`\n    \n    escape_char must be the same as that used in the call to escape."
        ],
        [
            "Determines whether this backend is allowed to send a notification to\n        the given user and notice_type."
        ],
        [
            "Returns a dictionary with the format identifier as the key. The values are\n        are fully rendered templates with the given context."
        ],
        [
            "Copy the attributes from a source object to a destination object."
        ],
        [
            "Returns DataFrameRow of the DataFrame given its index.\n\n        :param idx: the index of the row in the DataFrame.\n        :return: returns a DataFrameRow"
        ],
        [
            "The notice settings view.\n\n    Template: :template:`notification/notice_settings.html`\n\n    Context:\n\n        notice_types\n            A list of all :model:`notification.NoticeType` objects.\n\n        notice_settings\n            A dictionary containing ``column_headers`` for each ``NOTICE_MEDIA``\n            and ``rows`` containing a list of dictionaries: ``notice_type``, a\n            :model:`notification.NoticeType` object and ``cells``, a list of\n            tuples whose first value is suitable for use in forms and the second\n            value is ``True`` or ``False`` depending on a ``request.POST``\n            variable called ``form_label``, whose valid value is ``on``."
        ],
        [
            "Query Wolfram Alpha and return a Result object"
        ],
        [
            "Return list of all Pod objects in result"
        ],
        [
            "Find a node in the tree. If the node is not found it is added first and then returned.\n\n        :param args: a tuple\n        :return: returns the node"
        ],
        [
            "Returns site-specific notification language for this user. Raises\n    LanguageStoreNotAvailable if this site does not use translated\n    notifications."
        ],
        [
            "Creates a new notice.\n\n    This is intended to be how other apps create new notices.\n\n    notification.send(user, \"friends_invite_sent\", {\n        \"spam\": \"eggs\",\n        \"foo\": \"bar\",\n    )"
        ],
        [
            "A basic interface around both queue and send_now. This honors a global\n    flag NOTIFICATION_QUEUE_ALL that helps determine whether all calls should\n    be queued or not. A per call ``queue`` or ``now`` keyword argument can be\n    used to always override the default global behavior."
        ],
        [
            "Queue the notification in NoticeQueueBatch. This allows for large amounts\n    of user notifications to be deferred to a seperate process running outside\n    the webserver."
        ],
        [
            "A helper function to write lammps pair potentials to string. Assumes that\n    functions are vectorized.\n\n    Parameters\n    ----------\n    func: function\n       A function that will be evaluated for the force at each radius. Required to\n       be numpy vectorizable.\n    dfunc: function\n       Optional. A function that will be evaluated for the energy at each\n       radius. If not supplied the centered difference method will be\n       used. Required to be numpy vectorizable.\n    bounds: tuple, list\n       Optional. specifies min and max radius to evaluate the\n       potential. Default 1 length unit, 10 length unit.\n    samples: int\n       Number of points to evaluate potential. Default 1000. Note that\n       a low number of sample points will reduce accuracy.\n    tollerance: float\n       Value used to centered difference differentiation.\n    keyword: string\n       Lammps keyword to use to pair potential. This keyword will need\n       to be used in the lammps pair_coeff. Default ``PAIR``\n    filename: string\n       Optional. filename to write lammps table potential as. Default\n       ``lammps.table`` it is highly recomended to change the value.\n\n    A file for each unique pair potential is required."
        ],
        [
            "Write tersoff potential file from parameters to string\n\n    Parameters\n    ----------\n    parameters: dict\n       keys are tuple of elements with the values being the parameters length 14"
        ],
        [
            "Aggregate the rows of each group into a single value.\n\n        :param clazz: name of a class that extends class Callable\n        :type clazz: class\n        :param new_col: name of the new column\n        :type new_col: str\n        :param args: list of column names of the object that\n         function should be applied to\n        :type args: varargs\n        :return: returns a new dataframe object with the aggregated value\n        :rtype: DataFrame"
        ],
        [
            "Checks if elements of set2 are in set1.\n\n    :param set1: a set of values\n    :param set2: a set of values\n    :param warn: the error message that should be thrown\n     when the sets are NOT disjoint\n    :return: returns true no elements of set2 are in set1"
        ],
        [
            "Checks if all elements from set2 are in set1.\n\n    :param set1:  a set of values\n    :param set2:  a set of values\n    :param warn: the error message that should be thrown \n     when the sets are not containd\n    :return: returns true if all values of set2 are in set1"
        ],
        [
            "Serialize object back to XML string.\n\n        Returns:\n            str: String which should be same as original input, if everything\\\n                 works as expected."
        ],
        [
            "Parse MARC XML document to dicts, which are contained in\n        self.controlfields and self.datafields.\n\n        Args:\n            xml (str or HTMLElement): input data\n\n        Also detect if this is oai marc format or not (see elf.oai_marc)."
        ],
        [
            "Parse control fields.\n\n        Args:\n            fields (list): list of HTMLElements\n            tag_id (str):  parameter name, which holds the information, about\n                           field name this is normally \"tag\", but in case of\n                           oai_marc \"id\"."
        ],
        [
            "Parse data fields.\n\n        Args:\n            fields (list): of HTMLElements\n            tag_id (str): parameter name, which holds the information, about\n                          field name this is normally \"tag\", but in case of\n                          oai_marc \"id\"\n            sub_id (str): id of parameter, which holds informations about\n                          subfield name this is normally \"code\" but in case of\n                          oai_marc \"label\""
        ],
        [
            "This method is used mainly internally, but it can be handy if you work\n        with with raw MARC XML object and not using getters.\n\n        Args:\n            num (int): Which indicator you need (1/2).\n            is_oai (bool/None): If None, :attr:`.oai_marc` is\n                   used.\n\n        Returns:\n            str: current name of ``i1``/``ind1`` parameter based on \\\n                 :attr:`oai_marc` property."
        ],
        [
            "Return content of given `subfield` in `datafield`.\n\n        Args:\n            datafield (str): Section name (for example \"001\", \"100\", \"700\").\n            subfield (str):  Subfield name (for example \"a\", \"1\", etc..).\n            i1 (str, default None): Optional i1/ind1 parameter value, which\n               will be used for search.\n            i2 (str, default None): Optional i2/ind2 parameter value, which\n               will be used for search.\n            exception (bool): If ``True``, :exc:`~exceptions.KeyError` is\n                      raised when method couldn't found given `datafield` /\n                      `subfield`. If ``False``, blank array ``[]`` is returned.\n\n        Returns:\n            list: of :class:`.MARCSubrecord`.\n\n        Raises:\n            KeyError: If the subfield or datafield couldn't be found.\n\n        Note:\n            MARCSubrecord is practically same thing as string, but has defined\n            :meth:`.MARCSubrecord.i1` and :attr:`.MARCSubrecord.i2`\n            methods.\n\n            You may need to be able to get this, because MARC XML depends on\n            i/ind parameters from time to time (names of authors for example)."
        ],
        [
            "Get the given param from each of the DOFs for a joint."
        ],
        [
            "Set the given param for each of the DOFs for a joint."
        ],
        [
            "Given an angle and an axis, create a quaternion."
        ],
        [
            "Given a set of bodies, compute their center of mass in world coordinates."
        ],
        [
            "Set the state of this body.\n\n        Parameters\n        ----------\n        state : BodyState tuple\n            The desired state of the body."
        ],
        [
            "Set the rotation of this body using a rotation matrix.\n\n        Parameters\n        ----------\n        rotation : sequence of 9 floats\n            The desired rotation matrix for this body."
        ],
        [
            "Convert a body-relative offset to world coordinates.\n\n        Parameters\n        ----------\n        position : 3-tuple of float\n            A tuple giving body-relative offsets.\n\n        Returns\n        -------\n        position : 3-tuple of float\n            A tuple giving the world coordinates of the given offset."
        ],
        [
            "Convert a point in world coordinates to a body-relative offset.\n\n        Parameters\n        ----------\n        position : 3-tuple of float\n            A world coordinates position.\n\n        Returns\n        -------\n        offset : 3-tuple of float\n            A tuple giving the body-relative offset of the given position."
        ],
        [
            "Convert a relative body offset to world coordinates.\n\n        Parameters\n        ----------\n        offset : 3-tuple of float\n            The offset of the desired point, given as a relative fraction of the\n            size of this body. For example, offset (0, 0, 0) is the center of\n            the body, while (0.5, -0.2, 0.1) describes a point halfway from the\n            center towards the maximum x-extent of the body, 20% of the way from\n            the center towards the minimum y-extent, and 10% of the way from the\n            center towards the maximum z-extent.\n\n        Returns\n        -------\n        position : 3-tuple of float\n            A position in world coordinates of the given body offset."
        ],
        [
            "Add a force to this body.\n\n        Parameters\n        ----------\n        force : 3-tuple of float\n            A vector giving the forces along each world or body coordinate axis.\n        relative : bool, optional\n            If False, the force values are assumed to be given in the world\n            coordinate frame. If True, they are assumed to be given in the\n            body-relative coordinate frame. Defaults to False.\n        position : 3-tuple of float, optional\n            If given, apply the force at this location in world coordinates.\n            Defaults to the current position of the body.\n        relative_position : 3-tuple of float, optional\n            If given, apply the force at this relative location on the body. If\n            given, this method ignores the ``position`` parameter."
        ],
        [
            "Add a torque to this body.\n\n        Parameters\n        ----------\n        force : 3-tuple of float\n            A vector giving the torque along each world or body coordinate axis.\n        relative : bool, optional\n            If False, the torque values are assumed to be given in the world\n            coordinate frame. If True, they are assumed to be given in the\n            body-relative coordinate frame. Defaults to False."
        ],
        [
            "Connect this body to another one using a joint.\n\n        This method creates a joint to fasten this body to the other one. See\n        :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body : :class:`Body` or str, optional\n            The other body to join with this one. If not given, connects this\n            body to the world."
        ],
        [
            "Move another body next to this one and join them together.\n\n        This method will move the ``other_body`` so that the anchor points for\n        the joint coincide. It then creates a joint to fasten the two bodies\n        together. See :func:`World.move_next_to` and :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body : :class:`Body` or str\n            The other body to join with this one.\n        offset : 3-tuple of float, optional\n            The body-relative offset where the anchor for the joint should be\n            placed. Defaults to (0, 0, 0). See :func:`World.move_next_to` for a\n            description of how offsets are specified.\n        other_offset : 3-tuple of float, optional\n            The offset on the second body where the joint anchor should be\n            placed. Defaults to (0, 0, 0). Like ``offset``, this is given as an\n            offset relative to the size and shape of ``other_body``."
        ],
        [
            "List of positions for linear degrees of freedom."
        ],
        [
            "List of position rates for linear degrees of freedom."
        ],
        [
            "List of angles for rotational degrees of freedom."
        ],
        [
            "List of angle rates for rotational degrees of freedom."
        ],
        [
            "List of axes for this object's degrees of freedom."
        ],
        [
            "Set the lo stop values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        lo_stops : float or sequence of float\n            A lo stop value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians."
        ],
        [
            "Set the hi stop values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        hi_stops : float or sequence of float\n            A hi stop value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians."
        ],
        [
            "Set the target velocities for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        velocities : float or sequence of float\n            A target velocity value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians / second."
        ],
        [
            "Set the maximum forces for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        max_forces : float or sequence of float\n            A maximum force value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom."
        ],
        [
            "Set the ERP values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        erps : float or sequence of float\n            An ERP value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom."
        ],
        [
            "Set the CFM values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom."
        ],
        [
            "Set the CFM values for this object's DOF limits.\n\n        Parameters\n        ----------\n        stop_cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom limits, or a list\n            containing one such value for each degree of freedom limit."
        ],
        [
            "Set the ERP values for this object's DOF limits.\n\n        Parameters\n        ----------\n        stop_erps : float or sequence of float\n            An ERP value to set on all degrees of freedom limits, or a list\n            containing one such value for each degree of freedom limit."
        ],
        [
            "Set the linear axis of displacement for this joint.\n\n        Parameters\n        ----------\n        axes : list containing one 3-tuple of floats\n            A list of the axes for this joint. For a slider joint, which has one\n            degree of freedom, this must contain one 3-tuple specifying the X,\n            Y, and Z axis for the joint."
        ],
        [
            "Set the angular axis of rotation for this joint.\n\n        Parameters\n        ----------\n        axes : list containing one 3-tuple of floats\n            A list of the axes for this joint. For a hinge joint, which has one\n            degree of freedom, this must contain one 3-tuple specifying the X,\n            Y, and Z axis for the joint."
        ],
        [
            "A list of axes of rotation for this joint."
        ],
        [
            "Create a new body.\n\n        Parameters\n        ----------\n        shape : str\n            The \"shape\" of the body to be created. This should name a type of\n            body object, e.g., \"box\" or \"cap\".\n        name : str, optional\n            The name to use for this body. If not given, a default name will be\n            constructed of the form \"{shape}{# of objects in the world}\".\n\n        Returns\n        -------\n        body : :class:`Body`\n            The created body object."
        ],
        [
            "Create a new joint that connects two bodies together.\n\n        Parameters\n        ----------\n        shape : str\n            The \"shape\" of the joint to use for joining together two bodies.\n            This should name a type of joint, such as \"ball\" or \"piston\".\n        body_a : str or :class:`Body`\n            The first body to join together with this joint. If a string is\n            given, it will be used as the name of a body to look up in the\n            world.\n        body_b : str or :class:`Body`, optional\n            If given, identifies the second body to join together with\n            ``body_a``. If not given, ``body_a`` is joined to the world.\n        name : str, optional\n            If given, use this name for the created joint. If not given, a name\n            will be constructed of the form\n            \"{body_a.name}^{shape}^{body_b.name}\".\n\n        Returns\n        -------\n        joint : :class:`Joint`\n            The joint object that was created."
        ],
        [
            "Move one body to be near another one.\n\n        After moving, the location described by ``offset_a`` on ``body_a`` will\n        be coincident with the location described by ``offset_b`` on ``body_b``.\n\n        Parameters\n        ----------\n        body_a : str or :class:`Body`\n            The body to use as a reference for moving the other body. If this is\n            a string, it is treated as the name of a body to look up in the\n            world.\n        body_b : str or :class:`Body`\n            The body to move next to ``body_a``. If this is a string, it is\n            treated as the name of a body to look up in the world.\n        offset_a : 3-tuple of float\n            The offset of the anchor point, given as a relative fraction of the\n            size of ``body_a``. See :func:`Body.relative_offset_to_world`.\n        offset_b : 3-tuple of float\n            The offset of the anchor point, given as a relative fraction of the\n            size of ``body_b``.\n\n        Returns\n        -------\n        anchor : 3-tuple of float\n            The location of the shared point, which is often useful to use as a\n            joint anchor."
        ],
        [
            "Set the states of some bodies in the world.\n\n        Parameters\n        ----------\n        states : sequence of states\n            A complete state tuple for one or more bodies in the world. See\n            :func:`get_body_states`."
        ],
        [
            "Step the world forward by one frame.\n\n        Parameters\n        ----------\n        substeps : int, optional\n            Split the step into this many sub-steps. This helps to prevent the\n            time delta for an update from being too large."
        ],
        [
            "Determine whether the given bodies are currently connected.\n\n        Parameters\n        ----------\n        body_a : str or :class:`Body`\n            One body to test for connectedness. If this is a string, it is\n            treated as the name of a body to look up.\n        body_b : str or :class:`Body`\n            One body to test for connectedness. If this is a string, it is\n            treated as the name of a body to look up.\n\n        Returns\n        -------\n        connected : bool\n            Return True iff the two bodies are connected."
        ],
        [
            "Parse an AMC motion capture data file.\n\n    Parameters\n    ----------\n    source : file\n        A file-like object that contains AMC motion capture text.\n\n    Yields\n    ------\n    frame : dict\n        Yields a series of motion capture frames. Each frame is a dictionary\n        that maps a bone name to a list of the DOF configurations for that bone."
        ],
        [
            "Traverse the bone hierarchy and create physics bodies."
        ],
        [
            "Traverse the bone hierarchy and create physics joints."
        ],
        [
            "Parse informations about corporations from given field identified\n        by `datafield` parameter.\n\n        Args:\n            datafield (str): MARC field ID (\"``110``\", \"``610``\", etc..)\n            subfield (str):  MARC subfield ID with name, which is typically\n                             stored in \"``a``\" subfield.\n            roles (str): specify which roles you need. Set to ``[\"any\"]`` for\n                         any role, ``[\"dst\"]`` for distributors, etc.. For\n                         details, see\n                         http://www.loc.gov/marc/relators/relaterm.html\n\n        Returns:\n            list: :class:`Corporation` objects."
        ],
        [
            "Parse persons from given datafield.\n\n        Args:\n            datafield (str): code of datafield (\"010\", \"730\", etc..)\n            subfield (char):  code of subfield (\"a\", \"z\", \"4\", etc..)\n            role (list of str): set to [\"any\"] for any role, [\"aut\"] for\n                 authors, etc.. For details see\n                 http://www.loc.gov/marc/relators/relaterm.html\n\n        Main records for persons are: \"100\", \"600\" and \"700\", subrecords \"c\".\n\n        Returns:\n            list: Person objects."
        ],
        [
            "Get list of VALID ISBN.\n\n        Returns:\n            list: List with *valid* ISBN strings."
        ],
        [
            "Content of field ``856u42``. Typically URL pointing to producers\n        homepage.\n\n        Returns:\n            list: List of URLs defined by producer."
        ],
        [
            "URL's, which may point to edeposit, aleph, kramerius and so on.\n\n        Fields ``856u40``, ``998a`` and ``URLu``.\n\n        Returns:\n            list: List of internal URLs."
        ],
        [
            "r'''Create a callable that implements a PID controller.\n\n    A PID controller returns a control signal :math:`u(t)` given a history of\n    error measurements :math:`e(0) \\dots e(t)`, using proportional (P), integral\n    (I), and derivative (D) terms, according to:\n\n    .. math::\n\n       u(t) = kp * e(t) + ki * \\int_{s=0}^t e(s) ds + kd * \\frac{de(s)}{ds}(t)\n\n    The proportional term is just the current error, the integral term is the\n    sum of all error measurements, and the derivative term is the instantaneous\n    derivative of the error measurement.\n\n    Parameters\n    ----------\n    kp : float\n        The weight associated with the proportional term of the PID controller.\n    ki : float\n        The weight associated with the integral term of the PID controller.\n    kd : float\n        The weight associated with the derivative term of the PID controller.\n    smooth : float in [0, 1]\n        Derivative values will be smoothed with this exponential average. A\n        value of 1 never incorporates new derivative information, a value of 0.5\n        uses the mean of the historic and new information, and a value of 0\n        discards historic information (i.e., the derivative in this case will be\n        unsmoothed). The default is 0.1.\n\n    Returns\n    -------\n    controller : callable (float, float) -> float\n        Returns a function that accepts an error measurement and a delta-time\n        value since the previous measurement, and returns a control signal."
        ],
        [
            "Given a sequence of sequences, return a flat numpy array.\n\n    Parameters\n    ----------\n    iterables : sequence of sequence of number\n        A sequence of tuples or lists containing numbers. Typically these come\n        from something that represents each joint in a skeleton, like angle.\n\n    Returns\n    -------\n    ndarray :\n        An array of flattened data from each of the source iterables."
        ],
        [
            "Load a skeleton definition from a file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton. See :class:`pagoda.parser.Parser` for more\n            information about the format of the text file."
        ],
        [
            "Load a skeleton definition from a text file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton. See :class:`pagoda.parser.BodyParser` for\n            more information about the format of the text file."
        ],
        [
            "Load a skeleton definition from an ASF text file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton, in ASF format."
        ],
        [
            "Set PID parameters for all joints in the skeleton.\n\n        Parameters for this method are passed directly to the `pid` constructor."
        ],
        [
            "Get a list of all current joint torques in the skeleton."
        ],
        [
            "Get a list of the indices for a specific joint.\n\n        Parameters\n        ----------\n        name : str\n            The name of the joint to look up.\n\n        Returns\n        -------\n        list of int :\n            A list of the index values for quantities related to the named\n            joint. Often useful for getting, say, the angles for a specific\n            joint in the skeleton."
        ],
        [
            "Get a list of the indices for a specific body.\n\n        Parameters\n        ----------\n        name : str\n            The name of the body to look up.\n        step : int, optional\n            The number of numbers for each body. Defaults to 3, should be set\n            to 4 for body rotation (since quaternions have 4 values).\n\n        Returns\n        -------\n        list of int :\n            A list of the index values for quantities related to the named body."
        ],
        [
            "Get the current joint separations for the skeleton.\n\n        Returns\n        -------\n        distances : list of float\n            A list expressing the distance between the two joint anchor points,\n            for each joint in the skeleton. These quantities describe how\n            \"exploded\" the bodies in the skeleton are; a value of 0 indicates\n            that the constraints are perfectly satisfied for that joint."
        ],
        [
            "Enable the joint motors in this skeleton.\n\n        This method sets the maximum force that can be applied by each joint to\n        attain the desired target velocities. It also enables torque feedback\n        for all joint motors.\n\n        Parameters\n        ----------\n        max_force : float\n            The maximum force that each joint is allowed to apply to attain its\n            target velocity."
        ],
        [
            "Move each joint toward a target angle.\n\n        This method uses a PID controller to set a target angular velocity for\n        each degree of freedom in the skeleton, based on the difference between\n        the current and the target angle for the respective DOF.\n\n        PID parameters are by default set to achieve a tiny bit less than\n        complete convergence in one time step, using only the P term (i.e., the\n        P coefficient is set to 1 - \\delta, while I and D coefficients are set\n        to 0). PID parameters can be updated by calling the `set_pid_params`\n        method.\n\n        Parameters\n        ----------\n        angles : list of float\n            A list of the target angles for every joint in the skeleton."
        ],
        [
            "Add torques for each degree of freedom in the skeleton.\n\n        Parameters\n        ----------\n        torques : list of float\n            A list of the torques to add to each degree of freedom in the\n            skeleton."
        ],
        [
            "Return the names of our marker labels in canonical order."
        ],
        [
            "Load marker data from a CSV file.\n\n        The file will be imported using Pandas, which must be installed to use\n        this method. (``pip install pandas``)\n\n        The first line of the CSV file will be used for header information. The\n        \"time\" column will be used as the index for the data frame. There must\n        be columns named 'markerAB-foo-x','markerAB-foo-y','markerAB-foo-z', and\n        'markerAB-foo-c' for marker 'foo' to be included in the model.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the CSV file to load."
        ],
        [
            "Load marker data from a C3D file.\n\n        The file will be imported using the c3d module, which must be installed\n        to use this method. (``pip install c3d``)\n\n        Parameters\n        ----------\n        filename : str\n            Name of the C3D file to load.\n        start_frame : int, optional\n            Discard the first N frames. Defaults to 0.\n        max_frames : int, optional\n            Maximum number of frames to load. Defaults to loading all frames."
        ],
        [
            "Process data to produce velocity and dropout information."
        ],
        [
            "Create physics bodies corresponding to each marker in our data."
        ],
        [
            "Load attachment configuration from the given text source.\n\n        The attachment configuration file has a simple format. After discarding\n        Unix-style comments (any part of a line that starts with the pound (#)\n        character), each line in the file is then expected to have the following\n        format::\n\n            marker-name body-name X Y Z\n\n        The marker name must correspond to an existing \"channel\" in our marker\n        data. The body name must correspond to a rigid body in the skeleton. The\n        X, Y, and Z coordinates specify the body-relative offsets where the\n        marker should be attached: 0 corresponds to the center of the body along\n        the given axis, while -1 and 1 correspond to the minimal (maximal,\n        respectively) extent of the body's bounding box along the corresponding\n        dimension.\n\n        Parameters\n        ----------\n        source : str or file-like\n            A filename or file-like object that we can use to obtain text\n            configuration that describes how markers are attached to skeleton\n            bodies.\n\n        skeleton : :class:`pagoda.skeleton.Skeleton`\n            The skeleton to attach our marker data to."
        ],
        [
            "Attach marker bodies to the corresponding skeleton bodies.\n\n        Attachments are only made for markers that are not in a dropout state in\n        the given frame.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data we will use for attaching marker bodies."
        ],
        [
            "Reposition markers to a specific frame of data.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data where we should reposition marker bodies. Markers\n            will be positioned in the appropriate places in world coordinates.\n            In addition, linear velocities of the markers will be set according\n            to the data as long as there are no dropouts in neighboring frames."
        ],
        [
            "Get a list of the distances between markers and their attachments.\n\n        Returns\n        -------\n        distances : ndarray of shape (num-markers, 3)\n            Array of distances for each marker joint in our attachment setup. If\n            a marker does not currently have an associated joint (e.g. because\n            it is not currently visible) this will contain NaN for that row."
        ],
        [
            "Return an array of the forces exerted by marker springs.\n\n        Notes\n        -----\n\n        The forces exerted by the marker springs can be approximated by::\n\n          F = kp * dx\n\n        where ``dx`` is the current array of marker distances. An even more\n        accurate value is computed by approximating the velocity of the spring\n        displacement::\n\n          F = kp * dx + kd * (dx - dx_tm1) / dt\n\n        where ``dx_tm1`` is an array of distances from the previous time step.\n\n        Parameters\n        ----------\n        dx_tm1 : ndarray\n            An array of distances from markers to their attachment targets,\n            measured at the previous time step.\n\n        Returns\n        -------\n        F : ndarray\n            An array of forces that the markers are exerting on the skeleton."
        ],
        [
            "Create and configure a skeleton in our model.\n\n        Parameters\n        ----------\n        filename : str\n            The name of a file containing skeleton configuration data.\n        pid_params : dict, optional\n            If given, use this dictionary to set the PID controller\n            parameters on each joint in the skeleton. See\n            :func:`pagoda.skeleton.pid` for more information."
        ],
        [
            "Load marker data and attachment preferences into the model.\n\n        Parameters\n        ----------\n        filename : str\n            The name of a file containing marker data. This currently needs to\n            be either a .C3D or a .CSV file. CSV files must adhere to a fairly\n            strict column naming convention; see :func:`Markers.load_csv` for\n            more information.\n        attachments : str\n            The name of a text file specifying how markers are attached to\n            skeleton bodies.\n        max_frames : number, optional\n            Only read in this many frames of marker data. By default, the entire\n            data file is read into memory.\n\n        Returns\n        -------\n        markers : :class:`Markers`\n            Returns a markers object containing loaded marker data as well as\n            skeleton attachment configuration."
        ],
        [
            "Advance the physics world by one step.\n\n        Typically this is called as part of a :class:`pagoda.viewer.Viewer`, but\n        it can also be called manually (or some other stepping mechanism\n        entirely can be used)."
        ],
        [
            "Settle the skeleton to our marker data at a specific frame.\n\n        Parameters\n        ----------\n        frame_no : int, optional\n            Settle the skeleton to marker data at this frame. Defaults to 0.\n        max_distance : float, optional\n            The settling process will stop when the mean marker distance falls\n            below this threshold. Defaults to 0.1m (10cm). Setting this too\n            small prevents the settling process from finishing (it will loop\n            indefinitely), and setting it too large prevents the skeleton from\n            settling to a stable state near the markers.\n        max_iters : int, optional\n            Attempt to settle markers for at most this many iterations. Defaults\n            to 1000.\n        states : list of body states, optional\n            If given, set the bodies in our skeleton to these kinematic states\n            before starting the settling process."
        ],
        [
            "Iterate over a set of marker data, dragging its skeleton along.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start following marker data after this frame. Defaults to 0.\n        end : int, optional\n            Stop following marker data after this frame. Defaults to the end of\n            the marker data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data."
        ],
        [
            "Update the simulator to a specific frame of marker data.\n\n        This method returns a generator of body states for the skeleton! This\n        generator must be exhausted (e.g., by consuming this call in a for loop)\n        for the simulator to work properly.\n\n        This process involves the following steps:\n\n        - Move the markers to their new location:\n          - Detach from the skeleton\n          - Update marker locations\n          - Reattach to the skeleton\n        - Detect ODE collisions\n        - Yield the states of the bodies in the skeleton\n        - Advance the ODE world one step\n\n        Parameters\n        ----------\n        frame_no : int\n            Step to this frame of marker data.\n        dt : float, optional\n            Step with this time duration. Defaults to ``self.dt``.\n\n        Returns\n        -------\n        states : sequence of state tuples\n            A generator of a sequence of one body state for the skeleton. This\n            generator must be exhausted for the simulation to work properly."
        ],
        [
            "Follow a set of marker data, yielding kinematic joint angles.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start following marker data after this frame. Defaults to 0.\n        end : int, optional\n            Stop following marker data after this frame. Defaults to the end of\n            the marker data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        max_force : float, optional\n            Allow each degree of freedom in the skeleton to exert at most this\n            force when attempting to maintain its equilibrium position. This\n            defaults to 20N. Set this value higher to simulate a stiff skeleton\n            while following marker data.\n\n        Returns\n        -------\n        angles : sequence of angle frames\n            Returns a generator of joint angle data for the skeleton. One set of\n            joint angles will be generated for each frame of marker data between\n            `start` and `end`."
        ],
        [
            "Follow a set of angle data, yielding dynamic joint torques.\n\n        Parameters\n        ----------\n        angles : ndarray (num-frames x num-dofs)\n            Follow angle data provided by this array of angle values.\n        start : int, optional\n            Start following angle data after this frame. Defaults to the start\n            of the angle data.\n        end : int, optional\n            Stop following angle data after this frame. Defaults to the end of\n            the angle data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        max_force : float, optional\n            Allow each degree of freedom in the skeleton to exert at most this\n            force when attempting to follow the given joint angles. Defaults to\n            100N. Setting this value to be large results in more accurate\n            following but can cause oscillations in the PID controllers,\n            resulting in noisy torques.\n\n        Returns\n        -------\n        torques : sequence of torque frames\n            Returns a generator of joint torque data for the skeleton. One set\n            of joint torques will be generated for each frame of angle data\n            between `start` and `end`."
        ],
        [
            "Move the body according to a set of torque data."
        ],
        [
            "Sort values, but put numbers after alphabetically sorted words.\n\n    This function is here to make outputs diff-compatible with Aleph.\n\n    Example::\n        >>> sorted([\"b\", \"1\", \"a\"])\n        ['1', 'a', 'b']\n        >>> resorted([\"b\", \"1\", \"a\"])\n        ['a', 'b', '1']\n\n    Args:\n        values (iterable): any iterable object/list/tuple/whatever.\n\n    Returns:\n        list of sorted values, but with numbers after words"
        ],
        [
            "Draw all bodies in the world."
        ],
        [
            "Get room stream to listen for messages.\n\n        Kwargs:\n            error_callback (func): Callback to call when an error occurred (parameters: exception)\n            live (bool): If True, issue a live stream, otherwise an offline stream\n\n        Returns:\n            :class:`Stream`. Stream"
        ],
        [
            "Get list of users in the room.\n\n        Kwargs:\n            sort (bool): If True, sort rooms by name\n\n        Returns:\n            array. List of users"
        ],
        [
            "Set the room name.\n\n        Args:\n            name (str): Name\n\n        Returns:\n            bool. Success"
        ],
        [
            "Set the room topic.\n\n        Args:\n            topic (str): Topic\n\n        Returns:\n            bool. Success"
        ],
        [
            "Post a message.\n\n        Args:\n            message (:class:`Message` or string): Message\n\n        Returns:\n            bool. Success"
        ],
        [
            "Returns a list of paths specified by the XDG_CONFIG_DIRS environment\n        variable or the appropriate default.\n\n        The list is sorted by precedence, with the most important item coming\n        *last* (required by the existing config_resolver logic)."
        ],
        [
            "Returns the value specified in the XDG_CONFIG_HOME environment variable\n        or the appropriate default."
        ],
        [
            "Returns the filename which is effectively used by the application. If\n        overridden by an environment variable, it will return that filename."
        ],
        [
            "Check if ``filename`` can be read. Will return boolean which is True if\n        the file can be read, False otherwise."
        ],
        [
            "Searches for an appropriate config file. If found, loads the file into\n        the current instance. This method can also be used to reload a\n        configuration. Note that you may want to set ``reload`` to ``True`` to\n        clear the configuration before loading in that case.  Without doing\n        that, values will remain available even if they have been removed from\n        the config files.\n\n        :param reload: if set to ``True``, the existing values are cleared\n                       before reloading.\n        :param require_load: If set to ``True`` this will raise a\n                             :py:exc:`IOError` if no config file has been found\n                             to load."
        ],
        [
            "Get styles."
        ],
        [
            "Create a connection with given settings.\n\n        Args:\n            settings (dict): A dictionary of settings\n\n        Returns:\n            :class:`Connection`. The connection"
        ],
        [
            "Issue a PUT request.\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (dict): Dictionary of parameter and values\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n        \n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception"
        ],
        [
            "Issue a POST request.\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (dict): Dictionary of parameter and values\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n            listener (func): callback called when uploading a file\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n        \n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception"
        ],
        [
            "Issue a GET request.\n\n        Kwargs:\n            url (str): Destination URL\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n\n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception"
        ],
        [
            "Get headers.\n\n        Returns:\n            tuple: Headers"
        ],
        [
            "Get URL used for authentication\n\n        Returns:\n            string: URL"
        ],
        [
            "Parses a response.\n\n        Args:\n            text (str): Text to parse\n\n        Kwargs:\n            key (str): Key to look for, if any\n\n        Returns:\n            Parsed value\n\n        Raises:\n            ValueError"
        ],
        [
            "Build a request for twisted\n\n        Args:\n            method (str): Request method (GET/POST/PUT/DELETE/etc.) If not specified, it will be POST if post_data is not None\n            url (str): Destination URL (full, or relative)\n\n        Kwargs:\n            extra_headers (dict): Headers (override default connection headers, if any)\n            body_producer (:class:`twisted.web.iweb.IBodyProducer`): Object producing request body\n            full_url (bool): If False, URL is relative\n\n        Returns:\n            tuple. Tuple with two elements: reactor, and request"
        ],
        [
            "Issue a request.\n\n        Args:\n            method (str): Request method (GET/POST/PUT/DELETE/etc.) If not specified, it will be POST if post_data is not None\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (str): A string of what to POST\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n            listener (func): callback called when uploading a file\n            full_return (bool): If set to True, get a full response (with success, data, info, body)\n\n        Returns:\n            dict. Response. If full_return==True, a dict with keys: success, data, info, body, otherwise the parsed data\n\n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError"
        ],
        [
            "Build destination URL.\n\n        Kwargs:\n            url (str): Destination URL\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            str. URL"
        ],
        [
            "Tells if this message is a text message.\n\n        Returns:\n            bool. Success"
        ],
        [
            "Get rooms list.\n\n        Kwargs:\n            sort (bool): If True, sort rooms by name\n\n        Returns:\n            array. List of rooms (each room is a dict)"
        ],
        [
            "Get a room by name.\n\n        Returns:\n            :class:`Room`. Room\n\n        Raises:\n            RoomNotFoundException"
        ],
        [
            "Get room.\n\n        Returns:\n            :class:`Room`. Room"
        ],
        [
            "Get user.\n\n        Returns:\n            :class:`User`. User"
        ],
        [
            "Search transcripts.\n\n        Args:\n            terms (str): Terms for search\n\n        Returns:\n            array. Messages"
        ],
        [
            "Attach an observer.\n\n        Args:\n            observer (func): A function to be called when new messages arrive\n\n        Returns:\n            :class:`Stream`. Current instance to allow chaining"
        ],
        [
            "Called when incoming messages arrive.\n\n        Args:\n            messages (tuple): Messages (each message is a dict)"
        ],
        [
            "Fetch new messages."
        ],
        [
            "Called when new messages arrive.\n\n        Args:\n            messages (tuple): Messages"
        ],
        [
            "Called when a connection is made, and used to send out headers"
        ],
        [
            "Callback issued by twisted when new line arrives.\n\n        Args:\n            line (str): Incoming line"
        ],
        [
            "Process data.\n\n        Args:\n            data (str): Incoming data"
        ],
        [
            "Get a dictionary of CSL styles."
        ],
        [
            "Start producing.\n\n        Args:\n            consumer: Consumer"
        ],
        [
            "Cleanup code after asked to stop producing.\n\n        Kwargs:\n            forced (bool): If True, we were forced to stop"
        ],
        [
            "Send a block of bytes to the consumer.\n\n        Args:\n            block (str): Block of bytes"
        ],
        [
            "Returns total length for this request.\n\n        Returns:\n            int. Length"
        ],
        [
            "Build headers for each field."
        ],
        [
            "Returns the file size for given file field.\n\n        Args:\n            field (str): File field\n\n        Returns:\n            int. File size"
        ],
        [
            "Generate a path value of type result_type.\n\n    result_type can either be bytes or text_type"
        ],
        [
            "Given an ASCII str, returns a path of the given type."
        ],
        [
            "Generates a root component for a path."
        ],
        [
            "A strategy which generates filesystem path values.\n\n    The generated values include everything which the builtin\n    :func:`python:open` function accepts i.e. which won't lead to\n    :exc:`ValueError` or :exc:`TypeError` being raised.\n\n    Note that the range of the returned values depends on the operating\n    system, the Python version, and the filesystem encoding as returned by\n    :func:`sys.getfilesystemencoding`.\n\n    :param allow_pathlike:\n        If :obj:`python:None` makes the strategy include objects implementing\n        the :class:`python:os.PathLike` interface when Python >= 3.6 is used.\n        If :obj:`python:False` no pathlike objects will be generated. If\n        :obj:`python:True` pathlike will be generated (Python >= 3.6 required)\n\n    :type allow_pathlike: :obj:`python:bool` or :obj:`python:None`\n\n    .. versionadded:: 3.15"
        ],
        [
            "exec compiled code"
        ],
        [
            "replace all blocks in extends with current blocks"
        ],
        [
            "flush all buffered string into code"
        ],
        [
            "Add POST data.\n\n        Args:\n            data (dict): key => value dictionary"
        ],
        [
            "Given some error text it will log the text if self.log_errors is True\n\n        :param text: Error text to log"
        ],
        [
            "Processes the texts using TweeboParse and returns them in CoNLL format.\n\n        :param texts: The List of Strings to be processed by TweeboParse.\n        :param retry_count: The number of times it has retried for. Default\n                            0 does not require setting, main purpose is for\n                            recursion.\n        :return: A list of CoNLL formated strings.\n        :raises ServerError: Caused when the server is not running.\n        :raises :py:class:`requests.exceptions.HTTPError`: Caused when the\n                input texts is not formated correctly e.g. When you give it a\n                String not a list of Strings.\n        :raises :py:class:`json.JSONDecodeError`: Caused if after self.retries\n                attempts to parse the data it cannot decode the data.\n\n        :Example:"
        ],
        [
            "Set entity data\n\n        Args:\n            data (dict): Entity data\n            datetime_fields (array): Fields that should be parsed as datetimes"
        ],
        [
            "validates XML text"
        ],
        [
            "validates XML name"
        ],
        [
            "Prepare the actors, the world, and the messaging system to begin \n        playing the game.\n        \n        This method is guaranteed to be called exactly once upon entering the \n        game stage."
        ],
        [
            "Sequentially update the actors, the world, and the messaging system.  \n        The theater terminates once all of the actors indicate that they are done."
        ],
        [
            "Give the actors, the world, and the messaging system a chance to react \n        to the end of the game."
        ],
        [
            "Template variables."
        ],
        [
            "Use when application is starting."
        ],
        [
            "Catch a connection asyncrounosly."
        ],
        [
            "Initialize self."
        ],
        [
            "Asyncronously wait for a connection from the pool."
        ],
        [
            "Release waiters."
        ],
        [
            "Listen for an id from the server.\n\n        At the beginning of a game, each client receives an IdFactory from the \n        server.  This factory are used to give id numbers that are guaranteed \n        to be unique to tokens that created locally.  This method checks to see if such \n        a factory has been received.  If it hasn't, this method does not block \n        and immediately returns False.  If it has, this method returns True \n        after saving the factory internally.  At this point it is safe to enter \n        the GameStage."
        ],
        [
            "Respond when the server indicates that the client is out of sync.\n\n        The server can request a sync when this client sends a message that \n        fails the check() on the server.  If the reason for the failure isn't \n        very serious, then the server can decide to send it as usual in the \n        interest of a smooth gameplay experience.  When this happens, the \n        server sends out an extra response providing the clients with the\n        information they need to resync themselves."
        ],
        [
            "Manage the response when the server rejects a message.\n\n        An undo is when required this client sends a message that the server \n        refuses to pass on to the other clients playing the game.  When this \n        happens, the client must undo the changes that the message made to the \n        world before being sent or crash.  Note that unlike sync requests, undo \n        requests are only reported to the client that sent the offending \n        message."
        ],
        [
            "Relay messages from the forum on the server to the client represented \n        by this actor."
        ],
        [
            "Create a new DataItem."
        ],
        [
            "Raise an ApiUsageError if the given object is not a token that is currently \n    participating in the game.  To be participating in the game, the given \n    token must have an id number and be associated with the world."
        ],
        [
            "Iterate through each member of the class being created and add a \n        safety check to every method that isn't marked as read-only."
        ],
        [
            "Register the given callback to be called whenever the method with the \n        given name is called.  You can easily take advantage of this feature in \n        token extensions by using the @watch_token decorator."
        ],
        [
            "Clear all the internal data the token needed while it was part of \n        the world.\n\n        Note that this method doesn't actually remove the token from the \n        world.  That's what World._remove_token() does.  This method is just \n        responsible for setting the internal state of the token being removed."
        ],
        [
            "Allow tokens to modify the world for the duration of a with-block.\n\n        It's important that tokens only modify the world at appropriate times, \n        otherwise the changes they make may not be communicated across the \n        network to other clients.  To help catch and prevent these kinds of \n        errors, the game engine keeps the world locked most of the time and \n        only briefly unlocks it (using this method) when tokens are allowed to \n        make changes.  When the world is locked, token methods that aren't \n        marked as being read-only can't be called.  When the world is unlocked, \n        any token method can be called.  These checks can be disabled by \n        running python with optimization enabled.\n\n        You should never call this method manually from within your own game.  \n        This method is intended to be used by the game engine, which was \n        carefully designed to allow the world to be modified only when safe.  \n        Calling this method yourself disables an important safety check."
        ],
        [
            "Converts XML tree to event generator"
        ],
        [
            "Converts events stream into lXML tree"
        ],
        [
            "Parses file content into events stream"
        ],
        [
            "selects sub-tree events"
        ],
        [
            "merges each run of successive text events into one text event"
        ],
        [
            "locates ENTER peer for each EXIT object. Convenient when selectively\n    filtering out XML markup"
        ],
        [
            "construct BusinessDate instance from datetime.date instance,\n        raise ValueError exception if not possible\n\n        :param datetime.date datetime_date: calendar day\n        :return bool:"
        ],
        [
            "construct datetime.date instance represented calendar date of BusinessDate instance\n\n        :return datetime.date:"
        ],
        [
            "addition of a period object\n\n        :param BusinessDate d:\n        :param p:\n        :type p: BusinessPeriod or str\n        :param list holiday_obj:\n        :return bankdate:"
        ],
        [
            "addition of a number of months\n\n        :param BusinessDate d:\n        :param int month_int:\n        :return bankdate:"
        ],
        [
            "private method for the addition of business days, used in the addition of a BusinessPeriod only\n\n        :param BusinessDate d:\n        :param int days_int:\n        :param list holiday_obj:\n        :return: BusinessDate"
        ],
        [
            "Parses as much as possible until it encounters a matching closing quote.\n    \n    By default matches any_token, but can be provided with a more specific parser if required.\n    Returns a string"
        ],
        [
            "returns number of days for the given year and month\n\n    :param int year: calendar year\n    :param int month: calendar month\n    :return int:"
        ],
        [
            "Initialize the application."
        ],
        [
            "Register connection's middleware and prepare self database."
        ],
        [
            "Close all connections."
        ],
        [
            "Register a model in self."
        ],
        [
            "Manage a database connection."
        ],
        [
            "Write your migrations here.\n\n    > Model = migrator.orm['name']\n\n    > migrator.sql(sql)\n    > migrator.create_table(Model)\n    > migrator.drop_table(Model, cascade=True)\n    > migrator.add_columns(Model, **fields)\n    > migrator.change_columns(Model, **fields)\n    > migrator.drop_columns(Model, *field_names, cascade=True)\n    > migrator.rename_column(Model, old_field_name, new_field_name)\n    > migrator.rename_table(Model, new_table_name)\n    > migrator.add_index(Model, *col_names, unique=False)\n    > migrator.drop_index(Model, index_name)\n    > migrator.add_not_null(Model, field_name)\n    > migrator.drop_not_null(Model, field_name)\n    > migrator.add_default(Model, field_name, default)"
        ],
        [
            "Runs a series of parsers in sequence passing the result of each parser to the next.\n    The result of the last parser is returned."
        ],
        [
            "Returns the current token if is found in the collection provided.\n    \n    Fails otherwise."
        ],
        [
            "Returns the current token if it is not found in the collection provided.\n    \n    The negative of one_of."
        ],
        [
            "Returns the current token if it satisfies the guard function provided.\n    \n    Fails otherwise.\n    This is the a generalisation of one_of."
        ],
        [
            "Succeeds if the given parser cannot consume input"
        ],
        [
            "Applies the parser to input zero or more times.\n    \n    Returns a list of parser results."
        ],
        [
            "Consumes as many of these as it can until it term is encountered.\n    \n    Returns a tuple of the list of these results and the term result"
        ],
        [
            "Like many_until but must consume at least one of these."
        ],
        [
            "Like sep but must consume at least one of parser."
        ],
        [
            "fills the internal buffer from the source iterator"
        ],
        [
            "Advances to and returns the next token or returns EndOfFile"
        ],
        [
            "Run a game being developed with the kxg game engine.\n\nUsage:\n    {exe_name} sandbox [<num_ais>] [-v...]\n    {exe_name} client [--host HOST] [--port PORT] [-v...]\n    {exe_name} server <num_guis> [<num_ais>] [--host HOST] [--port PORT] [-v...] \n    {exe_name} debug <num_guis> [<num_ais>] [--host HOST] [--port PORT] [-v...]\n    {exe_name} --help\n\nCommands:\n    sandbox\n        Play a single-player game with the specified number of AIs.  None of \n        the multiplayer machinery will be used.\n\n    client\n        Launch a client that will try to connect to a server on the given host \n        and port.  Once it connects and the game starts, the client will allow \n        you to play the game against any other connected clients.\n\n    server\n        Launch a server that will manage a game between the given number of \n        human and AI players.  The human players must connect using this \n        command's client mode.\n\n    debug\n        Debug a multiplayer game locally.  This command launches a server and \n        the given number of clients all in different processes, and configures \n        the logging system such that the output from each process can be easily \n        distinguished.\n\nArguments:\n    <num_guis>\n        The number of human players that will be playing the game.  Only needed \n        by commands that will launch some sort of multiplayer server.\n\n    <num_ais>\n        The number of AI players that will be playing the game.  Only needed by \n        commands that will launch single-player games or multiplayer servers.\n\nOptions:\n    -x --host HOST          [default: {default_host}]\n        The address of the machine running the server.  Must be accessible from \n        the machines running the clients.\n\n    -p --port PORT          [default: {default_port}]\n        The port that the server should listen on.  Don't specify a value less \n        than 1024 unless the server is running with root permissions.\n\n    -v --verbose \n        Have the game engine log more information about what it's doing.  You \n        can specify this option several times to get more and more information.\n\nThis command is provided so that you can start writing your game with the least \npossible amount of boilerplate code.  However, the clients and servers provided \nby this command are not capable of running a production game.  Once you have \nwritten your game and want to give it a polished set of menus and options, \nyou'll have to write new Stage subclasses encapsulating that logic and you'll \nhave to call those stages yourself by interacting more directly with the \nTheater class.  The online documentation has more information on this process."
        ],
        [
            "Poll the queues that the worker can use to communicate with the \n        supervisor, until all the workers are done and all the queues are \n        empty.  Handle messages as they appear."
        ],
        [
            "Return database field type."
        ],
        [
            "Parse value from database."
        ],
        [
            "Parse the fsapi endpoint from the device url."
        ],
        [
            "Create a session on the frontier silicon device."
        ],
        [
            "Execute a frontier silicon API call."
        ],
        [
            "Helper method for setting a value by using the fsapi API."
        ],
        [
            "Helper method for fetching a text value."
        ],
        [
            "Helper method for fetching a integer value."
        ],
        [
            "Helper method for fetching a long value. Result is integer."
        ],
        [
            "Check if the device is on."
        ],
        [
            "Power on or off the device."
        ],
        [
            "Get the modes supported by this device."
        ],
        [
            "Read the maximum volume level of the device."
        ],
        [
            "Check if the device is muted."
        ],
        [
            "Mute or unmute the device."
        ],
        [
            "Get the play status of the device."
        ],
        [
            "Get the equaliser modes supported by this device."
        ],
        [
            "Set device sleep timer."
        ],
        [
            "Assumes that start and stop are already in 'buffer' coordinates. value is a byte iterable.\n        value_len is fractional."
        ],
        [
            "Parse genotype from VCF line data"
        ],
        [
            "toIndex - An optional method which will return the value prepped for index.\n\n\t\t\tBy default, \"toStorage\" will be called. If you provide \"hashIndex=True\" on the constructor,\n\t\t\tthe field will be md5summed for indexing purposes. This is useful for large strings, etc."
        ],
        [
            "copy - Create a copy of this IRField.\n\n\t\t\t  Each subclass should implement this, as you'll need to pass in the args to constructor.\n\n\t\t\t@return <IRField (or subclass)> - Another IRField that has all the same values as this one."
        ],
        [
            "objHasUnsavedChanges - Check if any object has unsaved changes, cascading."
        ],
        [
            "Check that a value has a certain JSON type.\n\n    Raise TypeError if the type does not match.\n\n    Supported types: str, int, float, bool, list, dict, and None.\n    float will match any number, int will only match numbers without\n    fractional part.\n\n    The special type JList(x) will match a list value where each\n    item is of type x:\n\n    >>> assert_json_type([1, 2, 3], JList(int))"
        ],
        [
            "Load json or yaml data from file handle.\n\n        Args:\n            fh (file): File handle to load from.\n\n        Examlple:\n            >>> with open('data.json', 'r') as json:\n            >>>    jsdata = composite.load(json)\n            >>>\n            >>> with open('data.yml', 'r') as yml:\n            >>>    ymldata = composite.load(yml)"
        ],
        [
            "Load json from file handle.\n\n        Args:\n            fh (file): File handle to load from.\n\n        Examlple:\n            >>> with open('data.json', 'r') as json:\n            >>>    data = composite.load(json)"
        ],
        [
            "Recursively compute intersection of data. For dictionaries, items\n        for specific keys will be reduced to unique items. For lists, items\n        will be reduced to unique items. This method is meant to be analogous\n        to set.intersection for composite objects.\n\n        Args:\n            other (composite): Other composite object to intersect with.\n            recursive (bool): Whether or not to perform the operation recursively,\n                for all nested composite objects."
        ],
        [
            "Recursively compute union of data. For dictionaries, items\n        for specific keys will be combined into a list, depending on the\n        status of the overwrite= parameter. For lists, items will be appended\n        and reduced to unique items. This method is meant to be analogous\n        to set.union for composite objects.\n\n        Args:\n            other (composite): Other composite object to union with.\n            recursive (bool): Whether or not to perform the operation recursively,\n                for all nested composite objects.\n            overwrite (bool): Whether or not to overwrite entries with the same\n                key in a nested dictionary."
        ],
        [
            "Append to object, if object is list."
        ],
        [
            "Extend list from object, if object is list."
        ],
        [
            "Write composite object to file handle in JSON format.\n\n        Args:\n            fh (file): File handle to write to.\n            pretty (bool): Sort keys and indent in output."
        ],
        [
            "Return list of files in filetree."
        ],
        [
            "Prune leaves of filetree according to specified\n        regular expression.\n\n        Args:\n            regex (str): Regular expression to use in pruning tree."
        ],
        [
            "Returns the value this reference is pointing to. This method uses 'ctx' to resolve the reference and return\n        the value this reference references.\n        If the call was already made, it returns a cached result.\n        It also makes sure there's no cyclic reference, and if so raises CyclicReferenceError."
        ],
        [
            "delete - Delete all objects in this list.\n\n\t\t\t@return <int> - Number of objects deleted"
        ],
        [
            "save - Save all objects in this list"
        ],
        [
            "reload - Reload all objects in this list. \n\t\t\t\tUpdates in-place. To just fetch all these objects again, use \"refetch\"\n\n\t\t\t@return - List (same order as current objects) of either exception (KeyError) if operation failed,\n\t\t\t  or a dict of fields changed -> (old, new)"
        ],
        [
            "refetch - Fetch a fresh copy of all items in this list.\n\t\t\t\tReturns a new list. To update in-place, use \"reload\".\n\n\t\t\t@return IRQueryableList<IndexedRedisModel> - List of fetched items"
        ],
        [
            "Renders as a str"
        ],
        [
            "Returns the elements HTML start tag"
        ],
        [
            "Returns a repr of an object and falls back to a minimal representation of type and ID if the call to repr raised\n    an error.\n\n    :param obj: object to safe repr\n    :returns: repr string or '(type<id> repr error)' string\n    :rtype: str"
        ],
        [
            "Match a genome VCF to variants in the ClinVar VCF file\n\n    Acts as a generator, yielding tuples of:\n    (ClinVarVCFLine, ClinVarAllele, zygosity)\n\n    'zygosity' is a string and corresponds to the genome's zygosity for that\n    ClinVarAllele. It can be either: 'Het' (heterozygous), 'Hom' (homozygous),\n    or 'Hem' (hemizygous, e.g. X chromosome in XY individuals)."
        ],
        [
            "Return Allele data as dict object."
        ],
        [
            "Create list of Alleles from VCF line data"
        ],
        [
            "Parse the VCF info field"
        ],
        [
            "Dict representation of parsed VCF data"
        ],
        [
            "Very lightweight parsing of a vcf line to get position.\n\n        Returns a dict containing:\n        'chrom': index of chromosome (int), indicates sort order\n        'pos': position on chromosome (int)"
        ],
        [
            "_toStorage - Convert the value to a string representation for storage.\n\n\t\t\t@param value - The value of the item to convert\n\t\t\t@return A string value suitable for storing."
        ],
        [
            "Navigate an open ftplib.FTP to appropriate directory for ClinVar VCF files.\n\n    Args:\n        ftp:   (type: ftplib.FTP) an open connection to ftp.ncbi.nlm.nih.gov\n        build: (type: string) genome build, either 'b37' or 'b38'"
        ],
        [
            "Return ClinVarAllele data as dict object."
        ],
        [
            "Parse frequency data in ClinVar VCF"
        ],
        [
            "Parse alleles for ClinVar VCF, overrides parent method."
        ],
        [
            "Returns back a class decorator that enables registering Blox to this factory"
        ],
        [
            "Decorator for warning user of depricated functions before use.\n\n    Args:\n        newmethod (str): Name of method to use instead."
        ],
        [
            "setDefaultRedisConnectionParams - Sets the default parameters used when connecting to Redis.\n\n\t\t  This should be the args to redis.Redis in dict (kwargs) form.\n\n\t\t  @param connectionParams <dict> - A dict of connection parameters.\n\t\t    Common keys are:\n\n\t\t       host <str> - hostname/ip of Redis server (default '127.0.0.1')\n\t\t       port <int> - Port number\t\t\t(default 6379)\n\t\t       db  <int>  - Redis DB number\t\t(default 0)\n\n\t\t   Omitting any of those keys will ensure the default value listed is used.\n\n\t\t  This connection info will be used by default for all connections to Redis, unless explicitly set otherwise.\n\t\t  The common way to override is to define REDIS_CONNECTION_PARAMS on a model, or use AltConnectedModel = MyModel.connectAlt( PARAMS )\n\n\t\t  Any omitted fields in these connection overrides will inherit the value from the global default.\n\n\t\t  For example, if your global default connection params define host = 'example.com', port=15000, and db=0, \n\t\t    and then one of your models has\n\t\t       \n\t\t       REDIS_CONNECTION_PARAMS = { 'db' : 1 }\n\t\t    \n\t\t    as an attribute, then that model's connection will inherit host='example.com\" and port=15000 but override db and use db=1\n\n\n\t\t    NOTE: Calling this function will clear the connection_pool attribute of all stored managed connections, disconnect all managed connections,\n\t\t      and close-out the connection pool.\n\t\t     It may not be safe to call this function while other threads are potentially hitting Redis (not that it would make sense anyway...)\n\n\t\t     @see clearRedisPools   for more info"
        ],
        [
            "clearRedisPools - Disconnect all managed connection pools, \n\t\t   and clear the connectiobn_pool attribute on all stored managed connection pools.\n\n\t\t   A \"managed\" connection pool is one where REDIS_CONNECTION_PARAMS does not define the \"connection_pool\" attribute.\n\t\t   If you define your own pools, IndexedRedis will use them and leave them alone.\n\n\t\t  This method will be called automatically after calling setDefaultRedisConnectionParams.\n\n\t\t  Otherwise, you shouldn't have to call it.. Maybe as some sort of disaster-recovery call.."
        ],
        [
            "getRedisPool - Returns and possibly also creates a Redis connection pool\n\t\t\tbased on the REDIS_CONNECTION_PARAMS passed in.\n\n\t\t\tThe goal of this method is to keep a small connection pool rolling\n\t\t\tto each unique Redis instance, otherwise during network issues etc\n\t\t\tpython-redis will leak connections and in short-order can exhaust\n\t\t\tall the ports on a system. There's probably also some minor\n\t\t\tperformance gain in sharing Pools.\n\n\t\t\tWill modify \"params\", if \"host\" and/or \"port\" are missing, will fill\n\t\t\tthem in with defaults, and prior to return will set \"connection_pool\"\n\t\t\ton params, which will allow immediate return on the next call,\n\t\t\tand allow access to the pool directly from the model object.\n\n\t\t\t@param params <dict> - REDIS_CONNECTION_PARAMS - kwargs to redis.Redis\n\n\t\t\t@return redis.ConnectionPool corrosponding to this unique server."
        ],
        [
            "pprint - Pretty-print a dict representation of this object.\n\n\t\t\t@param stream <file/None> - Either a stream to output, or None to default to sys.stdout"
        ],
        [
            "hasUnsavedChanges - Check if any unsaved changes are present in this model, or if it has never been saved.\n\n\t\t\t@param cascadeObjects <bool> default False, if True will check if any foreign linked objects themselves have unsaved changes (recursively).\n\t\t\t\tOtherwise, will just check if the pk has changed.\n\n\t\t\t@return <bool> - True if any fields have changed since last fetch, or if never saved. Otherwise, False"
        ],
        [
            "diff - Compare the field values on two IndexedRedisModels.\n\n\t\t\t@param firstObj <IndexedRedisModel instance> - First object (or self)\n\n\t\t\t@param otherObj <IndexedRedisModel instance> - Second object\n\n\t\t\t@param includeMeta <bool> - If meta information (like pk) should be in the diff results.\n\n\n\t\t\t@return <dict> - Dict of  'field' : ( value_firstObjForField, value_otherObjForField ).\n\t\t\t\t\n\t\t\t\tKeys are names of fields with different values.\n\t\t\t\tValue is a tuple of ( value_firstObjForField, value_otherObjForField )\n\n\t\t\tCan be called statically, like: IndexedRedisModel.diff ( obj1, obj2 )\n\n\t\t\t  or in reference to an obj   : obj1.diff(obj2)"
        ],
        [
            "save - Save this object.\n\t\t\t\n\t\t\tWill perform an \"insert\" if this object had not been saved before,\n\t\t\t  otherwise will update JUST the fields changed on THIS INSTANCE of the model.\n\n\t\t\t  i.e. If you have two processes fetch the same object and change different fields, they will not overwrite\n\t\t\t  eachother, but only save the ones each process changed.\n\n\t\t\tIf you want to save multiple objects of type MyModel in a single transaction,\n\t\t\tand you have those objects in a list, myObjs, you can do the following:\n\n\t\t\t\tMyModel.saver.save(myObjs)\n\n\t\t\t@param cascadeSave <bool> Default True - If True, any Foreign models linked as attributes that have been altered\n\t\t\t   or created will be saved with this object. If False, only this object (and the reference to an already-saved foreign model) will be saved.\n\n\t\t\t@see #IndexedRedisSave.save\n\n\t\t\t@return <list> - Single element list, id of saved object (if successful)"
        ],
        [
            "hasSameValues - Check if this and another model have the same fields and values.\n\n\t\t\tThis does NOT include id, so the models can have the same values but be different objects in the database.\n\n\t\t\t@param other <IndexedRedisModel> - Another model\n\n\t\t\t@param cascadeObject <bool> default True - If True, foreign link values with changes will be considered a difference.\n\t\t\t\tOtherwise, only the immediate values are checked.\n\n\t\t\t@return <bool> - True if all fields have the same value, otherwise False"
        ],
        [
            "copy - Copies this object.\n\n                    @param copyPrimaryKey <bool> default False - If True, any changes to the copy will save over-top the existing entry in Redis.\n                        If False, only the data is copied, and nothing is saved.\n\n\t\t    @param copyValues <bool> default False - If True, every field value on this object will be explicitly copied. If False,\n\t\t      an object will be created with the same values, and depending on the type may share the same reference.\n\t\t      \n\t\t      This is the difference between a copy and a deepcopy.\n\n\t            @return <IndexedRedisModel> - Copy of this object, per above\n\n\t\t    If you need a copy that IS linked, @see IndexedRedisModel.copy"
        ],
        [
            "saveToExternal - Saves this object to a different Redis than that specified by REDIS_CONNECTION_PARAMS on this model.\n\n\t\t\t@param redisCon <dict/redis.Redis> - Either a dict of connection params, a la REDIS_CONNECTION_PARAMS, or an existing Redis connection.\n\t\t\t\tIf you are doing a lot of bulk copies, it is recommended that you create a Redis connection and pass it in rather than establish a new\n\t\t\t\tconnection with each call.\n\n\t\t\t@note - You will generate a new primary key relative to the external Redis environment. If you need to reference a \"shared\" primary key, it is better\n\t\t\t\t\tto use an indexed field than the internal pk."
        ],
        [
            "reload - Reload this object from the database, overriding any local changes and merging in any updates.\n\n\n\t\t    @param cascadeObjects <bool> Default True. If True, foreign-linked objects will be reloaded if their values have changed\n\t\t      since last save/fetch. If False, only if the pk changed will the foreign linked objects be reloaded.\n\n                    @raises KeyError - if this object has not been saved (no primary key)\n\n                    @return - Dict with the keys that were updated. Key is field name that was updated,\n\t\t       and value is tuple of (old value, new value). \n\n\t\t    NOTE: Currently, this will cause a fetch of all Foreign Link objects, one level"
        ],
        [
            "copyModel - Copy this model, and return that copy.\n\n\t\t\t  The copied model will have all the same data, but will have a fresh instance of the FIELDS array and all members,\n\t\t\t    and the INDEXED_FIELDS array.\n\t\t\t  \n\t\t\t  This is useful for converting, like changing field types or whatever, where you can load from one model and save into the other.\n\n\t\t\t@return <IndexedRedisModel> - A copy class of this model class with a unique name."
        ],
        [
            "connectAlt - Create a class of this model which will use an alternate connection than the one specified by REDIS_CONNECTION_PARAMS on this model.\n\n\t\t\t@param redisConnectionParams <dict> - Dictionary of arguments to redis.Redis, same as REDIS_CONNECTION_PARAMS.\n\n\t\t\t@return - A class that can be used in all the same ways as the existing IndexedRedisModel, but that connects to a different instance.\n\n\t\t\t  The fields and key will be the same here, but the connection will be different. use #copyModel if you want an independent class for the model"
        ],
        [
            "_get_new_connection - Get a new connection\n\t\t\tinternal"
        ],
        [
            "_get_connection - Maybe get a new connection, or reuse if passed in.\n\t\t\t\tWill share a connection with a model\n\t\t\tinternal"
        ],
        [
            "_add_id_to_keys - Adds primary key to table\n\t\t\tinternal"
        ],
        [
            "_rem_id_from_keys - Remove primary key from table\n\t\t\tinternal"
        ],
        [
            "_add_id_to_index - Adds an id to an index\n\t\t\tinternal"
        ],
        [
            "_rem_id_from_index - Removes an id from an index\n\t\t\tinternal"
        ],
        [
            "_get_key_for_index - Returns the key name that would hold the indexes on a value\n\t\t\tInternal - does not validate that indexedFields is actually indexed. Trusts you. Don't let it down.\n\n\t\t\t@param indexedField - string of field name\n\t\t\t@param val - Value of field\n\n\t\t\t@return - Key name string, potentially hashed."
        ],
        [
            "_compat_rem_str_id_from_index - Used in compat_convertHashedIndexes to remove the old string repr of a field,\n\t\t\t\tin order to later add the hashed value,"
        ],
        [
            "_peekNextID - Look at, but don't increment the primary key for this model.\n\t\t\t\tInternal.\n\n\t\t\t@return int - next pk"
        ],
        [
            "Internal for handling filters; the guts of .filter and .filterInline"
        ],
        [
            "count - gets the number of records matching the filter criteria\n\n\t\t\tExample:\n\t\t\t\ttheCount = Model.objects.filter(field1='value').count()"
        ],
        [
            "exists - Tests whether a record holding the given primary key exists.\n\n\t\t\t@param pk - Primary key (see getPk method)\n\n\t\t\tExample usage: Waiting for an object to be deleted without fetching the object or running a filter. \n\n\t\t\tThis is a very cheap operation.\n\n\t\t\t@return <bool> - True if object with given pk exists, otherwise False"
        ],
        [
            "getPrimaryKeys - Returns all primary keys matching current filterset.\n\n\t\t\t@param sortByAge <bool> - If False, return will be a set and may not be ordered.\n\t\t\t\tIf True, return will be a list and is guarenteed to represent objects oldest->newest\n\n\t\t\t@return <set> - A set of all primary keys associated with current filters."
        ],
        [
            "all - Get the underlying objects which match the filter criteria.\n\n\t\t\tExample:   objs = Model.objects.filter(field1='value', field2='value2').all()\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@return - Objects of the Model instance associated with this query."
        ],
        [
            "allOnlyFields - Get the objects which match the filter criteria, only fetching given fields.\n\n\t\t\t@param fields - List of fields to fetch\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\n\t\t\t@return - Partial objects with only the given fields fetched"
        ],
        [
            "allOnlyIndexedFields - Get the objects which match the filter criteria, only fetching indexed fields.\n\n\t\t\t@return - Partial objects with only the indexed fields fetched"
        ],
        [
            "Random - Returns a random record in current filterset.\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@return - Instance of Model object, or None if no items math current filters"
        ],
        [
            "delete - Deletes all entries matching the filter criteria"
        ],
        [
            "get - Get a single value with the internal primary key.\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@param pk - internal primary key (can be found via .getPk() on an item)"
        ],
        [
            "_doCascadeFetch - Takes an object and performs a cascading fetch on all foreign links, and all theirs, and so on.\n\n\t\t\t@param obj <IndexedRedisModel> - A fetched model"
        ],
        [
            "getMultiple - Gets multiple objects with a single atomic operation\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@param pks - list of internal keys"
        ],
        [
            "getOnlyFields - Gets only certain fields from a paticular primary key. For working on entire filter set, see allOnlyFields\n\n\t\t\t@param pk <int> - Primary Key\n\n\t\t\t@param fields list<str> - List of fields\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\n\t\t\treturn - Partial objects with only fields applied"
        ],
        [
            "getMultipleOnlyFields - Gets only certain fields from a list of  primary keys. For working on entire filter set, see allOnlyFields\n\n\t\t\t@param pks list<str> - Primary Keys\n\n\t\t\t@param fields list<str> - List of fields\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\treturn - List of partial objects with only fields applied"
        ],
        [
            "compat_convertHashedIndexes - Reindex fields, used for when you change the propery \"hashIndex\" on one or more fields.\n\n\t\t\tFor each field, this will delete both the hash and unhashed keys to an object, \n\t\t\t  and then save a hashed or unhashed value, depending on that field's value for \"hashIndex\".\n\n\t\t\tFor an IndexedRedisModel class named \"MyModel\", call as \"MyModel.objects.compat_convertHashedIndexes()\"\n\n\t\t\tNOTE: This works one object at a time (regardless of #fetchAll), so that an unhashable object does not trash all data.\n\n\t\t\tThis method is intended to be used while your application is offline,\n\t\t\t  as it doesn't make sense to be changing your model while applications are actively using it.\n\n\t\t\t@param fetchAll <bool>, Default True - If True, all objects will be fetched first, then converted.\n\t\t\t  This is generally what you want to do, as it is more efficient. If you are memory contrainted,\n\t\t\t  you can set this to \"False\", and it will fetch one object at a time, convert it, and save it back."
        ],
        [
            "_doSave - Internal function to save a single object. Don't call this directly. \n\t\t\t            Use \"save\" instead.\n\n\t\t\t  If a pipeline is provided, the operations (setting values, updating indexes, etc)\n\t\t\t    will be queued into that pipeline.\n\t\t\t  Otherwise, everything will be executed right away.\n\n\t\t\t  @param obj - Object to save\n\t\t\t  @param isInsert - Bool, if insert or update. Either way, obj._id is expected to be set.\n\t\t\t  @param conn - Redis connection\n\t\t\t  @param pipeline - Optional pipeline, if present the items will be queued onto it. Otherwise, go directly to conn."
        ],
        [
            "compat_convertHashedIndexes - Reindex all fields for the provided objects, where the field value is hashed or not.\n\t\t\tIf the field is unhashable, do not allow.\n\n\t\t\tNOTE: This works one object at a time. It is intended to be used while your application is offline,\n\t\t\t  as it doesn't make sense to be changing your model while applications are actively using it.\n\n\t\t\t@param objs <IndexedRedisModel objects to convert>\n\t\t\t@param conn <redis.Redis or None> - Specific Redis connection or None to reuse."
        ],
        [
            "deleteOne - Delete one object\n\n\t\t\t@param obj - object to delete\n\t\t\t@param conn - Connection to reuse, or None\n\n\t\t\t@return - number of items deleted (0 or 1)"
        ],
        [
            "deleteByPk - Delete object associated with given primary key"
        ],
        [
            "deleteMultiple - Delete multiple objects\n\n\t\t\t@param objs - List of objects\n\n\t\t\t@return - Number of objects deleted"
        ],
        [
            "deleteMultipleByPks - Delete multiple objects given their primary keys\n\n\t\t\t@param pks - List of primary keys\n\n\t\t\t@return - Number of objects deleted"
        ],
        [
            "Returns a blox template from an html string"
        ],
        [
            "Returns a blox template from a file stream object"
        ],
        [
            "Returns a blox template from a valid file path"
        ],
        [
            "Accumulate all dictionary and named arguments as\n    keyword argument dictionary. This is generally useful for\n    functions that try to automatically resolve inputs.\n\n    Examples:\n        >>> @keywords\n        >>> def test(*args, **kwargs):\n        >>>     return kwargs\n        >>>\n        >>> print test({'one': 1}, two=2)\n        {'one': 1, 'two': 2}"
        ],
        [
            "getCompressMod - Return the module used for compression on this field\n\n\t\t\t@return <module> - The module for compression"
        ],
        [
            "toBytes - Convert a value to bytes using the encoding specified on this field\n\n\t\t\t@param value <str> - The field to convert to bytes\n\n\t\t\t@return <bytes> - The object encoded using the codec specified on this field.\n\n\t\t\tNOTE: This method may go away."
        ],
        [
            "Like functools.partial but instead of using the new kwargs, keeps the old ones."
        ],
        [
            "Callable to configure Bokeh's show method when a proxy must be\n    configured.\n\n    If port is None we're asking about the URL\n    for the origin header."
        ],
        [
            "Called at the start of notebook execution to setup the environment.\n\n    This will configure bokeh, and setup the logging library to be\n    reasonable."
        ],
        [
            "Creates a overview of the hosts per range."
        ],
        [
            "Create an OrderedDict\n\n    :param hierarchy: a dictionary\n    :param level: single key\n    :return: deeper dictionary"
        ],
        [
            "Groups line reference together\n\n    :param text: Text object\n    :type text: MyCapytains.resources.text.api\n    :param getreffs: Callback function to retrieve text\n    :type getreffs: function(level)\n    :param lines: Number of lines to use by group\n    :type lines: int\n    :return: List of grouped urn references with their human readable version\n    :rtype: [(str, str)]"
        ],
        [
            "Chunk a text at the passage level\n\n    :param text: Text object\n    :type text: MyCapytains.resources.text.api\n    :param getreffs: Callback function to retrieve text\n    :type getreffs: function(level)\n    :return: List of urn references with their human readable version\n    :rtype: [(str, str)]"
        ],
        [
            "Create a numpy.ndarray with all observed fields and\n    computed teff and luminosity values."
        ],
        [
            "Return the numpy array with rounded teff and luminosity columns."
        ],
        [
            "Checks the arguments to brutefore and spawns greenlets to perform the bruteforcing."
        ],
        [
            "Given a cluster create a Bokeh plot figure using the\n    cluster's image."
        ],
        [
            "Returns rounded teff and luminosity lists."
        ],
        [
            "Given a cluster create a Bokeh plot figure creating an\n    H-R diagram."
        ],
        [
            "Given a numpy array calculate what the ranges of the H-R\n    diagram should be."
        ],
        [
            "Given a numpy array create a Bokeh plot figure creating an\n    H-R diagram."
        ],
        [
            "Filter the cluster data catalog into the filtered_data\n        catalog, which is what is shown in the H-R diagram.\n\n        Filter on the values of the sliders, as well as the lasso\n        selection in the skyviewer."
        ],
        [
            "Creates a tempfile and starts the given editor, returns the data afterwards."
        ],
        [
            "This functions gives the user a way to change the data that is given as input."
        ],
        [
            "Performs a bruteforce for the given users, password, domain on the given host."
        ],
        [
            "Set the access and modified times of the file specified by path."
        ],
        [
            "Strip \\\\?\\ prefix in init phase"
        ],
        [
            "Return the path always without the \\\\?\\ prefix."
        ],
        [
            "Formats the output of another tool in the given way.\n        Has default styles for ranges, hosts and services."
        ],
        [
            "Print the given line to stdout"
        ],
        [
            "Gets the IP from the inet interfaces."
        ],
        [
            "Create a pandas DataFrame from a numpy ndarray.\n\n    By default use temp and lum with max rows of 32 and precision of 2.\n\n    arr - An numpy.ndarray.\n    columns - The columns to include in the pandas DataFrame. Defaults to\n              temperature and luminosity.\n    names - The column names for the pandas DataFrame. Defaults to\n            Temperature and Luminosity.\n    max_rows - If max_rows is an integer then set the pandas\n               display.max_rows option to that value. If max_rows\n               is True then set display.max_rows option  to 1000.\n    precision - An integer to set the pandas precision option."
        ],
        [
            "Strips labels."
        ],
        [
            "Remove namespace in the passed document in place."
        ],
        [
            "Check to see if this URI is retrievable by this Retriever implementation\n\n        :param uri: the URI of the resource to be retrieved\n        :type uri: str\n        :return: True if it can be, False if not\n        :rtype: bool"
        ],
        [
            "Decorator used to tag a method that should be used as a hook for the\n  specified `name` hook type."
        ],
        [
            "Subscribes `callable` to listen to events of `name` type. The\n    parameters passed to `callable` are dependent on the specific\n    event being triggered."
        ],
        [
            "Configures this engine based on the options array passed into\n    `argv`. If `argv` is ``None``, then ``sys.argv`` is used instead.\n    During configuration, the command line options are merged with\n    previously stored values. Then the logging subsystem and the\n    database model are initialized, and all storable settings are\n    serialized to configurations files."
        ],
        [
            "Alias for _assemble_with_columns"
        ],
        [
            "Execute a query with provided parameters \n\n        Parameters\n        :query:     SQL string with parameter placeholders\n        :commit:    If True, the query will commit\n        :returns:   List of rows"
        ],
        [
            "Handle provided columns and if necessary, convert columns to a list for \n        internal strage.\n\n        :columns: A sequence of columns for the table. Can be list, comma\n            -delimited string, or IntEnum."
        ],
        [
            "Execute a DML query \n\n        :sql_string:    An SQL string template\n        :*args:         Arguments to be passed for query parameters.\n        :commit:        Whether or not to commit the transaction after the query\n        :returns:       Psycopg2 result"
        ],
        [
            "Execute a SELECT statement \n\n        :sql_string:    An SQL string template\n        :columns:       A list of columns to be returned by the query\n        :*args:         Arguments to be passed for query parameters.\n        :returns:       Psycopg2 result"
        ],
        [
            "Retreive a single record from the table.  Lots of reasons this might be\n        best implemented in the model\n\n        :pk:            The primary key ID for the record\n        :returns:       List of single result"
        ],
        [
            "Creates the final payload based on the x86 and x64 meterpreters."
        ],
        [
            "Combines the files 1 and 2 into 3."
        ],
        [
            "Runs the checker.py scripts to detect the os."
        ],
        [
            "Starts the exploiting phase, you should run setup before running this function.\n            if auto is set, this function will fire the exploit to all systems. Otherwise a curses interface is shown."
        ],
        [
            "Exploits a single ip, exploit is based on the given operating system."
        ],
        [
            "Create server instance with an optional WebSocket handler\n\n    For pure WebSocket server ``app`` may be ``None`` but an attempt to access\n    any path other than ``ws_path`` will cause server error.\n    \n    :param host: hostname or IP\n    :type host: str\n    :param port: server port\n    :type port: int\n    :param app: WSGI application\n    :param server_class: WSGI server class, defaults to AsyncWsgiServer\n    :param handler_class: WSGI handler class, defaults to AsyncWsgiHandler\n    :param ws_handler_class: WebSocket hanlder class, defaults to ``None``\n    :param ws_path: WebSocket path on the server, defaults to '/ws'\n    :type ws_path: str, optional\n    :return: initialized server instance"
        ],
        [
            "Poll active sockets once\n\n        This method can be used to allow aborting server polling loop\n        on some condition.\n\n        :param timeout: polling timeout"
        ],
        [
            "Start serving HTTP requests\n\n        This method blocks the current thread.\n\n        :param poll_interval: polling timeout\n        :return:"
        ],
        [
            "write triples into a translation file."
        ],
        [
            "write triples to file."
        ],
        [
            "Returns protobuf mapcontainer. Read from translation file."
        ],
        [
            "Returns map with entity or relations from plain text."
        ],
        [
            "Prints an overview of the tags of the hosts."
        ],
        [
            "Main credentials tool"
        ],
        [
            "Provides an overview of the duplicate credentials."
        ],
        [
            "Register nemo and parses annotations\n\n        .. note:: Process parses the annotation and extends informations about the target URNs by retrieving resource in range\n\n        :param nemo: Nemo"
        ],
        [
            "Starts the loop to provide the data from jackal."
        ],
        [
            "Creates a search query based on the section of the config file."
        ],
        [
            "Creates the workers based on the given configfile to provide named pipes in the directory."
        ],
        [
            "Loads the config and handles the workers."
        ],
        [
            "Replace isocode by its language equivalent\n\n    :param isocode: Three character long language code\n    :param lang: Lang in which to return the language name\n    :return: Full Text Language Name"
        ],
        [
            "A function to construct a hierarchical dictionary representing the different citation layers of a text\n\n    :param reffs: passage references with human-readable equivalent\n    :type reffs: [(str, str)]\n    :param citation: Main Citation\n    :type citation: Citation\n    :return: nested dictionary representing where keys represent the names of the levels and the final values represent the passage reference\n    :rtype: OrderedDict"
        ],
        [
            "Take a string of form %citation_type|passage% and format it for human\n\n    :param string: String of formation %citation_type|passage%\n    :param lang: Language to translate to\n    :return: Human Readable string\n\n    .. note :: To Do : Use i18n tools and provide real i18n"
        ],
        [
            "Annotation filtering filter\n\n    :param annotations: List of annotations\n    :type annotations: [AnnotationResource]\n    :param type_uri: URI Type on which to filter\n    :type type_uri: str\n    :param number: Number of the annotation to return\n    :type number: int\n    :return: Annotation(s) matching the request\n    :rtype: [AnnotationResource] or AnnotationResource"
        ],
        [
            "Connect to a service to see if it is a http or https server."
        ],
        [
            "Retrieves services starts check_service in a gevent pool of 100."
        ],
        [
            "Imports the given nmap result."
        ],
        [
            "Start an nmap process with the given args on the given ips."
        ],
        [
            "Scans the given hosts with nmap."
        ],
        [
            "Scans available smb services in the database for smb signing and ms17-010."
        ],
        [
            "Function to create an overview of the services.\n        Will print a list of ports found an the number of times the port was seen."
        ],
        [
            "Rename endpoint function name to avoid conflict when namespacing is set to true\n\n    :param fn_name: Name of the route function\n    :param instance: Instance bound to the function\n    :return: Name of the new namespaced function name"
        ],
        [
            "Retrieve the best matching locale using request headers\n\n        .. note:: Probably one of the thing to enhance quickly.\n\n        :rtype: str"
        ],
        [
            "Transform input according to potentially registered XSLT\n\n        .. note:: Since 1.0.0, transform takes an objectId parameter which represent the passage which is called\n\n        .. note:: Due to XSLT not being able to be used twice, we rexsltise the xml at every call of xslt\n\n        .. warning:: Until a C libxslt error is fixed ( https://bugzilla.gnome.org/show_bug.cgi?id=620102 ), \\\n        it is not possible to use strip tags in the xslt given to this application\n\n        :param work: Work object containing metadata about the xml\n        :type work: MyCapytains.resources.inventory.Text\n        :param xml: XML to transform\n        :type xml: etree._Element\n        :param objectId: Object Identifier\n        :type objectId: str\n        :param subreference: Subreference\n        :type subreference: str\n        :return: String representation of transformed resource\n        :rtype: str"
        ],
        [
            "Request the api endpoint to retrieve information about the inventory\n\n        :return: Main Collection\n        :rtype: Collection"
        ],
        [
            "Retrieve and transform a list of references.\n\n        Returns the inventory collection object with its metadata and a callback function taking a level parameter \\\n        and returning a list of strings.\n\n        :param objectId: Collection Identifier\n        :type objectId: str\n        :param subreference: Subreference from which to retrieve children\n        :type subreference: str\n        :param collection: Collection object bearing metadata\n        :type collection: Collection\n        :param export_collection: Return collection metadata\n        :type export_collection: bool\n        :return: Returns either the list of references, or the text collection object with its references as tuple\n        :rtype: (Collection, [str]) or [str]"
        ],
        [
            "Retrieve the passage identified by the parameters\n\n        :param objectId: Collection Identifier\n        :type objectId: str\n        :param subreference: Subreference of the passage\n        :type subreference: str\n        :return: An object bearing metadata and its text\n        :rtype: InteractiveTextualNode"
        ],
        [
            "Get siblings of a browsed subreference\n\n        .. note:: Since 1.0.0c, there is no more prevnext dict. Nemo uses the list of original\\\n        chunked references to retrieve next and previous, or simply relies on the resolver to get siblings\\\n        when the subreference is not found in given original chunks.\n\n        :param objectId: Id of the object\n        :param subreference: Subreference of the object\n        :param passage: Current Passage\n        :return: Previous and next references\n        :rtype: (str, str)"
        ],
        [
            "Generates a SEO friendly string for given collection\n\n        :param collection: Collection object to generate string for\n        :param parent: Current collection parent\n        :return: SEO/URL Friendly string"
        ],
        [
            "Creates a CoINS Title string from information\n\n        :param collection: Collection to create coins from\n        :param text: Text/Passage object\n        :param subreference: Subreference\n        :param lang: Locale information\n        :return: Coins HTML title value"
        ],
        [
            "Build an ancestor or descendant dict view based on selected information\n\n        :param member: Current Member to build for\n        :param collection: Collection from which we retrieved it\n        :param lang: Language to express data in\n        :return:"
        ],
        [
            "Build member list for given collection\n\n        :param collection: Collection to build dict view of for its members\n        :param lang: Language to express data in\n        :return: List of basic objects"
        ],
        [
            "Build parents list for given collection\n\n        :param collection: Collection to build dict view of for its members\n        :param lang: Language to express data in\n        :return: List of basic objects"
        ],
        [
            "Retrieve the top collections of the inventory\n\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Collections information and template\n        :rtype: {str: Any}"
        ],
        [
            "Collection content browsing route function\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Template and collections contained in given collection\n        :rtype: {str: Any}"
        ],
        [
            "Text exemplar references browsing route function\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Template and required information about text with its references"
        ],
        [
            "Provides a redirect to the first passage of given objectId\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :return: Redirection to the first passage of given text"
        ],
        [
            "Retrieve the text of the passage\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :param subreference: Reference identifier\n        :type subreference: str\n        :return: Template, collections metadata and Markup object representing the text\n        :rtype: {str: Any}"
        ],
        [
            "Route for specific assets.\n\n        :param filetype: Asset Type\n        :param asset: Filename of an asset\n        :return: Response"
        ],
        [
            "Merge and register assets, both as routes and dictionary\n\n        :return: None"
        ],
        [
            "Create blueprint and register rules\n\n        :return: Blueprint of the current nemo app\n        :rtype: flask.Blueprint"
        ],
        [
            "Create a view\n\n        :param name: Name of the route function to use for the view.\n        :type name: str\n        :return: Route function which makes use of Nemo context (such as menu informations)\n        :rtype: function"
        ],
        [
            "Retrieve main parent collections of a repository\n\n        :param lang: Language to retrieve information in\n        :return: Sorted collections representations"
        ],
        [
            "This function is built to provide cache keys for templates\n\n        :param endpoint: Current endpoint\n        :param kwargs: Keyword Arguments\n        :return: tuple of i18n dependant cache key and i18n ignoring cache key\n        :rtype: tuple(str)"
        ],
        [
            "Render a route template and adds information to this route.\n\n        :param template: Template name.\n        :type template: str\n        :param kwargs: dictionary of named arguments used to be passed to the template\n        :type kwargs: dict\n        :return: Http Response with rendered template\n        :rtype: flask.Response"
        ],
        [
            "Register the app using Blueprint\n\n        :return: Nemo blueprint\n        :rtype: flask.Blueprint"
        ],
        [
            "Register filters for Jinja to use\n\n       .. note::  Extends the dictionary filters of jinja_env using self._filters list"
        ],
        [
            "Register plugins in Nemo instance\n\n        - Clear routes first if asked by one plugin\n        - Clear assets if asked by one plugin and replace by the last plugin registered static_folder\n        - Register each plugin\n            - Append plugin routes to registered routes\n            - Append plugin filters to registered filters\n            - Append templates directory to given namespaces\n            - Append assets (CSS, JS, statics) to given resources \n            - Append render view (if exists) to Nemo.render stack"
        ],
        [
            "Handle a list of references depending on the text identifier using the chunker dictionary.\n\n        :param text: Text object from which comes the references\n        :type text: MyCapytains.resources.texts.api.Text\n        :param reffs: List of references to transform\n        :type reffs: References\n        :return: Transformed list of references\n        :rtype: [str]"
        ],
        [
            "Obtains the data from the pipe and appends the given tag."
        ],
        [
            "Creates the section value if it does not exists and sets the value.\n            Use write_config to actually set the value."
        ],
        [
            "This function tries to retrieve the value from the configfile\n            otherwise will return a default."
        ],
        [
            "Returns the configuration directory"
        ],
        [
            "Write the current config to disk to store them."
        ],
        [
            "Track the specified remote branch if it is not already tracked."
        ],
        [
            "Checkout, update and branch from the specified branch."
        ],
        [
            "Returns the interface name of the first not link_local and not loopback interface."
        ],
        [
            "load_targets will load the services with smb signing disabled and if ldap is enabled the services with the ldap port open."
        ],
        [
            "write_targets will write the contents of ips and ldap_strings to the targets_file."
        ],
        [
            "Starts the ntlmrelayx.py and responder processes.\n            Assumes you have these programs in your path."
        ],
        [
            "Function that gets called on each event from pyinotify."
        ],
        [
            "Watches directory for changes"
        ],
        [
            "Terminate the processes."
        ],
        [
            "This function waits for the relay and responding processes to exit.\n            Captures KeyboardInterrupt to shutdown these processes."
        ],
        [
            "Retrieve annotations from the query provider\n\n        :param targets: The CTS URN(s) to query as the target of annotations\n        :type targets: [MyCapytain.common.reference.URN], URN or None\n        :param wildcard: Wildcard specifier for how to match the URN\n        :type wildcard: str\n        :param include: URI(s) of Annotation types to include in the results\n        :type include: list(str)\n        :param exclude: URI(s) of Annotation types to include in the results\n        :type exclude: list(str)\n        :param limit: The max number of results to return (Default is None for no limit)\n        :type limit: int\n        :param start: the starting record to return (Default is 1)\n        :type start: int \n        :param expand: Flag to state whether Annotations are expanded (Default is False)\n        :type expand: bool\n    \n        :return: Tuple representing the query results. The first element\n                 The first element is the number of total Annotations found\n                 The second element is the list of Annotations\n        :rtype: (int, list(Annotation)\n\n        .. note::\n\n            Wildcard should be one of the following value\n\n            - '.' to match exact,\n            - '.%' to match exact plus lower in the hierarchy\n            - '%.' to match exact + higher in the hierarchy\n            - '-' to match in the range\n            - '%.%' to match all"
        ],
        [
            "Make breadcrumbs for a route\n\n        :param kwargs: dictionary of named arguments used to construct the view\n        :type kwargs: dict\n        :return: List of dict items the view can use to construct the link.\n        :rtype: {str: list({ \"link\": str, \"title\", str, \"args\", dict})}"
        ],
        [
            "This function obtains hosts from core and starts a nessus scan on these hosts.\n        The nessus tag is appended to the host tags."
        ],
        [
            "Retrieves the uuid of the given template name."
        ],
        [
            "Creates a scan with the given host ips\n            Returns the scan id of the created object."
        ],
        [
            "Starts the scan identified by the scan_id.s"
        ],
        [
            "Bases the comparison of the datastores on URI alone."
        ],
        [
            "Adds a tag to the list of tags and makes sure the result list contains only unique results."
        ],
        [
            "Removes a tag from this object"
        ],
        [
            "Returns the result as a dictionary, provide the include_meta flag to als show information like index and doctype."
        ],
        [
            "Route to retrieve annotations by target\n\n        :param target_urn: The CTS URN for which to retrieve annotations  \n        :type target_urn: str\n        :return: a JSON string containing count and list of resources\n        :rtype: {str: Any}"
        ],
        [
            "Returns the label for a given Enum key"
        ],
        [
            "Returns the verbose name for a given enum value"
        ],
        [
            "Returns the configured DNS servers with the use f nmcli."
        ],
        [
            "Tries to perform a zone transfer."
        ],
        [
            "Resolves the list of domains and returns the ips."
        ],
        [
            "Parses the list of ips, turns these into ranges based on the netmask given.\n        Set include_public to True to include public IP adresses."
        ],
        [
            "Creates a connection based upon the given configuration object."
        ],
        [
            "Searches the elasticsearch instance to retrieve the requested documents."
        ],
        [
            "Uses the command line arguments to fill the search function and call it."
        ],
        [
            "Returns the number of results after filtering with the given arguments."
        ],
        [
            "Uses the command line arguments to fill the count function and call it."
        ],
        [
            "Returns a generator that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json."
        ],
        [
            "Resolves an ip adres to a range object, creating it if it doesn't exists."
        ],
        [
            "Argparser option with search functionality specific for ranges."
        ],
        [
            "Searches elasticsearch for objects with the same address, protocol, port and state."
        ],
        [
            "Resolves the given id to a user object, if it doesn't exists it will be created."
        ],
        [
            "Retrieves the domains of the users from elastic."
        ],
        [
            "Returns a list that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json."
        ],
        [
            "Consumes an ET protocol tree and converts it to state.Command commands"
        ],
        [
            "Initializes the indices"
        ],
        [
            "Parse the entry into a computer object."
        ],
        [
            "Parse the file and extract the computers, import the computers that resolve into jackal."
        ],
        [
            "Parses a single entry from the domaindump"
        ],
        [
            "Parses the domain users and groups files."
        ],
        [
            "Parses ldapdomaindump files and stores hosts and users in elasticsearch."
        ],
        [
            "Make an autocomplete API request\n\n    This can be used to find cities and/or hurricanes by name\n\n    :param string query: city\n    :param string country: restrict search to a specific country. Must be a two letter country code\n    :param boolean hurricanes: whether to search for hurricanes or not\n    :param boolean cities: whether to search for cities or not\n    :param integer timeout: timeout of the api request\n    :returns: result of the autocomplete API request\n    :rtype: dict"
        ],
        [
            "Make an API request\n\n    :param string key: API key to use\n    :param list features: features to request. It must be a subset of :data:`FEATURES`\n    :param string query: query to send\n    :param integer timeout: timeout of the request\n    :returns: result of the API request\n    :rtype: dict"
        ],
        [
            "Try to convert a string to unicode using different encodings"
        ],
        [
            "Handle HTTP GET requests on an authentication endpoint.\n\n    Authentication flow begins when ``params`` has a ``login`` key with a value\n    of ``start``. For instance, ``/auth/twitter?login=start``.\n\n    :param str provider: An provider to obtain a user ID from.\n    :param str request_url: The authentication endpoint/callback.\n    :param dict params: GET parameters from the query string.\n    :param str token_secret: An app secret to encode/decode JSON web tokens.\n    :param str token_cookie: The current JSON web token, if available.\n    :return: A dict containing any of the following possible keys:\n\n        ``status``: an HTTP status code the server should sent\n\n        ``redirect``: where the client should be directed to continue the flow\n\n        ``set_token_cookie``: contains a JSON web token and should be stored by\n        the client and passed in the next call.\n\n        ``provider_user_id``: the user ID from the login provider\n\n        ``provider_user_name``: the user name from the login provider"
        ],
        [
            "Method to call to get a serializable object for json.dump or jsonify based on the target\n\n        :return: dict"
        ],
        [
            "Read the contents of the Annotation Resource\n\n        :return: the contents of the resource\n        :rtype: str or bytes or flask.response"
        ],
        [
            "index all triples into indexes and return their mappings"
        ],
        [
            "recover triples from mapping."
        ],
        [
            "Transform triple index into a 1-D numpy array."
        ],
        [
            "Packs a list of triple indexes into a 2D numpy array."
        ],
        [
            "If entity pairs in a relation is as close as another relations, only keep one relation of such set."
        ],
        [
            "Remove direct links in the training sets."
        ],
        [
            "Uses a union find to find segment."
        ],
        [
            "Create a usable data structure for serializing."
        ],
        [
            "Logs an operation done on an entity, possibly with other arguments"
        ],
        [
            "Logs a new state of an entity"
        ],
        [
            "Logs an update done on an entity"
        ],
        [
            "Logs an error"
        ],
        [
            "Decorator that provides a dictionary cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.DICT) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side dictionary cursor"
        ],
        [
            "Decorator that provides a cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor() coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side cursor"
        ],
        [
            "Decorator that provides a namedtuple cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side namedtuple cursor"
        ],
        [
            "Provides a transacted cursor which will run in autocommit=false mode\n\n    For any exception the transaction will be rolled back.\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side transacted named cursor"
        ],
        [
            "gives the number of records in the table\n\n        Args:\n            table: a string indicating the name of the table\n\n        Returns:\n            an integer indicating the number of records in the table"
        ],
        [
            "Creates an insert statement with only chosen fields\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n\n        Returns:\n            A 'Record' object with table columns as properties"
        ],
        [
            "Creates an update query with only chosen fields\n        Supports only a single field where clause\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted"
        ],
        [
            "Creates a delete query with where keys\n        Supports multiple where clause with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted"
        ],
        [
            "Creates a select query for selective columns with where keys\n        Supports multiple where claus with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            order_by: a string indicating column name to order the results on\n            columns: list of columns to select from\n            where_keys: list of dictionary\n            limit: the limit on the number of results\n            offset: offset on the results\n\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and across dictionaries get 'OR'-ed\n\n        Returns:\n            A list of 'Record' object with table columns as properties"
        ],
        [
            "Run a raw sql query\n\n        Args:\n            query : query string to execute\n            values : tuple of values to be used with the query\n\n        Returns:\n            result of query as list of named tuple"
        ],
        [
            "This method is used to append content of the `text`\n    argument to the `out` argument.\n\n    Depending on how many lines in the text, a\n    padding can be added to all lines except the first\n    one.\n\n    Concatenation result is appended to the `out` argument."
        ],
        [
            "This function should return unicode representation of the value"
        ],
        [
            "Helper function to traverse an element tree rooted at element, yielding nodes matching the query."
        ],
        [
            "Given a simplified XPath query string, returns an array of normalized query parts."
        ],
        [
            "Inserts a new element as a child of this element, before the specified index or sibling.\n\n        :param before: An :class:`XmlElement` or a numeric index to insert the new node before\n        :param name: The tag name to add\n        :param attrs: Attributes for the new tag\n        :param data: CDATA for the new tag\n        :returns: The newly-created element\n        :rtype: :class:`XmlElement`"
        ],
        [
            "A generator yielding children of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :param reverse: If ``True``, children will be yielded in reverse declaration order"
        ],
        [
            "Helper function to determine if this node matches the given predicate."
        ],
        [
            "Returns a canonical path to this element, relative to the root node.\n\n        :param include_root: If ``True``, include the root node in the path. Defaults to ``False``."
        ],
        [
            "Recursively find any descendants of this node with the given tag name. If a tag name is omitted, this will\n        yield every descendant node.\n\n        :param name: If specified, only consider elements with this tag name\n        :returns: A generator yielding descendants of this node"
        ],
        [
            "Returns the last child of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`"
        ],
        [
            "Yields all parents of this element, back to the root element.\n\n        :param name: If specified, only consider elements with this tag name"
        ],
        [
            "Returns the next sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`"
        ],
        [
            "Returns the previous sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`"
        ],
        [
            "Parses the HTML table into a list of dictionaries, each of which\n        represents a single observation."
        ],
        [
            "Calculates cache key based on `args` and `kwargs`.\n    `args` and `kwargs` must be instances of hashable types."
        ],
        [
            "Cache result of function execution into the django cache backend.\n    Calculate cache key based on `prefix`, `args` and `kwargs` of the function.\n    For using like object method set `method=True`."
        ],
        [
            "Wrapper around Django's ORM `get` functionality.\n    Wrap anything that raises ObjectDoesNotExist exception\n    and provide the default value if necessary.\n    `default` by default is None. `default` can be any callable,\n    if it is callable it will be called when ObjectDoesNotExist\n    exception will be raised."
        ],
        [
            "Turn column inputs from user into list of simple numbers.\n\n    Inputs can be:\n\n      - individual number: 1\n      - range: 1-3\n      - comma separated list: 1,2,3,4-6"
        ],
        [
            "Return only the part of the row which should be printed."
        ],
        [
            "Writes a single observation to the output file.\n\n        If the ``observation_data`` parameter is a dictionary, it is\n        converted to a list to keep a consisted field order (as described\n        in format specification). Otherwise it is assumed that the data\n        is a raw record ready to be written to file.\n\n        :param observation_data: a single observation as a dictionary or list"
        ],
        [
            "Takes a dictionary of observation data and converts it to a list\n        of fields according to AAVSO visual format specification.\n\n        :param cls: current class\n        :param observation_data: a single observation as a dictionary"
        ],
        [
            "Converts a raw input record to a dictionary of observation data.\n\n        :param cls: current class\n        :param row: a single observation as a list or tuple"
        ],
        [
            "Get the name of the view function used to prevent having to set the tag\n    manually for every endpoint"
        ],
        [
            "Downloads all variable star observations by a given observer.\n\n    Performs a series of HTTP requests to AAVSO's WebObs search and\n    downloads the results page by page. Each page is then passed to\n    :py:class:`~pyaavso.parsers.webobs.WebObsResultsParser` and parse results\n    are added to the final observation list."
        ],
        [
            "Generates likely unique image path using md5 hashes"
        ],
        [
            "Extract, transform, and load metadata from Lander-based projects.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    Raises\n    ------\n    NotLanderPageError\n        Raised when the LTD product cannot be interpreted as a Lander page\n        because the ``/metadata.jsonld`` file is absent. This implies that\n        the LTD product *could* be of a different format.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d"
        ],
        [
            "Upsert the technote resource into the projectmeta MongoDB collection.\n\n    Parameters\n    ----------\n    collection : `motor.motor_asyncio.AsyncIOMotorCollection`\n        The MongoDB collection.\n    jsonld : `dict`\n        The JSON-LD document that represents the document resource."
        ],
        [
            "Converts a Open511 JSON document to XML.\n\n    lang: the appropriate language code\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    Accepts only the full root-level JSON object from an Open511 response."
        ],
        [
            "Converts a Open511 JSON fragment to XML.\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    This won't provide a conforming document if you pass in a full JSON document;\n    it's for translating little fragments, and is mostly used internally."
        ],
        [
            "Given a dict deserialized from a GeoJSON object, returns an lxml Element\n    of the corresponding GML geometry."
        ],
        [
            "Transform a GEOS or OGR geometry object into an lxml Element\n    for the GML geometry."
        ],
        [
            "Delete latex comments from TeX source.\n\n    Parameters\n    ----------\n    tex_source : str\n        TeX source content.\n\n    Returns\n    -------\n    tex_source : str\n        TeX source without comments."
        ],
        [
            "r\"\"\"Replace macros in the TeX source with their content.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros. See\n        `lsstprojectmeta.tex.scraper.get_macros`.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source with known macros replaced.\n\n    Notes\n    -----\n    Macros with arguments are not supported.\n\n    Examples\n    --------\n    >>> macros = {r'\\handle': 'LDM-nnn'}\n    >>> sample = r'This is document \\handle.'\n    >>> replace_macros(sample, macros)\n    'This is document LDM-nnn.'\n\n    Any trailing slash after the macro command is also replaced by this\n    function.\n\n    >>> macros = {r'\\product': 'Data Management'}\n    >>> sample = r'\\title    [Test Plan]  { \\product\\ Test Plan}'\n    >>> replace_macros(sample, macros)\n    '\\\\title    [Test Plan]  { Data Management Test Plan}'"
        ],
        [
            "Ensures that the provided document is an lxml Element or json dict."
        ],
        [
            "Convert an Open511 document between formats.\n    input_doc - either an lxml open511 Element or a deserialized JSON dict\n    output_format - short string name of a valid output format, as listed above"
        ],
        [
            "Construct an `LsstLatexDoc` instance by reading and parsing the\n        LaTeX source.\n\n        Parameters\n        ----------\n        root_tex_path : `str`\n            Path to the LaTeX source on the filesystem. For multi-file LaTeX\n            projects this should be the path to the root document.\n\n        Notes\n        -----\n        This method implements the following pipeline:\n\n        1. `lsstprojectmeta.tex.normalizer.read_tex_file`\n        2. `lsstprojectmeta.tex.scraper.get_macros`\n        3. `lsstprojectmeta.tex.normalizer.replace_macros`\n\n        Thus ``input`` and ``includes`` are resolved along with simple macros."
        ],
        [
            "Get the document content in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content."
        ],
        [
            "Get the document title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document."
        ],
        [
            "Get the document short title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the short title is not available in\n            the document."
        ],
        [
            "Get the document abstract in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document."
        ],
        [
            "Get the document authors in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `list` of `str`\n            Sequence of author names in the specified output markup format."
        ],
        [
            "Parse documentclass options.\n\n        Sets the the ``_document_options`` attribute."
        ],
        [
            "Parse the title from TeX source.\n\n        Sets these attributes:\n\n        - ``_title``\n        - ``_short_title``"
        ],
        [
            "Parse the document handle.\n\n        Sets the ``_series``, ``_serial``, and ``_handle`` attributes."
        ],
        [
            "r\"\"\"Parse the author from TeX source.\n\n        Sets the ``_authors`` attribute.\n\n        Goal is to parse::\n\n           \\author{\n           A.~Author,\n           B.~Author,\n           and\n           C.~Author}\n\n        Into::\n\n           ['A. Author', 'B. Author', 'C. Author']"
        ],
        [
            "Parse the abstract from the TeX source.\n\n        Sets the ``_abstract`` attribute."
        ],
        [
            "Process a LaTeX snippet of content for better transformation\n        with pandoc.\n\n        Currently runs the CitationLinker to convert BibTeX citations to\n        href links."
        ],
        [
            "r\"\"\"Load the BibTeX bibliography referenced by the document.\n\n        This method triggered by the `bib_db` attribute and populates the\n        `_bib_db` private attribute.\n\n        The ``\\bibliography`` command is parsed to identify the bibliographies\n        referenced by the document."
        ],
        [
            "r\"\"\"Parse the ``\\date`` command, falling back to getting the\n        most recent Git commit date and the current datetime.\n\n        Result is available from the `revision_datetime` attribute."
        ],
        [
            "Create a JSON-LD representation of this LSST LaTeX document.\n\n        Parameters\n        ----------\n        url : `str`, optional\n            URL where this document is published to the web. Prefer\n            the LSST the Docs URL if possible.\n            Example: ``'https://ldm-151.lsst.io'``.\n        code_url : `str`, optional\n            Path the the document's repository, typically on GitHub.\n            Example: ``'https://github.com/lsst/LDM-151'``.\n        ci_url : `str`, optional\n            Path to the continuous integration service dashboard for this\n            document's repository.\n            Example: ``'https://travis-ci.org/lsst/LDM-151'``.\n        readme_url : `str`, optional\n            URL to the document repository's README file. Example:\n            ``https://raw.githubusercontent.com/lsst/LDM-151/master/README.rst``.\n        license_id : `str`, optional\n            License identifier, if known. The identifier should be from the\n            listing at https://spdx.org/licenses/. Example: ``CC-BY-4.0``.\n\n        Returns\n        -------\n        jsonld : `dict`\n            JSON-LD-formatted dictionary."
        ],
        [
            "Renames an existing database."
        ],
        [
            "Returns True if database server is running, False otherwise."
        ],
        [
            "Saves the state of a database to a file.\n\n        Parameters\n        ----------\n        name: str\n            the database to be backed up.\n        filename: str\n            path to a file where database backup will be written."
        ],
        [
            "Loads state of a backup file to a database.\n\n        Note\n        ----\n        If database name does not exist, it will be created.\n\n        Parameters\n        ----------\n        name: str\n            the database to which backup will be restored.\n        filename: str\n            path to a file contain a postgres database backup."
        ],
        [
            "Provides a connection string for database.\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection string (e.g. 'dbname=db1 user=user1 host=localhost port=5432')"
        ],
        [
            "Provides a connection string for database as a sqlalchemy compatible URL.\n\n        NB - this doesn't include special arguments related to SSL connectivity (which are outside the scope\n        of the connection URL format).\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection URL (e.g. postgresql://user1@localhost:5432/db1)"
        ],
        [
            "Connects the database client shell to the database.\n\n        Parameters\n        ----------\n        expect_module: str\n            the database to which backup will be restored."
        ],
        [
            "Returns settings from the server."
        ],
        [
            "Say something in the morning"
        ],
        [
            "Say something in the afternoon"
        ],
        [
            "Say something in the evening"
        ],
        [
            "Command line entrypoint to reduce technote metadata."
        ],
        [
            "Run a pipeline to process extract, transform, and load metadata for\n    multiple LSST the Docs-hosted projects\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    product_urls : `list` of `str`\n        List of LSST the Docs product URLs.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records."
        ],
        [
            "Ingest any kind of LSST document hosted on LSST the Docs from its\n    source.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_url : `str`\n        URL of the technote's product resource in the LTD Keeper API.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d"
        ],
        [
            "Allows a decorator to be called with or without keyword arguments."
        ],
        [
            "Create a GitHub token for an integration installation.\n\n    Parameters\n    ----------\n    installation_id : `int`\n        Installation ID. This is available in the URL of the integration's\n        **installation** ID.\n    integration_jwt : `bytes`\n        The integration's JSON Web Token (JWT). You can create this with\n        `create_jwt`.\n\n    Returns\n    -------\n    token_obj : `dict`\n        GitHub token object. Includes the fields:\n\n        - ``token``: the token string itself.\n        - ``expires_at``: date time string when the token expires.\n\n    Example\n    -------\n    The typical workflow for authenticating to an integration installation is:\n\n    .. code-block:: python\n\n       from dochubadapter.github import auth\n       jwt = auth.create_jwt(integration_id, private_key_path)\n       token_obj = auth.get_installation_token(installation_id, jwt)\n       print(token_obj['token'])\n\n    Notes\n    -----\n    See\n    https://developer.github.com/early-access/integrations/authentication/#as-an-installation\n    for more information"
        ],
        [
            "Create a JSON Web Token to authenticate a GitHub Integration or\n    installation.\n\n    Parameters\n    ----------\n    integration_id : `int`\n        Integration ID. This is available from the GitHub integration's\n        homepage.\n    private_key_path : `str`\n        Path to the integration's private key (a ``.pem`` file).\n\n    Returns\n    -------\n    jwt : `bytes`\n        JSON Web Token that is good for 9 minutes.\n\n    Notes\n    -----\n    The JWT is encoded with the RS256 algorithm. It includes a payload with\n    fields:\n\n    - ``'iat'``: The current time, as an `int` timestamp.\n    - ``'exp'``: Expiration time, as an `int timestamp. The expiration\n      time is set of 9 minutes in the future (maximum allowance is 10 minutes).\n    - ``'iss'``: The integration ID (`int`).\n\n    For more information, see\n    https://developer.github.com/early-access/integrations/authentication/."
        ],
        [
            "r\"\"\"Get all macro definitions from TeX source, supporting multiple\n    declaration patterns.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    This function uses the following function to scrape macros of different\n    types:\n\n    - `get_def_macros`\n    - `get_newcommand_macros`\n\n    This macro scraping has the following caveats:\n\n    - Macro definition (including content) must all occur on one line.\n    - Macros with arguments are not supported."
        ],
        [
            "r\"\"\"Get all ``\\def`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\def`` macros with arguments are not supported."
        ],
        [
            "r\"\"\"Get all ``\\newcommand`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\newcommand`` macros with arguments are not supported."
        ],
        [
            "Try to load and return a module\n\n    Will add DIRECTORY_NAME to sys.path and tries to import MODULE_NAME.\n\n    For example:\n    load(\"~/.yaz\", \"yaz_extension\")"
        ],
        [
            "Makes a naive datetime.datetime in a given time zone aware."
        ],
        [
            "Makes an aware datetime.datetime naive in a given time zone."
        ],
        [
            "Converts a datetime to the timezone of this Schedule."
        ],
        [
            "Returns the next Period this event is in effect, or None if the event\n        has no remaining periods."
        ],
        [
            "Returns an iterator of Period tuples for every day this event is in effect, between range_start\n        and range_end."
        ],
        [
            "Returns an iterator of Period tuples for continuous stretches of time during\n        which this event is in effect, between range_start and range_end."
        ],
        [
            "Does this schedule include the provided time?\n        query_date and query_time are date and time objects, interpreted\n        in this schedule's timezone"
        ],
        [
            "Returns an iterator of Period tuples for every day this schedule is in effect, between range_start\n        and range_end."
        ],
        [
            "A Period tuple representing the daily start and end time."
        ],
        [
            "A set of integers representing the weekdays the schedule recurs on,\n        with Monday = 0 and Sunday = 6."
        ],
        [
            "A context manager that creates a temporary database.\n\n    Useful for automated tests.\n\n    Parameters\n    ----------\n    db: object\n        a preconfigured DB object\n    name: str, optional\n        name of the database to be created. (default: globally unique name)"
        ],
        [
            "Asynchronously request a URL and get the encoded text content of the\n    body.\n\n    Parameters\n    ----------\n    url : `str`\n        URL to download.\n    session : `aiohttp.ClientSession`\n        An open aiohttp session.\n\n    Returns\n    -------\n    content : `str`\n        Content downloaded from the URL."
        ],
        [
            "Asynchronously download a set of lsst-texmf BibTeX bibliographies from\n    GitHub.\n\n    Parameters\n    ----------\n    bibtex_names : sequence of `str`\n        Names of lsst-texmf BibTeX files to download. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtexs : `list` of `str`\n        List of BibTeX file content, in the same order as ``bibtex_names``."
        ],
        [
            "Get content of lsst-texmf bibliographies.\n\n    BibTeX content is downloaded from GitHub (``master`` branch of\n    https://github.com/lsst/lsst-texmf or retrieved from an in-memory cache.\n\n    Parameters\n    ----------\n    bibtex_filenames : sequence of `str`, optional\n        List of lsst-texmf BibTeX files to retrieve. These can be the filenames\n        of lsst-bibtex files (for example, ``['lsst.bib', 'lsst-dm.bib']``)\n        or names without an extension (``['lsst', 'lsst-dm']``). The default\n        (recommended) is to get *all* lsst-texmf bibliographies:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtex : `dict`\n        Dictionary with keys that are bibtex file names (such as ``'lsst'``,\n        ``'lsst-dm'``). Values are the corresponding bibtex file content\n        (`str`)."
        ],
        [
            "Make a pybtex BibliographyData instance from standard lsst-texmf\n    bibliography files and user-supplied bibtex content.\n\n    Parameters\n    ----------\n    lsst_bib_names : sequence of `str`, optional\n        Names of lsst-texmf BibTeX files to include. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n        Default is `None`, which includes all lsst-texmf bibtex files.\n\n    bibtex : `str`\n        BibTeX source content not included in lsst-texmf. This can be content\n        from a import ``local.bib`` file.\n\n    Returns\n    -------\n    bibliography : `pybtex.database.BibliographyData`\n        A pybtex bibliography database that includes all given sources:\n        lsst-texmf bibliographies and ``bibtex``."
        ],
        [
            "Get a usable URL from a pybtex entry.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n\n    Returns\n    -------\n    url : `str`\n        Best available URL from the ``entry``.\n\n    Raises\n    ------\n    NoEntryUrlError\n        Raised when no URL can be made from the bibliography entry.\n\n    Notes\n    -----\n    The order of priority is:\n\n    1. ``url`` field\n    2. ``ls.st`` URL from the handle for ``@docushare`` entries.\n    3. ``adsurl``\n    4. DOI"
        ],
        [
            "Get and format author-year text from a pybtex entry to emulate\n    natbib citations.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n    parens : `bool`, optional\n        Whether to add parentheses around the year. Default is `False`.\n\n    Returns\n    -------\n    authoryear : `str`\n        The author-year citation text."
        ],
        [
            "Extract, transform, and load Sphinx-based technote metadata.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    Raises\n    ------\n    NotSphinxTechnoteError\n        Raised when the LTD product cannot be interpreted as a Sphinx-based\n        technote project because it's missing a metadata.yaml file in its\n        GitHub repository. This implies that the LTD product *could* be of a\n        different format.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d"
        ],
        [
            "Reduce a technote project's metadata from multiple sources into a\n    single JSON-LD resource.\n\n    Parameters\n    ----------\n    github_url : `str`\n        URL of the technote's GitHub repository.\n    metadata : `dict`\n        The parsed contents of ``metadata.yaml`` found in a technote's\n        repository.\n    github_data : `dict`\n        The contents of the ``technote_repo`` GitHub GraphQL API query.\n    ltd_product_data : `dict`\n        JSON dataset for the technote corresponding to the\n        ``/products/<product>`` of LTD Keeper.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d"
        ],
        [
            "Download the metadata.yaml file from a technote's GitHub repository."
        ],
        [
            "Return the timezone. If none is set use system timezone"
        ],
        [
            "Convert any timestamp into a datetime and save as _time"
        ],
        [
            "Return a dict that represents the DayOneEntry"
        ],
        [
            "Saves a DayOneEntry as a plist"
        ],
        [
            "Create and return full file path for DayOne entry"
        ],
        [
            "Combine many files into a single file on disk.  Defaults to using the 'time' dimension."
        ],
        [
            "The entry point for a yaz script\n\n    This will almost always be called from a python script in\n    the following manner:\n\n        if __name__ == \"__main__\":\n            yaz.main()\n\n    This function will perform the following steps:\n\n    1. It will load any additional python code from\n       the yaz_extension python module located in the\n       ~/.yaz directory when LOAD_YAZ_EXTENSION is True\n       and the yaz_extension module exists\n\n    2. It collects all yaz tasks and plugins.  When WHITE_LIST\n       is a non-empty list, only the tasks and plugins located\n       therein will be considered\n\n    3. It will parse arguments from ARGV, or the command line\n       when ARGV is not given, resulting in a yaz task or a parser\n       help message.\n\n    4. When a suitable task is found, this task is executed.  In\n       case of a task which is part of a plugin, i.e. class, then\n       this plugin is initialized, possibly resulting in other\n       plugins to also be initialized if there are marked as\n       `@yaz.dependency`."
        ],
        [
            "Returns a tree of Task instances\n\n    The tree is comprised of dictionaries containing strings for\n    keys and either dictionaries or Task instances for values.\n\n    When WHITE_LIST is given, only the tasks and plugins in this\n    list will become part of the task tree.  The WHITE_LIST may\n    contain either strings, corresponding to the task of plugin\n    __qualname__, or, preferable, the WHITE_LIST contains\n    links to the task function or plugin class instead."
        ],
        [
            "Declare a function or method to be a Yaz task\n\n    @yaz.task\n    def talk(message: str = \"Hello World!\"):\n        return message\n\n    Or... group multiple tasks together\n\n    class Tools(yaz.Plugin):\n        @yaz.task\n        def say(self, message: str = \"Hello World!\"):\n            return message\n\n        @yaz.task(option__choices=[\"A\", \"B\", \"C\"])\n        def choose(self, option: str = \"A\"):\n            return option"
        ],
        [
            "Returns a list of parameters"
        ],
        [
            "Returns the configuration for KEY"
        ],
        [
            "Returns an instance of a fully initialized plugin class\n\n    Every plugin class is kept in a plugin cache, effectively making\n    every plugin into a singleton object.\n\n    When a plugin has a yaz.dependency decorator, it will be called\n    as well, before the instance is returned."
        ],
        [
            "Convert an Open511 XML document or document fragment to JSON.\n\n    Takes an lxml Element object. Returns a dict ready to be JSON-serialized."
        ],
        [
            "Given an lxml Element of a GML geometry, returns a dict in GeoJSON format."
        ],
        [
            "Translates a deprecated GML 2.0 geometry to GeoJSON"
        ],
        [
            "Panflute filter function that converts content wrapped in a Para to\n    Plain.\n\n    Use this filter with pandoc as::\n\n        pandoc [..] --filter=lsstprojectmeta-deparagraph\n\n    Only lone paragraphs are affected. Para elements with siblings (like a\n    second Para) are left unaffected.\n\n    This filter is useful for processing strings like titles or author names so\n    that the output isn't wrapped in paragraph tags. For example, without\n    this filter, pandoc converts a string ``\"The title\"`` to\n    ``<p>The title</p>`` in HTML. These ``<p>`` tags aren't useful if you\n    intend to put the title text in ``<h1>`` tags using your own templating\n    system."
        ],
        [
            "Recursively generate of all the subclasses of class cls."
        ],
        [
            "List unique elements, preserving order. Remember only the element just seen."
        ],
        [
            "Returns a masked array with anything outside of values masked.\n    The minv and maxv parameters take precendence over any dict values.\n    The valid_range attribute takes precendence over the valid_min and\n    valid_max attributes."
        ],
        [
            "If input object is an ndarray it will be converted into a list"
        ],
        [
            "If input object is an ndarray it will be converted into a dict\n        holding dtype, shape and the data, base64 encoded."
        ],
        [
            "leftSibling\n        previousSibling\n        leftSib\n        prevSib\n        lsib\n        psib\n        \n        have the same parent,and on the left"
        ],
        [
            "rightSibling\n        nextSibling\n        rightSib\n        nextSib\n        rsib\n        nsib\n        \n        have the same parent,and on the right"
        ],
        [
            "leftCousin\n        previousCousin\n        leftCin\n        prevCin\n        lcin\n        pcin\n        \n        parents are neighbors,and on the left"
        ],
        [
            "rightCousin\n        nextCousin\n        rightCin\n        nextCin\n        rcin\n        ncin\n        \n        parents are neighbors,and on the right"
        ],
        [
            "_creat_child_desc\n            update depth,parent_breadth_path,parent_path,sib_seq,path,lsib_path,rsib_path,lcin_path,rcin_path"
        ],
        [
            "_upgrade_breadth_info\n            update breadth, breadth_path, and add desc to desc_level"
        ],
        [
            "Parse command content from the LaTeX source.\n\n        Parameters\n        ----------\n        source : `str`\n            The full source of the tex document.\n\n        Yields\n        ------\n        parsed_command : `ParsedCommand`\n            Yields parsed commands instances for each occurence of the command\n            in the source."
        ],
        [
            "Parse a single command.\n\n        Parameters\n        ----------\n        source : `str`\n            The full source of the tex document.\n        start_index : `int`\n            Character index in ``source`` where the command begins.\n\n        Returns\n        -------\n        parsed_command : `ParsedCommand`\n            The parsed command from the source at the given index."
        ],
        [
            "r\"\"\"Attempt to parse a single token on the first line of this source.\n\n        This method is used for parsing whitespace-delimited arguments, like\n        ``\\input file``. The source should ideally contain `` file`` along\n        with a newline character.\n\n        >>> source = 'Line 1\\n' r'\\input test.tex' '\\nLine 2'\n        >>> LatexCommand._parse_whitespace_argument(source, 'input')\n        'test.tex'\n\n        Bracket delimited arguments (``\\input{test.tex}``) are handled in\n        the normal logic of `_parse_command`."
        ],
        [
            "Returns a list of TMDDEventConverter elements.\n\n        doc is an XML Element containing one or more <FEU> events"
        ],
        [
            "Mostly ripped from nc3tonc4 in netCDF4-python.\n        Added ability to skip dimension and variables.\n        Removed all of the unpacking logic for shorts."
        ],
        [
            "Returns a Pandas DataFrame of the data.\n        This always returns positive down depths"
        ],
        [
            "Load a pre-made query.\n\n        These queries are distributed with lsstprojectmeta. See\n        :file:`lsstrojectmeta/data/githubv4/README.rst` inside the\n        package repository for details on available queries.\n\n        Parameters\n        ----------\n        query_name : `str`\n            Name of the query, such as ``'technote_repo'``.\n\n        Returns\n        -------\n        github_query : `GitHubQuery\n            A GitHub query or mutation object that you can pass to\n            `github_request` to execute the request itself."
        ],
        [
            "Obtain the timestamp for the most recent commit to a given file in a\n    Git repository.\n\n    Parameters\n    ----------\n    filepath : `str`\n        Absolute or repository-relative path for a file.\n    repo_path : `str`, optional\n        Path to the Git repository. Leave as `None` to use the current working\n        directory or if a ``repo`` argument is provided.\n    repo : `git.Repo`, optional\n        A `git.Repo` instance.\n\n    Returns\n    -------\n    commit_timestamp : `datetime.datetime`\n        The datetime of the most recent commit to the given file.\n\n    Raises\n    ------\n    IOError\n        Raised if the ``filepath`` does not exist in the Git repository."
        ],
        [
            "Get the datetime for the most recent commit to a project that\n    affected certain types of content.\n\n    Parameters\n    ----------\n    extensions : sequence of 'str'\n        Extensions of files to consider in getting the most recent commit\n        date. For example, ``('rst', 'svg', 'png')`` are content extensions\n        for a Sphinx project. **Extension comparision is case sensitive.** add\n        uppercase variants to match uppercase extensions.\n    acceptance_callback : callable\n        Callable function whose sole argument is a file path, and returns\n        `True` or `False` depending on whether the file's commit date should\n        be considered or not. This callback is only run on files that are\n        included by ``extensions``. Thus this callback is a way to exclude\n        specific files that would otherwise be included by their extension.\n    root_dir : 'str`, optional\n        Only content contained within this root directory is considered.\n        This directory must be, or be contained by, a Git repository. This is\n        the current working directory by default.\n\n    Returns\n    -------\n    commit_date : `datetime.datetime`\n        Datetime of the most recent content commit.\n\n    Raises\n    ------\n    RuntimeError\n        Raised if no content files are found."
        ],
        [
            "Iterative over relative filepaths of files in a directory, and\n    sub-directories, with the given extension.\n\n    Parameters\n    ----------\n    extname : `str`\n        Extension name (such as 'txt' or 'rst'). Extension comparison is\n        case sensitive.\n    root_dir : 'str`, optional\n        Root directory. Current working directory by default.\n\n    Yields\n    ------\n    filepath : `str`\n        File path, relative to ``root_dir``, with the given extension."
        ],
        [
            "Returns variables that match specific conditions.\n\n        * Can pass in key=value parameters and variables are returned that\n        contain all of the matches.  For example,\n\n        >>> # Get variables with x-axis attribute.\n        >>> vs = nc.get_variables_by_attributes(axis='X')\n        >>> # Get variables with matching \"standard_name\" attribute.\n        >>> nc.get_variables_by_attributes(standard_name='northward_sea_water_velocity')\n\n        * Can pass in key=callable parameter and variables are returned if the\n        callable returns True.  The callable should accept a single parameter,\n        the attribute value.  None is given as the attribute value when the\n        attribute does not exist on the variable. For example,\n\n        >>> # Get Axis variables.\n        >>> vs = nc.get_variables_by_attributes(axis=lambda v: v in ['X', 'Y', 'Z', 'T'])\n        >>> # Get variables that don't have an \"axis\" attribute.\n        >>> vs = nc.get_variables_by_attributes(axis=lambda v: v is None)\n        >>> # Get variables that have a \"grid_mapping\" attribute.\n        >>> vs = nc.get_variables_by_attributes(grid_mapping=lambda v: v is not None)"
        ],
        [
            "vfuncs can be any callable that accepts a single argument, the\n        Variable object, and returns a dictionary of new attributes to\n        set. These will overwrite existing attributes"
        ],
        [
            "Decorate a function that uses pypandoc to ensure that pandoc is\n    installed if necessary."
        ],
        [
            "Convert text from one markup format to another using pandoc.\n\n    This function is a thin wrapper around `pypandoc.convert_text`.\n\n    Parameters\n    ----------\n    content : `str`\n        Original content.\n\n    from_fmt : `str`\n        Format of the original ``content``. Format identifier must be one of\n        those known by Pandoc. See https://pandoc.org/MANUAL.html for details.\n\n    to_fmt : `str`\n        Output format for the content.\n\n    deparagraph : `bool`, optional\n        If `True`, then the\n        `lsstprojectmeta.pandoc.filters.deparagraph.deparagraph` filter is\n        used to remove paragraph (``<p>``, for example) tags around a single\n        paragraph of content. That filter does not affect content that\n        consists of multiple blocks (several paragraphs, or lists, for\n        example). Default is `False`.\n\n        For example, **without** this filter Pandoc will convert\n        the string ``\"Title text\"`` to ``\"<p>Title text</p>\"`` in HTML. The\n        paragraph tags aren't useful if you intend to wrap the converted\n        content in different tags, like ``<h1>``, using your own templating\n        system.\n\n        **With** this filter, Pandoc will convert the string ``\"Title text\"``\n        to ``\"Title text\"`` in HTML.\n\n    mathjax : `bool`, optional\n        If `True` then Pandoc will markup output content to work with MathJax.\n        Default is False.\n\n    smart : `bool`, optional\n        If `True` (default) then ascii characters will be converted to unicode\n        characters like smart quotes and em dashes.\n\n    extra_args : `list`, optional\n        Sequence of Pandoc arguments command line arguments (such as\n        ``'--normalize'``). The ``deparagraph``, ``mathjax``, and ``smart``\n        arguments are convenience arguments that are equivalent to items\n        in ``extra_args``.\n\n    Returns\n    -------\n    output : `str`\n        Content in the output (``to_fmt``) format.\n\n    Notes\n    -----\n    This function will automatically install Pandoc if it is not available.\n    See `ensure_pandoc`."
        ],
        [
            "Convert lsstdoc-class LaTeX to another markup format.\n\n    This function is a thin wrapper around `convert_text` that automatically\n    includes common lsstdoc LaTeX macros.\n\n    Parameters\n    ----------\n    content : `str`\n        Original content.\n\n    to_fmt : `str`\n        Output format for the content (see https://pandoc.org/MANUAL.html).\n        For example, 'html5'.\n\n    deparagraph : `bool`, optional\n        If `True`, then the\n        `lsstprojectmeta.pandoc.filters.deparagraph.deparagraph` filter is\n        used to remove paragraph (``<p>``, for example) tags around a single\n        paragraph of content. That filter does not affect content that\n        consists of multiple blocks (several paragraphs, or lists, for\n        example). Default is `False`.\n\n        For example, **without** this filter Pandoc will convert\n        the string ``\"Title text\"`` to ``\"<p>Title text</p>\"`` in HTML. The\n        paragraph tags aren't useful if you intend to wrap the converted\n        content in different tags, like ``<h1>``, using your own templating\n        system.\n\n        **With** this filter, Pandoc will convert the string ``\"Title text\"``\n        to ``\"Title text\"`` in HTML.\n\n    mathjax : `bool`, optional\n        If `True` then Pandoc will markup output content to work with MathJax.\n        Default is False.\n\n    smart : `bool`, optional\n        If `True` (default) then ascii characters will be converted to unicode\n        characters like smart quotes and em dashes.\n\n    extra_args : `list`, optional\n        Sequence of Pandoc arguments command line arguments (such as\n        ``'--normalize'``). The ``deparagraph``, ``mathjax``, and ``smart``\n        arguments are convenience arguments that are equivalent to items\n        in ``extra_args``.\n\n    Returns\n    -------\n    output : `str`\n        Content in the output (``to_fmt``) format.\n\n    Notes\n    -----\n    This function will automatically install Pandoc if it is not available.\n    See `ensure_pandoc`."
        ],
        [
            "Decode a JSON-LD dataset, including decoding datetime\n    strings into `datetime.datetime` objects.\n\n    Parameters\n    ----------\n    encoded_dataset : `str`\n        The JSON-LD dataset encoded as a string.\n\n    Returns\n    -------\n    jsonld_dataset : `dict`\n        A JSON-LD dataset.\n\n    Examples\n    --------\n\n    >>> doc = '{\"dt\": \"2018-01-01T12:00:00Z\"}'\n    >>> decode_jsonld(doc)\n    {'dt': datetime.datetime(2018, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)}"
        ],
        [
            "Encode values as JSON strings.\n\n        This method overrides the default implementation from\n        `json.JSONEncoder`."
        ],
        [
            "Get all git repositories within this environment"
        ],
        [
            "Install a python package using pip"
        ],
        [
            "Update a python package using pip"
        ],
        [
            "Returns the nb quantiles for datas in a dataframe"
        ],
        [
            "Returns the root mean square error betwwen a and b"
        ],
        [
            "Returns the normalized mean square error of a and b"
        ],
        [
            "Returns the mean fractionalized bias error"
        ],
        [
            "Returns the factor of exceedance"
        ],
        [
            "Computes the correlation between a and b, says the Pearson's correlation\n    coefficient R"
        ],
        [
            "Geometric mean bias"
        ],
        [
            "Geometric mean variance"
        ],
        [
            "Figure of merit in time"
        ],
        [
            "Performs several stats on a against b, typically a is the predictions\n    array, and b the observations array\n\n    Returns:\n        A dataFrame of stat name, stat description, result"
        ],
        [
            "Path to environments site-packages"
        ],
        [
            "Prior to activating, store everything necessary to deactivate this\n        environment."
        ],
        [
            "Do some serious mangling to the current python environment...\n        This is necessary to activate an environment via python."
        ],
        [
            "Remove this environment"
        ],
        [
            "Command used to launch this application module"
        ],
        [
            "Create a virtual environment. You can pass either the name of a new\n    environment to create in your CPENV_HOME directory OR specify a full path\n    to create an environment outisde your CPENV_HOME.\n\n    Create an environment in CPENV_HOME::\n\n        >>> cpenv.create('myenv')\n\n    Create an environment elsewhere::\n\n        >>> cpenv.create('~/custom_location/myenv')\n\n    :param name_or_path: Name or full path of environment\n    :param config: Environment configuration including dependencies etc..."
        ],
        [
            "Remove an environment or module\n\n    :param name_or_path: name or path to environment or module"
        ],
        [
            "Activates and launches a module\n\n    :param module_name: name of module to launch"
        ],
        [
            "Deactivates an environment by restoring all env vars to a clean state\n    stored prior to activating environments"
        ],
        [
            "Returns a list of available modules."
        ],
        [
            "Add a module to CPENV_ACTIVE_MODULES environment variable"
        ],
        [
            "Remove a module from CPENV_ACTIVE_MODULES environment variable"
        ],
        [
            "Format a list of environments and modules for terminal output"
        ],
        [
            "Show context info"
        ],
        [
            "Activate an environment"
        ],
        [
            "Create a new environment."
        ],
        [
            "Remove an environment"
        ],
        [
            "Add an environment to the cache. Allows you to activate the environment\n    by name instead of by full path"
        ],
        [
            "Remove a cached environment. Removed paths will no longer be able to\n    be activated by name"
        ],
        [
            "Create a new template module.\n\n    You can also specify a filesystem path like \"./modules/new_module\""
        ],
        [
            "Add a module to an environment. PATH can be a git repository path or\n    a filesystem path."
        ],
        [
            "Copy a global module to the active environment."
        ],
        [
            "Resolves VirtualEnvironments with a relative or absolute path"
        ],
        [
            "Resolves VirtualEnvironments in CPENV_HOME"
        ],
        [
            "Resolves VirtualEnvironments in EnvironmentCache"
        ],
        [
            "Resolves module in previously resolved environment."
        ],
        [
            "Resolves modules in currently active environment."
        ],
        [
            "Resolves environment from .cpenv file...recursively walks up the tree\n    in attempt to find a .cpenv file"
        ],
        [
            "Returns a view of the array with axes transposed.\n\n    For a 1-D array, this has no effect.\n    For a 2-D array, this is the usual matrix transpose.\n    For an n-D array, if axes are given, their order indicates how the\n    axes are permuted\n\n    Args:\n      a (array_like): Input array.\n      axes (list of int, optional): By default, reverse the dimensions,\n        otherwise permute the axes according to the values given."
        ],
        [
            "Roll the specified axis backwards, until it lies in a given position.\n\n    Args:\n      a (array_like): Input array.\n      axis (int): The axis to roll backwards.  The positions of the other axes \n        do not change relative to one another.\n      start (int, optional): The axis is rolled until it lies before this \n        position.  The default, 0, results in a \"complete\" roll.\n\n    Returns:\n      res (ndarray)"
        ],
        [
            "Insert a new axis, corresponding to a given position in the array shape\n\n    Args:\n      a (array_like): Input array.\n      axis (int): Position (amongst axes) where new axis is to be inserted."
        ],
        [
            "Join a sequence of arrays together. \n    Will aim to join `ndarray`, `RemoteArray`, and `DistArray` without moving \n    their data, if they happen to be on different engines.\n\n    Args:\n      tup (sequence of array_like): Arrays to be concatenated. They must have\n        the same shape, except in the dimension corresponding to `axis`.\n      axis (int, optional): The axis along which the arrays will be joined.\n\n    Returns: \n      res: `ndarray`, if inputs were all local\n           `RemoteArray`, if inputs were all on the same remote engine\n           `DistArray`, if inputs were already scattered on different engines"
        ],
        [
            "Return the shape that would result from broadcasting the inputs"
        ],
        [
            "Compute the arithmetic mean along the specified axis.\n\n    Returns the average of the array elements.  The average is taken over\n    the flattened array by default, otherwise over the specified axis.\n    `float64` intermediate and return values are used for integer inputs.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose mean is desired. If `a` is not an\n        array, a conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the means are computed. The default is to\n        compute the mean of the flattened array.\n        If this is a tuple of ints, a mean is performed over multiple axes,\n        instead of a single axis or all the axes as before.\n    dtype : data-type, optional\n        Type to use in computing the mean.  For integer inputs, the default\n        is `float64`; for floating point inputs, it is the same as the\n        input dtype.\n    out : ndarray, optional\n        Alternate output array in which to place the result.  The default\n        is ``None``; if provided, it must have the same shape as the\n        expected output, but the type will be cast if necessary.\n        See `doc.ufuncs` for details.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original `arr`.\n\n    Returns\n    -------\n    m : ndarray, see dtype parameter above\n\n    Notes\n    -----\n    np.mean fails to pass the keepdims parameter to ndarray subclasses.\n    That is the main reason we implement this function."
        ],
        [
            "`ax` is a valid candidate for a distributed axis if the given\n        subarray shapes are all the same when ignoring axis `ax`"
        ],
        [
            "Returns True if successful, False if failure"
        ],
        [
            "Return a command to launch a subshell"
        ],
        [
            "Generate a prompt with a given prefix\n\n    linux/osx: [prefix] user@host cwd $\n          win: [prefix] cwd:"
        ],
        [
            "Launch a subshell"
        ],
        [
            "Append a file to file repository.\n\n        For file monitoring, monitor instance needs file.\n        Please put the name of file to `file` argument.\n\n        :param file: the name of file you want monitor."
        ],
        [
            "Append files to file repository.\n        \n        ModificationMonitor can append files to repository using this.\n        Please put the list of file names to `filelist` argument.\n\n        :param filelist: the list of file nmaes"
        ],
        [
            "Run file modification monitor.\n\n        The monitor can catch file modification using timestamp and file body. \n        Monitor has timestamp data and file body data. And insert timestamp \n        data and file body data before into while roop. In while roop, monitor \n        get new timestamp and file body, and then monitor compare new timestamp\n        to originaltimestamp. If new timestamp and file body differ original,\n        monitor regard thease changes as `modification`. Then monitor create\n        instance of FileModificationObjectManager and FileModificationObject,\n        and monitor insert FileModificationObject to FileModificationObject-\n        Manager. Then, yield this object.\n\n        :param sleep: How times do you sleep in while roop."
        ],
        [
            "Decorator that invokes `add_status_job`.\n\n        ::\n\n            @app.status_job\n            def postgresql():\n                # query/ping postgres\n\n            @app.status_job(name=\"Active Directory\")\n            def active_directory():\n                # query active directory\n\n            @app.status_job(timeout=5)\n            def paypal():\n                # query paypal, timeout after 5 seconds"
        ],
        [
            "Page through text by feeding it to another program.  Invoking a\n    pager through this might support colors."
        ],
        [
            "Calcul du profil annuel\n\n    Param\u00e8tres:\n    df: DataFrame de donn\u00e9es dont l'index est une s\u00e9rie temporelle\n        (cf module xair par exemple)\n    func: function permettant le calcul. Soit un nom de fonction numpy ('mean', 'max', ...)\n        soit la fonction elle-m\u00eame (np.mean, np.max, ...)\n    Retourne:\n    Un DataFrame de moyennes par mois"
        ],
        [
            "Attempt to run a global hook by name with args"
        ],
        [
            "Calcule de moyennes glissantes\n\n    Param\u00e8tres:\n    df: DataFrame de mesures sur lequel appliqu\u00e9 le calcul\n    sur: (int, par d\u00e9faut 8) Nombre d'observations sur lequel s'appuiera le\n    calcul\n    rep: (float, d\u00e9faut 0.75) Taux de r\u00e9pr\u00e9sentativit\u00e9 en dessous duquel le\n    calcul renverra NaN\n\n    Retourne:\n    Un DataFrame des moyennes glissantes calcul\u00e9es"
        ],
        [
            "Calcul de l'AOT40 du 1er mai au 31 juillet\n\n    *AOT40 : AOT 40 ( exprim\u00e9 en micro g/m\u00b3 par heure ) signifie la somme des\n    diff\u00e9rences entre les concentrations horaires sup\u00e9rieures \u00e0 40 parties par\n    milliard ( 40 ppb soit 80 micro g/m\u00b3 ), durant une p\u00e9riode donn\u00e9e en\n    utilisant uniquement les valeurs sur 1 heure mesur\u00e9es quotidiennement\n    entre 8 heures (d\u00e9but de la mesure) et 20 heures (pile, fin de la mesure) CET,\n    ce qui correspond \u00e0 de 8h \u00e0 19h TU (donnant bien 12h de mesures, 8h donnant\n    la moyenne horaire de 7h01 \u00e0 8h00)\n\n    Param\u00e8tres:\n    df: DataFrame de mesures sur lequel appliqu\u00e9 le calcul\n    nb_an: (int) Nombre d'ann\u00e9es contenu dans le df, et servant \u00e0 diviser le\n    r\u00e9sultat retourn\u00e9\n\n    Retourne:\n    Un DataFrame de r\u00e9sultat de calcul"
        ],
        [
            "Validate all the entries in the environment cache."
        ],
        [
            "Load the environment cache from disk."
        ],
        [
            "Save the environment cache to disk."
        ],
        [
            "Prompts a user for input.  This is a convenience function that can\n    be used to prompt a user for input later.\n\n    If the user aborts the input by sending a interrupt signal, this\n    function will catch it and raise a :exc:`Abort` exception.\n\n    .. versionadded:: 6.0\n       Added unicode support for cmd.exe on Windows.\n\n    .. versionadded:: 4.0\n       Added the `err` parameter.\n\n    :param text: the text to show for the prompt.\n    :param default: the default value to use if no input happens.  If this\n                    is not given it will prompt until it's aborted.\n    :param hide_input: if this is set to true then the input value will\n                       be hidden.\n    :param confirmation_prompt: asks for confirmation for the value.\n    :param type: the type to use to check the value against.\n    :param value_proc: if this parameter is provided it's a function that\n                       is invoked instead of the type conversion to\n                       convert a value.\n    :param prompt_suffix: a suffix that should be added to the prompt.\n    :param show_default: shows or hides the default value in the prompt.\n    :param err: if set to true the file defaults to ``stderr`` instead of\n                ``stdout``, the same as with echo."
        ],
        [
            "This function takes a text and shows it via an environment specific\n    pager on stdout.\n\n    .. versionchanged:: 3.0\n       Added the `color` flag.\n\n    :param text: the text to page.\n    :param color: controls if the pager supports ANSI colors or not.  The\n                  default is autodetection."
        ],
        [
            "Prepare all iPython engines for distributed object processing.\n\n    Args:\n      client (ipyparallel.Client, optional): If None, will create a client\n        using the default ipyparallel profile."
        ],
        [
            "Retrieve objects that have been distributed, making them local again"
        ],
        [
            "Apply a function in parallel to each element of the input"
        ],
        [
            "Configure engines so that remote methods returning values of type\n        `real_type` will instead return by proxy, as type `proxy_type`"
        ],
        [
            "Returns True if path is a git repository."
        ],
        [
            "Returns True if path is in CPENV_HOME"
        ],
        [
            "Returns True if path contains a .cpenv file"
        ],
        [
            "Get environment path from redirect file"
        ],
        [
            "Returns an absolute expanded path"
        ],
        [
            "Like os.path.join but also expands and normalizes path parts."
        ],
        [
            "Like os.path.join but acts relative to this packages bin path."
        ],
        [
            "Like os.makedirs but keeps quiet if path already exists"
        ],
        [
            "Walk down a directory tree. Same as os.walk but allows for a depth limit\n    via depth argument"
        ],
        [
            "Walk up a directory tree"
        ],
        [
            "Preprocess a dict to be used as environment variables.\n\n    :param d: dict to be processed"
        ],
        [
            "Add a sequence value to env dict"
        ],
        [
            "Join a bunch of dicts"
        ],
        [
            "Convert a dict containing environment variables into a standard dict.\n    Variables containing multiple values will be split into a list based on\n    the argument passed to pathsep.\n\n    :param env: Environment dict like os.environ.data\n    :param pathsep: Path separator used to split variables"
        ],
        [
            "Convert a python dict to a dict containing valid environment variable\n    values.\n\n    :param d: Dict to convert to an env dict\n    :param pathsep: Path separator used to join lists(default os.pathsep)"
        ],
        [
            "Expand all environment variables in an environment dict\n\n    :param env: Environment dict"
        ],
        [
            "Returns an unused random filepath."
        ],
        [
            "Encode current environment as yaml and store in path or a temporary\n    file. Return the path to the stored environment."
        ],
        [
            "Returns the URL to the upstream data source for the given URI based on configuration"
        ],
        [
            "Return request object for calling the upstream"
        ],
        [
            "Returns time to live in seconds. 0 means no caching.\n\n        Criteria:\n        - response code 200\n        - read-only method (GET, HEAD, OPTIONS)\n        Plus http headers:\n        - cache-control: option1, option2, ...\n          where options are:\n          private | public\n          no-cache\n          no-store\n          max-age: seconds\n          s-maxage: seconds\n          must-revalidate\n          proxy-revalidate\n        - expires: Thu, 01 Dec 1983 20:00:00 GMT\n        - pragma: no-cache (=cache-control: no-cache)\n\n        See http://www.mobify.com/blog/beginners-guide-to-http-cache-headers/\n\n        TODO: tests"
        ],
        [
            "Guarantee the existence of a basic MANIFEST.in.\n\n    manifest doc: http://docs.python.org/distutils/sourcedist.html#manifest\n\n    `options.paved.dist.manifest.include`: set of files (or globs) to include with the `include` directive.\n\n    `options.paved.dist.manifest.recursive_include`: set of files (or globs) to include with the `recursive-include` directive.\n\n    `options.paved.dist.manifest.prune`: set of files (or globs) to exclude with the `prune` directive.\n\n    `options.paved.dist.manifest.include_sphinx_docroot`: True -> sphinx docroot is added as `graft`\n\n    `options.paved.dist.manifest.include_sphinx_docroot`: True -> sphinx builddir is added as `prune`"
        ],
        [
            "Format a pathname\n\n    :param str pathname: Pathname to format\n    :param int max_length: Maximum length of result pathname (> 3)\n    :return: Formatted pathname\n    :rtype: str\n    :raises ValueError: If *max_length* is not larger than 3\n\n    This function formats a pathname so it is not longer than *max_length*\n    characters. The resulting pathname is returned. It does so by replacing\n    characters at the start of the *pathname* with three dots, if necessary.\n    The idea is that the end of the *pathname* is the most important part\n    to be able to identify the file."
        ],
        [
            "Format a UUID string\n\n    :param str uuid: UUID to format\n    :param int max_length: Maximum length of result string (> 3)\n    :return: Formatted UUID\n    :rtype: str\n    :raises ValueError: If *max_length* is not larger than 3\n\n    This function formats a UUID so it is not longer than *max_length*\n    characters. The resulting string is returned. It does so by replacing\n    characters at the end of the *uuid* with three dots, if necessary.\n    The idea is that the start of the *uuid* is the most important part\n    to be able to identify the related entity.\n\n    The default *max_length* is 10, which will result in a string\n    containing the first 7 characters of the *uuid* passed in. Most of\n    the time, such a string is still unique within a collection of UUIDs."
        ],
        [
            "attempts to get next and previous on updates"
        ],
        [
            "Notify the client of the result of handling a request\n\n    The payload contains two elements:\n\n    - client_id\n    - result\n\n    The *client_id* is the id of the client to notify. It is assumed\n    that the notifier service is able to identify the client by this id\n    and that it can pass the *result* to it.\n\n    The *result* always contains a *status_code* element. In case the\n    message passed in is not None, it will also contain a *message*\n    element.\n\n    In case the notifier service does not exist or returns an error,\n    an error message will be logged to *stderr*."
        ],
        [
            "Retrieves the setting value whose name is indicated by name_hyphen.\n\n        Values starting with $ are assumed to reference environment variables,\n        and the value stored in environment variables is retrieved. It's an\n        error if thes corresponding environment variable it not set."
        ],
        [
            "This method does the work of updating settings. Can be passed with\n        enforce_helpstring = False which you may want if allowing end users to\n        add arbitrary metadata via the settings system.\n\n        Preferable to use update_settings (without leading _) in code to do the\n        right thing and always have docstrings."
        ],
        [
            "Return a combined dictionary of setting values and attribute values."
        ],
        [
            "Detect if we get a class or a name, convert a name to a class."
        ],
        [
            "Asserts that the class has a docstring, returning it if successful."
        ],
        [
            "Get absolute path to resource, works for dev and for PyInstaller"
        ],
        [
            "Add new block of logbook selection windows. Only 5 allowed."
        ],
        [
            "Remove logbook menu set."
        ],
        [
            "Return selected log books by type."
        ],
        [
            "Verify enetered user name is on accepted MCC logbook list."
        ],
        [
            "Parse xml elements for pretty printing"
        ],
        [
            "Convert supplied QPixmap object to image file."
        ],
        [
            "Process user inputs and subit logbook entry when user clicks Submit button"
        ],
        [
            "Process log information and push to selected logbooks."
        ],
        [
            "Create graphical objects for menus."
        ],
        [
            "Display menus and connect even signals."
        ],
        [
            "Add or change list of logbooks."
        ],
        [
            "Remove unwanted logbooks from list."
        ],
        [
            "Populate log program list to correspond with log type selection."
        ],
        [
            "Add menus to parent gui."
        ],
        [
            "Iteratively remove graphical objects from layout."
        ],
        [
            "Adds labels to a plot."
        ],
        [
            "Determine the URL corresponding to Python object"
        ],
        [
            "Update the database with model schema. Shorthand for `paver manage syncdb`."
        ],
        [
            "Run the dev server.\n\n    Uses `django_extensions <http://pypi.python.org/pypi/django-extensions/0.5>`, if\n    available, to provide `runserver_plus`.\n\n    Set the command to use with `options.paved.django.runserver`\n    Set the port to use with `options.paved.django.runserver_port`"
        ],
        [
            "Run South's schemamigration command."
        ],
        [
            "This static method validates a BioMapMapper definition.\n        It returns None on success and throws an exception otherwise."
        ],
        [
            "The main method of this class and the essence of the package.\n        It allows to \"map\" stuff.\n\n        Args:\n\n            ID_s: Nested lists with strings as leafs (plain strings also possible)\n            FROM (str): Origin key for the mapping (default: main key)\n            TO (str): Destination key for the mapping (default: main key)\n            target_as_set (bool): Whether to summarize the output as a set (removes duplicates)\n            no_match_sub: Object representing the status of an ID not being able to be matched\n                          (default: None)\n\n        Returns:\n\n            Mapping: a mapping object capturing the result of the mapping request"
        ],
        [
            "Returns all data entries for a particular key. Default is the main key.\n\n        Args:\n\n            key (str): key whose values to return (default: main key)\n\n        Returns:\n\n            List of all data entries for the key"
        ],
        [
            "Returns list of strings split by input delimeter\n\n        Argument:\n        line - Input line to cut"
        ],
        [
            "Get Existing Message\n\n        http://dev.wheniwork.com/#get-existing-message"
        ],
        [
            "Creates a message\n\n        http://dev.wheniwork.com/#create/update-message"
        ],
        [
            "Modify an existing message.\n\n        http://dev.wheniwork.com/#create/update-message"
        ],
        [
            "Delete existing messages.\n\n        http://dev.wheniwork.com/#delete-existing-message"
        ],
        [
            "Returns site data.\n\n        http://dev.wheniwork.com/#get-existing-site"
        ],
        [
            "Returns a list of sites.\n\n        http://dev.wheniwork.com/#listing-sites"
        ],
        [
            "Creates a site\n\n        http://dev.wheniwork.com/#create-update-site"
        ],
        [
            "Returns a link to a view that moves the passed in object up in rank.\n\n    :param obj:\n        Object to move\n    :param link_text:\n        Text to display in the link.  Defaults to \"up\"\n    :returns:\n        HTML link code to view for moving the object"
        ],
        [
            "Returns a link to a view that moves the passed in object down in rank.\n\n    :param obj:\n        Object to move\n    :param link_text:\n        Text to display in the link.  Defaults to \"down\"\n    :returns:\n        HTML link code to view for moving the object"
        ],
        [
            "Shows a figure with a typical orientation so that x and y axes are set up as expected."
        ],
        [
            "Shifts indicies as needed to account for one based indexing\n\n    Positive indicies need to be reduced by one to match with zero based\n    indexing.\n\n    Zero is not a valid input, and as such will throw a value error.\n\n    Arguments:\n        index -     index to shift"
        ],
        [
            "Returns selected positions from cut input source in desired\n        arrangement.\n\n        Argument:\n            line -      input to cut"
        ],
        [
            "Processes positions to account for ranges\n\n        Arguments:\n            positions -     list of positions and/or ranges to process"
        ],
        [
            "Performs cut for range from start position to end\n\n        Arguments:\n            line -              input to cut\n            start -             start of range\n            current_position -  current position in main cut function"
        ],
        [
            "Creates list of values in a range with output delimiters.\n\n        Arguments:\n            start -     range start\n            end -       range end"
        ],
        [
            "Locks the file by writing a '.lock' file.\n       Returns True when the file is locked and\n       False when the file was locked already"
        ],
        [
            "Unlocks the file by remove a '.lock' file.\n       Returns True when the file is unlocked and\n       False when the file was unlocked already"
        ],
        [
            "Initiate the local catalog and push it the cloud"
        ],
        [
            "Initiate the local catalog by downloading the cloud catalog"
        ],
        [
            "Return nodes in the path between 'a' and 'b' going from\n        parent to child NOT including 'a'"
        ],
        [
            "Index of the last occurrence of x in the sequence."
        ],
        [
            "Create and save an admin user.\n\n    :param username:\n        Admin account's username.  Defaults to 'admin'\n    :param email:\n        Admin account's email address.  Defaults to 'admin@admin.com'\n    :param password:\n        Admin account's password.  Defaults to 'admin'\n    :returns:\n        Django user with staff and superuser privileges"
        ],
        [
            "Returns a list of the messages from the django MessageMiddleware\n    package contained within the given response.  This is to be used during\n    unit testing when trying to see if a message was set properly in a view.\n\n    :param response: HttpResponse object, likely obtained through a\n        test client.get() or client.post() call\n\n    :returns: a list of tuples (message_string, message_level), one for each\n        message in the response context"
        ],
        [
            "Authenticates the superuser account via the web login."
        ],
        [
            "Does a django test client ``get`` against the given url after\n        logging in the admin first.\n\n        :param url:\n            URL to fetch\n        :param response_code:\n            Expected response code from the URL fetch.  This value is\n            asserted.  Defaults to 200\n        :param headers:\n            Optional dictionary of headers to send in the request\n        :param follow:\n            When True, the get call will follow any redirect requests.\n            Defaults to False.\n        :returns:\n            Django testing ``Response`` object"
        ],
        [
            "Does a django test client ``post`` against the given url after\n        logging in the admin first.\n\n        :param url:\n            URL to fetch\n        :param data:\n            Dictionary to form contents to post\n        :param response_code:\n            Expected response code from the URL fetch.  This value is\n            asserted.  Defaults to 200\n        :param headers:\n            Optional dictionary of headers to send in with the request\n        :returns:\n            Django testing ``Response`` object"
        ],
        [
            "Returns the value displayed in the column on the web interface for\n        a given instance.\n\n        :param admin_model:\n            Instance of a :class:`admin.ModelAdmin` object that is responsible\n            for displaying the change list\n        :param instance:\n            Object instance that is the row in the admin change list\n        :field_name:\n            Name of the field/column to fetch"
        ],
        [
            "Highest value of input image."
        ],
        [
            "Lowest value of input image."
        ],
        [
            "spawns a greenlet that does not print exceptions to the screen.\n    if you use this function you MUST use this module's join or joinall otherwise the exception will be lost"
        ],
        [
            "Returns usage string with no trailing whitespace."
        ],
        [
            "Setup argparser to process arguments and generate help"
        ],
        [
            "Opens connection to S3 returning bucket and key"
        ],
        [
            "Upload a local file to S3."
        ],
        [
            "Download a remote file from S3."
        ],
        [
            "Creates an ical .ics file for an event using python-card-me."
        ],
        [
            "Returns a list view of all comments for a given event.\n    Combines event comments and update comments in one list."
        ],
        [
            "Returns a list view of updates for a given event.\n    If the event is over, it will be in chronological order.\n    If the event is upcoming or still going,\n    it will be in reverse chronological order."
        ],
        [
            "Displays list of videos for given event."
        ],
        [
            "Public form to add an event."
        ],
        [
            "Adds a memory to an event."
        ],
        [
            "Inserts Interpreter Library of imports into sketch in a very non-consensual way"
        ],
        [
            "Sets the beam moments directly.\n\n        Parameters\n        ----------\n        sx : float\n            Beam moment where :math:`\\\\text{sx}^2 = \\\\langle x^2 \\\\rangle`.\n        sxp : float\n            Beam moment where :math:`\\\\text{sxp}^2 = \\\\langle x'^2 \\\\rangle`.\n        sxxp : float\n            Beam moment where :math:`\\\\text{sxxp} = \\\\langle x x' \\\\rangle`."
        ],
        [
            "Sets the beam moments indirectly using Courant-Snyder parameters.\n\n        Parameters\n        ----------\n        beta : float\n            Courant-Snyder parameter :math:`\\\\beta`.\n        alpha : float\n            Courant-Snyder parameter :math:`\\\\alpha`.\n        emit : float\n            Beam emittance :math:`\\\\epsilon`.\n        emit_n : float\n            Normalized beam emittance :math:`\\\\gamma \\\\epsilon`."
        ],
        [
            "Given a slice object, return appropriate values for use in the range function\n\n    :param slice_obj: The slice object or integer provided in the `[]` notation\n    :param length: For negative indexing we need to know the max length of the object."
        ],
        [
            "Helper to add error to messages field. It fills placeholder with extra call parameters\n        or values from message_value map.\n\n        :param error_code: Error code to use\n        :rparam error_code: str\n        :param value: Value checked\n        :param kwargs: Map of values to use in placeholders"
        ],
        [
            "File copy that support compress and decompress of zip files"
        ],
        [
            "Apply to the 'catalog' the changesets in the metafile list 'changesets"
        ],
        [
            "Validate that an event with this name on this date does not exist."
        ],
        [
            "When entering the context, spawns a greenlet that sleeps for `interval` seconds between `callback` executions.\n    When leaving the context stops the greenlet.\n    The yielded object is the `GeventLoop` object so the loop can be stopped from within the context.\n\n    For example:\n    ```\n    with loop_in_background(60.0, purge_cache) as purge_cache_job:\n        ...\n        ...\n        if should_stop_cache():\n            purge_cache_job.stop()\n    ```"
        ],
        [
            "Main loop - used internally."
        ],
        [
            "Starts the loop. Calling a running loop is an error."
        ],
        [
            "Kills the running loop and waits till it gets killed."
        ],
        [
            "Used to plot a set of coordinates.\n\n\n    Parameters\n    ----------\n    x, y : :class:`numpy.ndarray`\n        1-D ndarrays of lengths N and M, respectively, specifying pixel centers\n    z : :class:`numpy.ndarray`\n        An (M, N) ndarray or masked array of values to be colormapped, or a (M, N, 3) RGB array, or a (M, N, 4) RGBA array.\n    ax : :class:`matplotlib.axes.Axes`, optional\n        The axis to plot to.\n    fig : :class:`matplotlib.figure.Figure`, optional\n        The figure to plot to.\n    cmap : :class:`matplotlib.colors.Colormap`, optional\n        The colormap to use.\n    alpha : float, optional\n        The transparency to use.\n    scalex : bool, optional\n        To set the x limits to available data\n    scaley : bool, optional\n        To set the y limits to available data\n    add_cbar : bool, optional\n        Whether ot add a colorbar or not.\n\n    Returns\n    -------\n    img : :class:`matplotlib.image.NonUniformImage`\n        Object representing the :class:`matplotlib.image.NonUniformImage`."
        ],
        [
            "Fix common spacing errors caused by LaTeX's habit\n        of using an inter-sentence space after any full stop."
        ],
        [
            "Transform hyphens to various kinds of dashes"
        ],
        [
            "Replace target with replacement"
        ],
        [
            "Regex substitute target with replacement"
        ],
        [
            "Call the Sphinx Makefile with the specified targets.\n\n    `options.paved.docs.path`: the path to the Sphinx folder (where the Makefile resides)."
        ],
        [
            "Upload the docs to a remote location via rsync.\n\n    `options.paved.docs.rsync_location`: the target location to rsync files to.\n\n    `options.paved.docs.path`: the path to the Sphinx folder (where the Makefile resides).\n\n    `options.paved.docs.build_rel`: the path of the documentation\n        build folder, relative to `options.paved.docs.path`."
        ],
        [
            "Push Sphinx docs to github_ gh-pages branch.\n\n     1. Create file .nojekyll\n     2. Push the branch to origin/gh-pages\n        after committing using ghp-import_\n\n    Requirements:\n     - easy_install ghp-import\n\n    Options:\n     - `options.paved.docs.*` is not used\n     - `options.sphinx.docroot` is used (default=docs)\n     - `options.sphinx.builddir` is used (default=.build)\n\n    .. warning::\n        This will DESTROY your gh-pages branch.\n        If you love it, you'll want to take backups\n        before playing with this. This script assumes\n        that gh-pages is 100% derivative. You should\n        never edit files in your gh-pages branch by hand\n        if you're using this script because you will\n        lose your work.\n\n    .. _github: https://github.com\n    .. _ghp-import: https://github.com/davisp/ghp-import"
        ],
        [
            "Open your web browser and display the generated html documentation."
        ],
        [
            "Tries to minimize the length of CSS code passed as parameter. Returns string."
        ],
        [
            "Return an open file-object to the index file"
        ],
        [
            "Create the tasks on the server"
        ],
        [
            "Update existing tasks on the server"
        ],
        [
            "Reconcile this collection with the server."
        ],
        [
            "Prompts the user for yes or no."
        ],
        [
            "Prompts the user with custom options."
        ],
        [
            "Reading the configure file and adds non-existing attributes to 'args"
        ],
        [
            "Writing the configure file with the attributes in 'args"
        ],
        [
            "Create a new instance of a game. Note, a mode MUST be provided and MUST be of\n        type GameMode.\n\n        :param mode: <required>"
        ],
        [
            "Bumps the Version given a target\n\n        The target can be either MAJOR, MINOR or PATCH"
        ],
        [
            "Returns a copy of this object"
        ],
        [
            "Returns a Tag with a given revision"
        ],
        [
            "Parses a string into a Tag"
        ],
        [
            "Tiles open figures."
        ],
        [
            "When a Comment is added, updates the Update to set \"last_updated\" time"
        ],
        [
            "Adds useful global items to the context for use in templates.\n\n    * *request*: the request object\n    * *HOST*: host name of server\n    * *IN_ADMIN*: True if you are in the django admin area"
        ],
        [
            "Create the challenge on the server"
        ],
        [
            "Update existing challenge on the server"
        ],
        [
            "Check if a challenge exists on the server"
        ],
        [
            "Returns position data.\n\n        http://dev.wheniwork.com/#get-existing-position"
        ],
        [
            "Returns a list of positions.\n\n        http://dev.wheniwork.com/#listing-positions"
        ],
        [
            "Creates a position\n\n        http://dev.wheniwork.com/#create-update-position"
        ],
        [
            "Print \"Source Lines of Code\" and export to file.\n\n    Export is hudson_ plugin_ compatible: sloccount.sc\n\n    requirements:\n     - sloccount_ should be installed.\n     - tee and pipes are used\n\n    options.paved.pycheck.sloccount.param\n\n    .. _sloccount: http://www.dwheeler.com/sloccount/\n    .. _hudson: http://hudson-ci.org/\n    .. _plugin: http://wiki.hudson-ci.org/display/HUDSON/SLOCCount+Plugin"
        ],
        [
            "passive check of python programs by pyflakes.\n\n    requirements:\n     - pyflakes_ should be installed. ``easy_install pyflakes``\n\n    options.paved.pycheck.pyflakes.param\n\n    .. _pyflakes: http://pypi.python.org/pypi/pyflakes"
        ],
        [
            "Handle HTTP exception\n\n    :param werkzeug.exceptions.HTTPException exception: Raised exception\n\n    A response is returned, as formatted by the :py:func:`response` function."
        ],
        [
            "Returns True if the value given is a valid CSS colour, i.e. matches one\n    of the regular expressions in the module or is in the list of\n    predetefined values by the browser."
        ],
        [
            "Reynold number utility function that return Reynold number for vehicle at specific length and speed.\n    Optionally, it can also take account of temperature effect of sea water.\n\n        Kinematic viscosity from: http://web.mit.edu/seawater/2017_MIT_Seawater_Property_Tables_r2.pdf\n\n    :param length: metres length of the vehicle\n    :param speed: m/s speed of the vehicle\n    :param temperature: degree C \n    :return: Reynolds number of the vehicle (dimensionless)"
        ],
        [
            "Froude number utility function that return Froude number for vehicle at specific length and speed.\n\n    :param speed: m/s speed of the vehicle\n    :param length: metres length of the vehicle\n    :return: Froude number of the vehicle (dimensionless)"
        ],
        [
            "Residual resistance coefficient estimation from slenderness function, prismatic coefficient and Froude number.\n\n    :param slenderness: Slenderness coefficient dimensionless :math:`L/(\u2207^{1/3})` where L is length of ship, \u2207 is displacement\n    :param prismatic_coef: Prismatic coefficient dimensionless :math:`\u2207/(L\\cdot A_m)` where L is length of ship, \u2207 is displacement Am is midsection area of the ship\n    :param froude_number: Froude number of the ship dimensionless \n    :return: Residual resistance of the ship"
        ],
        [
            "Assign values for the main dimension of a ship.\n\n        :param length: metres length of the vehicle\n        :param draught: metres draught of the vehicle\n        :param beam: metres beam of the vehicle\n        :param speed: m/s speed of the vehicle\n        :param slenderness_coefficient: Slenderness coefficient dimensionless :math:`L/(\u2207^{1/3})` where L is length of ship,\n            \u2207 is displacement\n        :param prismatic_coefficient: Prismatic coefficient dimensionless :math:`\u2207/(L\\cdot A_m)` where L is length of ship,\n            \u2207 is displacement Am is midsection area of the ship"
        ],
        [
            "Return resistance of the vehicle.\n\n        :return: newton the resistance of the ship"
        ],
        [
            "Return the maximum deck area of the ship\n\n        :param water_plane_coef: optional water plane coefficient\n        :return: Area of the deck"
        ],
        [
            "Total propulsion power of the ship.\n\n        :param propulsion_eff: Shaft efficiency of the ship\n        :param sea_margin: Sea margin take account of interaction between ship and the sea, e.g. wave\n        :return: Watts shaft propulsion power of the ship"
        ],
        [
            "Configure the api to use given url and token or to get them from the\n        Config."
        ],
        [
            "Send zipfile to TMC for given exercise"
        ],
        [
            "Ensures that the request url is valid.\n        Sometimes we have URLs that the server gives that are preformatted,\n        sometimes we need to form our own."
        ],
        [
            "Extract json from a response.\n            Assumes response is valid otherwise.\n            Internal use only."
        ],
        [
            "Wrapper for gevent.joinall if the greenlet that waits for the joins is killed, it kills all the greenlets it\n    joins for."
        ],
        [
            "Creates an error from the given code, and args and kwargs.\n\n    :param code: The acknowledgement code\n    :param args: Exception args\n    :param kwargs: Exception kwargs\n    :return: the error for the given acknowledgement code"
        ],
        [
            "Creates an error Acknowledgement message.\n        The message's code and message are taken from this exception.\n\n        :return: the message representing this exception"
        ],
        [
            "Clean up extra files littering the source tree.\n\n    options.paved.clean.dirs: directories to search recursively\n    options.paved.clean.patterns: patterns to search for and remove"
        ],
        [
            "print paver options.\n\n    Prettified by json.\n    `long_description` is removed"
        ],
        [
            "\\\n        Parses a binary protobuf message into a Message object."
        ],
        [
            "Creates a new ``Node`` based on the extending class and adds it as\n        a child to this ``Node``.\n\n        :param kwargs: \n            arguments for constructing the data object associated with this\n            ``Node``\n        :returns: \n            extender of the ``Node`` class"
        ],
        [
            "Returns a list of the ancestors of this node."
        ],
        [
            "Returns a list of the ancestors of this node but does not pass the\n        root node, even if the root has parents due to cycles."
        ],
        [
            "Returns a list of descendents of this node."
        ],
        [
            "Returns True if it is legal to remove this node and still leave the\n        graph as a single connected entity, not splitting it into a forest.\n        Only nodes with no children or those who cause a cycle can be deleted."
        ],
        [
            "Removes the node and all descendents without looping back past the\n        root.  Note this does not remove the associated data objects.\n\n        :returns:\n            list of :class:`BaseDataNode` subclassers associated with the\n            removed ``Node`` objects."
        ],
        [
            "Returns a list of nodes that would be removed if prune were called\n        on this element."
        ],
        [
            "Called to verify that the given rule can become a child of the\n        current node.  \n\n        :raises AttributeError: \n            if the child is not allowed"
        ],
        [
            "Returns location data.\n\n        http://dev.wheniwork.com/#get-existing-location"
        ],
        [
            "Returns a list of locations.\n\n        http://dev.wheniwork.com/#listing-locations"
        ],
        [
            "The reduced chi-square of the linear least squares"
        ],
        [
            "Create the task on the server"
        ],
        [
            "Update existing task on the server"
        ],
        [
            "Retrieve a task from the server"
        ],
        [
            "Formats a string with color"
        ],
        [
            "Returns user profile data.\n\n        http://dev.wheniwork.com/#get-existing-user"
        ],
        [
            "Returns a list of users.\n\n        http://dev.wheniwork.com/#listing-users"
        ],
        [
            "Attempt to set the virtualenv activate command, if it hasn't been specified."
        ],
        [
            "Recursively update the destination dict-like object with the source dict-like object.\n\n    Useful for merging options and Bunches together!\n\n    Based on:\n    http://code.activestate.com/recipes/499335-recursively-update-a-dictionary-without-hitting-py/#c1"
        ],
        [
            "Send the given arguments to `pip install`."
        ],
        [
            "When I Work GET method. Return representation of the requested\n        resource."
        ],
        [
            "When I Work PUT method."
        ],
        [
            "When I Work POST method."
        ],
        [
            "When I Work DELETE method."
        ],
        [
            "Creates a shift\n\n        http://dev.wheniwork.com/#create/update-shift"
        ],
        [
            "Delete existing shifts.\n\n        http://dev.wheniwork.com/#delete-shift"
        ],
        [
            "Returns combined list of event and update comments."
        ],
        [
            "Returns chained list of event and update images."
        ],
        [
            "Gets count of all images from both event and updates."
        ],
        [
            "Gets images and videos to populate top assets.\n\n        Map is built separately."
        ],
        [
            "Decorated methods progress will be displayed to the user as a spinner.\n        Mostly for slower functions that do some network IO."
        ],
        [
            "Launches a new menu. Wraps curses nicely so exceptions won't screw with\n        the terminal too much."
        ],
        [
            "Overridden method that handles that re-ranking of objects and the\n        integrity of the ``rank`` field.\n\n        :param rerank:\n            Added parameter, if True will rerank other objects based on the\n            change in this save.  Defaults to True."
        ],
        [
            "Removes any blank ranks in the order."
        ],
        [
            "Returns the field names of a Django model object.\n\n    :param obj: the Django model class or object instance to get the fields\n        from\n    :param ignore_auto: ignore any fields of type AutoField. Defaults to True\n    :param ignore_relations: ignore any fields that involve relations such as\n        the ForeignKey or ManyToManyField\n    :param exclude: exclude anything in this list from the results\n\n    :returns: generator of found field names"
        ],
        [
            "Register all HTTP error code error handlers\n\n    Currently, errors are handled by the JSON error handler."
        ],
        [
            "Plots but automatically resizes x axis.\n\n    .. versionadded:: 1.4\n\n    Parameters\n    ----------\n    args\n        Passed on to :meth:`matplotlib.axis.Axis.plot`.\n    ax : :class:`matplotlib.axis.Axis`, optional\n        The axis to plot to.\n    kwargs\n        Passed on to :meth:`matplotlib.axis.Axis.plot`."
        ],
        [
            "Create a vector of values over an interval with a specified step size.\n\n    Parameters\n    ----------\n\n    start : float\n        The beginning of the interval.\n    stop : float\n        The end of the interval.\n    step : float\n        The step size.\n\n    Returns\n    -------\n    vector : :class:`numpy.ndarray`\n        The vector of values."
        ],
        [
            "Passes the selected course as the first argument to func."
        ],
        [
            "Passes the selected exercise as the first argument to func."
        ],
        [
            "If func returns False the program exits immediately."
        ],
        [
            "Configure tmc.py to use your account."
        ],
        [
            "Download the exercises from the server."
        ],
        [
            "Go to the next exercise."
        ],
        [
            "Spawns a process with `command path-of-exercise`"
        ],
        [
            "Select a course or an exercise."
        ],
        [
            "Submit the selected exercise to the server."
        ],
        [
            "Sends the selected exercise to the TMC pastebin."
        ],
        [
            "Update the data of courses and or exercises from server."
        ],
        [
            "Determine the type of x"
        ],
        [
            "map for a directory"
        ],
        [
            "Apply the types on the elements of the line"
        ],
        [
            "Convert a file to a .csv file"
        ],
        [
            "Returns a link to the django admin change list with a filter set to\n    only the object given.\n\n    :param obj:\n        Object to create the admin change list display link for\n    :param display:\n        Text to display in the link.  Defaults to string call of the object\n    :returns:\n        Text containing HTML for a link"
        ],
        [
            "Returns string representation of an object, either the default or based\n    on the display template passed in."
        ],
        [
            "Adds a ``list_display`` attribute that appears as a link to the\n        django admin change page for the type of object being shown. Supports\n        double underscore attribute name dereferencing.\n\n        :param attr:\n            Name of the attribute to dereference from the corresponding\n            object, i.e. what will be lined to.  This name supports double\n            underscore object link referencing for ``models.ForeignKey``\n            members.\n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``attr``\n\n        :param display:\n            What to display as the text for the link being shown.  If not\n            given it defaults to the string representation of the object for\n            the row: ``str(obj)`` .  This parameter supports django\n            templating, the context for which contains a dictionary key named\n            \"obj\" with the value being the object for the row.\n\n        Example usage:\n\n        .. code-block:: python\n\n            # ---- admin.py file ----\n\n            base = fancy_modeladmin('id')\n            base.add_link('author', 'Our Authors',\n                '{{obj.name}} (id={{obj.id}})')\n\n            @admin.register(Book)\n            class BookAdmin(base):\n                pass\n\n        The django admin change page for the Book class would have a column\n        for \"id\" and another titled \"Our Authors\". The \"Our Authors\" column\n        would have a link for each Author object referenced by \"book.author\".\n        The link would go to the Author django admin change listing. The\n        display of the link would be the name of the author with the id in\n        brakcets, e.g. \"Douglas Adams (id=42)\""
        ],
        [
            "Adds a ``list_display`` attribute showing an object.  Supports\n        double underscore attribute name dereferencing.\n\n        :param attr:\n            Name of the attribute to dereference from the corresponding\n            object, i.e. what will be lined to.  This name supports double\n            underscore object link referencing for ``models.ForeignKey``\n            members.\n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``attr``\n\n        :param display:\n            What to display as the text for the link being shown.  If not\n            given it defaults to the string representation of the object for\n            the row: ``str(obj)``.  This parameter supports django templating,\n            the context for which contains a dictionary key named \"obj\" with\n            the value being the object for the row."
        ],
        [
            "Adds a ``list_display`` attribute showing a field in the object\n        using a python %formatted string.\n\n        :param field:\n            Name of the field in the object.\n\n        :param format_string:\n            A old-style (to remain python 2.x compatible) % string formatter\n            with a single variable reference. The named ``field`` attribute\n            will be passed to the formatter using the \"%\" operator. \n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``field``"
        ],
        [
            "View decorator that enforces that the method was called using POST.\n    This decorator can be called with or without parameters.  As it is\n    expected to wrap a view, the first argument of the method being wrapped is\n    expected to be a ``request`` object.\n\n    .. code-block:: python\n\n        @post_required\n        def some_view(request):\n            pass\n\n\n        @post_required(['firstname', 'lastname'])\n        def some_view(request):\n            pass\n\n    The optional parameter contains a single list which specifies the names of\n    the expected fields in the POST dictionary.  The list is not exclusive,\n    you can pass in fields that are not checked by the decorator.\n\n    :param options:\n        List of the names of expected POST keys."
        ],
        [
            "View decorator that enforces that the method was called using POST and\n    contains a field containing a JSON dictionary. This method should\n    only be used to wrap views and assumes the first argument of the method\n    being wrapped is a ``request`` object.\n\n    .. code-block:: python\n\n        @json_post_required('data', 'json_data')\n        def some_view(request):\n            username = request.json_data['username']\n\n    :param field:\n        The name of the POST field that contains a JSON dictionary\n    :param request_name:\n        [optional] Name of the parameter on the request to put the\n        deserialized JSON data. If not given the field name is used"
        ],
        [
            "Divergence of matched beam"
        ],
        [
            "The plasma density in SI units."
        ],
        [
            "Semver tag triggered deployment helper"
        ],
        [
            "Performs some environment checks prior to the program's execution"
        ],
        [
            "Prints latest tag's information"
        ],
        [
            "Prompts user before proceeding"
        ],
        [
            "Gets an array from datasets.\n\n    .. versionadded:: 1.4"
        ],
        [
            "Get the current directory state"
        ],
        [
            "Add one tick to progress bar"
        ],
        [
            "Push k to the top of the list\n\n        >>> l = DLL()\n        >>> l.push(1)\n        >>> l\n        [1]\n        >>> l.push(2)\n        >>> l\n        [2, 1]\n        >>> l.push(3)\n        >>> l\n        [3, 2, 1]"
        ],
        [
            "Call this method to increment the named counter.  This is atomic on\n        the database.\n\n        :param name:\n            Name for a previously created ``Counter`` object"
        ],
        [
            "print loading message on screen\n\n        .. note::\n\n            loading message only write to `sys.stdout`\n\n\n        :param int wait: seconds to wait\n        :param str message: message to print\n        :return: None"
        ],
        [
            "print warn type message,\n        if file handle is `sys.stdout`, print color message\n\n\n        :param str message: message to print\n        :param file fh: file handle,default is `sys.stdout`\n        :param str prefix: message prefix,default is `[warn]`\n        :param str suffix: message suffix ,default is `...`\n        :return: None"
        ],
        [
            "print error type message\n        if file handle is `sys.stderr`, print color message\n\n        :param str message: message to print\n        :param file fh: file handle, default is `sys.stdout`\n        :param str prefix: message prefix,default is `[error]`\n        :param str suffix: message suffix ,default is '...'\n        :return: None"
        ],
        [
            "a built-in wrapper make dry-run easier.\n        you should use this instead use `os.system`\n\n        .. note::\n\n            to use it,you need add '--dry-run' option in\n            your argparser options\n\n\n        :param str cmd: command to execute\n        :param bool fake_code: only display command\n            when is True,default is False\n        :return:"
        ],
        [
            "Returns a Corrected URL to be used for a Request\n        as per the REST API."
        ],
        [
            "Main method.\n\n    This method holds what you want to execute when\n    the script is run on command line."
        ],
        [
            "Pickle and compress."
        ],
        [
            "Decompress and unpickle."
        ],
        [
            "Displays the contact form and sends the email"
        ],
        [
            "try use gitconfig info.\n        author,email etc."
        ],
        [
            "Init project."
        ],
        [
            "In general, you don't need to overwrite this method.\n\n        :param options:\n        :return:"
        ],
        [
            "Return a new filename to use as the combined file name for a\n    bunch of files, based on the SHA of their contents.\n    A precondition is that they all have the same file extension\n\n    Given that the list of files can have different paths, we aim to use the\n    most common path.\n\n    Example:\n      /somewhere/else/foo.js\n      /somewhere/bar.js\n      /somewhere/different/too/foobar.js\n    The result will be\n      /somewhere/148713695b4a4b9083e506086f061f9c.js\n\n    Another thing to note, if the filenames have timestamps in them, combine\n    them all and use the highest timestamp."
        ],
        [
            "Extract the oritentation EXIF tag from the image, which should be a PIL Image instance,\n    and if there is an orientation tag that would rotate the image, apply that rotation to\n    the Image instance given to do an in-place rotation.\n\n    :param Image im: Image instance to inspect\n    :return: A possibly transposed image instance"
        ],
        [
            "Start a new piece"
        ],
        [
            "Start a new site."
        ],
        [
            "Publish the site"
        ],
        [
            "Returns a list of the branches"
        ],
        [
            "Returns the currently active branch"
        ],
        [
            "Create a patch between tags"
        ],
        [
            "Create a callable that applies ``func`` to a value in a sequence.\n\n    If the value is not a sequence or is an empty sequence then ``None`` is\n    returned.\n\n    :type  func: `callable`\n    :param func: Callable to be applied to each result.\n\n    :type  n: `int`\n    :param n: Index of the value to apply ``func`` to."
        ],
        [
            "Create a callable that applies ``func`` to every value in a sequence.\n\n    If the value is not a sequence then an empty list is returned.\n\n    :type  func: `callable`\n    :param func: Callable to be applied to the first result."
        ],
        [
            "Parse a value as text.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat ``bytes`` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `unicode`\n    :return: Parsed text or ``None`` if ``value`` is neither `bytes` nor\n        `unicode`."
        ],
        [
            "Parse a value as an integer.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse\n\n    :type  base: `unicode` or `bytes`\n    :param base: Base to assume ``value`` is specified in.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat ``bytes`` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `int`\n    :return: Parsed integer or ``None`` if ``value`` could not be parsed as an\n        integer."
        ],
        [
            "Parse a value as a boolean.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse.\n\n    :type  true: `tuple` of `unicode`\n    :param true: Values to compare, ignoring case, for ``True`` values.\n\n    :type  false: `tuple` of `unicode`\n    :param false: Values to compare, ignoring case, for ``False`` values.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `bool`\n    :return: Parsed boolean or ``None`` if ``value`` did not match ``true`` or\n        ``false`` values."
        ],
        [
            "Parse a value as a delimited list.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse.\n\n    :type  parser: `callable` taking a `unicode` parameter\n    :param parser: Callable to map over the delimited text values.\n\n    :type  delimiter: `unicode`\n    :param delimiter: Delimiter text.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `list`\n    :return: List of parsed values."
        ],
        [
            "Parse a value as a POSIX timestamp in seconds.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse, which should be the number of seconds\n        since the epoch.\n\n    :type  _divisor: `float`\n    :param _divisor: Number to divide the value by.\n\n    :type  tz: `tzinfo`\n    :param tz: Timezone, defaults to UTC.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `datetime.datetime`\n    :return: Parsed datetime or ``None`` if ``value`` could not be parsed."
        ],
        [
            "Parse query parameters.\n\n    :type  expected: `dict` mapping `bytes` to `callable`\n    :param expected: Mapping of query argument names to argument parsing\n        callables.\n\n    :type  query: `dict` mapping `bytes` to `list` of `bytes`\n    :param query: Mapping of query argument names to lists of argument values,\n        this is the form that Twisted Web's `IRequest.args\n        <twisted:twisted.web.iweb.IRequest.args>` value takes.\n\n    :rtype: `dict` mapping `bytes` to `object`\n    :return: Mapping of query argument names to parsed argument values."
        ],
        [
            "Put metrics to cloudwatch. Metric shoult be instance or list of\n        instances of CloudWatchMetric"
        ],
        [
            "Render a given resource.\n\n    See `IResource.render <twisted:twisted.web.resource.IResource.render>`."
        ],
        [
            "Adapt a result to `IResource`.\n\n        Several adaptions are tried they are, in order: ``None``,\n        `IRenderable <twisted:twisted.web.iweb.IRenderable>`, `IResource\n        <twisted:twisted.web.resource.IResource>`, and `URLPath\n        <twisted:twisted.python.urlpath.URLPath>`. Anything else is returned as\n        is.\n\n        A `URLPath <twisted:twisted.python.urlpath.URLPath>` is treated as\n        a redirect."
        ],
        [
            "Handle the result from `IResource.render`.\n\n        If the result is a `Deferred` then return `NOT_DONE_YET` and add\n        a callback to write the result to the request when it arrives."
        ],
        [
            "Negotiate a handler based on the content types acceptable to the\n        client.\n\n        :rtype: 2-`tuple` of `twisted.web.iweb.IResource` and `bytes`\n        :return: Pair of a resource and the content type."
        ],
        [
            "Parse and sort an ``Accept`` header.\n\n    The header is sorted according to the ``q`` parameter for each header value.\n\n    @rtype: `OrderedDict` mapping `bytes` to `dict`\n    @return: Mapping of media types to header parameters."
        ],
        [
            "Split an HTTP header whose components are separated with commas.\n\n    Each component is then split on semicolons and the component arguments\n    converted into a `dict`.\n\n    @return: `list` of 2-`tuple` of `bytes`, `dict`\n    @return: List of header arguments and mapping of component argument names\n        to values."
        ],
        [
            "Extract an encoding from a ``Content-Type`` header.\n\n    @type  requestHeaders: `twisted.web.http_headers.Headers`\n    @param requestHeaders: Request headers.\n\n    @type  encoding: `bytes`\n    @param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one. Defaults to ``UTF-8``.\n\n    @rtype: `bytes`\n    @return: Content encoding."
        ],
        [
            "Create a nil-safe callable decorator.\n\n    If the wrapped callable receives ``None`` as its argument, it will return\n    ``None`` immediately."
        ],
        [
            "Get or set `Settings._wrapped`\n\n    :param str path: a python module file,\n        if user set it,write config to `Settings._wrapped`\n    :param str with_path: search path\n    :return: A instance of `Settings`"
        ],
        [
            "bind user variable to `_wrapped`\n\n        .. note::\n\n            you don't need call this method by yourself.\n\n            program will call it in  `cliez.parser.parse`\n\n\n        .. expection::\n\n            if path is not correct,will cause an `ImportError`\n\n\n        :param str mod_path: module path, *use dot style,'mod.mod1'*\n        :param str with_path: add path to `sys.path`,\n            if path is file,use its parent.\n        :return: A instance of `Settings`"
        ],
        [
            "Get the version from version module without importing more than\n    necessary."
        ],
        [
            "send a transaction immediately. Failed transactions are picked up by the TxBroadcaster\n\n        :param ip: specific peer IP to send tx to\n        :param port: port of specific peer\n        :param use_open_peers: use Arky's broadcast method"
        ],
        [
            "check if a tx is confirmed, else resend it.\n\n        :param use_open_peers: select random peers fro api/peers endpoint"
        ],
        [
            "Get sub-command list\n\n    .. note::\n\n        Don't use logger handle this function errors.\n\n        Because the error should be a code error,not runtime error.\n\n\n    :return: `list` matched sub-parser"
        ],
        [
            "Add class options to argparser options.\n\n    :param cliez.component.Component klass: subclass of Component\n    :param Namespace sub_parsers:\n    :param str default_epilog: default_epilog\n    :param list general_arguments: global options, defined by user\n    :return: Namespace subparser"
        ],
        [
            "parser cliez app\n\n    :param argparse.ArgumentParser parser: an instance\n        of argparse.ArgumentParser\n    :param argv: argument list,default is `sys.argv`\n    :type argv: list or tuple\n\n    :param str settings: settings option name,\n        default is settings.\n\n    :param object no_args_func: a callable object.if no sub-parser matched,\n        parser will call it.\n\n    :return:  an instance of `cliez.component.Component` or its subclass"
        ],
        [
            "Convert Hump style to underscore\n\n    :param name: Hump Character\n    :return: str"
        ],
        [
            "Fetches fuel prices for all stations."
        ],
        [
            "Gets the fuel prices for a specific fuel station."
        ],
        [
            "Gets all the fuel prices within the specified radius."
        ],
        [
            "Gets the fuel price trends for the given location and fuel types."
        ],
        [
            "Fetches API reference data.\n\n        :param modified_since: The response will be empty if no\n        changes have been made to the reference data since this\n        timestamp, otherwise all reference data will be returned."
        ],
        [
            "Called before template is applied."
        ],
        [
            "Match a route parameter.\n\n    `Any` is a synonym for `Text`.\n\n    :type  name: `bytes`\n    :param name: Route parameter name.\n\n    :type  encoding: `bytes`\n    :param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one.\n\n    :return: ``callable`` suitable for use with `route` or `subroute`."
        ],
        [
            "Match an integer route parameter.\n\n    :type  name: `bytes`\n    :param name: Route parameter name.\n\n    :type  base: `int`\n    :param base: Base to interpret the value in.\n\n    :type  encoding: `bytes`\n    :param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one.\n\n    :return: ``callable`` suitable for use with `route` or `subroute`."
        ],
        [
            "Match a request path against our path components.\n\n    The path components are always matched relative to their parent is in the\n    resource hierarchy, in other words it is only possible to match URIs nested\n    more deeply than the parent resource.\n\n    :type  components: ``iterable`` of `bytes` or `callable`\n    :param components: Iterable of path components, to match against the\n        request, either static strings or dynamic parameters. As a convenience,\n        a single `bytes` component containing ``/`` may be given instead of\n        manually separating the components. If no components are given the null\n        route is matched, this is the case where ``segments`` is empty.\n\n    :type  segments: ``sequence`` of `bytes`\n    :param segments: Sequence of path segments, from the request, to match\n        against.\n\n    :type  partialMatching: `bool`\n    :param partialMatching: Allow partial matching against the request path?\n\n    :rtype: 2-`tuple` of `dict` keyed on `bytes` and `list` of `bytes`\n    :return: Pair of parameter results, mapping parameter names to processed\n        values, and a list of the remaining request path segments. If there is\n        no route match the result will be ``None`` and the original request path\n        segments."
        ],
        [
            "Decorate a router-producing callable to instead produce a resource.\n\n    This simply produces a new callable that invokes the original callable, and\n    calls ``resource`` on the ``routerAttribute``.\n\n    If the router producer has multiple routers the attribute can be altered to\n    choose the appropriate one, for example:\n\n    .. code-block:: python\n\n        class _ComplexRouter(object):\n            router = Router()\n            privateRouter = Router()\n\n            @router.route('/')\n            def publicRoot(self, request, params):\n                return SomethingPublic(...)\n\n            @privateRouter.route('/')\n            def privateRoot(self, request, params):\n                return SomethingPrivate(...)\n\n        PublicResource = routedResource(_ComplexRouter)\n        PrivateResource = routedResource(_ComplexRouter, 'privateRouter')\n\n    :type  f: ``callable``\n    :param f: Callable producing an object with a `Router` attribute, for\n        example, a type.\n\n    :type  routerAttribute: `str`\n    :param routerAttribute: Name of the `Router` attribute on the result of\n        calling ``f``.\n\n    :rtype: `callable`\n    :return: Callable producing an `IResource`."
        ],
        [
            "Create a new `Router` instance, with it's own set of routes, for\n        ``obj``."
        ],
        [
            "Add a route handler and matcher to the collection of possible routes."
        ],
        [
            "See `txspinneret.route.route`.\n\n        This decorator can be stacked with itself to specify multiple routes\n        with a single handler."
        ],
        [
            "See `txspinneret.route.subroute`.\n\n        This decorator can be stacked with itself to specify multiple routes\n        with a single handler."
        ],
        [
            "Create a NamedTemporaryFile instance to be passed to atomic_writer"
        ],
        [
            "Open a NamedTemoraryFile handle in a context manager"
        ],
        [
            "Read entry from JSON file"
        ],
        [
            "Save entry to JSON file"
        ],
        [
            "Update entry by UUID in the JSON file"
        ],
        [
            "Get the number of the shell command."
        ],
        [
            "Execute a shell command."
        ],
        [
            "Simple program that creates an temp S3 link."
        ],
        [
            "Poll ``self.stdout`` and return True if it is readable.\n\n        :param float timeout: seconds to wait I/O\n        :return: True if readable, else False\n        :rtype: boolean"
        ],
        [
            "Retrieve a list of characters and escape codes where each escape\n        code uses only one index. The indexes will not match up with the\n        indexes in the original string."
        ],
        [
            "Strip all color codes from a string.\n        Returns empty string for \"falsey\" inputs."
        ],
        [
            "Called when builder group collect files\n        Resolves absolute url if relative passed\n\n        :type asset: static_bundle.builders.Asset\n        :type builder: static_bundle.builders.StandardBuilder"
        ],
        [
            "Add single file or list of files to bundle\n\n        :type: file_path: str|unicode"
        ],
        [
            "Add directory or directories list to bundle\n\n        :param exclusions: List of excluded paths\n\n        :type path: str|unicode\n        :type exclusions: list"
        ],
        [
            "Add custom path objects\n\n        :type: path_object: static_bundle.paths.AbstractPath"
        ],
        [
            "Add prepare handler to bundle\n\n        :type: prepare_handler: static_bundle.handlers.AbstractPrepareHandler"
        ],
        [
            "Called when builder run collect files in builder group\n\n        :rtype: list[static_bundle.files.StaticFileResult]"
        ],
        [
            "Get the number of files in the folder."
        ],
        [
            "Register the contents as JSON"
        ],
        [
            "Translate the data with the translation table"
        ],
        [
            "Get the data in JSON form"
        ],
        [
            "Get the data as JSON tuples"
        ],
        [
            "Issues a GET request against the API, properly formatting the params\n\n        :param url: a string, the url you are requesting\n        :param params: a dict, the key-value of all the paramaters needed\n                       in the request\n        :returns: a dict parsed of the JSON response"
        ],
        [
            "Issues a POST request against the API, allows for multipart data uploads\n\n        :param url: a string, the url you are requesting\n        :param params: a dict, the key-value of all the parameters needed\n                       in the request\n        :param files: a list, the list of tuples of files\n\n        :returns: a dict parsed of the JSON response"
        ],
        [
            "Go through the env var map, transferring the values to this object\n        as attributes.\n\n        :raises: RuntimeError if a required env var isn't defined."
        ],
        [
            "Create a temporary directory with input data for the test.\n    The directory contents is copied from a directory with the same name as the module located in the same directory of\n    the test module."
        ],
        [
            "Compare two files contents. If the files differ, show the diff and write a nice HTML\n        diff file into the data directory.\n\n        Searches for the filenames both inside and outside the data directory (in that order).\n\n        :param unicode obtained_fn: basename to obtained file into the data directory, or full path.\n\n        :param unicode expected_fn: basename to expected file into the data directory, or full path.\n\n        :param bool binary:\n            Thread both files as binary files.\n\n        :param unicode encoding:\n            File's encoding. If not None, contents obtained from file will be decoded using this\n            `encoding`.\n\n        :param callable fix_callback:\n            A callback to \"fix\" the contents of the obtained (first) file.\n            This callback receives a list of strings (lines) and must also return a list of lines,\n            changed as needed.\n            The resulting lines will be used to compare with the contents of expected_fn.\n\n        :param bool binary:\n            .. seealso:: zerotk.easyfs.GetFileContents"
        ],
        [
            "Returns a nice side-by-side diff of the given files, as a string."
        ],
        [
            "Add a peer or multiple peers to the PEERS variable, takes a single string or a list.\n\n        :param peer(list or string)"
        ],
        [
            "remove one or multiple peers from PEERS variable\n\n        :param peer(list or string):"
        ],
        [
            "check the status of the network and the peers\n\n        :return: network_height, peer_status"
        ],
        [
            "broadcasts a transaction to the peerslist using ark-js library"
        ],
        [
            "Exposes a given service to this API."
        ],
        [
            "Main entry point, expects doctopt arg dict as argd."
        ],
        [
            "Print a message only if DEBUG is truthy."
        ],
        [
            "Parse a string as an integer.\n        Exit with a message on failure."
        ],
        [
            "If `s` is a file name, read the file and return it's content.\n        Otherwise, return the original string.\n        Returns None if the file was opened, but errored during reading."
        ],
        [
            "Wait for response until timeout.\n        If timeout is specified to None, ``self.timeout`` is used.\n\n        :param float timeout: seconds to wait I/O"
        ],
        [
            "If the file-object is not seekable, return  ArchiveTemp of the fileobject,\n    otherwise return the file-object itself"
        ],
        [
            "Setup before_request, after_request handlers for tracing."
        ],
        [
            "Records the starting time of this reqeust."
        ],
        [
            "Calculates the request duration, and adds a transaction\n        ID to the header."
        ],
        [
            "Insert spaces between words until it is wide enough for `width`."
        ],
        [
            "Prepend or append text to lines. Yields each line."
        ],
        [
            "Format block by splitting on individual characters."
        ],
        [
            "Format block by wrapping on spaces."
        ],
        [
            "Remove spaces in between words until it is small enough for\n            `width`.\n            This will always leave at least one space between words,\n            so it may not be able to get below `width` characters."
        ],
        [
            "Check IP trough the httpBL API\n\n        :param ip: ipv4 ip address\n        :return: httpBL results or None if any error is occurred"
        ],
        [
            "Check if IP is a threat\n\n        :param result: httpBL results; if None, then results from last check_ip() used (optional)\n        :param harmless_age: harmless age for check if httpBL age is older (optional)\n        :param threat_score: threat score for check if httpBL threat is lower (optional)\n        :param threat_type:  threat type, if not equal httpBL score type, then return False (optional)\n        :return: True or False"
        ],
        [
            "Check if IP is suspicious\n\n        :param result: httpBL results; if None, then results from last check_ip() used (optional)\n        :return: True or False"
        ],
        [
            "Invalidate httpBL cache for IP address\n\n        :param ip: ipv4 IP address"
        ],
        [
            "Invalidate httpBL cache"
        ],
        [
            "Runs the consumer."
        ],
        [
            "Upload the next batch of items, return whether successful."
        ],
        [
            "Return the next batch of items to upload."
        ],
        [
            "Get a single item from the queue."
        ],
        [
            "Attempt to upload the batch and retry before raising an error"
        ],
        [
            "Translate camelCase into underscore format.\n\n    >>> _camelcase_to_underscore('minutesBetweenSummaries')\n    'minutes_between_summaries'"
        ],
        [
            "Creates the Trello endpoint tree.\n\n    >>> r = {'1': { \\\n                 'actions': {'METHODS': {'GET'}}, \\\n                 'boards': { \\\n                     'members': {'METHODS': {'DELETE'}}}} \\\n            }\n    >>> r == create_tree([ \\\n                 'GET /1/actions/[idAction]', \\\n                 'DELETE /1/boards/[board_id]/members/[idMember]'])\n    True"
        ],
        [
            "Prints the complete YAML."
        ],
        [
            "Connect by wmi and run wql."
        ],
        [
            "Wrapper for the other log methods, decide which one based on the\n        URL parameter."
        ],
        [
            "Write to a local log file"
        ],
        [
            "Write to a remote host via HTTP POST"
        ],
        [
            "Helper method to store username and password"
        ],
        [
            "Set connection parameters. Call set_connection with no arguments to clear."
        ],
        [
            "Set delegate parameters. Call set_delegate with no arguments to clear."
        ],
        [
            "Takes a single address and returns the current balance."
        ],
        [
            "returns a list of named tuples,  x.timestamp, x.amount including block rewards"
        ],
        [
            "Massages the 'true' and 'false' strings to bool equivalents.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :rtype: bool\n    :return: True or False, depending on the value."
        ],
        [
            "If the value is ``None``, fail validation.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :raises: ValueError if the config value is None."
        ],
        [
            "Make sure the value evaluates to boolean True.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :raises: ValueError if the config value evaluates to boolean False."
        ],
        [
            "Convert an evar value into a Python logging level constant.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :return: A validated string.\n    :raises: ValueError if the log level is invalid."
        ],
        [
            "Register a new range type as a PostgreSQL range.\n\n        >>> register_range_type(\"int4range\", intrange, conn)\n\n    The above will make sure intrange is regarded as an int4range for queries\n    and that int4ranges will be cast into intrange when fetching rows.\n\n    pgrange should be the full name including schema for the custom range type.\n\n    Note that adaption is global, meaning if a range type is passed to a regular\n    psycopg2 connection it will adapt it to its proper range type. Parsing of\n    rows from the database however is not global and just set on a per connection\n    basis."
        ],
        [
            "Acquires the correct error for a given response.\n\n  :param requests.Response response: HTTP error response\n  :returns: the appropriate error for a given response\n  :rtype: APIError"
        ],
        [
            "Converts the request parameters to Python.\n\n    :param request: <pyramid.request.Request> || <dict>\n\n    :return: <dict>"
        ],
        [
            "Extracts ORB context information from the request.\n\n    :param request: <pyramid.request.Request>\n    :param model: <orb.Model> || None\n\n    :return: {<str> key: <variant> value} values, <orb.Context>"
        ],
        [
            "Handles real-time updates to the order book."
        ],
        [
            "Used exclusively as a thread which keeps the WebSocket alive."
        ],
        [
            "Connects and subscribes to the WebSocket Feed."
        ],
        [
            "Marks a view function as being exempt from the cached httpbl view protection."
        ],
        [
            "Hook point for overriding how the CounterPool gets its connection to\n        AWS."
        ],
        [
            "Hook point for overriding how the CounterPool determines the schema\n        to be used when creating a missing table."
        ],
        [
            "Hook point for overriding how the CounterPool creates a new table\n        in DynamooDB"
        ],
        [
            "Hook point for overriding how the CounterPool transforms table_name\n        into a boto DynamoDB Table object."
        ],
        [
            "Hook point for overriding how the CouterPool creates a DynamoDB item\n        for a given counter when an existing item can't be found."
        ],
        [
            "Hook point for overriding how the CouterPool fetches a DynamoDB item\n        for a given counter."
        ],
        [
            "Gets the DynamoDB item behind a counter and ties it to a Counter\n        instace."
        ],
        [
            "Use an event to build a many-to-one relationship on a class.\n\n    This makes use of the :meth:`.References._reference_table` method\n    to generate a full foreign key relationship to the remote table."
        ],
        [
            "Use an event to build a one-to-many relationship on a class.\n\n    This makes use of the :meth:`.References._reference_table` method\n    to generate a full foreign key relationship from the remote table."
        ],
        [
            "Djeffify data between tags"
        ],
        [
            "Create a foreign key reference from the local class to the given remote\n        table.\n\n        Adds column references to the declarative class and adds a\n        ForeignKeyConstraint."
        ],
        [
            "Path join helper method\n    Join paths if list passed\n\n    :type path: str|unicode|list\n    :rtype: str|unicode"
        ],
        [
            "Read helper method\n\n    :type file_path: str|unicode\n    :type encoding: str|unicode\n    :rtype: str|unicode"
        ],
        [
            "Write helper method\n\n    :type file_path: str|unicode\n    :type contents: str|unicode\n    :type encoding: str|unicode"
        ],
        [
            "Copy file helper method\n\n    :type src: str|unicode\n    :type dest: str|unicode"
        ],
        [
            "Split file name and extension\n\n    :type path: str|unicode\n    :rtype: one str|unicode"
        ],
        [
            "Helper method for absolute and relative paths resolution\n        Split passed path and return each directory parts\n\n        example: \"/usr/share/dir\"\n        return: [\"usr\", \"share\", \"dir\"]\n\n        @type path: one of (unicode, str)\n        @rtype: list"
        ],
        [
            "Creates fully qualified endpoint URIs.\n\n    :param parts: the string parts that form the request URI"
        ],
        [
            "Makes sure we have proper ISO 8601 time.\n\n    :param time: either already ISO 8601 a string or datetime.datetime\n    :returns: ISO 8601 time\n    :rtype: str"
        ],
        [
            "Returns the given response or raises an APIError for non-2xx responses.\n\n    :param requests.Response response: HTTP response\n    :returns: requested data\n    :rtype: requests.Response\n    :raises APIError: for non-2xx responses"
        ],
        [
            "Checks if a next message is possible.\n\n    :returns: True if a next message is possible, otherwise False\n    :rtype: bool"
        ],
        [
            "Colors text with code and given format"
        ],
        [
            "Registers the given message type in the local database.\n\n    Args:\n      message: a message.Message, to be registered.\n\n    Returns:\n      The provided message."
        ],
        [
            "Insert object before index.\n\n        :param int index: index to insert in\n        :param string value: path to insert"
        ],
        [
            "Parse runtime path representation to list.\n\n        :param string string: runtime path string\n        :return: list of runtime paths\n        :rtype: list of string"
        ],
        [
            "Add some bundle to build group\n\n        :type bundle: static_bundle.bundles.AbstractBundle\n        @rtype: BuildGroup"
        ],
        [
            "Return collected files links\n\n        :rtype: list[static_bundle.files.StaticFileResult]"
        ],
        [
            "Asset minifier\n        Uses default minifier in bundle if it's not defined\n\n        :rtype: static_bundle.minifiers.DefaultMinifier|None"
        ],
        [
            "Render all includes in asset by names\n\n        :type name: str|unicode\n        :rtype: str|unicode"
        ],
        [
            "Return links without build files"
        ],
        [
            "Coerce everything to strings.\n    All objects representing time get output according to default_date_fmt."
        ],
        [
            "Initialize the zlogger.\n\n    Sets up a rotating file handler to the specified path and file with\n    the given size and backup count limits, sets the default\n    application_name, server_hostname, and default/whitelist fields.\n\n    :param path: path to write the log file\n    :param target: name of the log file\n    :param logger_name: name of the logger (defaults to root)\n    :param level: log level for this logger (defaults to logging.DEBUG)\n    :param maxBytes: size of the file before rotation (default 1MB)\n    :param application_name: app name to add to each log entry\n    :param server_hostname: hostname to add to each log entry\n    :param fields: default/whitelist fields.\n    :type path: string\n    :type target: string\n    :type logger_name: string\n    :type level: int\n    :type maxBytes: int\n    :type backupCount: int\n    :type application_name: string\n    :type server_hostname: string\n    :type fields: dict"
        ],
        [
            "formats a logging.Record into a standard json log entry\n\n        :param record: record to be formatted\n        :type record: logging.Record\n        :return: the formatted json string\n        :rtype: string"
        ],
        [
            "Initialize the model for a Pyramid app.\n\n    Activate this setup using ``config.include('baka_model')``."
        ],
        [
            "Return absolute and relative path for file\n\n        :type root_path: str|unicode\n        :type file_name: str|unicode\n        :type input_dir: str|unicode\n        :rtype: tuple"
        ],
        [
            "Adds an EnumDescriptor to the pool.\n\n    This method also registers the FileDescriptor associated with the message.\n\n    Args:\n      enum_desc: An EnumDescriptor."
        ],
        [
            "Gets the FileDescriptor for the file containing the specified symbol.\n\n    Args:\n      symbol: The name of the symbol to search for.\n\n    Returns:\n      A FileDescriptor that contains the specified symbol.\n\n    Raises:\n      KeyError: if the file can not be found in the pool."
        ],
        [
            "Loads the named descriptor from the pool.\n\n    Args:\n      full_name: The full name of the descriptor to load.\n\n    Returns:\n      The descriptor for the named type."
        ],
        [
            "Loads the named enum descriptor from the pool.\n\n    Args:\n      full_name: The full name of the enum descriptor to load.\n\n    Returns:\n      The enum descriptor for the named type."
        ],
        [
            "Loads the named extension descriptor from the pool.\n\n    Args:\n      full_name: The full name of the extension descriptor to load.\n\n    Returns:\n      A FieldDescriptor, describing the named extension."
        ],
        [
            "Make a protobuf EnumDescriptor given an EnumDescriptorProto protobuf.\n\n    Args:\n      enum_proto: The descriptor_pb2.EnumDescriptorProto protobuf message.\n      package: Optional package name for the new message EnumDescriptor.\n      file_desc: The file containing the enum descriptor.\n      containing_type: The type containing this enum.\n      scope: Scope containing available types.\n\n    Returns:\n      The added descriptor"
        ],
        [
            "Creates a field descriptor from a FieldDescriptorProto.\n\n    For message and enum type fields, this method will do a look up\n    in the pool for the appropriate descriptor for that type. If it\n    is unavailable, it will fall back to the _source function to\n    create it. If this type is still unavailable, construction will\n    fail.\n\n    Args:\n      field_proto: The proto describing the field.\n      message_name: The name of the containing message.\n      index: Index of the field\n      is_extension: Indication that this field is for an extension.\n\n    Returns:\n      An initialized FieldDescriptor object"
        ],
        [
            "Get a ``sqlalchemy.orm.Session`` instance backed by a transaction.\n\n    This function will hook the session to the transaction manager which\n    will take care of committing any changes.\n\n    - When using pyramid_tm it will automatically be committed or aborted\n      depending on whether an exception is raised.\n\n    - When using scripts you should wrap the session in a manager yourself.\n      For example::\n\n          import transaction\n\n          engine = get_engine(settings)\n          session_factory = get_session_factory(engine)\n          with transaction.manager:\n              dbsession = get_tm_session(session_factory, transaction.manager)"
        ],
        [
            "Generate a random string of the specified length.\n\n    The returned string is composed of an alphabet that shouldn't include any\n    characters that are easily mistakeable for one another (I, 1, O, 0), and\n    hopefully won't accidentally contain any English-language curse words."
        ],
        [
            "Require that the named `field` has the right `data_type`"
        ],
        [
            "Forces a flush from the internal queue to the server"
        ],
        [
            "Use all decompressor possible to make the stream"
        ],
        [
            "Manage a Marv site"
        ],
        [
            "Returns a decoder for a MessageSet item.\n\n  The parameter is the _extensions_by_number map for the message class.\n\n  The message set message looks like this:\n    message MessageSet {\n      repeated group Item = 1 {\n        required int32 type_id = 2;\n        required string message = 3;\n      }\n    }"
        ],
        [
            "Flask like implementation of getting the applicaiton name via\n    the filename of the including file"
        ],
        [
            "Given a Python function name, return the function it refers to."
        ],
        [
            "Add a function to the function list, in order."
        ],
        [
            "Return the mapping of a document according to the function list."
        ],
        [
            "Reduce several mapped documents by several reduction functions."
        ],
        [
            "Re-reduce a set of values, with a list of rereduction functions."
        ],
        [
            "Validate...this function is undocumented, but still in CouchDB."
        ],
        [
            "The main function called to handle a request."
        ],
        [
            "Log an event on the CouchDB server."
        ],
        [
            "Generates a universally unique ID.\n    Any arguments only create more randomness."
        ],
        [
            "revoke_token removes the access token from the data_store"
        ],
        [
            "_auth - internal method to ensure the client_id and client_secret passed with\n        the nonce match"
        ],
        [
            "_validate_request_code - internal method for verifying the the given nonce.\n        also removes the nonce from the data_store, as they are intended for\n        one-time use."
        ],
        [
            "_generate_token - internal function for generating randomized alphanumberic\n        strings of a given length"
        ],
        [
            "Merge multiple ordered so that within-ordered order is preserved"
        ],
        [
            "Helps us validate the parameters for the request\n\n    :param valid_options: a list of strings of valid options for the\n                          api request\n    :param params: a dict, the key-value store which we really only care about\n                   the key which has tells us what the user is using for the\n                   API request\n\n    :returns: None or throws an exception if the validation fails"
        ],
        [
            "Get current datetime for every file."
        ],
        [
            "run your main spider here\n        as for branch spider result data, you can return everything or do whatever with it\n        in your own code\n\n        :return: None"
        ],
        [
            "Read version info from a file without importing it"
        ],
        [
            "Make a protobuf Descriptor given a DescriptorProto protobuf.\n\n  Handles nested descriptors. Note that this is limited to the scope of defining\n  a message inside of another message. Composite fields can currently only be\n  resolved if the message is defined in the same scope as the field.\n\n  Args:\n    desc_proto: The descriptor_pb2.DescriptorProto protobuf message.\n    package: Optional package name for the new message Descriptor (string).\n    build_file_if_cpp: Update the C++ descriptor pool if api matches.\n                       Set to False on recursion, so no duplicates are created.\n    syntax: The syntax/semantics that should be used.  Set to \"proto3\" to get\n            proto3 field presence semantics.\n  Returns:\n    A Descriptor for protobuf messages."
        ],
        [
            "Returns the root if this is a nested type, or itself if its the root."
        ],
        [
            "Searches for the specified method, and returns its descriptor."
        ],
        [
            "Converts protobuf message to JSON format.\n\n  Args:\n    message: The protocol buffers message instance to serialize.\n    including_default_value_fields: If True, singular primitive fields,\n        repeated fields, and map fields will always be serialized.  If\n        False, only serialize non-empty fields.  Singular message fields\n        and oneof fields are not affected by this option.\n\n  Returns:\n    A string containing the JSON formatted protocol buffer message."
        ],
        [
            "Converts message to an object according to Proto3 JSON Specification."
        ],
        [
            "Converts Struct message according to Proto3 JSON Specification."
        ],
        [
            "Parses a JSON representation of a protocol message into a message.\n\n  Args:\n    text: Message JSON representation.\n    message: A protocol beffer message to merge into.\n\n  Returns:\n    The same message passed as argument.\n\n  Raises::\n    ParseError: On JSON parsing problems."
        ],
        [
            "Convert field value pairs into regular message.\n\n  Args:\n    js: A JSON object to convert the field value pairs.\n    message: A regular protocol message to record the data.\n\n  Raises:\n    ParseError: In case of problems converting."
        ],
        [
            "Convert a JSON object into a message.\n\n  Args:\n    value: A JSON object.\n    message: A WKT or regular protocol message to record the data.\n\n  Raises:\n    ParseError: In case of convert problems."
        ],
        [
            "Convert a JSON representation into Value message."
        ],
        [
            "Convert a JSON representation into ListValue message."
        ],
        [
            "Convert a JSON representation into Struct message."
        ],
        [
            "Update config options with the provided dictionary of options."
        ],
        [
            "Completes measuring time interval and updates counter."
        ],
        [
            "Converts Duration to string format.\n\n    Returns:\n      A string converted from self. The string format will contains\n      3, 6, or 9 fractional digits depending on the precision required to\n      represent the exact Duration value. For example: \"1s\", \"1.010s\",\n      \"1.000000100s\", \"-3.100s\""
        ],
        [
            "Converts a string to Duration.\n\n    Args:\n      value: A string to be converted. The string must end with 's'. Any\n          fractional digits (or none) are accepted as long as they fit into\n          precision. For example: \"1s\", \"1.01s\", \"1.0000001s\", \"-3.100s\n\n    Raises:\n      ParseError: On parsing problems."
        ],
        [
            "Converts string to FieldMask according to proto3 JSON spec."
        ],
        [
            "Return a CouchDB document, given its ID, revision and database name."
        ],
        [
            "Give reST format README for pypi."
        ],
        [
            "remove records from collection whose parameters match kwargs"
        ],
        [
            "Resolve the URL to this point.\n\n        >>> trello = TrelloAPIV1('APIKEY')\n        >>> trello.batch._url\n        '1/batch'\n        >>> trello.boards(board_id='BOARD_ID')._url\n        '1/boards/BOARD_ID'\n        >>> trello.boards(board_id='BOARD_ID')(field='FIELD')._url\n        '1/boards/BOARD_ID/FIELD'\n        >>> trello.boards(board_id='BOARD_ID').cards(filter='FILTER')._url\n        '1/boards/BOARD_ID/cards/FILTER'"
        ],
        [
            "Makes the HTTP request."
        ],
        [
            "Skips over a field value.\n\n  Args:\n    tokenizer: A tokenizer to parse the field name and values.\n\n  Raises:\n    ParseError: In case an invalid field value is found."
        ],
        [
            "Parses an integer.\n\n  Args:\n    text: The text to parse.\n    is_signed: True if a signed integer must be parsed.\n    is_long: True if a long integer must be parsed.\n\n  Returns:\n    The integer value.\n\n  Raises:\n    ValueError: Thrown Iff the text is not a valid integer."
        ],
        [
            "Convert protobuf message to text format.\n\n    Args:\n      message: The protocol buffers message."
        ],
        [
            "Converts an text representation of a protocol message into a message.\n\n    Args:\n      lines: Lines of a message's text representation.\n      message: A protocol buffer message to merge into.\n\n    Raises:\n      ParseError: On text parsing problems."
        ],
        [
            "Merges a single scalar field into a message.\n\n    Args:\n      tokenizer: A tokenizer to parse the field value.\n      message: The message of which field is a member.\n      field: The descriptor of the field to be merged.\n\n    Raises:\n      ParseError: In case of text parsing problems."
        ],
        [
            "Consumes protocol message field identifier.\n\n    Returns:\n      Identifier string.\n\n    Raises:\n      ParseError: If an identifier couldn't be consumed."
        ],
        [
            "Consumes a signed 32bit integer number.\n\n    Returns:\n      The integer parsed.\n\n    Raises:\n      ParseError: If a signed 32bit integer couldn't be consumed."
        ],
        [
            "Consumes an floating point number.\n\n    Returns:\n      The number parsed.\n\n    Raises:\n      ParseError: If a floating point number couldn't be consumed."
        ],
        [
            "Consumes a boolean value.\n\n    Returns:\n      The bool parsed.\n\n    Raises:\n      ParseError: If a boolean value couldn't be consumed."
        ],
        [
            "Consume one token of a string literal.\n\n    String literals (whether bytes or text) can come in multiple adjacent\n    tokens which are automatically concatenated, like in C or Python.  This\n    method only consumes one token.\n\n    Returns:\n      The token parsed.\n    Raises:\n      ParseError: When the wrong format data is found."
        ],
        [
            "convert ark timestamp to unix timestamp"
        ],
        [
            "Close the connection."
        ],
        [
            "Replace macros with content defined in the config.\n\n        :param content: Markdown content\n\n        :returns: Markdown content without macros"
        ],
        [
            "Return a pathname possibly with a number appended to it so that it is\r\n\tunique in the directory."
        ],
        [
            "Append numbers in sequential order to the filename or folder name\r\n\tNumbers should be appended before the extension on a filename."
        ],
        [
            "Custom version of splitext that doesn't perform splitext on directories"
        ],
        [
            "Set the modified time of a file"
        ],
        [
            "Get the modified time for a file as a datetime instance"
        ],
        [
            "wrap a function that returns a dir, making sure it exists"
        ],
        [
            "Check whether a file is presumed hidden, either because\r\n\tthe pathname starts with dot or because the platform\r\n\tindicates such."
        ],
        [
            "Get closer to your EOL"
        ],
        [
            "Open a connection over the serial line and receive data lines"
        ],
        [
            "create & start main thread\n\n        :return: None"
        ],
        [
            "Scans through all children of node and gathers the\n    text. If node has non-text child-nodes then\n    NotTextNodeError is raised."
        ],
        [
            "Get the number of credits remaining at AmbientSMS"
        ],
        [
            "Send a mesage via the AmbientSMS API server"
        ],
        [
            "Inteface for sending web requests to the AmbientSMS API Server"
        ],
        [
            "Called for each file\n        Must return file content\n        Can be wrapped\n\n        :type f: static_bundle.files.StaticFileResult\n        :type text: str|unicode\n        :rtype: str|unicode"
        ],
        [
            "Return True if the class is a date type."
        ],
        [
            "Convert a date or time to a datetime. If when is a date then it sets the time to midnight. If\n    when is a time it sets the date to the epoch. If when is None or a datetime it returns when.\n    Otherwise a TypeError is raised. Returned datetimes have tzinfo set to None unless when is a\n    datetime with tzinfo set in which case it remains the same."
        ],
        [
            "Return a date, time, or datetime converted to a datetime in the given timezone. If when is a\n    datetime and has no timezone it is assumed to be local time. Date and time objects are also\n    assumed to be UTC. The tz value defaults to UTC. Raise TypeError if when cannot be converted to\n    a datetime."
        ],
        [
            "Return a Unix timestamp in seconds for the provided datetime. The `totz` function is called\n    on the datetime to convert it to the provided timezone. It will be converted to UTC if no\n    timezone is provided."
        ],
        [
            "Return a Unix timestamp in milliseconds for the provided datetime. The `totz` function is\n    called on the datetime to convert it to the provided timezone. It will be converted to UTC if\n    no timezone is provided."
        ],
        [
            "Return the datetime representation of the provided Unix timestamp. By defaults the timestamp is\n    interpreted as UTC. If tzin is set it will be interpreted as this timestamp instead. By default\n    the output datetime will have UTC time. If tzout is set it will be converted in this timezone\n    instead."
        ],
        [
            "Return the Unix timestamp in milliseconds as a datetime object. If tz is set it will be\n    converted to the requested timezone otherwise it defaults to UTC."
        ],
        [
            "Return the datetime truncated to the precision of the provided unit."
        ],
        [
            "Return the date for the day of this week."
        ],
        [
            "Internal function that determines EOL_STYLE_NATIVE constant with the proper value for the\n    current platform."
        ],
        [
            "Normalizes a path maintaining the final slashes.\n\n    Some environment variables need the final slash in order to work.\n\n    Ex. The SOURCES_DIR set by subversion must end with a slash because of the way it is used\n    in the Visual Studio projects.\n\n    :param unicode path:\n        The path to normalize.\n\n    :rtype: unicode\n    :returns:\n        Normalized path"
        ],
        [
            "Returns a version of a path that is unique.\n\n    Given two paths path1 and path2:\n        CanonicalPath(path1) == CanonicalPath(path2) if and only if they represent the same file on\n        the host OS. Takes account of case, slashes and relative paths.\n\n    :param unicode path:\n        The original path.\n\n    :rtype: unicode\n    :returns:\n        The unique path."
        ],
        [
            "Replaces all slashes and backslashes with the target separator\n\n    StandardPath:\n        We are defining that the standard-path is the one with only back-slashes in it, either\n        on Windows or any other platform.\n\n    :param bool strip:\n        If True, removes additional slashes from the end of the path."
        ],
        [
            "Copy a file from source to target.\n\n    :param  source_filename:\n        @see _DoCopyFile\n\n    :param  target_filename:\n        @see _DoCopyFile\n\n    :param bool md5_check:\n        If True, checks md5 files (of both source and target files), if they match, skip this copy\n        and return MD5_SKIP\n\n        Md5 files are assumed to be {source, target} + '.md5'\n\n        If any file is missing (source, target or md5), the copy will always be made.\n\n    :param  copy_symlink:\n        @see _DoCopyFile\n\n    :raises FileAlreadyExistsError:\n        If target_filename already exists, and override is False\n\n    :raises NotImplementedProtocol:\n        If file protocol is not accepted\n\n        Protocols allowed are:\n            source_filename: local, ftp, http\n            target_filename: local, ftp\n\n    :rtype: None | MD5_SKIP\n    :returns:\n        MD5_SKIP if the file was not copied because there was a matching .md5 file\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information"
        ],
        [
            "Copy a file locally to a directory.\n\n    :param unicode source_filename:\n        The filename to copy from.\n\n    :param unicode target_filename:\n        The filename to copy to.\n\n    :param bool copy_symlink:\n        If True and source_filename is a symlink, target_filename will also be created as\n        a symlink.\n\n        If False, the file being linked will be copied instead."
        ],
        [
            "Copy files from the given source to the target.\n\n    :param unicode source_dir:\n        A filename, URL or a file mask.\n        Ex.\n            x:\\coilib50\n            x:\\coilib50\\*\n            http://server/directory/file\n            ftp://server/directory/file\n\n\n    :param unicode target_dir:\n        A directory or an URL\n        Ex.\n            d:\\Temp\n            ftp://server/directory\n\n    :param bool create_target_dir:\n        If True, creates the target path if it doesn't exists.\n\n    :param bool md5_check:\n        .. seealso:: CopyFile\n\n    :raises DirectoryNotFoundError:\n        If target_dir does not exist, and create_target_dir is False\n\n    .. seealso:: CopyFile for documentation on accepted protocols\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information"
        ],
        [
            "Copies files into directories, according to a file mapping\n\n    :param list(tuple(unicode,unicode)) file_mapping:\n        A list of mappings between the directory in the target and the source.\n        For syntax, @see: ExtendedPathMask\n\n    :rtype: list(tuple(unicode,unicode))\n    :returns:\n        List of files copied. (source_filename, target_filename)\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information"
        ],
        [
            "Recursively copy a directory tree.\n\n    :param unicode source_dir:\n        Where files will come from\n\n    :param unicode target_dir:\n        Where files will go to\n\n    :param bool override:\n        If True and target_dir already exists, it will be deleted before copying.\n\n    :raises NotImplementedForRemotePathError:\n        If trying to copy to/from remote directories"
        ],
        [
            "Deletes the given local filename.\n\n    .. note:: If file doesn't exist this method has no effect.\n\n    :param unicode target_filename:\n        A local filename\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a non-local path\n\n    :raises FileOnlyActionError:\n        Raised when filename refers to a directory."
        ],
        [
            "Appends content to a local file.\n\n    :param unicode filename:\n\n    :param unicode contents:\n\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n        Replaces the EOL by the appropriate EOL depending on the eol_style value.\n        Considers that all content is using only \"\\n\" as EOL.\n\n    :param unicode encoding:\n        Target file's content encoding.\n        Defaults to sys.getfilesystemencoding()\n\n    :param bool binary:\n        If True, content is appended in binary mode. In this case, `contents` must be `bytes` and not\n        `unicode`\n\n    :raises NotImplementedForRemotePathError:\n        If trying to modify a non-local path\n\n    :raises ValueError:\n        If trying to mix unicode `contents` without `encoding`, or `encoding` without\n        unicode `contents`"
        ],
        [
            "Moves a file.\n\n    :param unicode source_filename:\n\n    :param unicode target_filename:\n\n    :raises NotImplementedForRemotePathError:\n        If trying to operate with non-local files."
        ],
        [
            "Moves a directory.\n\n    :param unicode source_dir:\n\n    :param unicode target_dir:\n\n    :raises NotImplementedError:\n        If trying to move anything other than:\n            Local dir -> local dir\n            FTP dir -> FTP dir (same host)"
        ],
        [
            "Reads a file and returns its contents. Works for both local and remote files.\n\n    :param unicode filename:\n\n    :param bool binary:\n        If True returns the file as is, ignore any EOL conversion.\n\n    :param unicode encoding:\n        File's encoding. If not None, contents obtained from file will be decoded using this\n        `encoding`.\n\n    :param None|''|'\\n'|'\\r'|'\\r\\n' newline:\n        Controls universal newlines.\n        See 'io.open' newline parameter documentation for more details.\n\n    :returns str|unicode:\n        The file's contents.\n        Returns unicode string when `encoding` is not None.\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information"
        ],
        [
            "Reads a file and returns its contents as a list of lines. Works for both local and remote files.\n\n    :param unicode filename:\n\n    :param None|''|'\\n'|'\\r'|'\\r\\n' newline:\n        Controls universal newlines.\n        See 'io.open' newline parameter documentation for more details.\n\n    :param unicode encoding:\n        File's encoding. If not None, contents obtained from file will be decoded using this\n        `encoding`.\n\n    :returns list(unicode):\n        The file's lines\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information"
        ],
        [
            "Lists the files in the given directory\n\n    :type directory: unicode | unicode\n    :param directory:\n        A directory or URL\n\n    :rtype: list(unicode) | list(unicode)\n    :returns:\n        List of filenames/directories found in the given directory.\n        Returns None if the given directory does not exists.\n\n        If `directory` is a unicode string, all files returned will also be unicode\n\n    :raises NotImplementedProtocol:\n        If file protocol is not local or FTP\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information"
        ],
        [
            "Create a file with the given contents.\n\n    :param unicode filename:\n        Filename and path to be created.\n\n    :param unicode contents:\n        The file contents as a string.\n\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n        Replaces the EOL by the appropriate EOL depending on the eol_style value.\n        Considers that all content is using only \"\\n\" as EOL.\n\n    :param bool create_dir:\n        If True, also creates directories needed in filename's path\n\n    :param unicode encoding:\n        Target file's content encoding. Defaults to sys.getfilesystemencoding()\n        Ignored if `binary` = True\n\n    :param bool binary:\n        If True, file is created in binary mode. In this case, `contents` must be `bytes` and not\n        `unicode`\n\n    :return unicode:\n        Returns the name of the file created.\n\n    :raises NotImplementedProtocol:\n        If file protocol is not local or FTP\n\n    :raises ValueError:\n        If trying to mix unicode `contents` without `encoding`, or `encoding` without\n        unicode `contents`\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information"
        ],
        [
            "Replaces all occurrences of \"old\" by \"new\" in the given file.\n\n    :param unicode filename:\n        The name of the file.\n\n    :param unicode old:\n        The string to search for.\n\n    :param unicode new:\n        Replacement string.\n\n    :return unicode:\n        The new contents of the file."
        ],
        [
            "Create directory including any missing intermediate directory.\n\n    :param unicode directory:\n\n    :return unicode|urlparse.ParseResult:\n        Returns the created directory or url (see urlparse).\n\n    :raises NotImplementedProtocol:\n        If protocol is not local or FTP.\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information"
        ],
        [
            "Deletes a directory.\n\n    :param unicode directory:\n\n    :param bool skip_on_error:\n        If True, ignore any errors when trying to delete directory (for example, directory not\n        found)\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a remote directory."
        ],
        [
            "On Windows, returns a list of mapped network drives\n\n    :return: tuple(string, string, bool)\n        For each mapped netword drive, return 3 values tuple:\n            - the local drive\n            - the remote path-\n            - True if the mapping is enabled (warning: not reliable)"
        ],
        [
            "Create a symbolic link at `link_path` pointing to `target_path`.\n\n    :param unicode target_path:\n        Link target\n\n    :param unicode link_path:\n        Fullpath to link name\n\n    :param bool override:\n        If True and `link_path` already exists as a link, that link is overridden."
        ],
        [
            "Read the target of the symbolic link at `path`.\n\n    :param unicode path:\n        Path to a symbolic link\n\n    :returns unicode:\n        Target of a symbolic link"
        ],
        [
            "Checks if a given path is local, raise an exception if not.\n\n    This is used in filesystem functions that do not support remote operations yet.\n\n    :param unicode path:\n\n    :raises NotImplementedForRemotePathError:\n        If the given path is not local"
        ],
        [
            "Replaces eol on each line by the given eol_style.\n\n    :param unicode contents:\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:"
        ],
        [
            "Verifies if a filename match with given patterns.\n\n    :param str filename: The filename to match.\n    :param list(str) masks: The patterns to search in the filename.\n    :return bool:\n        True if the filename has matched with one pattern, False otherwise."
        ],
        [
            "Searches for files in a given directory that match with the given patterns.\n\n    :param str dir_: the directory root, to search the files.\n    :param list(str) in_filters: a list with patterns to match (default = all). E.g.: ['*.py']\n    :param list(str) out_filters: a list with patterns to ignore (default = none). E.g.: ['*.py']\n    :param bool recursive: if True search in subdirectories, otherwise, just in the root.\n    :param bool include_root_dir: if True, includes the directory being searched in the returned paths\n    :param bool standard_paths: if True, always uses unix path separators \"/\"\n    :return list(str):\n        A list of strings with the files that matched (with the full path in the filesystem)."
        ],
        [
            "os.path.expanduser wrapper, necessary because it cannot handle unicode strings properly.\n\n    This is not necessary in Python 3.\n\n    :param path:\n        .. seealso:: os.path.expanduser"
        ],
        [
            "Helper to iterate over the files in a directory putting those in the passed StringIO in ini\n    format.\n\n    :param unicode directory:\n        The directory for which the hash should be done.\n\n    :param StringIO stringio:\n        The string to which the dump should be put.\n\n    :param unicode base:\n        If provided should be added (along with a '/') before the name=hash of file.\n\n    :param unicode exclude:\n        Pattern to match files to exclude from the hashing. E.g.: *.gz\n\n    :param unicode include:\n        Pattern to match files to include in the hashing. E.g.: *.zip"
        ],
        [
            "Iterator for random hexadecimal hashes\n\n    :param iterator_size:\n        Amount of hashes return before this iterator stops.\n        Goes on forever if `iterator_size` is negative.\n\n    :param int hash_length:\n        Size of each hash returned.\n\n    :return generator(unicode):"
        ],
        [
            "A context manager to replace and restore a value using a getter and setter.\n\n    :param object obj: The object to replace/restore.\n    :param object key: The key to replace/restore in the object.\n    :param object value: The value to replace.\n\n    Example::\n\n      with PushPop2(sys.modules, 'alpha', None):\n        pytest.raises(ImportError):\n          import alpha"
        ],
        [
            "Return the database specifier for a database string.\n    \n    This accepts a database name or URL, and returns a database specifier in the\n    format accepted by ``specifier_to_db``. It is recommended that you consult\n    the documentation for that function for an explanation of the format."
        ],
        [
            "Return a CouchDB database instance from a database string."
        ],
        [
            "Make sure a DB specifier exists, creating it if necessary."
        ],
        [
            "Exclude NoSet objec\n\n    .. code-block::\n\n        >>> coerce(NoSet, 'value')\n        'value'"
        ],
        [
            "Parse a hub key into a dictionary of component parts\n\n    :param key: str, a hub key\n    :returns: dict, hub key split into parts\n    :raises: ValueError"
        ],
        [
            "Raise an exception if string doesn't match a part's regex\n\n    :param string: str\n    :param part: a key in the PARTS dict\n    :raises: ValueError, TypeError"
        ],
        [
            "apply default settings to commands\n            not static, shadow \"self\" in eval"
        ],
        [
            "add commands to parser"
        ],
        [
            "get config for subparser and create commands"
        ],
        [
            "custom command line  action to show version"
        ],
        [
            "custom command line action to check file exist"
        ],
        [
            "Return the consumer and oauth tokens with three-legged OAuth process and\n    save in a yaml file in the user's home directory."
        ],
        [
            "Adds properties for all fields in this protocol message type."
        ],
        [
            "Unpacks Any message and returns the unpacked message.\n\n  This internal method is differnt from public Any Unpack method which takes\n  the target message as argument. _InternalUnpackAny method does not have\n  target message type and need to find the message type in descriptor pool.\n\n  Args:\n    msg: An Any message to be unpacked.\n\n  Returns:\n    The unpacked message."
        ]
    ]
}